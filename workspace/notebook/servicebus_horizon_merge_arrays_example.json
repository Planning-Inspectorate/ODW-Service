{
	"name": "servicebus_horizon_merge_arrays_example",
	"properties": {
		"description": "Sample notebook showing how to join a flat table with a spark table with arrays.",
		"folder": {
			"name": "archive/utils"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "84ff81d6-f0c9-4edc-9dcc-b74f4f0bd4ca"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 32,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Sample notebook showing how to join a flat table with a spark table with arrays\r\n",
					"\r\n",
					"The example below shows the following:  \r\n",
					"\r\n",
					"You have a table from Service Bus which has an array field for attachmentIds.  \r\n",
					"You have a similar table from Horizon with attachmentIds in a flat schema - 1 row per attachmentId.  \r\n",
					"You want to merge them together into a final table, e.g. curated layer, joining on the primary key, in this case \"caseId\".  \r\n",
					"\r\n",
					"1. Create spark dataframes for each table.  \r\n",
					"2. Explode the service bus array field \"attachmentIds\" so you have 1 attachmentId per row.  \r\n",
					"3. Union the 2 dataframes as they have the same column names.  \r\n",
					"4. Group all the attachmentIds into an array using the collect_list function, grouping by the primary key, \"caseId\".  \r\n",
					"5. Use this final dataframe to load / create the final table.  \r\n",
					"\r\n",
					"**NB: this is just an example of how this can be achieved. Each use case should be examined to see if this approach is appropriate for that particular entity**"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import IntegerType, ArrayType, StructType, StructField\r\n",
					"from pyspark.sql import Row\r\n",
					"from pyspark.sql.functions import *"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"service_bus_table = \"odw_standardised_db.spark_table_sb\"\r\n",
					"horizon_table = \"odw_standardised_db.spark_table_horizon\"\r\n",
					"spark_table_final = \"odw_standardised_db.spark_table_final\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"service_bus_schema = StructType([\r\n",
					"    StructField(\"caseId\", IntegerType(), nullable=False),\r\n",
					"    StructField(\"attachmentIds\", ArrayType(IntegerType()), nullable=True)\r\n",
					"])"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"service_bus_data = [\r\n",
					"    Row(caseId=1, attachmentIds=[101, 102, 103]),\r\n",
					"    Row(caseId=2, attachmentIds=[201, 202]),\r\n",
					"    Row(caseId=3, attachmentIds=[301])\r\n",
					"]"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.sql(f\"drop table if exists {service_bus_table}\")\r\n",
					"spark.sql(f\"drop table if exists {horizon_table}\")\r\n",
					"spark.sql(f\"drop table if exists {spark_table_final}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"service_bus_df = spark.createDataFrame(service_bus_data, schema=service_bus_schema)\r\n",
					"service_bus_df.write.saveAsTable(f\"{service_bus_table}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"horizon_schema = StructType([\r\n",
					"    StructField(\"caseId\", IntegerType(), nullable=False),\r\n",
					"    StructField(\"attachmentId\", IntegerType(), nullable=False)\r\n",
					"])"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"horizon_data = [\r\n",
					"    Row(caseId=1, attachmentId=1010),\r\n",
					"    Row(caseId=1, attachmentId=1020),\r\n",
					"    Row(caseId=1, attachmentId=1030),\r\n",
					"    Row(caseId=2, attachmentId=2010),\r\n",
					"    Row(caseId=2, attachmentId=2020),\r\n",
					"    Row(caseId=3, attachmentId=3010)\r\n",
					"]"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"horizon_df = spark.createDataFrame(horizon_data, schema=horizon_schema)\r\n",
					"horizon_df.write.saveAsTable(f\"{horizon_table}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"service_bus_df = spark.sql(f\"select * from {service_bus_table}\")\r\n",
					"display(service_bus_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"horizon_df = spark.sql(f\"select * from {horizon_table}\")\r\n",
					"display(horizon_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"service_bus_df_explode = service_bus_df.select(\"caseId\", explode(\"attachmentIds\").alias(\"attachmentId\"))\r\n",
					"display(service_bus_df_explode)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"final_df = service_bus_df_explode.union(horizon_df).groupBy(\"caseId\").agg(collect_list(\"attachmentId\").alias(\"attachmentIds\"))\r\n",
					"display(final_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"final_df.printSchema()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"final_df.write.saveAsTable(f\"{spark_table_final}\")"
				],
				"execution_count": null
			}
		]
	}
}