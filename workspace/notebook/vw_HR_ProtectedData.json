{
	"name": "vw_HR_ProtectedData",
	"properties": {
		"folder": {
			"name": "odw-curated/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "59c85c0e-851d-41d9-9c4c-cfd692da31e7"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Harmoised layer and build a view & table for Power BI use.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;01-April-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Protected Data Covered in here;"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"%run utils/py_logging_decorator"
				],
				"execution_count": 22
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# ProtectedData"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import json\n",
					"\n",
					"# Initialize simple result dictionary\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"try:\n",
					"    logInfo(\"Starting HR Protected Data setup with ON-PREMISES MATCHING logic\")\n",
					"    \n",
					"    # Get storage account \n",
					"    logInfo(\"Getting storage account configuration\")\n",
					"    storage_account = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"    logInfo(f\"Using storage account: {storage_account}\")\n",
					"    \n",
					"    # Fix the path\n",
					"    storage_account = storage_account.rstrip('/')\n",
					"    delta_table_path = f\"abfss://odw-curated@{storage_account}/saphr/pbi_HR_ProtectedData\"\n",
					"    logInfo(f\"Delta table will be created at: {delta_table_path}\")\n",
					"    \n",
					"    # First, check base data availability\n",
					"    logInfo(\"Checking base protected data availability\")\n",
					"    base_data_check = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as total_records\n",
					"        FROM odw_harmonised_db.sap_hr_protected_data\n",
					"        WHERE Report_MonthEnd_Date IS NOT NULL\n",
					"    \"\"\").collect()[0]\n",
					"    \n",
					"    logInfo(f\"Base data check - Total records: {base_data_check['total_records']}\")\n",
					"    \n",
					"    if base_data_check['total_records'] == 0:\n",
					"        raise Exception(\"No protected data found in source table odw_harmonised_db.sap_hr_protected_data\")\n",
					"    \n",
					"    # Create/refresh view with ON-PREMISES MATCHING logic\n",
					"    logInfo(\"Creating view odw_curated_db.vw_HR_ProtectedData with ON-PREMISES MATCHING logic\")\n",
					"    logInfo(\"IMPORTANT: Using UPPER() for case-insensitive matching - Spark SQL LIKE is case-sensitive unlike SQL Server\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE VIEW odw_curated_db.vw_HR_ProtectedData\n",
					"    AS\n",
					"    \n",
					"    WITH Dates AS (\n",
					"        SELECT DISTINCT\n",
					"            Report_MonthEnd_Date AS Dt\n",
					"        FROM odw_harmonised_db.sap_hr_protected_data\n",
					"        WHERE Report_MonthEnd_Date IS NOT NULL\n",
					"    ),\n",
					"    \n",
					"    MinMaxDates AS (\n",
					"        SELECT \n",
					"            MIN(Dt) AS StartDate,\n",
					"            MAX(Dt) AS EndDate\n",
					"        FROM Dates\n",
					"    ),\n",
					"    \n",
					"    dimDate AS (\n",
					"        SELECT \n",
					"            b.date,\n",
					"            b.month_name,\n",
					"            b.month_int,\n",
					"            \n",
					"            -- MATCH ON-PREM: Latest Month Logic \n",
					"            -- Original: DATEADD(M,-1,DATEADD(D,1,EOMONTH((select EndDate FROM MinMaxDates),0)))\n",
					"            -- Spark SQL: ADD_MONTHS(DATE_ADD(LAST_DAY((SELECT EndDate FROM MinMaxDates)), 1), -1)\n",
					"            -- Logic: Get end of max month -> add 1 day -> subtract 1 month = first day of max month\n",
					"            CASE \n",
					"                WHEN b.first_of_month = ADD_MONTHS(DATE_ADD(LAST_DAY((SELECT EndDate FROM MinMaxDates)), 1), -1)\n",
					"                THEN 'Latest'\n",
					"                ELSE CONCAT(b.month_name, ' - ', RIGHT(CAST(b.year_int AS STRING), 2))\n",
					"            END AS Month_Year_Latest,\n",
					"            \n",
					"            -- MATCH ON-PREM: Sort Key Logic\n",
					"            CASE \n",
					"                WHEN b.first_of_month = ADD_MONTHS(DATE_ADD(LAST_DAY((SELECT EndDate FROM MinMaxDates)), 1), -1)\n",
					"                THEN CAST(DATE_FORMAT(b.date, 'yyyyMM') AS BIGINT) * -1\n",
					"                ELSE -999999\n",
					"            END AS MonthYearLatest_SortKey,\n",
					"            \n",
					"            b.year_int AS Year,\n",
					"            \n",
					"            -- MATCH ON-PREM: Runtime Financial Year Calculation (UK FY: Apr-Mar)\n",
					"            CASE \n",
					"                WHEN MONTH(b.date) > 3 THEN YEAR(b.date)\n",
					"                WHEN MONTH(b.date) <= 3 THEN YEAR(b.date) - 1\n",
					"                ELSE YEAR(b.date)\n",
					"            END AS financial_year,\n",
					"            \n",
					"            -- MATCH ON-PREM: FY_Calc logic based on current date\n",
					"            CASE \n",
					"                WHEN CURRENT_DATE() >= DATE(CONCAT(YEAR(CURRENT_DATE()), '-04-01')) THEN\n",
					"                    CASE \n",
					"                        WHEN MONTH(b.date) > 3 AND YEAR(b.date) = YEAR(CURRENT_DATE()) THEN 'Current'\n",
					"                        WHEN MONTH(b.date) <= 3 AND YEAR(b.date) = YEAR(CURRENT_DATE()) + 1 THEN 'Current'\n",
					"                        WHEN MONTH(b.date) > 3 AND YEAR(b.date) = YEAR(CURRENT_DATE()) - 1 THEN 'Previous'\n",
					"                        WHEN MONTH(b.date) <= 3 AND YEAR(b.date) = YEAR(CURRENT_DATE()) THEN 'Previous'\n",
					"                        ELSE NULL\n",
					"                    END\n",
					"                WHEN CURRENT_DATE() >= DATE(CONCAT(YEAR(CURRENT_DATE()) - 1, '-04-01')) THEN\n",
					"                    CASE \n",
					"                        WHEN MONTH(b.date) > 3 AND YEAR(b.date) = YEAR(CURRENT_DATE()) - 1 THEN 'Current'\n",
					"                        WHEN MONTH(b.date) <= 3 AND YEAR(b.date) = YEAR(CURRENT_DATE()) THEN 'Current'\n",
					"                        WHEN MONTH(b.date) > 3 AND YEAR(b.date) = YEAR(CURRENT_DATE()) - 2 THEN 'Previous'\n",
					"                        WHEN MONTH(b.date) <= 3 AND YEAR(b.date) = YEAR(CURRENT_DATE()) - 1 THEN 'Previous'\n",
					"                        ELSE NULL\n",
					"                    END\n",
					"                ELSE NULL\n",
					"            END AS FY_Calc,\n",
					"            \n",
					"            DATE_FORMAT(b.date, 'yyyyMM') AS SortKey,\n",
					"            CONCAT(b.month_name, ' - ', RIGHT(CAST(b.year_int AS STRING), 2)) AS Month_Year\n",
					"            \n",
					"        FROM odw_harmonised_db.live_dim_date b\n",
					"        WHERE b.date IN (SELECT Dt FROM Dates)\n",
					"    ),\n",
					"    \n",
					"    -- Add FY Latest calculation\n",
					"    dimDateWithFY AS (\n",
					"        SELECT *,\n",
					"            -- MATCH ON-PREM: FY Latest Logic\n",
					"            CASE \n",
					"                WHEN FY_Calc = 'Current' THEN 'Latest'\n",
					"                ELSE CONCAT('FY-', RIGHT(CAST(financial_year AS STRING), 2))\n",
					"            END AS FY_Latest,\n",
					"            \n",
					"            -- MATCH ON-PREM: FY Latest Sort Key\n",
					"            CASE \n",
					"                WHEN FY_Calc = 'Current' THEN 2099\n",
					"                ELSE financial_year\n",
					"            END AS FY_Latest_SortKey\n",
					"            \n",
					"        FROM dimDate\n",
					"    ),\n",
					"    \n",
					"    Final AS (\n",
					"        SELECT \n",
					"            CAST(b.date AS date) AS PC_Date,\n",
					"            b.month_name AS PC_Month,\n",
					"            b.month_int,\n",
					"            b.Month_Year_Latest AS PC_Month_Latest,\n",
					"            b.MonthYearLatest_SortKey,\n",
					"            b.Year AS PC_Year,\n",
					"            b.financial_year AS PC_FY,\n",
					"            b.FY_Latest AS PC_FY_Latest,\n",
					"            b.FY_Latest_SortKey,\n",
					"            a.RefNo AS Ref_Number,\n",
					"            a.DisabilityText,\n",
					"            \n",
					"            -- MATCH ON-PREM: Disability Logic (Case-Insensitive)\n",
					"            CASE \n",
					"                WHEN a.DisabilityText IS NULL THEN 'No Record'\n",
					"                WHEN a.DisabilityText = '0' THEN 'No Record'\n",
					"                WHEN UPPER(a.DisabilityText) = 'DISABLED' THEN 'Yes'\n",
					"                WHEN UPPER(a.DisabilityText) = 'HAS A DISABILITY (OR PREVIOUSLY HAD ONE)' THEN 'Yes'\n",
					"                WHEN UPPER(a.DisabilityText) = 'NOT DISABLED' THEN 'No'\n",
					"                WHEN UPPER(a.DisabilityText) = 'DOES NOT HAVE A DISABILITY' THEN 'No'\n",
					"                WHEN UPPER(a.DisabilityText) IN ('PREFER NOT TO SAY','NOT DISCLOSED','DO NOT WISH TO DISCLOSE') THEN 'Unknown'\n",
					"                ELSE 'No Record'\n",
					"            END AS Disabled,\n",
					"            \n",
					"            -- MATCH ON-PREM: Disability Group Logic (Case-Insensitive)\n",
					"            CASE UPPER(a.DisabilityText)\n",
					"                WHEN 'PREFER NOT TO SAY' THEN 'Undeclared'\n",
					"                WHEN 'DISABLED' THEN 'Disabled'\n",
					"                WHEN 'HAS A DISABILITY (OR PREVIOUSLY HAD ONE)' THEN 'Disabled'\n",
					"                WHEN 'NOT DISABLED' THEN 'Not Disabled'\n",
					"                WHEN 'DOES NOT HAVE A DISABILITY' THEN 'Not Disabled'\n",
					"                ELSE 'Not reported'\n",
					"            END AS Disability_group,\n",
					"            \n",
					"            a.EthnicOrigin,\n",
					"            \n",
					"            -- MATCH ON-PREM: Ethnicity Logic (Case-Insensitive)\n",
					"            CASE \n",
					"                WHEN a.EthnicOrigin IS NULL THEN 'No Record'\n",
					"                WHEN a.EthnicOrigin = '0' THEN 'No Record'\n",
					"                WHEN UPPER(a.EthnicOrigin) LIKE 'WHITE%' THEN 'White'\n",
					"                WHEN UPPER(a.EthnicOrigin) IN ('PREFER NOT TO SAY','NOT DISCLOSED','DO NOT WISH TO DISCLOSE','UNKNOWN','NOT KNOWN') THEN 'Unknown'\n",
					"                ELSE 'BME'\n",
					"            END AS Ethnicity,\n",
					"            \n",
					"            -- MATCH ON-PREM: Ethnicity Group Logic (Case-Insensitive)\n",
					"            CASE \n",
					"                WHEN UPPER(a.EthnicOrigin) LIKE 'ASIAN%' THEN 'Asian'\n",
					"                WHEN UPPER(a.EthnicOrigin) LIKE 'BLACK%' THEN 'Black'\n",
					"                WHEN UPPER(a.EthnicOrigin) = 'CHINESE' THEN 'Chinese'\n",
					"                WHEN UPPER(a.EthnicOrigin) = 'DO NOT WISH TO DISCLOSE' THEN 'Not declared'\n",
					"                WHEN UPPER(a.EthnicOrigin) = 'NOT DISCLOSED' THEN 'Not declared'\n",
					"                WHEN UPPER(a.EthnicOrigin) LIKE 'MIXED%' THEN 'Mixed'\n",
					"                WHEN UPPER(a.EthnicOrigin) LIKE 'OTHER%' THEN 'Other ethnicity'\n",
					"                WHEN UPPER(a.EthnicOrigin) = 'UNKNOWN' THEN 'Not reported'\n",
					"                WHEN UPPER(a.EthnicOrigin) = 'NOT KNOWN' THEN 'Not reported'\n",
					"                WHEN UPPER(a.EthnicOrigin) LIKE 'WHITE%' THEN 'White'\n",
					"                WHEN UPPER(a.EthnicOrigin) = 'PREFER NOT TO SAY' THEN 'Prefer not to say'\n",
					"                ELSE 'Not reported'\n",
					"            END AS Ethnicity_group,\n",
					"            \n",
					"            a.ReligiousDenominationKey,\n",
					"            \n",
					"            -- MATCH ON-PREM: Religion Logic (Case-Insensitive)\n",
					"            CASE \n",
					"                WHEN a.ReligiousDenominationKey IS NULL THEN 'No Record'\n",
					"                WHEN UPPER(a.ReligiousDenominationKey) IN ('NONE','AGNOSTIC / ATHEIST') OR LEFT(UPPER(a.ReligiousDenominationKey),3) IN ('ATH','AGN') THEN 'Agnostic / Atheist'\n",
					"                WHEN UPPER(a.ReligiousDenominationKey) = 'CHRISTIAN' THEN 'Christian'\n",
					"                WHEN UPPER(a.ReligiousDenominationKey) IN ('PREFER NOT TO SAY','NOT DISCLOSED','DO NOT WISH TO DISCLOSE','UNKNOWN') THEN 'Unknown'\n",
					"                ELSE 'Other Religion'\n",
					"            END AS Religion,\n",
					"            \n",
					"            -- MATCH ON-PREM: Religion Group Logic (Case-Insensitive)\n",
					"            CASE UPPER(a.ReligiousDenominationKey)\n",
					"                WHEN 'AGNOSTIC' THEN 'No religion'\n",
					"                WHEN 'ATHEIST' THEN 'No religion'\n",
					"                WHEN 'BUDDHIST' THEN 'Buddhist'\n",
					"                WHEN 'CHRISTIAN' THEN 'Christian'\n",
					"                WHEN 'DO NOT WISH TO DISCLOSE' THEN 'Not declared'\n",
					"                WHEN 'HINDU' THEN 'Hindu'\n",
					"                WHEN 'JEWISH' THEN 'Jewish'\n",
					"                WHEN 'MUSLIM' THEN 'Muslim'\n",
					"                WHEN 'NONE' THEN 'No religion'\n",
					"                WHEN 'NO RELIGION' THEN 'No religion'\n",
					"                WHEN 'OTHER (PLEASE SPECIFY)' THEN 'Any other religion'\n",
					"                WHEN 'SIKH' THEN 'Sikh'\n",
					"                ELSE 'Not reported'\n",
					"            END AS Religion_group,\n",
					"            \n",
					"            a.SxO,\n",
					"            \n",
					"            -- MATCH ON-PREM: Sexual Orientation Logic (Case-Insensitive)\n",
					"            CASE \n",
					"                WHEN a.SxO IS NULL THEN 'No Record'\n",
					"                WHEN UPPER(a.SxO) LIKE '%HETERO%' OR UPPER(a.SxO) LIKE '%STRAIGHT%' THEN 'Heterosexual'\n",
					"                WHEN LEFT(UPPER(a.SxO),3) IN ('GAY','LES','BIS') THEN 'LGBT'\n",
					"                WHEN UPPER(a.SxO) IN ('PREFER NOT TO SAY','NOT DISCLOSED','DO NOT WISH TO DISCLOSE','UNKNOWN','OTHER') THEN 'Unknown'\n",
					"                ELSE 'Other'\n",
					"            END AS Sexual_Orientation,\n",
					"            \n",
					"            -- MATCH ON-PREM: Sexual Orientation Group Logic (Case-Insensitive)\n",
					"            CASE \n",
					"                WHEN UPPER(a.SxO) = 'BISEXUAL' THEN 'Bisexual'\n",
					"                WHEN UPPER(a.SxO) LIKE 'GAY%' THEN 'Lesbian / Gay'\n",
					"                WHEN UPPER(a.SxO) LIKE '%HETERO%' OR UPPER(a.SxO) LIKE '%STRAIGHT%' THEN 'Hetero / Straight'\n",
					"                WHEN UPPER(a.SxO) = 'LESBIAN' THEN 'Lesbian / Gay'\n",
					"                WHEN UPPER(a.SxO) = 'NOT DISCLOSED' THEN 'Undeclared'\n",
					"                WHEN UPPER(a.SxO) = 'OTHER' THEN 'Other'\n",
					"                ELSE 'Not reported'\n",
					"            END AS Sexual_Orientation_group\n",
					"            \n",
					"        FROM odw_harmonised_db.sap_hr_protected_data a\n",
					"        LEFT OUTER JOIN dimDateWithFY b\n",
					"            ON CAST(a.Report_MonthEnd_Date AS DATE) = CAST(b.date AS DATE)\n",
					"        WHERE a.Report_MonthEnd_Date IS NOT NULL\n",
					"    )\n",
					"    \n",
					"    SELECT \n",
					"        *,\n",
					"        1 AS PC_Headcount,\n",
					"        \n",
					"        -- MATCH ON-PREM: Data Completeness Logic\n",
					"        CASE \n",
					"            WHEN Disabled IN ('No Record','Unknown') THEN 'No Record or Data not supplied for one or more categories.'\n",
					"            WHEN Ethnicity IN ('No Record','Unknown') THEN 'No Record or Data not supplied for one or more categories.'\n",
					"            WHEN Religion IN ('No Record','Unknown') THEN 'No Record or Data not supplied for one or more categories.'\n",
					"            WHEN Sexual_Orientation IN ('No Record','Unknown') THEN 'No Record or Data not supplied for one or more categories.'\n",
					"            ELSE 'Records Complete'\n",
					"        END AS Data_Completeness\n",
					"    FROM Final\n",
					"    \"\"\")\n",
					"    logInfo(\"Successfully created view odw_curated_db.vw_HR_ProtectedData with ON-PREMISES MATCHING logic\")\n",
					"    \n",
					"    # Count records in view\n",
					"    protected_data_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_curated_db.vw_HR_ProtectedData\").collect()[0]['count']\n",
					"    logInfo(f\"View contains {protected_data_count} protected data records\")\n",
					"    \n",
					"    if protected_data_count == 0:\n",
					"        raise Exception(\"View contains no records - check date dimension join\")\n",
					"    \n",
					"    # Test the Latest logic with detailed debugging\n",
					"    logInfo(\"Testing Latest month identification logic with debugging\")\n",
					"    latest_debug = spark.sql(\"\"\"\n",
					"        WITH Dates AS (\n",
					"            SELECT DISTINCT Report_MonthEnd_Date AS Dt\n",
					"            FROM odw_harmonised_db.sap_hr_protected_data\n",
					"            WHERE Report_MonthEnd_Date IS NOT NULL\n",
					"        ),\n",
					"        MinMaxDates AS (\n",
					"            SELECT MIN(Dt) AS StartDate, MAX(Dt) AS EndDate\n",
					"            FROM Dates\n",
					"        )\n",
					"        SELECT \n",
					"            (SELECT EndDate FROM MinMaxDates) as MaxEndDate,\n",
					"            LAST_DAY((SELECT EndDate FROM MinMaxDates)) as LastDayOfMaxMonth,\n",
					"            DATE_ADD(LAST_DAY((SELECT EndDate FROM MinMaxDates)), 1) as FirstDayOfNextMonth,\n",
					"            ADD_MONTHS(DATE_ADD(LAST_DAY((SELECT EndDate FROM MinMaxDates)), 1), -1) as CalculatedLatestFirstOfMonth\n",
					"    \"\"\").collect()[0]\n",
					"    \n",
					"    logInfo(f\"Date Debug - MaxEndDate: {latest_debug['MaxEndDate']}\")\n",
					"    logInfo(f\"Date Debug - LastDayOfMaxMonth: {latest_debug['LastDayOfMaxMonth']}\")  \n",
					"    logInfo(f\"Date Debug - FirstDayOfNextMonth: {latest_debug['FirstDayOfNextMonth']}\")\n",
					"    logInfo(f\"Date Debug - CalculatedLatestFirstOfMonth: {latest_debug['CalculatedLatestFirstOfMonth']}\")\n",
					"    \n",
					"    latest_test = spark.sql(\"\"\"\n",
					"        SELECT PC_Month_Latest, COUNT(*) as record_count\n",
					"        FROM odw_curated_db.vw_HR_ProtectedData \n",
					"        GROUP BY PC_Month_Latest\n",
					"        ORDER BY PC_Month_Latest\n",
					"    \"\"\").collect()\n",
					"    \n",
					"    for row in latest_test:\n",
					"        logInfo(f\"Month: {row['PC_Month_Latest']}, Records: {row['record_count']}\")\n",
					"    \n",
					"    # Drop the table if it exists\n",
					"    logInfo(\"Dropping table odw_curated_db.pbi_HR_ProtectedData if it exists\")\n",
					"    try:\n",
					"        spark.sql(\"DROP TABLE IF EXISTS odw_curated_db.pbi_HR_ProtectedData\")\n",
					"        logInfo(\"Table dropped or did not exist\")\n",
					"    except Exception as drop_error:\n",
					"        logInfo(f\"Note: Could not drop table (may not exist): {str(drop_error)}\")\n",
					"    \n",
					"    # Create table from view with specified location\n",
					"    logInfo(\"Creating table odw_curated_db.pbi_HR_ProtectedData from view\")\n",
					"    spark.sql(f\"\"\"\n",
					"    CREATE OR REPLACE TABLE odw_curated_db.pbi_HR_ProtectedData\n",
					"    USING delta\n",
					"    LOCATION '{delta_table_path}'\n",
					"    AS SELECT * FROM odw_curated_db.vw_HR_ProtectedData\n",
					"    \"\"\")\n",
					"    \n",
					"    # Count records in table - this is our final record count\n",
					"    table_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_curated_db.pbi_HR_ProtectedData\").collect()[0]['count']\n",
					"    result[\"record_count\"] = table_count\n",
					"    logInfo(f\"Created table with {table_count} records\")\n",
					"    \n",
					"   \n",
					"\n",
					"except Exception as e:\n",
					"    # Capture error information\n",
					"    error_msg = f\"Error in HR Protected Data setup: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    \n",
					"    # Try to get current record count even in case of error\n",
					"    try:\n",
					"        error_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_curated_db.pbi_HR_ProtectedData\").collect()[0]['count']\n",
					"        result[\"record_count\"] = error_count\n",
					"    except:\n",
					"        result[\"record_count\"] = 0\n",
					"    \n",
					"    # Update result for error case\n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = error_msg[:300]  # Truncate to 300 characters\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()\n",
					"    \n",
					"    # Output the simple result as JSON for ADF to capture\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": 25
			}
		]
	}
}