{
	"name": "vw_HR_ProtectedData",
	"properties": {
		"folder": {
			"name": "odw-curated/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "6af41b0d-c990-4a98-8c6f-3fb16b4900f6"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Harmoised layer and build a view & table for Power BI use.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;01-April-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Protected Data Covered in here;"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"%run utils/py_logging_decorator"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# ProtectedData"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import json\n",
					"\n",
					"# Initialize simple result dictionary\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"try:\n",
					"    logInfo(\"Starting HR Protected Data setup with corrected Latest logic\")\n",
					"    \n",
					"    # Get storage account \n",
					"    logInfo(\"Getting storage account configuration\")\n",
					"    storage_account = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"    logInfo(f\"Using storage account: {storage_account}\")\n",
					"    \n",
					"    # Fix the path\n",
					"    storage_account = storage_account.rstrip('/')\n",
					"    delta_table_path = f\"abfss://odw-curated@{storage_account}/saphr/pbi_HR_ProtectedData\"\n",
					"    logInfo(f\"Delta table will be created at: {delta_table_path}\")\n",
					"    \n",
					"    # First, check base data availability\n",
					"    logInfo(\"Checking base protected data availability\")\n",
					"    base_data_check = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as total_records\n",
					"        FROM odw_harmonised_db.sap_hr_protected_data\n",
					"        WHERE Report_MonthEnd_Date IS NOT NULL\n",
					"    \"\"\").collect()[0]\n",
					"    \n",
					"    logInfo(f\"Base data check - Total records: {base_data_check['total_records']}\")\n",
					"    \n",
					"    if base_data_check['total_records'] == 0:\n",
					"        raise Exception(\"No protected data found in source table odw_harmonised_db.sap_hr_protected_data\")\n",
					"    \n",
					"    # Create/refresh view with FIXED Latest logic\n",
					"    logInfo(\"Creating view odw_curated_db.vw_HR_ProtectedData with corrected Latest logic\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE VIEW odw_curated_db.vw_HR_ProtectedData\n",
					"    AS\n",
					"    \n",
					"    WITH Dates AS (\n",
					"        SELECT DISTINCT\n",
					"            Report_MonthEnd_Date AS Dt\n",
					"        FROM odw_harmonised_db.sap_hr_protected_data\n",
					"        WHERE Report_MonthEnd_Date IS NOT NULL\n",
					"    ),\n",
					"    \n",
					"    MinMaxDates AS (\n",
					"        SELECT \n",
					"            MIN(Dt) AS StartDate,\n",
					"            MAX(Dt) AS EndDate\n",
					"        FROM Dates\n",
					"    ),\n",
					"    \n",
					"    Final AS (\n",
					"        SELECT \n",
					"            CAST(b.date AS date) AS PC_Date,\n",
					"            b.month_name AS PC_Month,\n",
					"            b.month_int,\n",
					"            \n",
					"            CASE \n",
					"                WHEN b.date = (SELECT MAX(Dt) FROM Dates) THEN 'Latest'\n",
					"                ELSE CONCAT(b.month_name, ' - ', RIGHT(CAST(b.year_int AS STRING), 2))\n",
					"            END AS PC_Month_Latest,\n",
					"            CAST(COALESCE(b.MonthYearSortKey, 0) AS BIGINT) * -1 AS MonthYearLatest_SortKey,\n",
					"            b.year_int AS PC_Year,\n",
					"            b.FY AS PC_FY,\n",
					"            COALESCE(b.FY_Latest, 'Unknown') AS PC_FY_Latest,\n",
					"            COALESCE(b.FY_Latest_SortKey, 0) * -1 as FY_Latest_SortKey,\n",
					"            a.RefNo AS Ref_Number,\n",
					"            a.DisabilityText,\n",
					"            CASE \n",
					"                WHEN a.DisabilityText IS NULL THEN 'No Record'\n",
					"                WHEN a.DisabilityText = '0' THEN 'No Record'\n",
					"                WHEN a.DisabilityText = 'DISABLED' THEN 'Yes'\n",
					"                WHEN a.DisabilityText = 'Has a disability (or previously had one)' THEN 'Yes'\n",
					"                WHEN a.DisabilityText = 'NOT DISABLED' THEN 'No'\n",
					"                WHEN a.DisabilityText = 'Does not have a disability' THEN 'No'\n",
					"                WHEN a.DisabilityText IN ('PREFER NOT TO SAY','Not disclosed','Do not wish to disclose') THEN 'Unknown'\n",
					"                ELSE 'No Record'\n",
					"            END AS Disabled,\n",
					"            CASE a.DisabilityText\n",
					"                WHEN 'PREFER NOT TO SAY' THEN 'Undeclared'\n",
					"                WHEN 'DISABLED' THEN 'Disabled'\n",
					"                WHEN 'Has a disability (or previously had one)' THEN 'Disabled'\n",
					"                WHEN 'NOT DISABLED' THEN 'Not Disabled'\n",
					"                WHEN 'Does not have a disability' THEN 'Not Disabled'\n",
					"                ELSE 'Not reported'\n",
					"            END AS Disability_group,\n",
					"            a.EthnicOrigin,\n",
					"            CASE \n",
					"                WHEN a.EthnicOrigin IS NULL THEN 'No Record'\n",
					"                WHEN a.EthnicOrigin = '0' THEN 'No Record'\n",
					"                WHEN a.EthnicOrigin LIKE 'White%' THEN 'White'\n",
					"                WHEN a.EthnicOrigin IN ('PREFER NOT TO SAY','Not disclosed','Do not wish to disclose','Unknown','Not known') THEN 'Unknown'\n",
					"                ELSE 'BME'\n",
					"            END AS Ethnicity,\n",
					"            CASE \n",
					"                WHEN a.EthnicOrigin LIKE 'Asian%' THEN 'Asian'\n",
					"                WHEN a.EthnicOrigin LIKE 'Black%' THEN 'Black'\n",
					"                WHEN a.EthnicOrigin = 'Chinese' THEN 'Chinese'\n",
					"                WHEN a.EthnicOrigin = 'Do not wish to disclose' THEN 'Not declared'\n",
					"                WHEN a.EthnicOrigin = 'Not disclosed' THEN 'Not declared'\n",
					"                WHEN a.EthnicOrigin LIKE 'Mixed%' THEN 'Mixed'\n",
					"                WHEN a.EthnicOrigin LIKE 'Other%' THEN 'Other ethnicity'\n",
					"                WHEN a.EthnicOrigin = 'Unknown' THEN 'Not reported'\n",
					"                WHEN a.EthnicOrigin = 'Not known' THEN 'Not reported'\n",
					"                WHEN a.EthnicOrigin LIKE 'White%' THEN 'White'\n",
					"                WHEN a.EthnicOrigin = 'Prefer not to say' THEN 'Prefer not to say'\n",
					"                ELSE 'Not reported'\n",
					"            END AS Ethnicity_group,\n",
					"            a.ReligiousDenominationKey,\n",
					"            CASE \n",
					"                WHEN a.ReligiousDenominationKey IS NULL THEN 'No Record'\n",
					"                WHEN a.ReligiousDenominationKey IN ('None','Agnostic / Atheist') OR LEFT(a.ReligiousDenominationKey,3) IN ('ath','agn') THEN 'Agnostic / Atheist'\n",
					"                WHEN a.ReligiousDenominationKey = 'Christian' THEN 'Christian'\n",
					"                WHEN a.ReligiousDenominationKey IN ('PREFER NOT TO SAY','Not disclosed','Do not wish to disclose','Unknown') THEN 'Unknown'\n",
					"                ELSE 'Other Religion'\n",
					"            END AS Religion,\n",
					"            CASE a.ReligiousDenominationKey\n",
					"                WHEN 'Agnostic' THEN 'No religion'\n",
					"                WHEN 'Atheist' THEN 'No religion'\n",
					"                WHEN 'Buddhist' THEN 'Buddhist'\n",
					"                WHEN 'Christian' THEN 'Christian'\n",
					"                WHEN 'Do not wish to disclose' THEN 'Not declared'\n",
					"                WHEN 'Hindu' THEN 'Hindu'\n",
					"                WHEN 'Jewish' THEN 'Jewish'\n",
					"                WHEN 'Muslim' THEN 'Muslim'\n",
					"                WHEN 'None' THEN 'No religion'\n",
					"                WHEN 'No religion' THEN 'No religion'\n",
					"                WHEN 'Other (please specify)' THEN 'Any other religion'\n",
					"                WHEN 'Sikh' THEN 'Sikh'\n",
					"                ELSE 'Not reported'\n",
					"            END AS Religion_group,\n",
					"            a.SxO,\n",
					"            CASE \n",
					"                WHEN a.SxO IS NULL THEN 'No Record'\n",
					"                WHEN a.SxO LIKE '%Hetero%' THEN 'Heterosexual'\n",
					"                WHEN LEFT(a.SxO,3) IN ('Gay','Les','Bis') THEN 'LGBT'\n",
					"                WHEN a.SxO IN ('PREFER NOT TO SAY','Not disclosed','Do not wish to disclose','Unknown','Other') THEN 'Unknown'\n",
					"                ELSE 'Other'\n",
					"            END AS Sexual_Orientation,\n",
					"            CASE \n",
					"                WHEN a.SxO = 'Bisexual' THEN 'Bisexual'\n",
					"                WHEN a.SxO = 'Gay Man' THEN 'Lesbian / Gay'\n",
					"                WHEN a.SxO LIKE 'Gay%' THEN 'Lesbian / Gay'\n",
					"                WHEN a.SxO LIKE '%Hetero%' THEN 'Hetero / Straight'\n",
					"                WHEN a.SxO = 'Lesbian' THEN 'Lesbian / Gay'\n",
					"                WHEN a.SxO = 'Not disclosed' THEN 'Undeclared'\n",
					"                WHEN a.SxO = 'Other' THEN 'Other'\n",
					"                ELSE 'Not reported'\n",
					"            END AS Sexual_Orientation_group,\n",
					"            CASE \n",
					"                WHEN a.SxO IN ('Bisexual', 'Gay Man', 'Gay', 'Lesbian', 'Other') THEN 'LGBTQ+' \n",
					"                WHEN a.SxO LIKE '%Hetero%' THEN 'Heterosexual'\n",
					"                ELSE COALESCE(a.SxO, 'Not reported')\n",
					"            END AS Sexual_Orientation_PINS_Grouping\n",
					"        FROM odw_harmonised_db.sap_hr_protected_data a\n",
					"        LEFT OUTER JOIN odw_harmonised_db.live_dim_date b\n",
					"            ON CAST(a.Report_MonthEnd_Date AS DATE) = CAST(b.date AS DATE)\n",
					"        WHERE a.Report_MonthEnd_Date IS NOT NULL\n",
					"    )\n",
					"    \n",
					"    SELECT \n",
					"        *,\n",
					"        1 AS PC_Headcount,\n",
					"        CASE \n",
					"            WHEN Disabled IN ('No Record','Unknown') THEN 'No Record or Data not supplied for one or more categories.'\n",
					"            WHEN Ethnicity IN ('No Record','Unknown') THEN 'No Record or Data not supplied for one or more categories.'\n",
					"            WHEN Religion IN ('No Record','Unknown') THEN 'No Record or Data not supplied for one or more categories.'\n",
					"            WHEN Sexual_Orientation IN ('No Record','Unknown') THEN 'No Record or Data not supplied for one or more categories.'\n",
					"            ELSE 'Records Complete'\n",
					"        END AS Data_Completeness,\n",
					"        CONCAT(\n",
					"            CAST(PC_Year AS STRING),\n",
					"            RIGHT(CONCAT('0', CAST(month_int AS STRING)), 2),\n",
					"            RIGHT(CONCAT('0', CAST(DAY(PC_Date) AS STRING)), 2)\n",
					"        ) AS PC_dim_date_key\n",
					"    FROM Final\n",
					"    \"\"\")\n",
					"    logInfo(\"Successfully created view odw_curated_db.vw_HR_ProtectedData with corrected Latest logic\")\n",
					"    \n",
					"    # Count records in view\n",
					"    protected_data_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_curated_db.vw_HR_ProtectedData\").collect()[0]['count']\n",
					"    logInfo(f\"View contains {protected_data_count} protected data records\")\n",
					"    \n",
					"    if protected_data_count == 0:\n",
					"        raise Exception(\"View contains no records - check date dimension join\")\n",
					"    \n",
					"    # Drop the table if it exists\n",
					"    logInfo(\"Dropping table odw_curated_db.pbi_HR_ProtectedData if it exists\")\n",
					"    try:\n",
					"        spark.sql(\"DROP TABLE IF EXISTS odw_curated_db.pbi_HR_ProtectedData\")\n",
					"        logInfo(\"Table dropped or did not exist\")\n",
					"    except Exception as drop_error:\n",
					"        logInfo(f\"Note: Could not drop table (may not exist): {str(drop_error)}\")\n",
					"    \n",
					"    # Create table from view with specified location\n",
					"    logInfo(\"Creating table odw_curated_db.pbi_HR_ProtectedData from view\")\n",
					"    spark.sql(f\"\"\"\n",
					"    CREATE OR REPLACE TABLE odw_curated_db.pbi_HR_ProtectedData\n",
					"    USING delta\n",
					"    LOCATION '{delta_table_path}'\n",
					"    AS SELECT * FROM odw_curated_db.vw_HR_ProtectedData\n",
					"    \"\"\")\n",
					"    \n",
					"    # Count records in table - this is our final record count\n",
					"    table_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_curated_db.pbi_HR_ProtectedData\").collect()[0]['count']\n",
					"    result[\"record_count\"] = table_count\n",
					"    logInfo(f\"Created table with {table_count} records\")\n",
					"    \n",
					"    logInfo(\"HR Protected Data setup completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Capture error information\n",
					"    error_msg = f\"Error in HR Protected Data setup: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    \n",
					"    # Try to get current record count even in case of error\n",
					"    try:\n",
					"        error_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_curated_db.pbi_HR_ProtectedData\").collect()[0]['count']\n",
					"        result[\"record_count\"] = error_count\n",
					"    except:\n",
					"        result[\"record_count\"] = 0\n",
					"    \n",
					"    # Update result for error case\n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = error_msg[:300]  # Truncate to 300 characters\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()\n",
					"    \n",
					"    # Output the simple result as JSON for ADF to capture\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": 7
			}
		]
	}
}