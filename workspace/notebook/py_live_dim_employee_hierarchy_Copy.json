{
	"name": "py_live_dim_employee_hierarchy_Copy",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "7539e3d2-b2d7-47d0-88ef-cf01e38483db"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this pyspark notebook is to read Delta tables from owb_standarsied_db to owb_harmonisied_db and load all records as Delta tables along with metadata columns.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Rohit Shukla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;04-Mar-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The functionality of this notebook is to Delta Table transform_inspector_address into owb-harmonisied_db &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;layer. Following stored procedures have been reverse engineered from MiPiNS using these stored prcedures : &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1. [Build].[sap_employee_hierarchy] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2. [Build].[dim_employee_hierarchy] and function &nbsp;&nbsp;\n",
					"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3. [Live].[udf_sap_LineManagement_H]\n",
					"\n",
					"**Spark Cluster Configuration** -> Apache Spark Version- 3.4, Python Version \t\t- 3.10, Delta Lake Version \t- 2.4\n",
					"\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"#import all libraries and initialise Spark Session\n",
					"import calendar\n",
					"from datetime import datetime, timedelta, date\n",
					"import requests\n",
					"import pyspark.sql.functions as F \n",
					"import re\n",
					"import json\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql.functions import lit, current_timestamp, to_date ,expr, md5, col, date_format,when, to_date,current_date,concat,cast,regexp_replace,coalesce,concat_ws,row_number, to_timestamp, max as spark_max\n",
					"from pprint import pprint as pp\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql import SparkSession\n",
					"spark = SparkSession.builder.getOrCreate()\n",
					"from delta.tables import DeltaTable\n",
					"#ignore FutureWarning messages \n",
					"import warnings\n",
					"warnings.filterwarnings(\"ignore\", message=\"iteritems is deprecated\")\n",
					""
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Get storage account"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"#Get Storage account name\n",
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					""
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define all storage path"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"# Define delta table names, database name and table paths\n",
					"source_database_name = \"odw_standardised_db\"\n",
					"target_database_name = \"odw_harmonised_db\"\n",
					"source_delta_table = f\"{source_database_name}.inspector_addresses_weekly\"\n",
					"target_delta_table = f\"{target_database_name}.live_dim_emp_hierarchy\"\n",
					"delta_table_path = f\"abfss://odw-harmonised@{storage_account}saphr/live_dim_emp_hierarchy\"\n",
					""
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Find Maximum Depth of Employee Hierarchy"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from notebookutils import mssparkutils\n",
					"import json\n",
					"\n",
					"result_data = {\n",
					"  \"table_details\": [\n",
					"    {\n",
					"      \"delta_table_name\": \"hr_absence_monthly\",\n",
					"      \"csv_file_name\": \"HR_Absence_20250528.csv\",\n",
					"      \"record_count\": 0,\n",
					"      \"table_result\": \"failed\"\n",
					"    },\n",
					"    {\n",
					"      \"delta_table_name\": \"inspector_addresses\",\n",
					"      \"csv_file_name\": \"Inspector_Addresses_20250528.csv\",\n",
					"      \"record_count\": 572,\n",
					"      \"table_result\": \"success\"\n",
					"    },\n",
					"    {\n",
					"      \"delta_table_name\": \"inspector_specialisms_monthly\",\n",
					"      \"csv_file_name\": \"Inspector_Specialisms_20250528.csv\",\n",
					"      \"record_count\": 2743,\n",
					"      \"table_result\": \"success\"\n",
					"    },\n",
					"    {\n",
					"      \"delta_table_name\": \"sap_email_monthly\",\n",
					"      \"csv_file_name\": \"SAP_PINS_Email_20250528.csv\",\n",
					"      \"record_count\": 1157,\n",
					"      \"table_result\": \"success\"\n",
					"    },\n",
					"    {\n",
					"      \"delta_table_name\": \"sap_hr_history_monthly\",\n",
					"      \"csv_file_name\": \"SAP_HR_History_20250528.csv\",\n",
					"      \"record_count\": 1157,\n",
					"      \"table_result\": \"success\"\n",
					"    },\n",
					"    {\n",
					"      \"delta_table_name\": \"sap_hr_leavers_monthly\",\n",
					"      \"csv_file_name\": \"SAP_HR_Leavers_20250528.csv\",\n",
					"      \"record_count\": 1167,\n",
					"      \"table_result\": \"success\"\n",
					"    },\n",
					"    {\n",
					"      \"delta_table_name\": \"sap_protected_monthly\",\n",
					"      \"csv_file_name\": \"SAP_Protected_20250528.csv\",\n",
					"      \"record_count\": 1001,\n",
					"      \"table_result\": \"success\"\n",
					"    }\n",
					"  ]\n",
					"}\n",
					"\n",
					"mssparkutils.notebook.exit(json.dumps(result_data))"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_harmonised_db.live_dim_emp_hierarchy\")\n",
					"\n",
					"#spark.sql(f\"delete from odw_harmonised_db.sap_hr_inspector_address\")\n",
					"\n",
					"# Read source data from Delta Tables\n",
					""
				],
				"execution_count": null
			}
		]
	}
}