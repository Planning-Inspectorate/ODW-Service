{
	"name": "py_absence_data_all_CR",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "2126a905-8831-4286-aaa9-efc7cb6d59cf"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Standardised layer and build a table for Curated Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;01-April-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This Notebook is designed to facilitate the monthly processing and harmonization of absence data. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The Notebook ensures that absence data is accurately transformed, stored, and made available for reporting and analysis."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Intialisations"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import logging"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"\n",
					"%run utils/py_logging_decorator"
				],
				"execution_count": 24
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Entity : absence_all"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import json\n",
					"\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"inserted_count\": 0,\n",
					"    \"updated_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"try:\n",
					"    # Set legacy time parser for compatibility\n",
					"    logInfo(\"Setting legacy time parser policy\")\n",
					"    spark.sql(\"\"\"SET spark.sql.legacy.timeParserPolicy = LEGACY\"\"\")\n",
					"    logInfo(\"Legacy time parser policy set successfully\")\n",
					"    \n",
					"    # Step 1: Create temporary view for source data with incremental filter\n",
					"    logInfo(\"Creating temporary view for source data with incremental logic\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW source_data AS\n",
					"    SELECT  \n",
					"        StaffNumber, AbsType, SicknessGroup, StartDate, EndDate, \n",
					"        AttendanceorAbsenceType, Days, Hrs, Start, Endtime, \n",
					"        Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys, \n",
					"        AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo, \n",
					"        RowID, IsActive\n",
					"    FROM\n",
					"    (\n",
					"        SELECT  \n",
					"            StaffNumber,\n",
					"            COALESCE(AbsType, '') AS AbsType,\n",
					"            COALESCE(SicknessGroup, '') AS SicknessGroup,\n",
					"            CAST(TO_DATE(StartDate,'dd/MM/yyyy') AS DATE) AS StartDate,\n",
					"            CAST(TO_DATE(EndDate,'dd/MM/yyyy') AS DATE) AS EndDate,\n",
					"            AttendanceorAbsenceType,\n",
					"            REPLACE(Days, ',', '') AS Days,\n",
					"            REPLACE(Hrs, ',', '') AS Hrs,\n",
					"            TO_DATE('31/12/1899','dd/MM/yyyy') AS Start,\n",
					"            TO_DATE('31/12/1899','dd/MM/yyyy') AS Endtime,  \n",
					"            Caldays,\n",
					"            WorkScheduleRule,\n",
					"            TRY_CAST(REPLACE(Wkhrs, ',', '') AS DOUBLE) AS Wkhrs,  \n",
					"            TRY_CAST(REPLACE(HrsDay, ',', '') AS DOUBLE) AS HrsDay,  \n",
					"            TRY_CAST(REPLACE(WkDys, ',', '') AS DOUBLE) AS WkDys,  \n",
					"            TO_DATE(AnnualLeaveStart,'dd/MM/yyyy') AS AnnualLeaveStart,\n",
					"            'saphr' AS SourceSystemID,\n",
					"            CURRENT_DATE() AS IngestionDate,\n",
					"            CURRENT_TIMESTAMP() AS ValidTo,\n",
					"            MD5(CONCAT_WS('|',\n",
					"                StaffNumber,            \n",
					"                COALESCE(AbsType, ''),                \n",
					"                COALESCE(SicknessGroup, ''),          \n",
					"                TO_DATE(StartDate,'dd/MM/yyyy'),              \n",
					"                TO_DATE(EndDate,'dd/MM/yyyy'),                \n",
					"                AttendanceorAbsenceType,\n",
					"                REPLACE(Days, ',', ''),                   \n",
					"                REPLACE(Hrs, ',', ''),                    \n",
					"                TO_DATE('31/12/1899','dd/MM/yyyy'),                  \n",
					"                TO_DATE('31/12/1899','dd/MM/yyyy'),                \n",
					"                Caldays,                \n",
					"                WorkScheduleRule,       \n",
					"                REPLACE(Wkhrs, ',', ''),                  \n",
					"                REPLACE(HrsDay, ',', ''),                 \n",
					"                REPLACE(WkDys, ',', '')\n",
					"            )) AS RowID,\n",
					"            'Y' AS IsActive,\n",
					"            -- Use externalCode for deduplication but don't include in final output\n",
					"            ROW_NUMBER() OVER (\n",
					"                PARTITION BY StaffNumber, StartDate, COALESCE(AbsType, ''), AttendanceorAbsenceType \n",
					"                ORDER BY COALESCE(lastModifiedDate, CURRENT_TIMESTAMP()) DESC\n",
					"            ) AS row_num\n",
					"        FROM odw_standardised_db.absence_data_historic\n",
					"        WHERE \n",
					"            StaffNumber IS NOT NULL \n",
					"            AND StartDate IS NOT NULL \n",
					"            AND EndDate IS NOT NULL\n",
					"            -- Incremental filter: only process records modified since last successful load\n",
					"            AND (\n",
					"                COALESCE(lastModifiedDate, CURRENT_TIMESTAMP()) > (\n",
					"                    SELECT COALESCE(MAX(ValidTo), CAST('1900-01-01' AS TIMESTAMP)) \n",
					"                    FROM odw_harmonised_db.sap_hr_absence_all\n",
					"                ) \n",
					"                OR lastModifiedDate IS NULL  -- Include records without modification date\n",
					"            )\n",
					"    ) ranked\n",
					"    WHERE row_num = 1\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step 2: MERGE operation using composite key\n",
					"    logInfo(\"Starting MERGE operation for incremental load\")\n",
					"    spark.sql(\"\"\"\n",
					"    MERGE INTO odw_harmonised_db.sap_hr_absence_all AS target\n",
					"    USING source_data AS source\n",
					"    ON target.StaffNumber = source.StaffNumber \n",
					"       AND target.StartDate = source.StartDate\n",
					"       AND target.AbsType = source.AbsType\n",
					"       AND target.AttendanceorAbsenceType = source.AttendanceorAbsenceType\n",
					"    \n",
					"    WHEN MATCHED AND target.RowID != source.RowID THEN\n",
					"        UPDATE SET\n",
					"            SicknessGroup = source.SicknessGroup,\n",
					"            EndDate = source.EndDate,\n",
					"            Days = source.Days,\n",
					"            Hrs = source.Hrs,\n",
					"            Start = source.Start,\n",
					"            Endtime = source.Endtime,\n",
					"            Caldays = source.Caldays,\n",
					"            WorkScheduleRule = source.WorkScheduleRule,\n",
					"            Wkhrs = source.Wkhrs,\n",
					"            HrsDay = source.HrsDay,\n",
					"            WkDys = source.WkDys,\n",
					"            AnnualLeaveStart = source.AnnualLeaveStart,\n",
					"            SourceSystemID = source.SourceSystemID,\n",
					"            IngestionDate = source.IngestionDate,\n",
					"            ValidTo = source.ValidTo,\n",
					"            RowID = source.RowID,\n",
					"            IsActive = source.IsActive\n",
					"    \n",
					"    WHEN NOT MATCHED THEN\n",
					"        INSERT (\n",
					"            StaffNumber, AbsType, SicknessGroup, StartDate, EndDate,\n",
					"            AttendanceorAbsenceType, Days, Hrs, Start, Endtime,\n",
					"            Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys,\n",
					"            AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo,\n",
					"            RowID, IsActive\n",
					"        )\n",
					"        VALUES (\n",
					"            source.StaffNumber, source.AbsType, source.SicknessGroup, \n",
					"            source.StartDate, source.EndDate, source.AttendanceorAbsenceType,\n",
					"            source.Days, source.Hrs, source.Start, source.Endtime,\n",
					"            source.Caldays, source.WorkScheduleRule, source.Wkhrs,\n",
					"            source.HrsDay, source.WkDys, source.AnnualLeaveStart,\n",
					"            source.SourceSystemID, source.IngestionDate, source.ValidTo,\n",
					"            source.RowID, source.IsActive\n",
					"        )\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step 3: Get counts for reporting\n",
					"    logInfo(\"Getting record counts for reporting\")\n",
					"    \n",
					"    source_record_count = spark.sql(\"SELECT COUNT(*) as count FROM source_data\").collect()[0]['count']\n",
					"    \n",
					"    inserted_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count FROM odw_harmonised_db.sap_hr_absence_all\n",
					"    WHERE IngestionDate = CURRENT_DATE()\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    updated_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count FROM odw_harmonised_db.sap_hr_absence_all\n",
					"    WHERE DATE(ValidTo) = CURRENT_DATE() AND IngestionDate != CURRENT_DATE()\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    final_record_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count FROM odw_harmonised_db.sap_hr_absence_all\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    result[\"record_count\"] = final_record_count\n",
					"    result[\"inserted_count\"] = inserted_count\n",
					"    result[\"updated_count\"] = updated_count\n",
					"    \n",
					"    logInfo(f\"Incremental load completed successfully:\")\n",
					"    logInfo(f\"- Source records processed: {source_record_count}\")\n",
					"    logInfo(f\"- Records inserted: {inserted_count}\")\n",
					"    logInfo(f\"- Records updated: {updated_count}\")\n",
					"    logInfo(f\"- Total records in target: {final_record_count}\")\n",
					"    \n",
					"    spark.sql(\"DROP VIEW IF EXISTS source_data\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error in incremental absence data processing: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    \n",
					"    truncated_error = (error_msg[:297] + '...') if len(error_msg) > 300 else error_msg\n",
					"    \n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = truncated_error\n",
					"    result[\"record_count\"] = -1\n",
					"    result[\"inserted_count\"] = -1\n",
					"    result[\"updated_count\"] = -1\n",
					"    \n",
					"    try:\n",
					"        spark.sql(\"DROP VIEW IF EXISTS source_data\")\n",
					"    except:\n",
					"        pass\n",
					"    \n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"import json\n",
					"\n",
					"# Initialize result dictionary\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"try:\n",
					"    # Set legacy time parser for compatibility\n",
					"    logInfo(\"Setting legacy time parser policy\")\n",
					"    spark.sql(\"\"\"SET spark.sql.legacy.timeParserPolicy = LEGACY\"\"\")\n",
					"    logInfo(\"Legacy time parser policy set successfully\")\n",
					"    \n",
					"    # Step 1: Delete all rows from the target table\n",
					"    logInfo(\"Starting deletion of all rows from odw_harmonised_db.sap_hr_absence_all\")\n",
					"    spark.sql(\"\"\"\n",
					"    DELETE FROM odw_harmonised_db.sap_hr_absence_all\n",
					"    \"\"\")\n",
					"    logInfo(\"Successfully deleted all rows from odw_harmonised_db.sap_hr_absence_all\")\n",
					"    \n",
					"    # Step 2: Insert data with deduplication built-in using a window function\n",
					"    logInfo(\"Starting data insertion with built-in deduplication\")\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.sap_hr_absence_all\n",
					"    SELECT  \n",
					"        StaffNumber, AbsType, SicknessGroup, StartDate, EndDate, \n",
					"        AttendanceorAbsenceType, Days, Hrs, Start, Endtime, \n",
					"        Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys, \n",
					"        AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo, \n",
					"        RowID, IsActive\n",
					"    FROM\n",
					"    (\n",
					"        SELECT  \n",
					"            StaffNumber,\n",
					"            COALESCE(AbsType, '') AS AbsType,\n",
					"            COALESCE(SicknessGroup, '') AS SicknessGroup,\n",
					"            cast(to_date(StartDate,'dd/MM/yyyy') as date) as StartDate,\n",
					"            cast(to_date(EndDate,'dd/MM/yyyy') as date) as EndDate,\n",
					"            AttendanceorAbsenceType,\n",
					"            REPLACE(Days, ',', '') AS Days,\n",
					"            REPLACE(Hrs, ',', '') AS Hrs,\n",
					"            to_date('31/12/1899','dd/MM/yyyy') as Start,\n",
					"            to_date('31/12/1899','dd/MM/yyyy') as Endtime,  \n",
					"            Caldays,\n",
					"            WorkScheduleRule,\n",
					"            TRY_CAST(REPLACE(Wkhrs, ',', '') AS DOUBLE) AS Wkhrs,  \n",
					"            TRY_CAST(REPLACE(HrsDay, ',', '') AS DOUBLE) AS HrsDay,  \n",
					"            TRY_CAST(REPLACE(WkDys, ',', '') AS DOUBLE) AS WkDys,  \n",
					"            to_date(AnnualLeaveStart,'dd/MM/yyyy') AS AnnualLeaveStart,\n",
					"            'saphr' AS SourceSystemID,\n",
					"            CURRENT_DATE() AS IngestionDate,\n",
					"            CURRENT_TIMESTAMP() AS ValidTo,\n",
					"            md5(concat_ws('|',\n",
					"                StaffNumber,            \n",
					"                COALESCE(AbsType, ''),                \n",
					"                COALESCE(SicknessGroup, ''),          \n",
					"                to_date(StartDate,'dd/MM/yyyy'),              \n",
					"                to_date(EndDate,'dd/MM/yyyy'),                \n",
					"                AttendanceorAbsenceType,\n",
					"                REPLACE(Days, ',', ''),                   \n",
					"                REPLACE(Hrs, ',', ''),                    \n",
					"                to_date('31/12/1899','dd/MM/yyyy'),                  \n",
					"                to_date('31/12/1899','dd/MM/yyyy'),                \n",
					"                Caldays,                \n",
					"                WorkScheduleRule,       \n",
					"                REPLACE(Wkhrs, ',', ''),                  \n",
					"                REPLACE(HrsDay, ',', ''),                 \n",
					"                REPLACE(WkDys, ',', '')\n",
					"            )) AS RowID,\n",
					"            'Y' AS IsActive,\n",
					"            ROW_NUMBER() OVER (PARTITION BY StaffNumber, StartDate ORDER BY StaffNumber, StartDate) AS row_num\n",
					"        FROM odw_standardised_db.absence_data_historic\n",
					"        WHERE \n",
					"            StaffNumber IS NOT NULL \n",
					"            AND StartDate IS NOT NULL \n",
					"            AND EndDate IS NOT NULL\n",
					"    ) ranked\n",
					"    WHERE row_num = 1\n",
					"    \"\"\")\n",
					"    \n",
					"    # Get final record count (only records inserted in this run)\n",
					"    final_record_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count FROM odw_harmonised_db.sap_hr_absence_all\n",
					"    WHERE IngestionDate = CURRENT_DATE()\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    result[\"record_count\"] = final_record_count\n",
					"    logInfo(f\"Successfully inserted {final_record_count} records into target table\")\n",
					"    \n",
					"    # Final success message\n",
					"    logInfo(\"Absence data processing completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Capture error information and limit to 300 characters\n",
					"    error_msg = f\"Error in absence data processing: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    \n",
					"    # Truncate error message if it exceeds 300 characters\n",
					"    truncated_error = (error_msg[:297] + '...') if len(error_msg) > 300 else error_msg\n",
					"    \n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = truncated_error\n",
					"    result[\"record_count\"] = -1  # Indicate failure with -1 count\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()\n",
					"    \n",
					"    # Output the result as JSON for ADF to capture\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"    SELECT  \n",
					"        StaffNumber,\n",
					"        COALESCE(AbsType, '') AS AbsType,\n",
					"        COALESCE(SicknessGroup, '') AS SicknessGroup,\n",
					"        CAST(TO_DATE(StartDate,'dd/MM/yyyy') AS DATE) AS StartDate,\n",
					"        CAST(TO_DATE(EndDate,'dd/MM/yyyy') AS DATE) AS EndDate,\n",
					"        AttendanceorAbsenceType,\n",
					"        REPLACE(Days, ',', '') AS Days,\n",
					"        REPLACE(Hrs, ',', '') AS Hrs,\n",
					"        TO_DATE('31/12/1899','dd/MM/yyyy') AS Start,\n",
					"        TO_DATE('31/12/1899','dd/MM/yyyy') AS Endtime,  \n",
					"        Caldays,\n",
					"        WorkScheduleRule,\n",
					"        TRY_CAST(REPLACE(Wkhrs, ',', '') AS DOUBLE) AS Wkhrs,  \n",
					"        TRY_CAST(REPLACE(HrsDay, ',', '') AS DOUBLE) AS HrsDay,  \n",
					"        TRY_CAST(REPLACE(WkDys, ',', '') AS DOUBLE) AS WkDys,  \n",
					"        TO_DATE(AnnualLeaveStart,'dd/MM/yyyy') AS AnnualLeaveStart,\n",
					"        'saphr' AS SourceSystemID,\n",
					"        CURRENT_DATE() AS IngestionDate,\n",
					"        CURRENT_TIMESTAMP() AS ValidTo,\n",
					"        \n",
					"        -- Extract status from AbsType (everything before first hyphen)\n",
					"        CASE \n",
					"            WHEN AbsType LIKE '%-%' THEN TRIM(SPLIT(AbsType, '-')[0])\n",
					"            ELSE 'UNKNOWN'\n",
					"        END AS ApprovalStatus,\n",
					"        \n",
					"        -- Extract UUID (last part after final hyphen, assuming 30+ characters)\n",
					"        CASE \n",
					"            WHEN AbsType RLIKE '.*-([a-f0-9]{30,})$' THEN \n",
					"                REGEXP_EXTRACT(AbsType, '.*-([a-f0-9]{30,})$', 1)\n",
					"            ELSE NULL\n",
					"        END AS RecordUUID,\n",
					"        \n",
					"        -- Extract last modified datetime\n",
					"        CASE \n",
					"            WHEN AbsType RLIKE '^[A-Z_]+-[0-9]{4}-[0-9]{2}-[0-9]{2}' THEN\n",
					"                TRY_CAST(\n",
					"                    REGEXP_EXTRACT(AbsType, '^[A-Z_]+-([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2})', 1)\n",
					"                    AS TIMESTAMP\n",
					"                )\n",
					"            ELSE CURRENT_TIMESTAMP()\n",
					"        END AS LastModifiedDate,\n",
					"        \n",
					"        -- Create row hash for change detection\n",
					"        MD5(CONCAT_WS('|',\n",
					"            StaffNumber,            \n",
					"            COALESCE(AbsType, ''),                \n",
					"            COALESCE(SicknessGroup, ''),          \n",
					"            TO_DATE(StartDate,'dd/MM/yyyy'),              \n",
					"            TO_DATE(EndDate,'dd/MM/yyyy'),                \n",
					"            AttendanceorAbsenceType,\n",
					"            REPLACE(Days, ',', ''),                   \n",
					"            REPLACE(Hrs, ',', ''),                    \n",
					"            Caldays,                \n",
					"            WorkScheduleRule,       \n",
					"            REPLACE(Wkhrs, ',', ''),                  \n",
					"            REPLACE(HrsDay, ',', ''),                 \n",
					"            REPLACE(WkDys, ',', '')\n",
					"        )) AS RowID,\n",
					"        'Y' AS IsActive\n",
					"        \n",
					"    FROM odw_standardised_db.absence_data_historic\n",
					"    WHERE \n",
					"        StaffNumber IS NOT NULL \n",
					"        AND StartDate IS NOT NULL \n",
					"        AND EndDate IS NOT NULL\n",
					"        AND AbsType IS NOT NULL"
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"source": [
					"import json\n",
					"\n",
					"# Initialize result dictionary\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"inserted_count\": 0,\n",
					"    \"updated_count\": 0,\n",
					"    \"deleted_count\": 0,\n",
					"    \"cancelled_deleted_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"try:\n",
					"    # Set legacy time parser for compatibility\n",
					"    logInfo(\"Setting legacy time parser policy\")\n",
					"    spark.sql(\"\"\"SET spark.sql.legacy.timeParserPolicy = LEGACY\"\"\")\n",
					"    logInfo(\"Legacy time parser policy set successfully\")\n",
					"    \n",
					"    # Step 1: Create staging view with parsed AbsType data\n",
					"    logInfo(\"Creating staging view with parsed AbsType data\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW staging_absence AS\n",
					"    SELECT  \n",
					"        StaffNumber,\n",
					"        COALESCE(AbsType, '') AS AbsType,\n",
					"        COALESCE(SicknessGroup, '') AS SicknessGroup,\n",
					"        CAST(TO_DATE(StartDate,'dd/MM/yyyy') AS DATE) AS StartDate,\n",
					"        CAST(TO_DATE(EndDate,'dd/MM/yyyy') AS DATE) AS EndDate,\n",
					"        AttendanceorAbsenceType,\n",
					"        REPLACE(Days, ',', '') AS Days,\n",
					"        REPLACE(Hrs, ',', '') AS Hrs,\n",
					"        TO_DATE('31/12/1899','dd/MM/yyyy') AS Start,\n",
					"        TO_DATE('31/12/1899','dd/MM/yyyy') AS Endtime,  \n",
					"        Caldays,\n",
					"        WorkScheduleRule,\n",
					"        TRY_CAST(REPLACE(Wkhrs, ',', '') AS DOUBLE) AS Wkhrs,  \n",
					"        TRY_CAST(REPLACE(HrsDay, ',', '') AS DOUBLE) AS HrsDay,  \n",
					"        TRY_CAST(REPLACE(WkDys, ',', '') AS DOUBLE) AS WkDys,  \n",
					"        TO_DATE(AnnualLeaveStart,'dd/MM/yyyy') AS AnnualLeaveStart,\n",
					"        'saphr' AS SourceSystemID,\n",
					"        CURRENT_DATE() AS IngestionDate,\n",
					"        CURRENT_TIMESTAMP() AS ValidTo,\n",
					"        \n",
					"        -- Extract status from AbsType (everything before first hyphen)\n",
					"        CASE \n",
					"            WHEN AbsType LIKE '%-%' THEN TRIM(SPLIT(AbsType, '-')[0])\n",
					"            ELSE 'UNKNOWN'\n",
					"        END AS ApprovalStatus,\n",
					"        \n",
					"        -- Extract UUID (last part after final hyphen, assuming 30+ characters)\n",
					"        CASE \n",
					"            WHEN AbsType RLIKE '.*-([a-f0-9]{30,})$' THEN \n",
					"                REGEXP_EXTRACT(AbsType, '.*-([a-f0-9]{30,})$', 1)\n",
					"            ELSE NULL\n",
					"        END AS RecordUUID,\n",
					"        \n",
					"        -- Extract last modified datetime\n",
					"        CASE \n",
					"            WHEN AbsType RLIKE '^[A-Z_]+-[0-9]{4}-[0-9]{2}-[0-9]{2}' THEN\n",
					"                TRY_CAST(\n",
					"                    REGEXP_EXTRACT(AbsType, '^[A-Z_]+-([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2})', 1)\n",
					"                    AS TIMESTAMP\n",
					"                )\n",
					"            ELSE CURRENT_TIMESTAMP()\n",
					"        END AS LastModifiedDate,\n",
					"        \n",
					"        -- Create row hash for change detection\n",
					"        MD5(CONCAT_WS('|',\n",
					"            StaffNumber,            \n",
					"            COALESCE(AbsType, ''),                \n",
					"            COALESCE(SicknessGroup, ''),          \n",
					"            TO_DATE(StartDate,'dd/MM/yyyy'),              \n",
					"            TO_DATE(EndDate,'dd/MM/yyyy'),                \n",
					"            AttendanceorAbsenceType,\n",
					"            REPLACE(Days, ',', ''),                   \n",
					"            REPLACE(Hrs, ',', ''),                    \n",
					"            Caldays,                \n",
					"            WorkScheduleRule,       \n",
					"            REPLACE(Wkhrs, ',', ''),                  \n",
					"            REPLACE(HrsDay, ',', ''),                 \n",
					"            REPLACE(WkDys, ',', '')\n",
					"        )) AS RowID,\n",
					"        'Y' AS IsActive\n",
					"        \n",
					"    FROM odw_standardised_db.absence_data_historic\n",
					"    WHERE \n",
					"        StaffNumber IS NOT NULL \n",
					"        AND StartDate IS NOT NULL \n",
					"        AND EndDate IS NOT NULL\n",
					"        AND AbsType IS NOT NULL\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step 2: Create view of latest records per UUID (handling duplicates)\n",
					"    logInfo(\"Creating view of latest records per UUID\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW latest_staging_records AS\n",
					"    SELECT *\n",
					"    FROM (\n",
					"        SELECT *,\n",
					"            ROW_NUMBER() OVER (\n",
					"                PARTITION BY RecordUUID \n",
					"                ORDER BY LastModifiedDate DESC, IngestionDate DESC\n",
					"            ) AS rn\n",
					"        FROM staging_absence\n",
					"        WHERE RecordUUID IS NOT NULL\n",
					"    ) ranked\n",
					"    WHERE rn = 1\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step 3: Create a snapshot of current target table\n",
					"    logInfo(\"Creating snapshot of current target table\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW current_target AS\n",
					"    SELECT * FROM odw_harmonised_db.sap_hr_absence_all\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step 4: Identify outdated records\n",
					"    logInfo(\"Identifying outdated records to replace\")\n",
					"    records_to_update = spark.sql(\"\"\"\n",
					"    SELECT DISTINCT t.RowID as old_rowid\n",
					"    FROM current_target t\n",
					"    INNER JOIN latest_staging_records s\n",
					"    ON t.RowID = s.RowID \n",
					"    WHERE s.LastModifiedDate > t.ValidTo\n",
					"    \"\"\")\n",
					"    \n",
					"    deleted_count = records_to_update.count()\n",
					"    logInfo(f\"Found {deleted_count} outdated records to replace\")\n",
					"    \n",
					"    # Step 5: Create view of records to keep (excluding outdated ones)\n",
					"    if deleted_count > 0:\n",
					"        old_rowids_list = [f\"'{row.old_rowid}'\" for row in records_to_update.collect()]\n",
					"        old_rowids_str = \",\".join(old_rowids_list)\n",
					"        \n",
					"        logInfo(f\"Creating view excluding {len(old_rowids_list)} outdated records\")\n",
					"        \n",
					"        spark.sql(f\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW records_to_keep AS\n",
					"        SELECT * FROM current_target\n",
					"        WHERE RowID NOT IN ({old_rowids_str})\n",
					"        \"\"\")\n",
					"    else:\n",
					"        logInfo(\"No outdated records found - keeping all current records\")\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW records_to_keep AS\n",
					"        SELECT * FROM current_target\n",
					"        \"\"\")\n",
					"    \n",
					"    # Step 6: Get new records from staging (those not in current target)\n",
					"    logInfo(\"Identifying new records to insert\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW new_records AS\n",
					"    SELECT \n",
					"        StaffNumber, AbsType, SicknessGroup, StartDate, EndDate,\n",
					"        AttendanceorAbsenceType, Days, Hrs, Start, Endtime,\n",
					"        Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys,\n",
					"        AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo,\n",
					"        RowID, IsActive\n",
					"    FROM latest_staging_records s\n",
					"    WHERE NOT EXISTS (\n",
					"        SELECT 1 FROM current_target t\n",
					"        WHERE t.RowID = s.RowID\n",
					"    )\n",
					"    \"\"\")\n",
					"    \n",
					"    new_records_count = spark.sql(\"SELECT COUNT(*) as count FROM new_records\").collect()[0]['count']\n",
					"    logInfo(f\"Found {new_records_count} new records to add\")\n",
					"    \n",
					"    # Step 7: Create final dataset combining kept records + new records\n",
					"    logInfo(\"Creating final dataset combining existing and new records\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW final_dataset AS\n",
					"    SELECT * FROM records_to_keep\n",
					"    UNION ALL\n",
					"    SELECT * FROM new_records\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step 8: Remove cancelled/rejected records from final dataset\n",
					"    logInfo(\"Filtering out cancelled/rejected records\")\n",
					"    \n",
					"    total_before_filter = spark.sql(\"SELECT COUNT(*) as count FROM final_dataset\").collect()[0]['count']\n",
					"    \n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW final_approved_dataset AS\n",
					"    SELECT * FROM final_dataset\n",
					"    WHERE NOT (\n",
					"        UPPER(CASE \n",
					"            WHEN AbsType LIKE '%-%' THEN TRIM(SPLIT(AbsType, '-')[0])\n",
					"            ELSE 'UNKNOWN'\n",
					"        END) IN ('CANCELLED', 'PENDING_CANCELLATION', 'REJECTED')\n",
					"    )\n",
					"    \"\"\")\n",
					"    \n",
					"    total_after_filter = spark.sql(\"SELECT COUNT(*) as count FROM final_approved_dataset\").collect()[0]['count']\n",
					"    cancelled_count = total_before_filter - total_after_filter\n",
					"    \n",
					"    logInfo(f\"Filtered out {cancelled_count} cancelled/rejected records\")\n",
					"    \n",
					"    # Step 9: TRUNCATE and INSERT - No overwrites!\n",
					"    logInfo(\"Refreshing target table with final approved dataset\")\n",
					"    \n",
					"    # Clear target table\n",
					"    spark.sql(\"delete from  odw_harmonised_db.sap_hr_absence_all\")\n",
					"    logInfo(\"Truncated target table\")\n",
					"    \n",
					"    # Insert final approved dataset\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.sap_hr_absence_all (\n",
					"        StaffNumber, AbsType, SicknessGroup, StartDate, EndDate,\n",
					"        AttendanceorAbsenceType, Days, Hrs, Start, Endtime,\n",
					"        Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys,\n",
					"        AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo,\n",
					"        RowID, IsActive\n",
					"    )\n",
					"    SELECT \n",
					"        StaffNumber, AbsType, SicknessGroup, StartDate, EndDate,\n",
					"        AttendanceorAbsenceType, Days, Hrs, Start, Endtime,\n",
					"        Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys,\n",
					"        AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo,\n",
					"        RowID, IsActive\n",
					"    FROM final_approved_dataset\n",
					"    \"\"\")\n",
					"    \n",
					"    final_record_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.sap_hr_absence_all\").collect()[0]['count']\n",
					"    logInfo(f\"Successfully loaded {final_record_count} total records into target table\")\n",
					"    \n",
					"    # Step 10: Calculate final counts for reporting\n",
					"    source_record_count = spark.sql(\"SELECT COUNT(*) as count FROM latest_staging_records\").collect()[0]['count']\n",
					"    \n",
					"    result[\"record_count\"] = final_record_count\n",
					"    result[\"inserted_count\"] = new_records_count\n",
					"    result[\"updated_count\"] = deleted_count  # Records that were replaced\n",
					"    result[\"deleted_count\"] = deleted_count\n",
					"    result[\"cancelled_deleted_count\"] = cancelled_count\n",
					"    \n",
					"    logInfo(f\"Incremental load completed successfully:\")\n",
					"    logInfo(f\"- Source records processed: {source_record_count}\")\n",
					"    logInfo(f\"- New records inserted: {new_records_count}\")\n",
					"    logInfo(f\"- Outdated records replaced: {deleted_count}\")\n",
					"    logInfo(f\"- Cancelled/rejected records removed: {cancelled_count}\")\n",
					"    logInfo(f\"- Total records in target: {final_record_count}\")\n",
					"    \n",
					"    # Clean up temporary views\n",
					"    spark.sql(\"DROP VIEW IF EXISTS staging_absence\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS latest_staging_records\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS current_target\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS records_to_keep\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS new_records\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS final_dataset\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS final_approved_dataset\")\n",
					"    \n",
					"    logInfo(\"Successfully completed incremental load process\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error in status-based incremental absence data processing: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    \n",
					"    truncated_error = (error_msg[:297] + '...') if len(error_msg) > 300 else error_msg\n",
					"    \n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = truncated_error\n",
					"    result[\"record_count\"] = -1\n",
					"    result[\"inserted_count\"] = -1\n",
					"    result[\"updated_count\"] = -1\n",
					"    result[\"deleted_count\"] = -1\n",
					"    result[\"cancelled_deleted_count\"] = -1\n",
					"    \n",
					"    # Clean up on error\n",
					"    try:\n",
					"        spark.sql(\"DROP VIEW IF EXISTS staging_absence\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS latest_staging_records\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS current_target\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS records_to_keep\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS new_records\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS final_dataset\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS final_approved_dataset\")\n",
					"    except:\n",
					"        pass\n",
					"    \n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}