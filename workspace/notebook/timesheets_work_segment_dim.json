{
	"name": "timesheets_work_segment_dim",
	"properties": {
		"folder": {
			"name": "odw-harmonised/Timesheets"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "6ccced66-e952-4cb0-a075-4a5b449a7d7a"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Check for new, updated or deleted data\n",
					"- This script checks for new, updated or deleted data by checking the source data (timesheets) against the target (odw_harmonised_db.timesheets_work_segment)\n",
					"- **New Data:** where a segment_id in the source does not exist as an SegmentID in the target. NewData flag is set to 'Y'\n",
					"- **Updated data:** Comparison occurs on segment_id in source and SegmentID in target where the row hash is different i.e. there is a change in one of the columns. NewData flag is set to 'Y'\n",
					"- **Deleted data:** where an SegmentID in the target exists but the same information doesn't exist in the source. DeletedData flag is set to 'Y'\n",
					"\n",
					"## View timesheets_work_segment_new is created"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"-- Build religion_dim table\r\n",
					"-- Gets modified or deleted from source rows\r\n",
					"\r\n",
					"CREATE OR REPLACE TEMPORARY VIEW timesheets_work_segment_new\r\n",
					"\r\n",
					"    AS\r\n",
					"\r\n",
					"-- gets data that matches of SourceID and flags that it is modified based on a row (md5) hash. Flags as \"NewData\"\r\n",
					"-- gets data that is in the target but not in source. Flags as \"DeletedData\"\r\n",
					"\r\n",
					"SELECT\r\n",
					"    CASE\r\n",
					"        WHEN T1.segment_id IS NULL\r\n",
					"        THEN T5.SegmentID\r\n",
					"    END as TimesheetsWorkSegmentID, -- surrogate key\r\n",
					"    T1.segment_id AS SegmentID,\r\n",
					"    T4.EmployeeID AS EmployeeID,\r\n",
					"    T1.segment_date AS SegmentDate,\r\n",
					"    T1.start_time AS StartTime,\r\n",
					"    T1.end_time AS EndTime,\r\n",
					"    T1.case_reference AS CaseReference,\r\n",
					"    T1.segment_type AS SegmentType,\r\n",
					"    T1.worked_minutes AS WorkedMinutes,\r\n",
					"    T1.created_at AS CreatedAt,\r\n",
					"    T1.updated_at AS UpdatedAt,\r\n",
					"    T1.case_display_name AS CaseDisplayName,\r\n",
					"    T2.SourceSystemID,\r\n",
					"    to_timestamp(T1.expected_from) AS IngestionDate,\r\n",
					"    NULL AS ValidTo,\r\n",
					"    md5(concat( IFNULL(T1.segment_id,'.'), IFNULL(T4.EmployeeID,'.'), IFNULL(T1.segment_date,'.'), IFNULL(T1.start_time,'.'), IFNULL(T1.end_time,'.'), \r\n",
					"                IFNULL(T1.case_reference,'.'), IFNULL(T1.segment_type,'.'), IFNULL(T1.worked_minutes,'.'), IFNULL(T1.created_at,'.'), \r\n",
					"                IFNULL(T1.updated_at,'.'), IFNULL(T1.case_display_name,'.'))) as RowID,\r\n",
					"    'Y' as IsActive, \r\n",
					"    T5.IsActive as HistoricIsActive\r\n",
					"\r\n",
					"FROM odw_standardised_db.timesheets_work_segment T1\r\n",
					"LEFT JOIN odw_harmonised_db.main_sourcesystem_fact T2 ON \"Timesheets\" = T2.Description and T2.IsActive = 'Y'\r\n",
					"LEFT JOIN odw_standardised_db.timesheets_employee T3 ON T1.employee_id = T3.employee_id\r\n",
					"LEFT JOIN odw_harmonised_db.hr_employee_fact T4 ON T3.staff_number = T4.EmployeeNumber AND T4.IsActive = 'Y'\r\n",
					"FULL JOIN odw_harmonised_db.timesheets_work_segment_dim T5 ON T1.segment_id = T5.SegmentID AND T5.IsActive = 'Y'\r\n",
					"\r\n",
					"WHERE\r\n",
					"        -- flags new data        \r\n",
					"        ( CASE\r\n",
					"            WHEN T1.segment_id = T5.SegmentID AND md5(concat(IFNULL(T1.segment_id,'.'), IFNULL(T4.EmployeeID,'.'), IFNULL(T1.segment_date,'.'), IFNULL(T1.start_time,'.'), \r\n",
					"                    IFNULL(T1.end_time,'.'), IFNULL(T1.case_reference,'.'), IFNULL(T1.segment_type,'.'), IFNULL(T1.worked_minutes,'.'), IFNULL(T1.created_at,'.'), \r\n",
					"                    IFNULL(T1.updated_at,'.'), IFNULL(T1.case_display_name,'.')))  <> T5.RowID     \r\n",
					"            THEN 'Y'\r\n",
					"            WHEN T5.TimesheetsWorkSegmentID IS NULL\r\n",
					"            THEN 'Y'\r\n",
					"            ELSE 'N'\r\n",
					"        END  = 'Y' ) \r\n",
					"    AND T1.segment_type_id IS NOT NULL\r\n",
					"    AND T1.expected_from = (SELECT MAX(expected_from) FROM odw_standardised_db.timesheets_work_segment);\r\n",
					";"
				],
				"execution_count": 28
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Run tests to check integrity of data by numbers\n",
					"- This script checks for the total number of current codes in the harmonised table, and compare against the numbers for new data to be added and data to be set as inactive.\n",
					"- **Changes tracking:** where it checks the data against active records in harmonised and compares with records to add and records to be set as inactive.\n",
					"- **Changes tolerance levels:** if the total ammount to be added and deleted surpasses the tolerance limit, it will interrupt the proces of loading the data"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# from pyspark.sql import SparkSession\r\n",
					"# spark = SparkSession.builder.getOrCreate()\r\n",
					"\r\n",
					"# Source_Number = spark.sql(\"SELECT COUNT(*) AS Source_Number FROM religion_grouped\")\r\n",
					"# Current_Number = spark.sql(\"SELECT COUNT (DISTINCT RowID) AS Current_Number FROM odw_harmonised_db.religion_dim where IsActive = 'Y' \")\r\n",
					"# New_Data_Number = spark.sql(\"SELECT COUNT (DISTINCT RowID) AS New_Data_Number FROM religion_dim_new WHERE NewData = 'Y'\")\r\n",
					"# Deleted_Data_Number = spark.sql(\"SELECT COUNT (DISTINCT RowID) AS Deleted_Data_Number FROM religion_dim_new WHERE DeletedData = 'Y'\")\r\n",
					"\r\n",
					"# Source_Number_Pandas = Source_Number.toPandas()\r\n",
					"# Current_Number_Pandas =  Current_Number.toPandas()\r\n",
					"# New_Data_Number_Pandas = New_Data_Number.toPandas()\r\n",
					"# Deleted_Data_Number_Pandas = Deleted_Data_Number.toPandas()\r\n",
					"\r\n",
					"# # checking if new total number of registers matches the previously loaded, plus New ones, minus Deleted ones\r\n",
					"# print(\"SET 1:\")\r\n",
					"# Total_Number = Source_Number_Pandas['Source_Number'].tolist() \r\n",
					"# Current_Loaded_Number = Current_Number_Pandas['Current_Number'].tolist() \r\n",
					"# New_Data_Number = New_Data_Number_Pandas['New_Data_Number'].tolist() \r\n",
					"# Deleted_Data_Number = Deleted_Data_Number_Pandas['Deleted_Data_Number'].tolist() \r\n",
					"\r\n",
					"# print(Total_Number) \r\n",
					"# print(Current_Loaded_Number) \r\n",
					"# print(New_Data_Number) \r\n",
					"# print(Deleted_Data_Number)\r\n",
					"\r\n",
					"# if Total_Number[0] != (Current_Loaded_Number[0] + New_Data_Number[0] - Deleted_Data_Number[0]):\r\n",
					"#     raise RuntimeError(\"Loading Number do not match\")\r\n",
					"# else:\r\n",
					"#     print(\"Loading number matches with existing codes plus new, minus deleted!\")\r\n",
					"\r\n",
					"# if New_Data_Number[0] > 1000:\r\n",
					"#     raise RuntimeError(\"ALERT: Too many new codes\")\r\n",
					"# else:\r\n",
					"#     print(\"New data under tolerance levels\")\r\n",
					"\r\n",
					"# if Deleted_Data_Number[0] > 500:\r\n",
					"#     raise RuntimeError(\"ALERT: Too many deleted codes\")\r\n",
					"# else:\r\n",
					"#     print(\"Unused codes under tolerance levels\")"
				],
				"execution_count": 29
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Run tests to check integrity of data by comparision of codes\n",
					"- This script checks for the total list of current codes in the harmonised table, and compare against the list for new data to be added and data to be set as inactive.\n",
					"- **Changes tracking:** where it checks the data against active records in harmonised and compares with records to add and records to be set as inactive."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# from pyspark.sql import SparkSession\r\n",
					"# spark = SparkSession.builder.getOrCreate()\r\n",
					"\r\n",
					"# Current_Records = spark.sql(\"SELECT DISTINCT RowID AS Current_Records FROM odw_harmonised_db.religion_dim where IsActive = 'Y' \")\r\n",
					"# New_Data_Records = spark.sql(\"SELECT DISTINCT RowID AS New_Data_Records FROM religion_dim_new WHERE NewData = 'Y'\")\r\n",
					"# Deleted_Data_Records = spark.sql(\"SELECT DISTINCT RowID AS Deleted_Data_Records FROM religion_dim_new WHERE DeletedData = 'Y'\")\r\n",
					"\r\n",
					"# Current_Records_Pandas =  Current_Records.toPandas()\r\n",
					"# New_Data_Records_Pandas = New_Data_Records.toPandas()\r\n",
					"# Deleted_Data_Records_Pandas = Deleted_Data_Records.toPandas()\r\n",
					"\r\n",
					"# # checking if a deleted records are correcly flagged, not existing in the new data, but existing inpreviously loaded\r\n",
					"# print(\"TEST 2:\")\r\n",
					"\r\n",
					"# Current_Records = Current_Records_Pandas['Current_Records'].tolist() \r\n",
					"# Deleted_Records = Deleted_Data_Records_Pandas['Deleted_Data_Records'].tolist()\r\n",
					"# New_Records = New_Data_Records_Pandas['New_Data_Records'].tolist()\r\n",
					"\r\n",
					"# print(Current_Records)\r\n",
					"# print(Deleted_Records)\r\n",
					"# print(New_Records)\r\n",
					"\r\n",
					"# for i in Deleted_Records:\r\n",
					"#     if i in Current_Records: \r\n",
					"#         print(i + \" to be deleted is in the current records\")\r\n",
					"#     else:\r\n",
					"#         raise RuntimeError(\"Records to Delete do not match\")\r\n",
					"\r\n",
					"# for j in New_Records:\r\n",
					"#     if j not in Current_Records: \r\n",
					"#         print(j + \" to be added is not in the current records\")\r\n",
					"#     else:\r\n",
					"#         raise RuntimeError(\"Records to Add do not match\")"
				],
				"execution_count": 30
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Dataset is created that contains changed data and corresponding target data\n",
					"- This script combines data that has been updated, Deleted or is new, with corresponding target data\n",
					"- View **table_new** is unioned to the target data filter to only those rows where changes have been detected\n",
					"## View table_changed_rows is created"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"-- Create new and updated dataset\r\n",
					"\r\n",
					"CREATE OR REPLACE TEMPORARY VIEW timesheets_work_segment_changed_rows\r\n",
					"\r\n",
					"    AS\r\n",
					"\r\n",
					"-- gets updated, deleted and new rows \r\n",
					"\r\n",
					"Select \r\n",
					"    TimesheetsWorkSegmentID,\r\n",
					"    SegmentID,\r\n",
					"    EmployeeID,\r\n",
					"    SegmentDate,\r\n",
					"    StartTime,\r\n",
					"    EndTime,\r\n",
					"    CaseReference,\r\n",
					"    SegmentType,\r\n",
					"    WorkedMinutes,\r\n",
					"    CreatedAt,\r\n",
					"    UpdatedAt,\r\n",
					"    CaseDisplayName,\r\n",
					"    SourceSystemID,\r\n",
					"    IngestionDate,\r\n",
					"    ValidTo,\r\n",
					"    RowID,\r\n",
					"    IsActive\r\n",
					"From timesheets_work_segment_new WHERE HistoricIsActive = 'Y' or HistoricIsActive IS NULL\r\n",
					"\r\n",
					"    UNION ALL\r\n",
					"\r\n",
					"-- gets original versions of updated rows so we can update EndDate and set IsActive flag to 'N'\r\n",
					"\r\n",
					"SELECT\r\n",
					"    TimesheetsWorkSegmentID,\r\n",
					"    SegmentID,\r\n",
					"    EmployeeID,\r\n",
					"    SegmentDate,\r\n",
					"    StartTime,\r\n",
					"    EndTime,\r\n",
					"    CaseReference,\r\n",
					"    SegmentType,\r\n",
					"    WorkedMinutes,\r\n",
					"    CreatedAt,\r\n",
					"    UpdatedAt,\r\n",
					"    CaseDisplayName,\r\n",
					"    SourceSystemID,\r\n",
					"    IngestionDate,\r\n",
					"    ValidTo,\r\n",
					"    RowID,\r\n",
					"    IsActive\r\n",
					"FROM odw_harmonised_db.timesheets_work_segment_dim\r\n",
					"WHERE SegmentID IN (SELECT SegmentID FROM timesheets_work_segment_new WHERE EmployeeID IS NULL)\r\n",
					"AND IsActive = 'Y'; \r\n",
					""
				],
				"execution_count": 31
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Cell to calculate ValidTo date for changed registers"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					}
				},
				"source": [
					"%%sql\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW Loading_month\n",
					"\n",
					"    AS\n",
					"\n",
					"SELECT DISTINCT\n",
					"    IngestionDate AS IngestionDate,\n",
					"    to_timestamp(date_sub(IngestionDate,1)) AS ClosingDate,\n",
					"    'Y' AS IsActive\n",
					"\n",
					"FROM timesheets_minutes_new;\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW timesheets_minutes_dim_changed_rows_final\n",
					"\n",
					"    AS\n",
					"\n",
					"SELECT \n",
					"    TimesheetsMinutesID,\n",
					"    StaffNumber,\n",
					"    EmployeeID,\n",
					"    BusinessArea,\n",
					"    CoreMinutes,\n",
					"    BalanceMinutes,\n",
					"    T1.SourceSystemID,\n",
					"    T1.IngestionDate,\n",
					"    T1.ValidTo,\n",
					"    T1.RowID,\n",
					"    T1.IsActive,\n",
					"    T2.ClosingDate\n",
					"FROM timesheets_minutes_dim_changed_rows T1\n",
					"FULL JOIN Loading_month T2 ON T1.IsActive = T2.IsActive\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# View religion_dim_changed_rows is used in a merge (Upsert) statement into the target table\n",
					"- **WHEN MATCHED** ON the Business Key, ValidTo is set to today -1 day and the IsActive flag is set to 'N'\n",
					"- **WHEN NOT MATCHED** ON the business key, insert rows\n",
					"## Table odw_harmonised.XXX_dim is updated"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"\r\n",
					"-- merge into table\r\n",
					"\r\n",
					"MERGE INTO odw_harmonised_db.timesheets_work_segment_dim AS Target\r\n",
					"USING timesheets_work_segment_changed_rows AS Source\r\n",
					"\r\n",
					"ON Source.TimesheetsWorkSegmentID = Target.TimesheetsWorkSegmentID\r\n",
					"\r\n",
					"-- updates existing rows\r\n",
					"\r\n",
					"WHEN MATCHED\r\n",
					"    THEN \r\n",
					"    UPDATE SET\r\n",
					"    Target.ValidTo = to_timestamp(date_sub(Source.IngestionDate,1)),\r\n",
					"    Target.IsActive = 'N'\r\n",
					"        \r\n",
					"-- Insert completely new rows\r\n",
					"WHEN NOT MATCHED \r\n",
					"    THEN INSERT * ; \r\n",
					""
				],
				"execution_count": 32
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Fix the IDs\n",
					"- No auto-increment feature is available in delta tables, therefore we need to create new IDs for the inserted rows\n",
					"- This is done by select the target data and using INSERT OVERWRITE to re-insert the data is a new Row Number\n",
					"## Table odw_harmonised.religion_dim is updated"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"-- Insert new ID\r\n",
					"\r\n",
					"INSERT OVERWRITE odw_harmonised_db.timesheets_work_segment_dim\r\n",
					"\r\n",
					"SELECT \r\n",
					"    ROW_NUMBER() OVER (ORDER BY TimesheetsWorkSegmentID NULLS LAST) AS TimesheetsWorkSegmentID,\r\n",
					"    SegmentID,\r\n",
					"    EmployeeID,\r\n",
					"    SegmentDate,\r\n",
					"    StartTime,\r\n",
					"    EndTime,\r\n",
					"    CaseReference,\r\n",
					"    SegmentType,\r\n",
					"    WorkedMinutes,\r\n",
					"    CreatedAt,\r\n",
					"    UpdatedAt,\r\n",
					"    CaseDisplayName, \r\n",
					"    SourceSystemID,\r\n",
					"    IngestionDate,\r\n",
					"    ValidTo,\r\n",
					"    RowID,\r\n",
					"    IsActive\r\n",
					"FROM odw_harmonised_db.timesheets_work_segment_dim;\r\n",
					""
				],
				"execution_count": 33
			}
		]
	}
}