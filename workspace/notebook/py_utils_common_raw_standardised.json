{
	"name": "py_utils_common_raw_standardised",
	"properties": {
		"folder": {
			"name": "utils/main"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "01714606-6d53-40af-82e3-72ebc8db8c65"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this pyspark notebook is to reads all recent files from the given odw-raw folder path and load the data into standardised_db lakehouse database's Delta tables\r\n",
					"\r\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \r\n",
					"Rohit Shukla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;19-Jan-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The functionality of this notebook is generic to cater to `.xlsx` and `.csv` files for creating Delta Tables.\r\n",
					"\r\n",
					"**Spark Cluster Configuration** -> Apache Spark Version- 3.4, Python Version \t\t- 3.10, Delta Lake Version \t- 2.4\r\n",
					"\r\n",
					"\r\n",
					"##### The input parameters are:\r\n",
					"###### Param_FileFolder_Path => This is a mandatory parameter which refers to a folder path of the entities like 'Timesheets', 'SapHrData'\r\n",
					"###### Param_File_Load_Type  => This is an optional parameter refers to a subfolders if there is any like Monthly,Daily, Quarterly etc.\r\n",
					"###### Param_Json_SchemaFolder_Name => This is a mandatory parameter which refers to a schema file in json format required to create delta tables.\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"#Define Input Parameters to get values from the pipleline\r\n",
					"Param_File_Load_Type = ''\r\n",
					"Param_FileFolder_Path = ''\r\n",
					"Param_Json_SchemaFolder_Name = ''"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Import all required Python Libraries"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"tags": []
				},
				"source": [
					"#import all libraries and initialise Spark Session\r\n",
					"import json\r\n",
					"import traceback\r\n",
					"import calendar\r\n",
					"import time\r\n",
					"from datetime import datetime, timedelta, date\r\n",
					"import requests\r\n",
					"import pyspark.sql.functions as F \r\n",
					"import os\r\n",
					"import re\r\n",
					"from itertools import chain\r\n",
					"from collections.abc import Mapping\r\n",
					"from operator import add\r\n",
					"from notebookutils import mssparkutils\r\n",
					"from pyspark.sql.functions import regexp_replace,lit, current_timestamp, to_date ,expr, md5, col, date_format, when, length, lpad,input_file_name,sha2,concat\r\n",
					"from pyspark.sql.types import *\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"spark = SparkSession.builder.getOrCreate()\r\n",
					"from delta.tables import DeltaTable\r\n",
					"#ignore FutureWarning messages \r\n",
					"import warnings\r\n",
					"warnings.filterwarnings(\"ignore\", message=\"iteritems is deprecated\")\r\n",
					""
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Get Storage account"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Get Storage account name\r\n",
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\r\n",
					"#print(storage_account)"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Enable message logging"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define all required folder paths"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define all Folder paths used in the notebook\r\n",
					"\r\n",
					"Param_Json_SchemaFolder_Name = Param_FileFolder_Path.lower()\r\n",
					"\r\n",
					"odw_raw_base_folder_path = f\"abfss://odw-raw@{storage_account}{Param_FileFolder_Path}/\"\r\n",
					"delta_table_base_path = f\"abfss://odw-standardised@{storage_account}{Param_Json_SchemaFolder_Name}\"\r\n",
					"json_schema_file_path = f\"abfss://odw-config@{storage_account}/orchestration/orchestration_saphr.json\"\r\n",
					"\r\n",
					"database_name = \"odw_standardised_db\"\r\n",
					"process_name = 'py_raw_to_std'\r\n",
					"\r\n",
					"logging_container = f\"abfss://logging@{storage_account}\"\r\n",
					"logging_table_name = 'tables_logs'\r\n",
					"ingestion_log_table_location = logging_container + logging_table_name\r\n",
					"\r\n",
					"# json result result dump list\r\n",
					"processing_results = []\r\n",
					"\r\n",
					""
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"# --- Load Orchestration Config ---\r\n",
					"df_orch = spark.read.option(\"multiline\", \"true\").json(json_schema_file_path)\r\n",
					"definitions = json.loads(df_orch.toJSON().first())[\"definitions\"]"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Find the recent Date subfolder path and construct the odw-raw path dynamically"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Get latest folder\r\n",
					"def get_latest_folder(path):\r\n",
					"    folders = [f.name for f in mssparkutils.fs.ls(path) if f.isDir]\r\n",
					"    folders = sorted([f for f in folders if re.match(r\"\\d{4}-\\d{2}-\\d{2}\", f)], reverse=True)\r\n",
					"    return folders[0] if folders else None"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define all files and dataframe processing related functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Function to check if filename has date pattern\r\n",
					"@logging_to_appins\r\n",
					"def has_date_pattern_in_filename(filename):\r\n",
					"    \"\"\"Check if filename contains a date pattern\"\"\"\r\n",
					"    date_patterns = [\r\n",
					"        r'_\\d{8}',           # _YYYYMMDD\r\n",
					"        r'_\\d{4}-\\d{2}-\\d{2}', # _YYYY-MM-DD\r\n",
					"        r'_\\d{4}_\\d{2}_\\d{2}', # _YYYY_MM_DD\r\n",
					"        r'\\d{8}',            # YYYYMMDD --anywhere in filename\r\n",
					"        r'\\d{4}-\\d{2}-\\d{2}', # YYYY-MM-DD --anywhere in filename\r\n",
					"        r'\\d{4}_\\d{2}_\\d{2}'  # YYYY_MM_DD --anywhere in filename\r\n",
					"    ]\r\n",
					"    \r\n",
					"    for pattern in date_patterns:\r\n",
					"        if re.search(pattern, filename):\r\n",
					"            return True\r\n",
					"    return False\r\n",
					"\r\n",
					"# Function to find files with same base name but different dates\r\n",
					"@logging_to_appins\r\n",
					"def find_multi_date_files(files, filename_start):\r\n",
					"    \"\"\"Find files that have the same base name but different dates\"\"\"\r\n",
					"    matching_files = []\r\n",
					"    base_files = []\r\n",
					"    \r\n",
					"    for file in files:\r\n",
					"        if file.startswith(filename_start):\r\n",
					"            if has_date_pattern_in_filename(file):\r\n",
					"                matching_files.append(file)\r\n",
					"            else:\r\n",
					"                base_files.append(file)\r\n",
					"    \r\n",
					"    # If we found files with date patterns, use those for multi-file processing\r\n",
					"    if len(matching_files) > 1:\r\n",
					"        return matching_files, 'multi_file'\r\n",
					"    elif len(matching_files) == 1:\r\n",
					"        return matching_files, 'single_file'\r\n",
					"    elif len(base_files) == 1:\r\n",
					"        # Fallback to single file without date pattern\r\n",
					"        return base_files, 'single_file'\r\n",
					"    elif len(base_files) > 1:\r\n",
					"        # Multiple files without clear date pattern - take the first one\r\n",
					"        return [base_files[0]], 'single_file'\r\n",
					"    else:\r\n",
					"        return [], 'none'\r\n",
					"\r\n",
					"# Function to extract date from filename with various date formats\r\n",
					"@logging_to_appins\r\n",
					"def extract_date_from_filename(filename):\r\n",
					"    \"\"\"Extract date from filename supporting multiple date formats\"\"\"\r\n",
					"    try:\r\n",
					"        # Pattern for YYYYMMDD format (8 digits)\r\n",
					"        date_pattern_8 = r'(\\d{8})'\r\n",
					"        # Pattern for YYYY-MM-DD format\r\n",
					"        date_pattern_dash = r'(\\d{4}-\\d{2}-\\d{2})'\r\n",
					"        # Pattern for YYYY_MM_DD format\r\n",
					"        date_pattern_underscore = r'(\\d{4}_\\d{2}_\\d{2})'\r\n",
					"        \r\n",
					"        match_8 = re.search(date_pattern_8, filename)\r\n",
					"        match_dash = re.search(date_pattern_dash, filename)\r\n",
					"        match_underscore = re.search(date_pattern_underscore, filename)\r\n",
					"        \r\n",
					"        if match_8:\r\n",
					"            date_str = match_8.group(1)\r\n",
					"            return datetime.strptime(date_str, '%Y%m%d')\r\n",
					"        elif match_dash:\r\n",
					"            date_str = match_dash.group(1)\r\n",
					"            return datetime.strptime(date_str, '%Y-%m-%d')\r\n",
					"        elif match_underscore:\r\n",
					"            date_str = match_underscore.group(1)\r\n",
					"            return datetime.strptime(date_str, '%Y_%m_%d')\r\n",
					"        else:\r\n",
					"            logInfo(f\"No date found in filename: {filename}\")\r\n",
					"            return datetime.min\r\n",
					"    except Exception as e:\r\n",
					"        logError(f\"Error extracting date from filename {filename}: {e}\")\r\n",
					"        return datetime.min\r\n",
					"\r\n",
					"# Function to check schema compatibility and determine write mode\r\n",
					"@logging_to_appins\r\n",
					"def determine_write_strategy(spark_df, delta_table_path, schema_path, table_name):\r\n",
					"    \"\"\"Determine the appropriate write strategy based on schema compatibility\"\"\"\r\n",
					"    try:\r\n",
					"        # Load expected schema from JSON\r\n",
					"        schema_json = json.loads(\"\".join([r.value for r in spark.read.text(schema_path).collect()]))\r\n",
					"        expected_fields = {f['name'].lower(): f['type'] for f in schema_json[\"fields\"]}\r\n",
					"        \r\n",
					"        # Get actual DataFrame schema (excluding metadata columns we always add)\r\n",
					"        excluded_columns = {'ingested_datetime', 'expected_from', 'expected_to'}\r\n",
					"        actual_fields = {field.name.lower(): str(field.dataType) for field in spark_df.schema.fields if field.name.lower() not in excluded_columns}\r\n",
					"        \r\n",
					"        # Check if Delta table exists\r\n",
					"        table_exists = DeltaTable.isDeltaTable(spark, delta_table_path)\r\n",
					"        \r\n",
					"        if not table_exists:\r\n",
					"            logInfo(f\"Delta table does not exist for {table_name}. Will create new table.\")\r\n",
					"            return \"create_new\", None\r\n",
					"        \r\n",
					"        # If table exists, get its current schema (excluding old tracking columns)\r\n",
					"        try:\r\n",
					"            current_table_df = spark.read.format(\"delta\").load(delta_table_path)\r\n",
					"            # Exclude old tracking columns and metadata columns from comparison\r\n",
					"            excluded_columns = {'source_file', 'file_processed_datetime', 'ingested_datetime', 'expected_from', 'expected_to'}\r\n",
					"            current_fields = {field.name.lower(): str(field.dataType) for field in current_table_df.schema.fields if field.name.lower() not in excluded_columns}\r\n",
					"        except Exception as e:\r\n",
					"            logError(f\"Error reading existing table schema: {e}\")\r\n",
					"            return \"create_new\", None\r\n",
					"        \r\n",
					"        # Compare only business data columns (not metadata or tracking columns)\r\n",
					"        missing_in_current = set(actual_fields.keys()) - set(current_fields.keys())\r\n",
					"        missing_in_actual = set(current_fields.keys()) - set(actual_fields.keys())\r\n",
					"        \r\n",
					"        if missing_in_current and not missing_in_actual:\r\n",
					"            logInfo(f\"New business columns detected in source data: {missing_in_current}\")\r\n",
					"            logInfo(f\"Will use MERGE schema option for {table_name}\")\r\n",
					"            return \"merge_schema\", missing_in_current\r\n",
					"        elif not missing_in_current and not missing_in_actual:\r\n",
					"            logInfo(f\"Business schema matches exactly for {table_name}. Will use standard overwrite/append.\")\r\n",
					"            return \"overwrite\", None\r\n",
					"        elif missing_in_actual and not missing_in_current:\r\n",
					"            logInfo(f\"Some columns missing in source data for {table_name}: {missing_in_actual}\")\r\n",
					"            logInfo(f\"Will use standard overwrite/append - missing columns will be null.\")\r\n",
					"            return \"overwrite\", None\r\n",
					"        else:\r\n",
					"            logInfo(f\"Schema differences detected for {table_name}\")\r\n",
					"            logInfo(f\"Missing in current table: {missing_in_current}\")\r\n",
					"            logInfo(f\"Missing in source data: {missing_in_actual}\")\r\n",
					"            logInfo(f\"Will use standard overwrite/append\")\r\n",
					"            return \"overwrite\", None\r\n",
					"            \r\n",
					"    except Exception as e:\r\n",
					"        logError(f\"Error determining write strategy for {table_name}: {e}\")\r\n",
					"        return \"overwrite\", None\r\n",
					"\r\n",
					"# Function to check if JSON schema has evolved vs existing Delta table\r\n",
					"@logging_to_appins\r\n",
					"def check_json_schema_evolution(delta_table_path, schema_path, table_name):\r\n",
					"    \"\"\"Check if JSON schema has evolved compared to existing Delta table schema\"\"\"\r\n",
					"    try:\r\n",
					"        # Check if Delta table exists\r\n",
					"        table_exists = DeltaTable.isDeltaTable(spark, delta_table_path)\r\n",
					"        \r\n",
					"        if not table_exists:\r\n",
					"            logInfo(f\"Delta table does not exist for {table_name}. Will create new table.\")\r\n",
					"            return False, \"create_new\"\r\n",
					"        \r\n",
					"        # Get JSON schema\r\n",
					"        schema_json = json.loads(\"\".join([r.value for r in spark.read.text(schema_path).collect()]))\r\n",
					"        expected_fields = set([f['name'] for f in schema_json[\"fields\"]])\r\n",
					"        \r\n",
					"        # Get existing table schema\r\n",
					"        try:\r\n",
					"            existing_table_df = spark.read.format(\"delta\").load(delta_table_path)\r\n",
					"            # Remove metadata columns from comparison\r\n",
					"            excluded_columns = {\"ingested_datetime\", \"expected_from\", \"expected_to\", \"source_file\", \"file_processed_datetime\"}\r\n",
					"            existing_business_fields = set(existing_table_df.columns) - excluded_columns\r\n",
					"            expected_business_fields = expected_fields - excluded_columns\r\n",
					"            \r\n",
					"            # Check for schema differences between JSON and existing table\r\n",
					"            if existing_business_fields != expected_business_fields:\r\n",
					"                missing_in_table = expected_business_fields - existing_business_fields\r\n",
					"                missing_in_json = existing_business_fields - expected_business_fields\r\n",
					"                \r\n",
					"                logInfo(f\"JSON schema evolution detected for {table_name}:\")\r\n",
					"                logInfo(f\"  Columns in JSON schema but not in existing table: {missing_in_table}\")\r\n",
					"                logInfo(f\"  Columns in existing table but not in JSON schema: {missing_in_json}\")\r\n",
					"                \r\n",
					"                if missing_in_json:\r\n",
					"                    # Columns have been removed from JSON schema - need schema reset\r\n",
					"                    logInfo(f\"JSON schema has removed columns. Will use schema reset for {table_name}\")\r\n",
					"                    return True, \"schema_reset\"\r\n",
					"                elif missing_in_table:\r\n",
					"                    # New columns added to JSON schema - can use merge schema\r\n",
					"                    logInfo(f\"JSON schema has new columns. Will use merge schema for {table_name}\")\r\n",
					"                    return False, \"merge_schema\"\r\n",
					"            \r\n",
					"            logInfo(f\"JSON schema matches existing table exactly for {table_name}. Will use standard overwrite.\")\r\n",
					"            return False, \"standard\"\r\n",
					"            \r\n",
					"        except Exception as e:\r\n",
					"            logError(f\"Error reading existing table schema for {table_name}: {e}\")\r\n",
					"            logInfo(f\"Will reset schema due to read error\")\r\n",
					"            return True, \"schema_reset\"\r\n",
					"            \r\n",
					"    except Exception as e:\r\n",
					"        logError(f\"Error checking JSON schema evolution for {table_name}: {e}\")\r\n",
					"        return False, \"standard\"\r\n",
					"\r\n",
					"# Function to reorder DataFrame columns to match JSON schema order\r\n",
					"def reorder_dataframe_to_match_schema(spark_df, schema_path, table_name):\r\n",
					"    \"\"\"Reorder DataFrame columns to exactly match JSON schema order\"\"\"\r\n",
					"    try:\r\n",
					"        # Get JSON schema column order\r\n",
					"        schema_json = json.loads(\"\".join([r.value for r in spark.read.text(schema_path).collect()]))\r\n",
					"        expected_columns = [f['name'] for f in schema_json[\"fields\"]]\r\n",
					"        \r\n",
					"        # Get current DataFrame columns\r\n",
					"        current_columns = spark_df.columns\r\n",
					"        \r\n",
					"        # Check if reordering is needed\r\n",
					"        if current_columns == expected_columns:            \r\n",
					"            return spark_df\r\n",
					"               \r\n",
					"        # Reorder columns to match JSON schema\r\n",
					"        reordered_df = spark_df.select(*expected_columns)\r\n",
					"        return reordered_df\r\n",
					"        \r\n",
					"    except Exception as e:\r\n",
					"        logError(f\"Error reordering DataFrame for {table_name}: {e}\")\r\n",
					"        return spark_df\r\n",
					"\r\n",
					"# Function to write DataFrame based on JSON schema evolution\r\n",
					"@logging_to_appins\r\n",
					"def write_dataframe_with_json_schema_compliance(spark_df, delta_table_path, full_table_name, schema_path, is_first_file=True):\r\n",
					"    \"\"\"Write DataFrame with JSON schema compliance - JSON schema is the ultimate truth\"\"\"\r\n",
					"    try:\r\n",
					"        # Ensure DataFrame column order matches JSON schema BEFORE any processing\r\n",
					"        spark_df = reorder_dataframe_to_match_schema(spark_df, schema_path, full_table_name)\r\n",
					"        \r\n",
					"        needs_reset, strategy = check_json_schema_evolution(delta_table_path, schema_path, full_table_name)\r\n",
					"        \r\n",
					"        # Always check if column order matches for any schema evolution\r\n",
					"        table_exists = DeltaTable.isDeltaTable(spark, delta_table_path)\r\n",
					"        force_schema_reset = False\r\n",
					"        \r\n",
					"        if table_exists and strategy != \"create_new\":\r\n",
					"            try:\r\n",
					"                existing_df = spark.read.format(\"delta\").load(delta_table_path)\r\n",
					"                existing_columns = [field.name for field in existing_df.schema.fields]\r\n",
					"                new_columns = [field.name for field in spark_df.schema.fields]\r\n",
					"                \r\n",
					"                # Check if column order is different\r\n",
					"                if existing_columns != new_columns:\r\n",
					"                    force_schema_reset = True\r\n",
					"            except Exception as e:\r\n",
					"                logInfo(f\"Error checking column order: {e}\")\r\n",
					"        \r\n",
					"        # Always use overwriteSchema for any schema changes\r\n",
					"        if needs_reset or strategy == \"schema_reset\" or force_schema_reset or strategy == \"merge_schema\":\r\n",
					"            # Any schema change requires complete table reset\r\n",
					"            create_table_if_not_exists(delta_table_path, full_table_name, schema_path)\r\n",
					"            spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(delta_table_path)\r\n",
					"            if strategy == \"merge_schema\":\r\n",
					"                logInfo(f\"Overwrote with schema evolution maintaining exact JSON schema order: {full_table_name}\")\r\n",
					"            else:\r\n",
					"                logInfo(f\"Reset Delta table schema to match exact JSON schema order: {full_table_name}\")\r\n",
					"            \r\n",
					"        elif strategy == \"create_new\":\r\n",
					"            # New table - create based on JSON schema\r\n",
					"            create_table_if_not_exists(delta_table_path, full_table_name, schema_path)\r\n",
					"            spark_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\r\n",
					"            logInfo(f\"Created new Delta table based on JSON schema: {full_table_name}\")\r\n",
					"            \r\n",
					"        else:\r\n",
					"            # Standard write (strategy == \"standard\") - but check column order\r\n",
					"            if force_schema_reset:\r\n",
					"                spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(delta_table_path)\r\n",
					"                logInfo(f\"Overwrote Delta table to fix column order to match JSON schema: {full_table_name}\")\r\n",
					"            elif is_first_file:\r\n",
					"                spark_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\r\n",
					"                logInfo(f\"Overwrote Delta table (JSON schema unchanged): {full_table_name}\")\r\n",
					"            else:\r\n",
					"                spark_df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\r\n",
					"                logInfo(f\"Appended to Delta table (JSON schema unchanged): {full_table_name}\")\r\n",
					"        \r\n",
					"        # Verify the final table schema after write\r\n",
					"        try:\r\n",
					"            final_df = spark.read.format(\"delta\").load(delta_table_path)\r\n",
					"            logInfo(f\"Final table schema after write for {full_table_name}:\")\r\n",
					"                        \r\n",
					"            # Check if the column order now matches JSON schema\r\n",
					"            final_columns = [field.name for field in final_df.schema.fields]\r\n",
					"            df_columns = [field.name for field in spark_df.schema.fields]\r\n",
					"            \r\n",
					"            # Also get JSON schema order for comparison\r\n",
					"            try:\r\n",
					"                schema_json = json.loads(\"\".join([r.value for r in spark.read.text(schema_path).collect()]))\r\n",
					"                json_schema_columns = [f['name'] for f in schema_json[\"fields\"]]\r\n",
					"                \r\n",
					"                if final_columns == json_schema_columns:\r\n",
					"                    logInfo(f\"Column order SUCCESS - table order matches JSON schema order exactly\")\r\n",
					"                else:\r\n",
					"                    logInfo(f\"Column order MISMATCH with JSON schema\")\r\n",
					"                    logInfo(f\"JSON schema order: {json_schema_columns}\")\r\n",
					"                    logInfo(f\"Table order: {final_columns}\")\r\n",
					"            except Exception as e:\r\n",
					"                logInfo(f\"Could not verify JSON schema order: {e}\")\r\n",
					"            \r\n",
					"            # Check if all columns are present\r\n",
					"            final_column_set = set(final_columns)\r\n",
					"            df_column_set = set(df_columns)\r\n",
					"            \r\n",
					"            if df_column_set.issubset(final_column_set):\r\n",
					"                logInfo(f\"Schema evolution SUCCESS - all DataFrame columns present in final table\")\r\n",
					"            else:\r\n",
					"                missing = df_column_set - final_column_set\r\n",
					"                logInfo(f\"Schema evolution FAILED - missing columns in final table: {missing}\")\r\n",
					"                \r\n",
					"        except Exception as e:\r\n",
					"            logError(f\"Could not verify final table schema: {e}\")\r\n",
					"        \r\n",
					"        return True\r\n",
					"        \r\n",
					"    except Exception as e:\r\n",
					"        logError(f\"Error writing DataFrame to {full_table_name}: {e}\")\r\n",
					"        return False\r\n",
					"\r\n",
					"@logging_to_appins\r\n",
					"def read_file(file_path):\r\n",
					"    try:\r\n",
					"        if file_path.endswith(\".csv\"):\r\n",
					"            return spark.read.option(\"header\", True).csv(file_path)\r\n",
					"\t\t\t\r\n",
					"        elif file_path.endswith(\".xlsx\"):\r\n",
					"            return spark.read.format(\"com.crealytics.spark.excel\") \\\r\n",
					"                             .option(\"header\", \"true\") \\\r\n",
					"                             .option(\"inferSchema\", \"true\") \\\r\n",
					"                             .load(file_path)        \r\n",
					"        else:\r\n",
					"            raise Exception(\"Unsupported file format\")\r\n",
					"    except Exception as e:\r\n",
					"        logError(f\"Failed to load file {file_path}: {e}\")\r\n",
					"        return None\r\n",
					"\r\n",
					"def clean_column_names(df):\r\n",
					"    cols = [re.sub(r\"[^a-zA-Z0-9_]+\", \"\", c).strip('_') for c in df.columns]\r\n",
					"    deduped = []\r\n",
					"    for i, c in enumerate(cols):\r\n",
					"        count = cols[:i].count(c)\r\n",
					"        deduped.append(f\"{c}{count + 1}\" if count else c)\r\n",
					"    return df.toDF(*deduped)\r\n",
					"\r\n",
					"# Reorder dataframe columns to bring additional metadata columns to the front\r\n",
					"def reorder_columns(df):\r\n",
					"    \"\"\"Reorders the columns so that metadata columns come first.\"\"\"\r\n",
					"    try:\r\n",
					"        metadata_cols = [\"ingested_datetime\", \"expected_from\", \"expected_to\"]\r\n",
					"        remaining_cols = [col for col in df.columns if col not in metadata_cols]\r\n",
					"        return df.select(metadata_cols + remaining_cols)\r\n",
					"    except Exception as e:\r\n",
					"        logError(f\"Error reordering columns: {e}\")\r\n",
					"        return df\r\n",
					"\r\n",
					"@logging_to_appins\r\n",
					"def create_table_if_not_exists(path, table_name, schema_path):\r\n",
					"    try:\r\n",
					"        if not DeltaTable.isDeltaTable(spark, path):\r\n",
					"            schema_json = json.loads(\"\".join([r.value for r in spark.read.text(schema_path).collect()]))\r\n",
					"            schema_str = \", \".join([f\"{f['name']} {f['type']}\" for f in schema_json[\"fields\"]])\r\n",
					"            spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} ({schema_str}) USING DELTA LOCATION '{path}'\")\r\n",
					"    except Exception as e:\r\n",
					"        logError(f\"Error creating table {table_name}: {e}\")\r\n",
					"\r\n",
					"@logging_to_appins\r\n",
					"def time_diff_seconds(start, end):\r\n",
					"    try:\r\n",
					"        if not start or not end:\r\n",
					"            return 0\r\n",
					"\r\n",
					"        # Parse strings into datetime objects if needed\r\n",
					"        if isinstance(start, str):\r\n",
					"            start = datetime.strptime(start, \"%Y-%m-%d %H:%M:%S.%f\")\r\n",
					"        if isinstance(end, str):\r\n",
					"            end = datetime.strptime(end, \"%Y-%m-%d %H:%M:%S.%f\")\r\n",
					"\r\n",
					"        diff_seconds = int((end - start).total_seconds())\r\n",
					"        return diff_seconds if diff_seconds > 0 else 0\r\n",
					"\r\n",
					"    except Exception as e:\r\n",
					"        return 0\r\n",
					"\r\n",
					"#This funtion handles datetime object and covert into string\r\n",
					"def datetime_handler(obj):\r\n",
					"    if isinstance(obj, datetime):\r\n",
					"        return obj.isoformat()\r\n",
					"    raise TypeError(f\"Object of type {obj.__class__.__name__} is not JSON serializable\")\r\n",
					""
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define Main Delta Table Ingestion Logic"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"@logging_to_appins\r\n",
					"def process_definitions(Param_File_Load_Type):\r\n",
					"    matched_definitions = []\r\n",
					"    unmatched_definitions = []\r\n",
					"    all_latest_files = []\r\n",
					"\r\n",
					"    # Filter only matched definitions\r\n",
					"    for definition in definitions:\r\n",
					"    \r\n",
					"        freq_folder = definition.get('Source_Frequency_Folder', '').lower()\r\n",
					"        source_folder = definition.get('Source_Folder', '').lower()\r\n",
					"        param_freq = (Param_File_Load_Type or '').lower()\r\n",
					"        param_path = (Param_FileFolder_Path or '').lower()\r\n",
					"\r\n",
					"        if Param_File_Load_Type and not (\r\n",
					"            freq_folder == param_freq and source_folder == param_path\r\n",
					"        ):\r\n",
					"            continue\r\n",
					"\r\n",
					"        source_path = f\"abfss://odw-raw@{storage_account}{Param_FileFolder_Path}/\"\r\n",
					"        \r\n",
					"        if Param_File_Load_Type:            \r\n",
					"            source_path += f\"{Param_File_Load_Type}/\"\r\n",
					"\r\n",
					"        try:\r\n",
					"            latest_folder = get_latest_folder(source_path)\r\n",
					"            if not latest_folder:\r\n",
					"                logInfo(f\"No folders in path {source_path}\")\r\n",
					"                continue\r\n",
					"\r\n",
					"            latest_path = f\"{source_path}{latest_folder}/\"\r\n",
					"            files = [f.name for f in mssparkutils.fs.ls(latest_path) if not f.isDir]\r\n",
					"            all_latest_files.extend(files)\r\n",
					"\r\n",
					"            # Automatically detect multi-file processing based on filename patterns\r\n",
					"            filename_start = definition['Source_Filename_Start']\r\n",
					"            \r\n",
					"            # Find files with the same base name but different dates\r\n",
					"            matching_files, processing_mode = find_multi_date_files(files, filename_start)\r\n",
					"            \r\n",
					"            logInfo(f\"File analysis for '{filename_start}': found {len(matching_files)} files, mode: {processing_mode}\")\r\n",
					"            logInfo(f\"Matching files: {matching_files}\")\r\n",
					"            \r\n",
					"            if processing_mode == 'multi_file':\r\n",
					"                # Multi-file processing mode - sort files by date\r\n",
					"                logInfo(f\"Enabling multi-file processing for {definition['Standardised_Table_Name']} - found {len(matching_files)} files with date patterns\")\r\n",
					"                \r\n",
					"                # Sort files by date extracted from filename\r\n",
					"                matching_files_with_dates = []\r\n",
					"                for file in matching_files:\r\n",
					"                    file_date = extract_date_from_filename(file)\r\n",
					"                    matching_files_with_dates.append((file, file_date))\r\n",
					"                \r\n",
					"                # Sort by date (oldest first for sequential processing)\r\n",
					"                matching_files_with_dates.sort(key=lambda x: x[1])\r\n",
					"                sorted_files = [file for file, _ in matching_files_with_dates]\r\n",
					"                \r\n",
					"                logInfo(f\"Files sorted by date: {sorted_files}\")\r\n",
					"                \r\n",
					"                definition['matched_files'] = [f\"{latest_path}{f}\" for f in sorted_files]\r\n",
					"                definition['latest_path'] = latest_path\r\n",
					"                definition['processing_mode'] = 'multi_file'\r\n",
					"                matched_definitions.append(definition)\r\n",
					"                logInfo(f\"Setup multi-file processing for {definition['Standardised_Table_Name']} with {len(sorted_files)} files\")\r\n",
					"                \r\n",
					"            elif processing_mode == 'single_file':\r\n",
					"                # Single file processing mode\r\n",
					"                matching_file = matching_files[0]\r\n",
					"                definition['matched_file'] = matching_file\r\n",
					"                definition['latest_path'] = latest_path\r\n",
					"                definition['processing_mode'] = 'single_file'\r\n",
					"                matched_definitions.append(definition)\r\n",
					"                logInfo(f\"Setup single-file processing for {definition['Standardised_Table_Name']} with file: {matching_file}\")\r\n",
					"                \r\n",
					"            else:\r\n",
					"                # No matching files found\r\n",
					"                unmatched_definitions.append(definition['Standardised_Table_Name'])\r\n",
					"                logInfo(f\"No files found matching pattern '{filename_start}' for {definition['Standardised_Table_Name']}\")\r\n",
					"\r\n",
					"        except Exception as e:\r\n",
					"            logError(f\"Could not read from {source_path}: {e}\")\r\n",
					"\r\n",
					"    #List filenames if nothing matched in Orchestration.json\r\n",
					"    processed_files = set()\r\n",
					"    for d in matched_definitions:\r\n",
					"        if d['processing_mode'] == 'multi_file':\r\n",
					"            processed_files.update([os.path.basename(f) for f in d['matched_files']])\r\n",
					"        else:\r\n",
					"            processed_files.add(d['matched_file'])\r\n",
					"    \r\n",
					"    unmatched_files = set(all_latest_files) - processed_files\r\n",
					"    if unmatched_files:\r\n",
					"        logError(f\"Files found in source but not defined in orchestration.json: {', '.join(unmatched_files)}\")\r\n",
					"\r\n",
					"    # Step 3: Process each matched definition\r\n",
					"    for definition in matched_definitions:\r\n",
					"        \r\n",
					"        if definition['processing_mode'] == 'multi_file':\r\n",
					"            # Process each file individually and create separate entries\r\n",
					"            file_index = 0\r\n",
					"            table_created = False  # Track if table has been successfully created\r\n",
					"            \r\n",
					"            for file_path in definition['matched_files']:\r\n",
					"                file_name = os.path.basename(file_path)\r\n",
					"                \r\n",
					"                # Create individual result entry for each file\r\n",
					"                result_entry = {\r\n",
					"                    \"delta_table_name\": definition['Standardised_Table_Name'],\r\n",
					"                    \"csv_file_name\": file_name,\r\n",
					"                    \"record_count\": 0,\r\n",
					"                    \"table_result\": \"failed\",\r\n",
					"                    \"start_exec_time\": \"\",\r\n",
					"                    \"end_exec_time\": \"\",\r\n",
					"                    \"total_exec_time\": \"\",\r\n",
					"                    \"error_message\": \"\"\r\n",
					"                }\r\n",
					"\r\n",
					"                try:\r\n",
					"                    start_exec_time = str(datetime.now())\r\n",
					"                    result_entry[\"start_exec_time\"] = start_exec_time\r\n",
					"                    \r\n",
					"                    # Process individual file\r\n",
					"                    sparkDF = read_file(file_path)\r\n",
					"                    if sparkDF is None:\r\n",
					"                        logError(f\"No data loaded for file: {file_name}\")\r\n",
					"                        result_entry[\"error_message\"] = f\"No data loaded for file: {file_name}\"\r\n",
					"                        result_entry[\"end_exec_time\"] = str(datetime.now())\r\n",
					"                        result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, datetime.now()))\r\n",
					"                        processing_results.append(result_entry)\r\n",
					"                        file_index += 1\r\n",
					"                        continue\r\n",
					"\r\n",
					"                    expected_from = datetime.now() - timedelta(days=definition['Expected_Within_Weekdays'])\r\n",
					"                    expected_to = datetime.now()\r\n",
					"\r\n",
					"                    sparkDF = clean_column_names(sparkDF)\r\n",
					"                    \r\n",
					"                    # Add metadata columns to standardised table\r\n",
					"                    sparkTableDF = sparkDF.withColumn(\"ingested_datetime\", current_timestamp()) \\\r\n",
					"                                          .withColumn(\"expected_from\", lit(expected_from)) \\\r\n",
					"                                          .withColumn(\"expected_to\", lit(expected_to))\r\n",
					"                    \r\n",
					"                    # Reorder metadata columns for the standardised delta table\r\n",
					"                    sparkTableDF = reorder_columns(sparkTableDF)\r\n",
					"\r\n",
					"                    delta_table_path = f\"abfss://odw-standardised@{storage_account}{Param_Json_SchemaFolder_Name}/{definition['Standardised_Table_Name']}\"\r\n",
					"                    full_table_name = f\"{database_name}.{definition['Standardised_Table_Name']}\"\r\n",
					"                    schema_path = f\"abfss://odw-config@{storage_account}{definition['Standardised_Table_Definition']}\"\r\n",
					"\r\n",
					"                    # Strict schema validation - JSON schema is the ultimate truth\r\n",
					"                    expected_schema_fields = [f['name'] for f in json.loads(\"\".join([r.value for r in spark.read.text(schema_path).collect()]))['fields']]\r\n",
					"                    actual_fields = sparkTableDF.columns\r\n",
					"                    \r\n",
					"                    # Remove metadata columns from comparison as they're always added by the system\r\n",
					"                    excluded_columns = {\"ingested_datetime\", \"expected_from\", \"expected_to\", \"source_file\", \"file_processed_datetime\"}\r\n",
					"                    actual_business_fields = set(actual_fields) - excluded_columns\r\n",
					"                    expected_business_fields = set(expected_schema_fields) - excluded_columns\r\n",
					"                    \r\n",
					"                    missing_columns = expected_business_fields - actual_business_fields\r\n",
					"                    extra_columns = actual_business_fields - expected_business_fields\r\n",
					"                    \r\n",
					"                    # STRICT VALIDATION: CSV must match JSON schema exactly\r\n",
					"                    if missing_columns:\r\n",
					"                        logError(f\"CSV file missing required columns from JSON schema for table {definition['Standardised_Table_Name']}: {', '.join(missing_columns)}\")\r\n",
					"                        logInfo(f\"Skipping ingestion for {definition['Standardised_Table_Name']} due to missing required columns.\")\r\n",
					"                        result_entry[\"error_message\"] = f\"Schema validation failed - CSV missing required columns: {', '.join(missing_columns)}\"\r\n",
					"                        result_entry[\"end_exec_time\"] = str(datetime.now())\r\n",
					"                        result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, datetime.now()))\r\n",
					"                        processing_results.append(result_entry)\r\n",
					"                        file_index += 1\r\n",
					"                        continue\r\n",
					"                    \r\n",
					"                    if extra_columns:\r\n",
					"                        logError(f\"CSV file has additional columns not defined in JSON schema for table {definition['Standardised_Table_Name']}: {', '.join(extra_columns)}\")\r\n",
					"                        logError(f\"JSON schema is the ultimate truth. CSV file must match JSON schema exactly.\")\r\n",
					"                        logInfo(f\"Skipping ingestion for {definition['Standardised_Table_Name']} due to extra columns in CSV.\")\r\n",
					"                        result_entry[\"error_message\"] = f\"Schema validation failed - CSV has additional columns not in JSON schema: {', '.join(extra_columns)}\"\r\n",
					"                        result_entry[\"end_exec_time\"] = str(datetime.now())\r\n",
					"                        result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, datetime.now()))\r\n",
					"                        processing_results.append(result_entry)\r\n",
					"                        file_index += 1\r\n",
					"                        continue\r\n",
					"                    \r\n",
					"                    # If we reach here, CSV exactly matches JSON schema\r\n",
					"                    logInfo(f\"CSV schema validation passed for {definition['Standardised_Table_Name']} - CSV matches JSON schema exactly.\")\r\n",
					"                    \r\n",
					"                    # Use JSON schema compliance for all files\r\n",
					"                    write_success = write_dataframe_with_json_schema_compliance(\r\n",
					"                        sparkTableDF, delta_table_path, full_table_name, schema_path, \r\n",
					"                        is_first_file=(not table_created)\r\n",
					"                    )\r\n",
					"                    \r\n",
					"                    if not write_success:\r\n",
					"                        raise Exception(f\"Failed to write data using comprehensive schema handling\")\r\n",
					"                    \r\n",
					"                    # Mark table as created after first successful write\r\n",
					"                    if not table_created:\r\n",
					"                        table_created = True\r\n",
					"\r\n",
					"                    # Count rows for validation\r\n",
					"                    rows_raw = sparkDF.count()\r\n",
					"                    \r\n",
					"                    end_exec_time = str(datetime.now())\r\n",
					"\r\n",
					"                    # Update json result entry with success data\r\n",
					"                    result_entry[\"record_count\"] = rows_raw  # Individual file record count\r\n",
					"                    result_entry[\"table_result\"] = \"success\"\r\n",
					"                    result_entry[\"end_exec_time\"] = end_exec_time\r\n",
					"                    result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, end_exec_time))\r\n",
					"\r\n",
					"                    logInfo(f\"Successfully processed file {file_name} with {rows_raw} rows for {definition['Standardised_Table_Name']}\")\r\n",
					"                \r\n",
					"                except Exception as e:\r\n",
					"                    \r\n",
					"                    #Code added to capture meaningful error message\r\n",
					"                    full_trace = traceback.format_exc()\r\n",
					"                    \r\n",
					"                    table_error_msg = str(e)\r\n",
					"\r\n",
					"                    complete_msg = table_error_msg + \"\\n\" + full_trace\r\n",
					"                    error_text = complete_msg[:300]           \r\n",
					"                    \r\n",
					"                    # Find the position of the last full stop before 300 characters\r\n",
					"                    last_period_index = error_text.rfind('.')\r\n",
					"\r\n",
					"                    # Use up to the last full stop, if found; else fall back to 300 chars\r\n",
					"                    if last_period_index != -1:\r\n",
					"                        error_message = error_text[:last_period_index + 1] \r\n",
					"                    else:\r\n",
					"                        error_message = error_text\r\n",
					"\r\n",
					"                    logError(f\"Failed processing file {file_name} for {definition['Standardised_Table_Name']} - {e}\")\r\n",
					"\r\n",
					"                    result_entry[\"error_message\"] = f\"Failed processing file {file_name} - {error_message}\"\r\n",
					"                    result_entry[\"end_exec_time\"] = str(datetime.now())\r\n",
					"                    result_entry[\"total_exec_time\"] = str(time_diff_seconds(result_entry[\"start_exec_time\"], datetime.now()))\r\n",
					"                \r\n",
					"                # Add result to the json dump\r\n",
					"                processing_results.append(result_entry)\r\n",
					"                file_index += 1\r\n",
					"        \r\n",
					"        else:\r\n",
					"            # Original single file processing\r\n",
					"            result_entry = {\r\n",
					"                \"delta_table_name\": definition['Standardised_Table_Name'],\r\n",
					"                \"csv_file_name\": definition.get('matched_file', 'unknown_file'),\r\n",
					"                \"record_count\": 0,\r\n",
					"                \"table_result\": \"failed\",\r\n",
					"                \"start_exec_time\": \"\",\r\n",
					"                \"end_exec_time\": \"\",\r\n",
					"                \"total_exec_time\": \"\",\r\n",
					"                \"error_message\": \"\"\r\n",
					"            }\r\n",
					"\r\n",
					"            try:\r\n",
					"                start_exec_time = str(datetime.now())\r\n",
					"                result_entry[\"start_exec_time\"] = start_exec_time\r\n",
					"                \r\n",
					"                # Original single file processing\r\n",
					"                sparkDF = read_file(f\"{definition['latest_path']}{definition['matched_file']}\")\r\n",
					"                \r\n",
					"                if sparkDF is None:\r\n",
					"                    logError(f\"No data loaded for: {definition['Standardised_Table_Name']}\")\r\n",
					"                    continue\r\n",
					"\r\n",
					"                expected_from = datetime.now() - timedelta(days=definition['Expected_Within_Weekdays'])\r\n",
					"                expected_to = datetime.now()\r\n",
					"\r\n",
					"                # Clean column names if not already done in multi-file processing\r\n",
					"                sparkDF = clean_column_names(sparkDF)\r\n",
					"                \r\n",
					"                # Add metadata columns to standardised table\r\n",
					"                sparkTableDF = sparkDF.withColumn(\"ingested_datetime\", current_timestamp()) \\\r\n",
					"                                      .withColumn(\"expected_from\", lit(expected_from)) \\\r\n",
					"                                      .withColumn(\"expected_to\", lit(expected_to))\r\n",
					"                \r\n",
					"                # Reorder metadata columns for the standardised delta table\r\n",
					"                sparkTableDF = reorder_columns(sparkTableDF)\r\n",
					"\r\n",
					"                delta_table_path = f\"abfss://odw-standardised@{storage_account}{Param_Json_SchemaFolder_Name}/{definition['Standardised_Table_Name']}\"\r\n",
					"                full_table_name = f\"{database_name}.{definition['Standardised_Table_Name']}\"\r\n",
					"                schema_path = f\"abfss://odw-config@{storage_account}{definition['Standardised_Table_Definition']}\"\r\n",
					"                \r\n",
					"                # Strict schema validation - JSON schema is the ultimate truth\r\n",
					"                expected_schema_fields = [f['name'] for f in json.loads(\"\".join([r.value for r in spark.read.text(schema_path).collect()]))['fields']]\r\n",
					"                actual_fields = sparkTableDF.columns\r\n",
					"                \r\n",
					"                # Remove metadata columns from comparison as they're always added by the system\r\n",
					"                excluded_columns = {\"ingested_datetime\", \"expected_from\", \"expected_to\", \"source_file\", \"file_processed_datetime\"}\r\n",
					"                actual_business_fields = set(actual_fields) - excluded_columns\r\n",
					"                expected_business_fields = set(expected_schema_fields) - excluded_columns\r\n",
					"                \r\n",
					"                missing_columns = expected_business_fields - actual_business_fields\r\n",
					"                extra_columns = actual_business_fields - expected_business_fields\r\n",
					"                \r\n",
					"                # STRICT VALIDATION: CSV must match JSON schema exactly\r\n",
					"                if missing_columns:\r\n",
					"                    logError(f\"CSV file missing required columns from JSON schema for table {definition['Standardised_Table_Name']}: {', '.join(missing_columns)}\")\r\n",
					"                    logInfo(f\"Skipping ingestion for {definition['Standardised_Table_Name']} due to missing required columns.\")\r\n",
					"                    result_entry[\"error_message\"] = f\"Schema validation failed - CSV missing required columns: {', '.join(missing_columns)}\"\r\n",
					"                    result_entry[\"end_exec_time\"] = str(datetime.now())\r\n",
					"                    result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, datetime.now()))\r\n",
					"                    processing_results.append(result_entry)\r\n",
					"                    continue\r\n",
					"                \r\n",
					"                if extra_columns:\r\n",
					"                    logError(f\"CSV file has additional columns not defined in JSON schema for table {definition['Standardised_Table_Name']}: {', '.join(extra_columns)}\")\r\n",
					"                    logError(f\"JSON schema is the ultimate truth. CSV file must match JSON schema exactly.\")\r\n",
					"                    logInfo(f\"Skipping ingestion for {definition['Standardised_Table_Name']} due to extra columns in CSV.\")\r\n",
					"                    result_entry[\"error_message\"] = f\"Schema validation failed - CSV has additional columns not in JSON schema: {', '.join(extra_columns)}\"\r\n",
					"                    result_entry[\"end_exec_time\"] = str(datetime.now())\r\n",
					"                    result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, datetime.now()))\r\n",
					"                    processing_results.append(result_entry)\r\n",
					"                    continue\r\n",
					"                \r\n",
					"                # If we reach here, CSV exactly matches JSON schema\r\n",
					"                logInfo(f\"CSV schema validation passed for {definition['Standardised_Table_Name']} - CSV matches JSON schema exactly.\")\r\n",
					"                \r\n",
					"                # Write using JSON schema compliance\r\n",
					"                write_success = write_dataframe_with_json_schema_compliance(\r\n",
					"                    sparkTableDF, delta_table_path, full_table_name, schema_path, is_first_file=True\r\n",
					"                )\r\n",
					"                \r\n",
					"                if not write_success:\r\n",
					"                    raise Exception(f\"Failed to write data using strategy: {write_strategy}\")\r\n",
					"\r\n",
					"                # Count rows for validation\r\n",
					"                rows_raw = sparkDF.count()\r\n",
					"                standardised_table_df = spark.read.format(\"delta\").load(delta_table_path)\r\n",
					"                rows_new = standardised_table_df.filter(\r\n",
					"                    (col(\"expected_from\") == expected_from) & \r\n",
					"                    (col(\"expected_to\") == expected_to)\r\n",
					"                ).count()\r\n",
					"                \r\n",
					"                end_exec_time = str(datetime.now())\r\n",
					"\r\n",
					"                # Update json result entry with success data\r\n",
					"                result_entry[\"record_count\"] = standardised_table_df.count()  # Total records in delta table\r\n",
					"                result_entry[\"table_result\"] = \"success\"\r\n",
					"                result_entry[\"end_exec_time\"] = end_exec_time\r\n",
					"                result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, end_exec_time))\r\n",
					"\r\n",
					"                if rows_raw <= rows_new:\r\n",
					"                    logInfo(f\"All rows successfully written to {definition['Standardised_Table_Name']} — Raw: {rows_raw}, Written: {rows_new}\")\r\n",
					"                else:\r\n",
					"                    logError(f\"Mismatch in row count for {definition['Standardised_Table_Name']}: expected {rows_raw}, got {rows_new}\")\r\n",
					"            \r\n",
					"            except Exception as e:\r\n",
					"                \r\n",
					"                #Code added to capture meaningful error message\r\n",
					"                full_trace = traceback.format_exc()\r\n",
					"                \r\n",
					"                table_error_msg = str(e)\r\n",
					"\r\n",
					"                complete_msg = table_error_msg + \"\\n\" + full_trace\r\n",
					"                error_text = complete_msg[:300]           \r\n",
					"                \r\n",
					"                # Find the position of the last full stop before 300 characters\r\n",
					"                last_period_index = error_text.rfind('.')\r\n",
					"\r\n",
					"                # Use up to the last full stop, if found; else fall back to 300 chars\r\n",
					"                if last_period_index != -1:\r\n",
					"                    error_message = error_text[:last_period_index + 1] \r\n",
					"                else:\r\n",
					"                    error_message = error_text\r\n",
					"\r\n",
					"                logError(f\"Failed processing for {definition['Standardised_Table_Name']} - {e}\")\r\n",
					"\r\n",
					"                result_entry[\"error_message\"] = f\"Failed processing for {definition['Standardised_Table_Name']} - {error_message} \"\r\n",
					"                result_entry[\"end_exec_time\"] = str(datetime.now())\r\n",
					"                result_entry[\"total_exec_time\"] = str(time_diff_seconds(result_entry[\"start_exec_time\"], datetime.now()))\r\n",
					"            \r\n",
					"            # Add result to the json dump\r\n",
					"            processing_results.append(result_entry)\r\n",
					""
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Execute main process"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# --- Run the main process ---\r\n",
					"process_definitions(Param_File_Load_Type)"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"source": [
					"# Prepare and return JSON result\r\n",
					"json_result = {\r\n",
					"    \"processing_summary\": {\r\n",
					"        \"total_tables_processed\": len(processing_results),\r\n",
					"        \"successful_overwrites\": len([r for r in processing_results if r[\"table_result\"] == \"success\"]),\r\n",
					"        \"failed_overwrites\": len([r for r in processing_results if r[\"table_result\"] == \"failed\"])\r\n",
					"    },\r\n",
					"    \"table_details\": processing_results\r\n",
					"}\r\n",
					"\r\n",
					"# Convert to JSON string\r\n",
					"result_json_str = json.dumps(json_result, indent=2, default=datetime_handler)\r\n",
					"\r\n",
					""
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"source": [
					"# Exit with the JSON result for pipeline consumption\r\n",
					"mssparkutils.notebook.exit(result_json_str)"
				],
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Dropping Delta tables if needed, Code commented"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"editable": false,
					"run_control": {
						"frozen": true
					},
					"collapsed": false
				},
				"source": [
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.hr_absence_monthly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.inspector_addresses_monthly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.inspector_specialisms_monthly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.sap_email_monthly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.sap_hr_history_monthly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.sap_hr_leavers_monthly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.sap_protected_monthly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.inspector_addresses_weekly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.inspector_specialisms_weekly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.sap_hr_weekly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.work_schedules\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.inspector_addresses_monthly\")\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					""
				],
				"execution_count": null
			}
		]
	}
}