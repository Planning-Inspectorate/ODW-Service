{
	"name": "py_sap_hr_leaver",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "f6464e47-750f-4e32-b313-b59fcdbc55c7"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Harmoised layer and build a table for Curated Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;25-02-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This template is designed to facilitate the monthly processing and harmonization of SAP HR Leavers data. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The template ensures that HR data is accurately transformed, stored, and made available for reporting and analysis."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Intialisations"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"from odw.core.util.logging_util import LoggingUtil"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## staging load"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"try:\n",
					"    logInfo(\"Starting SAP HR Leavers processing\")\n",
					"    \n",
					"    # Set legacy time parser policy\n",
					"    logInfo(\"Setting legacy time parser policy\")\n",
					"    spark.sql(\"\"\"\n",
					"    SET spark.sql.legacy.timeParserPolicy = LEGACY\n",
					"    \"\"\")\n",
					"    logInfo(\"Legacy time parser policy set successfully\")\n",
					"    \n",
					"    # Step 1: Clear the staging table first\n",
					"    logInfo(\"Starting deletion of all rows from odw_harmonised_db.stage_SAP_HR_Leavers\")\n",
					"    spark.sql(\"\"\"\n",
					"    DELETE FROM odw_harmonised_db.stage_SAP_HR_Leavers\n",
					"    \"\"\")\n",
					"    logInfo(\"Successfully deleted all rows from odw_harmonised_db.stage_SAP_HR_Leavers\")\n",
					"    \n",
					"    # Step 2: Insert data into the harmonised table with transformations\n",
					"    logInfo(\"Starting data insertion into odw_harmonised_db.stage_SAP_HR_Leavers\")\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.stage_SAP_HR_Leavers\n",
					"    (\n",
					"        PersNo,             \n",
					"        Lastname,           \n",
					"        Firstname,          \n",
					"        CoCd,               \n",
					"        CompanyCode,        \n",
					"        Loc,                \n",
					"        Location,           \n",
					"        PSgroup,            \n",
					"        PayBandDescription, \n",
					"        Orgunit,            \n",
					"        OrganizationalUnit, \n",
					"        PA,                 \n",
					"        PersonnelArea,      \n",
					"        PSubarea,           \n",
					"        PersonnelSubarea,   \n",
					"        WorkC,              \n",
					"        WorkContract,       \n",
					"        OrgStartDate,       \n",
					"        Leaving,            \n",
					"        Act,                \n",
					"        ActionType,         \n",
					"        ActR,               \n",
					"        ReasonforAction,    \n",
					"        S,                  \n",
					"        EmploymentStatus,   \n",
					"        EmployeeNo,         \n",
					"        Position,           \n",
					"        Position1,          \n",
					"        Annualsalary,       \n",
					"        Curr,               \n",
					"        UserID,             \n",
					"        EmailAddress,       \n",
					"        PersNo1,            \n",
					"        NameofManagerOM,    \n",
					"        ManagerPosition,    \n",
					"        ManagerPositionText,\n",
					"        LMEmail,\n",
					"        SourceSystemID,\n",
					"        IngestionDate,\n",
					"        ValidTo,\n",
					"        RowID,\n",
					"        IsActive\n",
					"    )              \n",
					"    SELECT \n",
					"        PersNo,             \n",
					"        Lastname,           \n",
					"        Firstname,          \n",
					"        CoCd,               \n",
					"        CompanyCode,        \n",
					"        Loc,                \n",
					"        Location,           \n",
					"        PSgroup,            \n",
					"        PayBandDescription, \n",
					"        Orgunit,            \n",
					"        OrganizationalUnit, \n",
					"        PA,                 \n",
					"        PersonnelArea,      \n",
					"        PSubarea,           \n",
					"        PersonnelSubarea,   \n",
					"        WorkC,              \n",
					"        WorkContract,       \n",
					"        CAST(TO_TIMESTAMP(OrgStartDate, 'dd/MM/yyyy') AS DATE) AS OrgStartDate,\n",
					"        CAST(TO_TIMESTAMP(Leaving, 'dd/MM/yyyy') AS DATE) AS Leaving,\n",
					"        Act,                \n",
					"        ActionType,         \n",
					"        ActR,               \n",
					"        ReasonforAction,    \n",
					"        S,                  \n",
					"        EmploymentStatus,   \n",
					"        EmployeeNo,         \n",
					"        Position,           \n",
					"        Position1,          \n",
					"        NULL AS Annualsalary,\n",
					"        Curr,               \n",
					"        UserID,             \n",
					"        EmailAddress,       \n",
					"        PersNo1,            \n",
					"        NameofManagerOM,    \n",
					"        ManagerPosition,    \n",
					"        ManagerPositionText,\n",
					"        LMEmail,\n",
					"        'saphr' AS SourceSystemID,\n",
					"        CURRENT_DATE() AS IngestionDate,\n",
					"        CURRENT_TIMESTAMP() AS ValidTo,\n",
					"        SHA2(\n",
					"            CONCAT(\n",
					"                COALESCE(PersNo, ''),\n",
					"                COALESCE(Lastname, ''),\n",
					"                COALESCE(Firstname, ''),\n",
					"                COALESCE(CoCd, ''),\n",
					"                COALESCE(Loc, ''),\n",
					"                COALESCE(PSgroup, ''),\n",
					"                COALESCE(Orgunit, ''),\n",
					"                COALESCE(PA, ''),\n",
					"                COALESCE(PSubarea, ''),\n",
					"                COALESCE(WorkC, ''),\n",
					"                COALESCE(OrgStartDate, ''),\n",
					"                COALESCE(Leaving, ''),\n",
					"                COALESCE(Act, ''),\n",
					"                COALESCE(ActR, ''),\n",
					"                COALESCE(S, ''),\n",
					"                COALESCE(Position, '')\n",
					"            ), 256\n",
					"        ) AS RowID,\n",
					"        'Y' AS IsActive\n",
					"    FROM \n",
					"        odw_standardised_db.sap_hr_leavers_monthly\n",
					"    \"\"\")\n",
					"    \n",
					"    # Get count of inserted records\n",
					"    insert_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.stage_SAP_HR_Leavers\").collect()[0]['count']\n",
					"    logInfo(f\"Successfully inserted {insert_count} records into odw_harmonised_db.stage_SAP_HR_Leavers\")\n",
					"    \n",
					"    # Step 3: Handle NULL EmployeeNo values\n",
					"    logInfo(\"Starting update to handle NULL EmployeeNo values\")\n",
					"    null_emp_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.stage_SAP_HR_Leavers WHERE EmployeeNo IS NULL\").collect()[0]['count']\n",
					"    \n",
					"    spark.sql(\"\"\"\n",
					"    UPDATE odw_harmonised_db.stage_SAP_HR_Leavers\n",
					"    SET EmployeeNo = '' \n",
					"    WHERE EmployeeNo IS NULL\n",
					"    \"\"\")\n",
					"    logInfo(f\"Successfully updated {null_emp_count} NULL EmployeeNo values\")\n",
					"    \n",
					"    # Step 4: Special case update for specific employee\n",
					"    logInfo(\"Starting update for special case employee PersNo 50426514\")\n",
					"    spark.sql(\"\"\"\n",
					"    UPDATE odw_harmonised_db.stage_SAP_HR_Leavers\n",
					"    SET Leaving = '2024-02-29'\n",
					"    WHERE PersNo = '50426514'\n",
					"    \"\"\")\n",
					"    logInfo(\"Successfully updated special case employee record\")\n",
					"    \n",
					"    # Step 5: Clear all Annualsalary values\n",
					"    logInfo(\"Starting update to clear all Annualsalary values\")\n",
					"    spark.sql(\"\"\"\n",
					"    UPDATE odw_harmonised_db.stage_SAP_HR_Leavers \n",
					"    SET Annualsalary = NULL\n",
					"    \"\"\")\n",
					"    logInfo(\"Successfully cleared all Annualsalary values\")\n",
					"    \n",
					"    logInfo(\"SAP HR Leavers processing completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail\n",
					"    logError(f\"Error in SAP HR Leavers processing: {str(e)}\")\n",
					"    logException(e)\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### odw_harmonised_db.SAP_HR_Leavers"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					},
					"collapsed": false
				},
				"source": [
					"import json\n",
					"\n",
					"# Initialize result dictionary\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"try:\n",
					"    logInfo(\"Starting SAP HR Leavers final table processing\")\n",
					"    \n",
					"    # Clear the target table\n",
					"    logInfo(\"Clearing target table odw_harmonised_db.load_SAP_HR_Leavers\")\n",
					"    spark.sql(\"\"\"\n",
					"    DELETE FROM odw_harmonised_db.load_SAP_HR_Leavers\n",
					"    \"\"\")\n",
					"    logInfo(\"Target table cleared successfully\")\n",
					"    \n",
					"    # Step 1: Update Annualsalary to NULL\n",
					"    logInfo(\"Setting Annualsalary values to NULL in staging table\")\n",
					"    update_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count FROM odw_harmonised_db.stage_SAP_HR_Leavers WHERE Annualsalary IS NOT NULL\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    spark.sql(\"\"\"\n",
					"    UPDATE odw_harmonised_db.stage_SAP_HR_Leavers \n",
					"    SET Annualsalary = NULL \n",
					"    WHERE Annualsalary IS NOT NULL\n",
					"    \"\"\")\n",
					"    logInfo(f\"Updated {update_count} Annualsalary values to NULL\")\n",
					"    \n",
					"    # Step 2: Calculate min and max dates first and save to temporary views\n",
					"    logInfo(\"Calculating date ranges for processing\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW temp_min_date AS\n",
					"    SELECT CONCAT(YEAR(MIN(Leaving)), '-04-01') AS min_date \n",
					"    FROM odw_harmonised_db.stage_SAP_HR_Leavers\n",
					"    \"\"\")\n",
					"    \n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW temp_max_date AS\n",
					"    SELECT CONCAT(YEAR(MAX(Leaving)) + 1, '-03-31') AS max_date \n",
					"    FROM odw_harmonised_db.stage_SAP_HR_Leavers\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step 3: Get the actual date values for use in the DELETE statement\n",
					"    min_date = spark.sql(\"SELECT min_date FROM temp_min_date\").collect()[0]['min_date']\n",
					"    max_date = spark.sql(\"SELECT max_date FROM temp_max_date\").collect()[0]['max_date']\n",
					"    logInfo(f\"Date range determined: {min_date} to {max_date}\")\n",
					"    \n",
					"    # Step 4: Use the values directly in your DELETE statement with the actual date values\n",
					"    logInfo(f\"Deleting existing records within date range {min_date} to {max_date}\")\n",
					"    delete_count = spark.sql(f\"\"\"\n",
					"    SELECT COUNT(*) as count FROM odw_harmonised_db.load_SAP_HR_Leavers\n",
					"    WHERE Leaving BETWEEN '{min_date}' AND '{max_date}'\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    spark.sql(f\"\"\"\n",
					"    DELETE FROM odw_harmonised_db.load_SAP_HR_Leavers\n",
					"    WHERE Leaving BETWEEN '{min_date}' AND '{max_date}'\n",
					"    \"\"\")\n",
					"    logInfo(f\"Deleted {delete_count} existing records within date range\")\n",
					"    \n",
					"    # Step 5: Create temporary view with deduplicated data\n",
					"    logInfo(\"Creating temporary view with deduplicated data\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW temp_deduplicated_leavers AS\n",
					"    SELECT * FROM (\n",
					"        SELECT \n",
					"            *,\n",
					"            ROW_NUMBER() OVER (PARTITION BY PersNo, Leaving, ManagerPosition\n",
					"                              ORDER BY PersNo, Leaving, ManagerPosition) AS row_num\n",
					"        FROM odw_harmonised_db.stage_SAP_HR_Leavers\n",
					"    ) t\n",
					"    WHERE row_num = 1\n",
					"    \"\"\")\n",
					"    \n",
					"    deduped_count = spark.sql(\"SELECT COUNT(*) as count FROM temp_deduplicated_leavers\").collect()[0]['count']\n",
					"    logInfo(f\"Deduplicated view created with {deduped_count} records\")\n",
					"    \n",
					"    # Step 6: Insert the deduplicated records with explicit casting for Annualsalary\n",
					"    logInfo(\"Inserting deduplicated records into final table\")\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.load_SAP_HR_Leavers\n",
					"    SELECT \n",
					"        PersNo,\n",
					"        Lastname,\n",
					"        Firstname,\n",
					"        CoCd,\n",
					"        CompanyCode,\n",
					"        Loc,\n",
					"        Location,\n",
					"        PSgroup,\n",
					"        PayBandDescription,\n",
					"        Orgunit,\n",
					"        OrganizationalUnit,\n",
					"        PA,\n",
					"        PersonnelArea,\n",
					"        PSubarea,\n",
					"        PersonnelSubarea,\n",
					"        WorkC,\n",
					"        WorkContract,\n",
					"        OrgStartDate,\n",
					"        Leaving,\n",
					"        Act,\n",
					"        ActionType,\n",
					"        ActR,\n",
					"        ReasonforAction,\n",
					"        S,\n",
					"        EmploymentStatus,\n",
					"        EmployeeNo,\n",
					"        Position,\n",
					"        Position1,\n",
					"        CAST(NULL AS DOUBLE) AS Annualsalary,  -- Cast to resolve the data type issue\n",
					"        Curr,\n",
					"        UserID,\n",
					"        EmailAddress,\n",
					"        PersNo1,\n",
					"        NameofManagerOM,\n",
					"        ManagerPosition,\n",
					"        ManagerPositionText,\n",
					"        LMEmail,\n",
					"        SourceSystemID,\n",
					"        IngestionDate,\n",
					"        ValidTo,\n",
					"        RowID,\n",
					"        IsActive\n",
					"    FROM temp_deduplicated_leavers\n",
					"    \"\"\")\n",
					"    \n",
					"    final_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count FROM odw_harmonised_db.load_SAP_HR_Leavers\n",
					"    WHERE IngestionDate = CURRENT_DATE()\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    result[\"record_count\"] = final_count\n",
					"    logInfo(f\"Successfully inserted {final_count} records into final table\")\n",
					"    \n",
					"    logInfo(\"SAP HR Leavers final table processing completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Capture error information and limit to 300 characters\n",
					"    error_msg = f\"Error in SAP HR Leavers processing: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    \n",
					"    # Truncate error message if it exceeds 300 characters\n",
					"    truncated_error = (error_msg[:297] + '...') if len(error_msg) > 300 else error_msg\n",
					"    \n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = truncated_error\n",
					"    result[\"record_count\"] = -1  # Indicate failure with -1 count\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()\n",
					"    \n",
					"    # Output the result as JSON for ADF to capture\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": null
			}
		]
	}
}