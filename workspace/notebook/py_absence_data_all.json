{
	"name": "py_absence_data_all",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "569185c9-6fd9-4b87-b0a0-983c9f26c93d"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Standardised layer and build a table for Curated Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;01-April-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This Notebook is designed to facilitate the monthly processing and harmonization of absence data. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The Notebook ensures that absence data is accurately transformed, stored, and made available for reporting and analysis."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Intialisations"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import logging"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"\n",
					"%run utils/py_logging_decorator"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Entity : absence_all"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    # Set legacy time parser for compatibility\n",
					"    logInfo(\"Setting legacy time parser policy\")\n",
					"    spark.sql(\"\"\"SET spark.sql.legacy.timeParserPolicy = LEGACY\"\"\")\n",
					"    logInfo(\"Legacy time parser policy set successfully\")\n",
					"    \n",
					"    # Step 1: Delete all rows from the target table\n",
					"    logInfo(\"Starting deletion of all rows from odw_harmonised_db.sap_hr_absence_all\")\n",
					"    spark.sql(\"\"\"\n",
					"    DELETE FROM odw_harmonised_db.sap_hr_absence_all\n",
					"    \"\"\")\n",
					"    logInfo(\"Successfully deleted all rows from odw_harmonised_db.sap_hr_absence_all\")\n",
					"    \n",
					"    # Step 2: Insert data into the target table with RowID generated inline\n",
					"    logInfo(\"Starting data insertion into odw_harmonised_db.sap_hr_absence_all from hr_absence_monthly\")\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.sap_hr_absence_all\n",
					"    (   \n",
					"        StaffNumber,            \n",
					"        AbsType,                \n",
					"        SicknessGroup,          \n",
					"        StartDate,              \n",
					"        EndDate,                \n",
					"        AttendanceorAbsenceType,\n",
					"        Days,                   \n",
					"        Hrs,                    \n",
					"        Start,                  \n",
					"        Endtime,                \n",
					"        Caldays,                \n",
					"        WorkScheduleRule,       \n",
					"        Wkhrs,                  \n",
					"        HrsDay,                 \n",
					"        WkDys,                  \n",
					"        AnnualLeaveStart,\n",
					"        SourceSystemID,\n",
					"        IngestionDate,\n",
					"        ValidTo,\n",
					"        RowID,\n",
					"        IsActive\n",
					"    )\n",
					"    SELECT  \n",
					"        StaffNumber,\n",
					"        COALESCE(AbsType, '') AS AbsType,\n",
					"        COALESCE(SicknessGroup, '') AS SicknessGroup,\n",
					"        cast(to_date(StartDate,'dd/MM/yyyy') as date) as StartDate,\n",
					"        cast(to_date(EndDate,'dd/MM/yyyy') as date) as EndDate,\n",
					"        AttendanceorAbsenceType,\n",
					"        REPLACE(Days, ',', '') AS Days,  -- Remove commas before casting\n",
					"        REPLACE(Hrs, ',', '') AS Hrs,    -- Remove commas before casting\n",
					"        to_date('31/12/1899','dd/MM/yyyy') as StartTime,\n",
					"        to_date('31/12/1899','dd/MM/yyyy') as Endtime,  \n",
					"        Caldays,\n",
					"        WorkScheduleRule,\n",
					"        TRY_CAST(REPLACE(Wkhrs, ',', '') AS DOUBLE) AS Wkhrs,  \n",
					"        TRY_CAST(REPLACE(HrsDay, ',', '') AS DOUBLE) AS HrsDay,  \n",
					"        TRY_CAST(REPLACE(WkDys, ',', '') AS DOUBLE) AS WkDys,  \n",
					"        to_date(AnnualLeaveStart,'dd/MM/yyyy') AS AnnualLeaveStart,\n",
					"        'saphr' AS SourceSystemID,\n",
					"        CURRENT_DATE() AS IngestionDate,\n",
					"        CURRENT_TIMESTAMP() AS ValidTo,\n",
					"        md5(concat_ws('|',\n",
					"            StaffNumber,            \n",
					"            COALESCE(AbsType, ''),                \n",
					"            COALESCE(SicknessGroup, ''),          \n",
					"            to_date(StartDate,'dd/MM/yyyy'),              \n",
					"            to_date(EndDate,'dd/MM/yyyy'),                \n",
					"            AttendanceorAbsenceType,\n",
					"            REPLACE(Days, ',', ''),                   \n",
					"            REPLACE(Hrs, ',', ''),                    \n",
					"            to_date('31/12/1899','dd/MM/yyyy'),                  \n",
					"            to_date('31/12/1899','dd/MM/yyyy'),                \n",
					"            Caldays,                \n",
					"            WorkScheduleRule,       \n",
					"            REPLACE(Wkhrs, ',', ''),                  \n",
					"            REPLACE(HrsDay, ',', ''),                 \n",
					"            REPLACE(WkDys, ',', '')\n",
					"        )) AS RowID,\n",
					"        'Y' AS IsActive\n",
					"    FROM odw_standardised_db.hr_absence_monthly\n",
					"    \"\"\")\n",
					"    \n",
					"    # Get count of inserted records\n",
					"    inserted_record_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.sap_hr_absence_all\").collect()[0]['count']\n",
					"    logInfo(f\"Successfully inserted {inserted_record_count} records into odw_harmonised_db.sap_hr_absence_all\")\n",
					"    \n",
					"    # Step 3: Remove rows with null values in key columns\n",
					"    logInfo(\"Starting removal of rows with null key values\")\n",
					"    null_rows_query = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count FROM odw_harmonised_db.sap_hr_absence_all\n",
					"    WHERE StaffNumber IS NULL OR StartDate IS NULL OR EndDate IS NULL\n",
					"    \"\"\").collect()[0]['count']\n",
					"    logInfo(f\"Found {null_rows_query} rows with null key values to remove\")\n",
					"    \n",
					"    spark.sql(\"\"\"\n",
					"    DELETE FROM odw_harmonised_db.sap_hr_absence_all\n",
					"    WHERE\n",
					"        StaffNumber IS NULL\n",
					"        OR StartDate IS NULL\n",
					"        OR EndDate IS NULL\n",
					"    \"\"\")\n",
					"    logInfo(\"Successfully removed rows with null key values\")\n",
					"    \n",
					"    # Step 4: Deduplication using the table rebuild approach\n",
					"    logInfo(\"Starting table rebuild deduplication approach\")\n",
					"    \n",
					"    # Count duplicates first for logging\n",
					"    duplicate_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count FROM (\n",
					"        SELECT StaffNumber, StartDate, COUNT(*) as cnt\n",
					"        FROM odw_harmonised_db.sap_hr_absence_all\n",
					"        GROUP BY StaffNumber, StartDate\n",
					"        HAVING COUNT(*) > 1\n",
					"    )\n",
					"    \"\"\").collect()[0]['count']\n",
					"    logInfo(f\"Found {duplicate_count} duplicate key combinations to process\")\n",
					"    \n",
					"    # Step a: Create a temporary table with deduplicated data\n",
					"    logInfo(\"Creating deduplicated temporary table\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW deduplicated_data AS\n",
					"    SELECT *\n",
					"    FROM (\n",
					"        SELECT \n",
					"            *,\n",
					"            ROW_NUMBER() OVER (PARTITION BY StaffNumber, StartDate ORDER BY RowID) AS row_num\n",
					"        FROM odw_harmonised_db.sap_hr_absence_all\n",
					"    ) ranked\n",
					"    WHERE row_num = 1\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step b: Count records in the temporary table for verification\n",
					"    dedup_count = spark.sql(\"SELECT COUNT(*) as count FROM deduplicated_data\").collect()[0]['count']\n",
					"    logInfo(f\"Deduplicated view contains {dedup_count} records\")\n",
					"    \n",
					"    # Step c: Clear the original table\n",
					"    logInfo(\"Clearing the original table for rebuild\")\n",
					"    spark.sql(\"\"\"\n",
					"    DELETE FROM odw_harmonised_db.sap_hr_absence_all\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step d: Insert deduplicated data back to the original table\n",
					"    logInfo(\"Inserting deduplicated data back to original table\")\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.sap_hr_absence_all\n",
					"    SELECT \n",
					"        StaffNumber,            \n",
					"        AbsType,                \n",
					"        SicknessGroup,          \n",
					"        StartDate,              \n",
					"        EndDate,                \n",
					"        AttendanceorAbsenceType,\n",
					"        Days,                   \n",
					"        Hrs,                    \n",
					"        Start,                  \n",
					"        Endtime,                \n",
					"        Caldays,                \n",
					"        WorkScheduleRule,       \n",
					"        Wkhrs,                  \n",
					"        HrsDay,                 \n",
					"        WkDys,                  \n",
					"        AnnualLeaveStart,\n",
					"        SourceSystemID,\n",
					"        IngestionDate,\n",
					"        ValidTo,\n",
					"        RowID,\n",
					"        IsActive\n",
					"    FROM deduplicated_data\n",
					"    \"\"\")\n",
					"    \n",
					"    # Get final record count\n",
					"    final_record_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.sap_hr_absence_all\").collect()[0]['count']\n",
					"    logInfo(f\"Successfully completed deduplication process. Final record count: {final_record_count}\")\n",
					"    \n",
					"    # Final success message\n",
					"    logInfo(\"Absence data processing completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail\n",
					"    logError(f\"Error in absence data processing: {str(e)}\")\n",
					"    logException(e)\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()"
				],
				"execution_count": null
			}
		]
	}
}