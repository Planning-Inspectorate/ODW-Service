{
	"name": "py_absence_data_all",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "5c8f4d11-c397-41da-8442-d77416b78923"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Standardised layer and build a table for Curated Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;01-April-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This Notebook is designed to facilitate the monthly processing and harmonization of absence data. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The Notebook ensures that absence data is accurately transformed, stored, and made available for reporting and analysis."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Intializations"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from odw.core.util.logging_util import LoggingUtil\n",
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from datetime import datetime, timedelta\n",
					"import json\n",
					"import re\n",
					"from pyspark.sql.functions import col\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Absence Incremental load"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"inserted_count\": 0,\n",
					"    \"updated_count\": 0,\n",
					"    \"deleted_count\": 0,\n",
					"    \"cancelled_deleted_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"# UUID extraction function for simplification\n",
					"def extract_uuid_from_abstype(abs_type):\n",
					"    \"\"\"\n",
					"    Extract UUID from AbsType field using simplified logic\n",
					"    \"\"\"\n",
					"    if abs_type is None:\n",
					"        return None\n",
					"    \n",
					"    # Pattern 1: Complex timestamp pattern\n",
					"    pattern1 = r'^[A-Z_]+-[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}[.][0-9]+-(.+)$'\n",
					"    match1 = re.match(pattern1, abs_type)\n",
					"    if match1:\n",
					"        return match1.group(1)\n",
					"    \n",
					"    # Pattern 2: 32-character hex string\n",
					"    pattern2 = r'[a-f0-9]{32}'\n",
					"    match2 = re.search(pattern2, abs_type)\n",
					"    if match2:\n",
					"        return match2.group(0)\n",
					"    \n",
					"    # Pattern 3: Suffix after last dash (4+ characters)\n",
					"    pattern3 = r'.*-([A-Za-z0-9_]{4,})$'\n",
					"    match3 = re.match(pattern3, abs_type)\n",
					"    if match3:\n",
					"        return match3.group(1)\n",
					"    \n",
					"    # Default: return MD5 hash\n",
					"    import hashlib\n",
					"    return hashlib.md5(abs_type.encode()).hexdigest()\n",
					"\n",
					"# Register UDF for use in SQL\n",
					"from pyspark.sql.types import StringType\n",
					"spark.udf.register(\"extract_uuid\", extract_uuid_from_abstype, StringType())\n",
					"\n",
					"# Set legacy time parser for compatibility\n",
					"LoggingUtil().log_info(\"Setting legacy time parser policy\")\n",
					"spark.sql(\"\"\"SET spark.sql.legacy.timeParserPolicy = LEGACY\"\"\")\n",
					"LoggingUtil().log_info(\"Legacy time parser policy set successfully\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Process Absence data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"try:\n",
					"    # Step 1: Create staging view with parsed AbsType data\n",
					"    LoggingUtil().log_info(\"Creating staging view with parsed AbsType data\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW staging_absence AS\n",
					"    SELECT  \n",
					"        StaffNumber,\n",
					"        COALESCE(AbsType, '') AS AbsType,\n",
					"        COALESCE(SicknessGroup, '') AS SicknessGroup,\n",
					"        CAST(TO_DATE(StartDate,'dd/MM/yyyy') AS DATE) AS StartDate,\n",
					"        CAST(TO_DATE(EndDate,'dd/MM/yyyy') AS DATE) AS EndDate,\n",
					"        AttendanceorAbsenceType,\n",
					"    ROUND(CAST(REPLACE(Days, ',', '') AS DOUBLE), 2) AS Days,\n",
					"    ROUND(CAST(REPLACE(Hrs, ',', '') AS DOUBLE), 2) AS Hrs,\n",
					"        TO_DATE('31/12/1899','dd/MM/yyyy') AS Start,\n",
					"        TO_DATE('31/12/1899','dd/MM/yyyy') AS Endtime,  \n",
					"        Caldays,\n",
					"        WorkScheduleRule,\n",
					"    ROUND(TRY_CAST(REPLACE(Wkhrs, ',', '') AS DOUBLE), 2) AS Wkhrs,  \n",
					"    ROUND(TRY_CAST(REPLACE(HrsDay, ',', '') AS DOUBLE), 2) AS HrsDay,  \n",
					"        TRY_CAST(REPLACE(WkDys, ',', '') AS DOUBLE) AS WkDys,  \n",
					"        TO_DATE(AnnualLeaveStart,'dd/MM/yyyy') AS AnnualLeaveStart,\n",
					"        'saphr' AS SourceSystemID,\n",
					"        CURRENT_DATE() AS IngestionDate,\n",
					"        CURRENT_TIMESTAMP() AS ValidTo,\n",
					"        \n",
					"        CASE \n",
					"            WHEN AbsType LIKE '%-%' THEN TRIM(SPLIT(AbsType, '-')[0])\n",
					"            ELSE 'UNKNOWN'\n",
					"        END AS ApprovalStatus,\n",
					"        \n",
					"        extract_uuid(AbsType) AS RecordUUID,\n",
					"        \n",
					"        CASE \n",
					"            WHEN AbsType RLIKE '^[A-Z_]+-[0-9]{4}-[0-9]{2}-[0-9]{2}' THEN\n",
					"                TRY_CAST(\n",
					"                    REGEXP_EXTRACT(AbsType, '^[A-Z_]+-([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2})', 1)\n",
					"                    AS TIMESTAMP\n",
					"                )\n",
					"            ELSE CURRENT_TIMESTAMP()\n",
					"        END AS LastModifiedDate,\n",
					"        \n",
					"        MD5(CONCAT_WS('|',\n",
					"            StaffNumber,            \n",
					"            COALESCE(AbsType, ''),                \n",
					"            COALESCE(SicknessGroup, ''),          \n",
					"            TO_DATE(StartDate,'dd/MM/yyyy'),              \n",
					"            TO_DATE(EndDate,'dd/MM/yyyy'),                \n",
					"            AttendanceorAbsenceType,\n",
					"            REPLACE(Days, ',', ''),                   \n",
					"            REPLACE(Hrs, ',', ''),                    \n",
					"            Caldays,                \n",
					"            WorkScheduleRule,       \n",
					"            REPLACE(Wkhrs, ',', ''),                  \n",
					"            REPLACE(HrsDay, ',', ''),                 \n",
					"            REPLACE(WkDys, ',', '')\n",
					"        )) AS RowID,\n",
					"        'Y' AS IsActive\n",
					"        \n",
					"    FROM odw_standardised_db.hr_absence_monthly\n",
					"    WHERE \n",
					"        StaffNumber IS NOT NULL \n",
					"        AND StartDate IS NOT NULL \n",
					"        AND EndDate IS NOT NULL\n",
					"        AND AbsType IS NOT NULL\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step 2: Create view of latest records per UUID (handling duplicates)\n",
					"    LoggingUtil().log_info(\"Creating view of latest records per UUID\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW latest_staging_records AS\n",
					"    SELECT *\n",
					"    FROM (\n",
					"        SELECT *,\n",
					"            ROW_NUMBER() OVER (\n",
					"                PARTITION BY RecordUUID \n",
					"                ORDER BY LastModifiedDate DESC, IngestionDate DESC\n",
					"            ) AS rn\n",
					"        FROM staging_absence\n",
					"        WHERE RecordUUID IS NOT NULL\n",
					"    ) ranked\n",
					"    WHERE rn = 1\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step 3: Create a snapshot of current target table\n",
					"    LoggingUtil().log_info(\"Creating snapshot of current target table\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW current_target AS\n",
					"    SELECT * FROM odw_harmonised_db.sap_hr_absence_all\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step 4: Identify outdated records\n",
					"    LoggingUtil().log_info(\"Identifying outdated records to replace\")\n",
					"    records_to_update = spark.sql(\"\"\"\n",
					"    SELECT DISTINCT t.RowID as old_rowid\n",
					"    FROM current_target t\n",
					"    INNER JOIN latest_staging_records s\n",
					"    ON t.RowID = s.RowID \n",
					"    WHERE s.LastModifiedDate > t.ValidTo\n",
					"    \"\"\")\n",
					"    \n",
					"    deleted_count = records_to_update.count()\n",
					"    LoggingUtil().log_info(f\"Found {deleted_count} outdated records to replace\")\n",
					"    \n",
					"    # Step 5: Create view of records to keep (excluding outdated ones) - SECURITY FIX APPLIED\n",
					"    if deleted_count > 0:\n",
					"        LoggingUtil().log_info(f\"Creating view excluding {deleted_count} outdated records\")\n",
					"        \n",
					"        # SECURE: Use DataFrame operations instead of f-string SQL to prevent SQL injection\n",
					"        outdated_rowids = [row.old_rowid for row in records_to_update.collect()]\n",
					"        current_target_df = spark.table(\"odw_harmonised_db.sap_hr_absence_all\")\n",
					"        records_to_keep_df = current_target_df.filter(~col(\"RowID\").isin(outdated_rowids))\n",
					"        records_to_keep_df.createOrReplaceTempView(\"records_to_keep\")\n",
					"    else:\n",
					"        LoggingUtil().log_info(\"No outdated records found - keeping all current records\")\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW records_to_keep AS\n",
					"        SELECT * FROM current_target\n",
					"        \"\"\")\n",
					"    \n",
					"    LoggingUtil().log_info(\"Data preparation and staging completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error in data preparation phase: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    \n",
					"    # Clean up on error in this phase\n",
					"    try:\n",
					"        spark.sql(\"DROP VIEW IF EXISTS staging_absence\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS latest_staging_records\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS current_target\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS records_to_keep\")\n",
					"    except:\n",
					"        pass\n",
					"    \n",
					"    raise e"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Final Processing and Data Loading"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    # Step 6: Get new records from staging (those not in current target)\n",
					"    LoggingUtil().log_info(\"Identifying new records to insert\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW new_records AS\n",
					"    SELECT \n",
					"        StaffNumber, AbsType, SicknessGroup, StartDate, EndDate,\n",
					"        AttendanceorAbsenceType, Days, Hrs, Start, Endtime,\n",
					"        Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys,\n",
					"        AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo,\n",
					"        RowID, IsActive\n",
					"    FROM latest_staging_records s\n",
					"    WHERE NOT EXISTS (\n",
					"        SELECT 1 FROM current_target t\n",
					"        WHERE t.RowID = s.RowID\n",
					"    )\n",
					"    \"\"\")\n",
					"    \n",
					"    new_records_count = spark.sql(\"SELECT COUNT(*) as count FROM new_records\").collect()[0]['count']\n",
					"    LoggingUtil().log_info(f\"Found {new_records_count} new records to add\")\n",
					"    \n",
					"    # Step 7: Create final dataset combining kept records + new records\n",
					"    LoggingUtil().log_info(\"Creating final dataset combining existing and new records\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW final_dataset AS\n",
					"    SELECT * FROM records_to_keep\n",
					"    UNION ALL\n",
					"    SELECT * FROM new_records\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step 8: Remove cancelled/rejected records from final dataset\n",
					"    LoggingUtil().log_info(\"Filtering out cancelled/rejected records\")\n",
					"    \n",
					"    total_before_filter = spark.sql(\"SELECT COUNT(*) as count FROM final_dataset\").collect()[0]['count']\n",
					"    \n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW final_approved_dataset AS\n",
					"    SELECT * FROM final_dataset\n",
					"    WHERE NOT (\n",
					"        UPPER(CASE \n",
					"            WHEN AbsType LIKE '%-%' THEN TRIM(SPLIT(AbsType, '-')[0])\n",
					"            ELSE 'UNKNOWN'\n",
					"        END) IN ('CANCELLED', 'PENDING_CANCELLATION', 'REJECTED')\n",
					"    )\n",
					"    \"\"\")\n",
					"    \n",
					"    total_after_filter = spark.sql(\"SELECT COUNT(*) as count FROM final_approved_dataset\").collect()[0]['count']\n",
					"    cancelled_count = total_before_filter - total_after_filter\n",
					"    \n",
					"    LoggingUtil().log_info(f\"Filtered out {cancelled_count} cancelled/rejected records\")\n",
					"    \n",
					"    # Step 9: TRUNCATE and INSERT - No overwrites!\n",
					"    LoggingUtil().log_info(\"Refreshing target table with final approved dataset\")\n",
					"    \n",
					"    # Clear target table\n",
					"    spark.sql(\"DELETE FROM odw_harmonised_db.sap_hr_absence_all\")\n",
					"    LoggingUtil().log_info(\"Truncated target table\")\n",
					"    \n",
					"    # Insert final approved dataset\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.sap_hr_absence_all (\n",
					"        StaffNumber, AbsType, SicknessGroup, StartDate, EndDate,\n",
					"        AttendanceorAbsenceType, Days, Hrs, Start, Endtime,\n",
					"        Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys,\n",
					"        AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo,\n",
					"        RowID, IsActive\n",
					"    )\n",
					"    SELECT \n",
					"        StaffNumber, AbsType, SicknessGroup, StartDate, EndDate,\n",
					"        AttendanceorAbsenceType, Days, Hrs, Start, Endtime,\n",
					"        Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys,\n",
					"        AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo,\n",
					"        RowID, IsActive\n",
					"    FROM final_approved_dataset\n",
					"    \"\"\")\n",
					"    \n",
					"    final_record_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.sap_hr_absence_all\").collect()[0]['count']\n",
					"    LoggingUtil().log_info(f\"Successfully loaded {final_record_count} total records into target table\")\n",
					"    \n",
					"    # Step 10: Calculate final counts for reporting\n",
					"    source_record_count = spark.sql(\"SELECT COUNT(*) as count FROM latest_staging_records\").collect()[0]['count']\n",
					"    \n",
					"    result[\"record_count\"] = final_record_count\n",
					"    result[\"inserted_count\"] = new_records_count\n",
					"    result[\"updated_count\"] = deleted_count  # Records that were replaced\n",
					"    result[\"deleted_count\"] = deleted_count\n",
					"    result[\"cancelled_deleted_count\"] = cancelled_count\n",
					"    \n",
					"    LoggingUtil().log_info(f\"Incremental load completed successfully:\")\n",
					"    LoggingUtil().log_info(f\"- Source records processed: {source_record_count}\")\n",
					"    LoggingUtil().log_info(f\"- New records inserted: {new_records_count}\")\n",
					"    LoggingUtil().log_info(f\"- Outdated records replaced: {deleted_count}\")\n",
					"    LoggingUtil().log_info(f\"- Cancelled/rejected records removed: {cancelled_count}\")\n",
					"    LoggingUtil().log_info(f\"- Total records in target: {final_record_count}\")\n",
					"    \n",
					"    # Clean up temporary views\n",
					"    spark.sql(\"DROP VIEW IF EXISTS staging_absence\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS latest_staging_records\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS current_target\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS records_to_keep\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS new_records\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS final_dataset\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS final_approved_dataset\")\n",
					"    \n",
					"    LoggingUtil().log_info(\"Successfully completed incremental load process\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error in final processing phase: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    \n",
					"    truncated_error = (error_msg[:297] + '...') if len(error_msg) > 300 else error_msg\n",
					"    \n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = truncated_error\n",
					"    result[\"record_count\"] = -1\n",
					"    result[\"inserted_count\"] = -1\n",
					"    result[\"updated_count\"] = -1\n",
					"    result[\"deleted_count\"] = -1\n",
					"    result[\"cancelled_deleted_count\"] = -1\n",
					"    \n",
					"    # Clean up on error\n",
					"    try:\n",
					"        spark.sql(\"DROP VIEW IF EXISTS staging_absence\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS latest_staging_records\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS current_target\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS records_to_keep\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS new_records\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS final_dataset\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS final_approved_dataset\")\n",
					"    except:\n",
					"        pass\n",
					"    \n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    LoggingUtil().log_info(\"Flushing logs\")\n",
					"    flushLogging()\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": null
			}
		]
	}
}