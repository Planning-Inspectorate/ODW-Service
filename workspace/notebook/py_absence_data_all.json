{
	"name": "py_absence_data_all",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b7cd5c00-c54c-45fc-8db0-e210f65e0470"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Standardised layer and build a table for Curated Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;01-April-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This Notebook is designed to facilitate the monthly processing and harmonization of absence data. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The Notebook ensures that absence data is accurately transformed, stored, and made available for reporting and analysis."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Intialisations"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import logging"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"\n",
					"%run utils/py_logging_decorator"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Absence Incremental load -old"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import json\n",
					"\n",
					"# Initialize result dictionary\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"inserted_count\": 0,\n",
					"    \"updated_count\": 0,\n",
					"    \"deleted_count\": 0,\n",
					"    \"cancelled_deleted_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"try:\n",
					"    # Set legacy time parser for compatibility\n",
					"    logInfo(\"Setting legacy time parser policy\")\n",
					"    spark.sql(\"\"\"SET spark.sql.legacy.timeParserPolicy = LEGACY\"\"\")\n",
					"    logInfo(\"Legacy time parser policy set successfully\")\n",
					"    \n",
					"    # Step 1: Create staging view with parsed AbsType data\n",
					"    logInfo(\"Creating staging view with parsed AbsType data\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW staging_absence AS\n",
					"    SELECT  \n",
					"        StaffNumber,\n",
					"        COALESCE(AbsType, '') AS AbsType,\n",
					"        COALESCE(SicknessGroup, '') AS SicknessGroup,\n",
					"        CAST(TO_DATE(StartDate,'dd/MM/yyyy') AS DATE) AS StartDate,\n",
					"        CAST(TO_DATE(EndDate,'dd/MM/yyyy') AS DATE) AS EndDate,\n",
					"        AttendanceorAbsenceType,\n",
					"        REPLACE(Days, ',', '') AS Days,\n",
					"        REPLACE(Hrs, ',', '') AS Hrs,\n",
					"        TO_DATE('31/12/1899','dd/MM/yyyy') AS Start,\n",
					"        TO_DATE('31/12/1899','dd/MM/yyyy') AS Endtime,  \n",
					"        Caldays,\n",
					"        WorkScheduleRule,\n",
					"        TRY_CAST(REPLACE(Wkhrs, ',', '') AS DOUBLE) AS Wkhrs,  \n",
					"        TRY_CAST(REPLACE(HrsDay, ',', '') AS DOUBLE) AS HrsDay,  \n",
					"        TRY_CAST(REPLACE(WkDys, ',', '') AS DOUBLE) AS WkDys,  \n",
					"        TO_DATE(AnnualLeaveStart,'dd/MM/yyyy') AS AnnualLeaveStart,\n",
					"        'saphr' AS SourceSystemID,\n",
					"        CURRENT_DATE() AS IngestionDate,\n",
					"        CURRENT_TIMESTAMP() AS ValidTo,\n",
					"        \n",
					"        -- Extract status from AbsType (everything before first hyphen)\n",
					"        CASE \n",
					"            WHEN AbsType LIKE '%-%' THEN TRIM(SPLIT(AbsType, '-')[0])\n",
					"            ELSE 'UNKNOWN'\n",
					"        END AS ApprovalStatus,\n",
					"        \n",
					"        -- Extract UUID (last part after final hyphen, assuming 30+ characters)\n",
					"        CASE \n",
					"            WHEN AbsType RLIKE '.*-([a-f0-9]{30,})$' THEN \n",
					"                REGEXP_EXTRACT(AbsType, '.*-([a-f0-9]{30,})$', 1)\n",
					"            ELSE NULL\n",
					"        END AS RecordUUID,\n",
					"        \n",
					"        -- Extract last modified datetime\n",
					"        CASE \n",
					"            WHEN AbsType RLIKE '^[A-Z_]+-[0-9]{4}-[0-9]{2}-[0-9]{2}' THEN\n",
					"                TRY_CAST(\n",
					"                    REGEXP_EXTRACT(AbsType, '^[A-Z_]+-([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2})', 1)\n",
					"                    AS TIMESTAMP\n",
					"                )\n",
					"            ELSE CURRENT_TIMESTAMP()\n",
					"        END AS LastModifiedDate,\n",
					"        \n",
					"        -- Create row hash for change detection\n",
					"        MD5(CONCAT_WS('|',\n",
					"            StaffNumber,            \n",
					"            COALESCE(AbsType, ''),                \n",
					"            COALESCE(SicknessGroup, ''),          \n",
					"            TO_DATE(StartDate,'dd/MM/yyyy'),              \n",
					"            TO_DATE(EndDate,'dd/MM/yyyy'),                \n",
					"            AttendanceorAbsenceType,\n",
					"            REPLACE(Days, ',', ''),                   \n",
					"            REPLACE(Hrs, ',', ''),                    \n",
					"            Caldays,                \n",
					"            WorkScheduleRule,       \n",
					"            REPLACE(Wkhrs, ',', ''),                  \n",
					"            REPLACE(HrsDay, ',', ''),                 \n",
					"            REPLACE(WkDys, ',', '')\n",
					"        )) AS RowID,\n",
					"        'Y' AS IsActive\n",
					"        \n",
					"    FROM odw_standardised_db.hr_absence_monthly\n",
					"    WHERE \n",
					"        StaffNumber IS NOT NULL \n",
					"        AND StartDate IS NOT NULL \n",
					"        AND EndDate IS NOT NULL\n",
					"        AND AbsType IS NOT NULL\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step 2: Create view of latest records per UUID (handling duplicates)\n",
					"    logInfo(\"Creating view of latest records per UUID\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW latest_staging_records AS\n",
					"    SELECT *\n",
					"    FROM (\n",
					"        SELECT *,\n",
					"            ROW_NUMBER() OVER (\n",
					"                PARTITION BY RecordUUID \n",
					"                ORDER BY LastModifiedDate DESC, IngestionDate DESC\n",
					"            ) AS rn\n",
					"        FROM staging_absence\n",
					"        WHERE RecordUUID IS NOT NULL\n",
					"    ) ranked\n",
					"    WHERE rn = 1\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step 3: Create a snapshot of current target table\n",
					"    logInfo(\"Creating snapshot of current target table\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW current_target AS\n",
					"    SELECT * FROM odw_harmonised_db.sap_hr_absence_all\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step 4: Identify outdated records\n",
					"    logInfo(\"Identifying outdated records to replace\")\n",
					"    records_to_update = spark.sql(\"\"\"\n",
					"    SELECT DISTINCT t.RowID as old_rowid\n",
					"    FROM current_target t\n",
					"    INNER JOIN latest_staging_records s\n",
					"    ON t.RowID = s.RowID \n",
					"    WHERE s.LastModifiedDate > t.ValidTo\n",
					"    \"\"\")\n",
					"    \n",
					"    deleted_count = records_to_update.count()\n",
					"    logInfo(f\"Found {deleted_count} outdated records to replace\")\n",
					"    \n",
					"    # Step 5: Create view of records to keep (excluding outdated ones)\n",
					"    if deleted_count > 0:\n",
					"        old_rowids_list = [f\"'{row.old_rowid}'\" for row in records_to_update.collect()]\n",
					"        old_rowids_str = \",\".join(old_rowids_list)\n",
					"        \n",
					"        logInfo(f\"Creating view excluding {len(old_rowids_list)} outdated records\")\n",
					"        \n",
					"        spark.sql(f\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW records_to_keep AS\n",
					"        SELECT * FROM current_target\n",
					"        WHERE RowID NOT IN ({old_rowids_str})\n",
					"        \"\"\")\n",
					"    else:\n",
					"        logInfo(\"No outdated records found - keeping all current records\")\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW records_to_keep AS\n",
					"        SELECT * FROM current_target\n",
					"        \"\"\")\n",
					"    \n",
					"    # Step 6: Get new records from staging (those not in current target)\n",
					"    logInfo(\"Identifying new records to insert\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW new_records AS\n",
					"    SELECT \n",
					"        StaffNumber, AbsType, SicknessGroup, StartDate, EndDate,\n",
					"        AttendanceorAbsenceType, Days, Hrs, Start, Endtime,\n",
					"        Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys,\n",
					"        AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo,\n",
					"        RowID, IsActive\n",
					"    FROM latest_staging_records s\n",
					"    WHERE NOT EXISTS (\n",
					"        SELECT 1 FROM current_target t\n",
					"        WHERE t.RowID = s.RowID\n",
					"    )\n",
					"    \"\"\")\n",
					"    \n",
					"    new_records_count = spark.sql(\"SELECT COUNT(*) as count FROM new_records\").collect()[0]['count']\n",
					"    logInfo(f\"Found {new_records_count} new records to add\")\n",
					"    \n",
					"    # Step 7: Create final dataset combining kept records + new records\n",
					"    logInfo(\"Creating final dataset combining existing and new records\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW final_dataset AS\n",
					"    SELECT * FROM records_to_keep\n",
					"    UNION ALL\n",
					"    SELECT * FROM new_records\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step 8: Remove cancelled/rejected records from final dataset\n",
					"    logInfo(\"Filtering out cancelled/rejected records\")\n",
					"    \n",
					"    total_before_filter = spark.sql(\"SELECT COUNT(*) as count FROM final_dataset\").collect()[0]['count']\n",
					"    \n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW final_approved_dataset AS\n",
					"    SELECT * FROM final_dataset\n",
					"    WHERE NOT (\n",
					"        UPPER(CASE \n",
					"            WHEN AbsType LIKE '%-%' THEN TRIM(SPLIT(AbsType, '-')[0])\n",
					"            ELSE 'UNKNOWN'\n",
					"        END) IN ('CANCELLED', 'PENDING_CANCELLATION', 'REJECTED')\n",
					"    )\n",
					"    \"\"\")\n",
					"    \n",
					"    total_after_filter = spark.sql(\"SELECT COUNT(*) as count FROM final_approved_dataset\").collect()[0]['count']\n",
					"    cancelled_count = total_before_filter - total_after_filter\n",
					"    \n",
					"    logInfo(f\"Filtered out {cancelled_count} cancelled/rejected records\")\n",
					"    \n",
					"    # Step 9: TRUNCATE and INSERT - No overwrites!\n",
					"    logInfo(\"Refreshing target table with final approved dataset\")\n",
					"    \n",
					"    # Clear target table\n",
					"    spark.sql(\"DELETE FROM odw_harmonised_db.sap_hr_absence_all\")\n",
					"    logInfo(\"Truncated target table\")\n",
					"    \n",
					"    # Insert final approved dataset\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.sap_hr_absence_all (\n",
					"        StaffNumber, AbsType, SicknessGroup, StartDate, EndDate,\n",
					"        AttendanceorAbsenceType, Days, Hrs, Start, Endtime,\n",
					"        Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys,\n",
					"        AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo,\n",
					"        RowID, IsActive\n",
					"    )\n",
					"    SELECT \n",
					"        StaffNumber, AbsType, SicknessGroup, StartDate, EndDate,\n",
					"        AttendanceorAbsenceType, Days, Hrs, Start, Endtime,\n",
					"        Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys,\n",
					"        AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo,\n",
					"        RowID, IsActive\n",
					"    FROM final_approved_dataset\n",
					"    \"\"\")\n",
					"    \n",
					"    final_record_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.sap_hr_absence_all\").collect()[0]['count']\n",
					"    logInfo(f\"Successfully loaded {final_record_count} total records into target table\")\n",
					"    \n",
					"    # Step 10: Calculate final counts for reporting\n",
					"    source_record_count = spark.sql(\"SELECT COUNT(*) as count FROM latest_staging_records\").collect()[0]['count']\n",
					"    \n",
					"    result[\"record_count\"] = final_record_count\n",
					"    result[\"inserted_count\"] = new_records_count\n",
					"    result[\"updated_count\"] = deleted_count  # Records that were replaced\n",
					"    result[\"deleted_count\"] = deleted_count\n",
					"    result[\"cancelled_deleted_count\"] = cancelled_count\n",
					"    \n",
					"    logInfo(f\"Incremental load completed successfully:\")\n",
					"    logInfo(f\"- Source records processed: {source_record_count}\")\n",
					"    logInfo(f\"- New records inserted: {new_records_count}\")\n",
					"    logInfo(f\"- Outdated records replaced: {deleted_count}\")\n",
					"    logInfo(f\"- Cancelled/rejected records removed: {cancelled_count}\")\n",
					"    logInfo(f\"- Total records in target: {final_record_count}\")\n",
					"    \n",
					"    # Clean up temporary views\n",
					"    spark.sql(\"DROP VIEW IF EXISTS staging_absence\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS latest_staging_records\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS current_target\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS records_to_keep\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS new_records\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS final_dataset\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS final_approved_dataset\")\n",
					"    \n",
					"    logInfo(\"Successfully completed incremental load process\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error in status-based incremental absence data processing: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    \n",
					"    truncated_error = (error_msg[:297] + '...') if len(error_msg) > 300 else error_msg\n",
					"    \n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = truncated_error\n",
					"    result[\"record_count\"] = -1\n",
					"    result[\"inserted_count\"] = -1\n",
					"    result[\"updated_count\"] = -1\n",
					"    result[\"deleted_count\"] = -1\n",
					"    result[\"cancelled_deleted_count\"] = -1\n",
					"    \n",
					"    # Clean up on error\n",
					"    try:\n",
					"        spark.sql(\"DROP VIEW IF EXISTS staging_absence\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS latest_staging_records\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS current_target\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS records_to_keep\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS new_records\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS final_dataset\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS final_approved_dataset\")\n",
					"    except:\n",
					"        pass\n",
					"    \n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"source": [
					"import json\n",
					"\n",
					"# Initialize result dictionary\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"inserted_count\": 0,\n",
					"    \"updated_count\": 0,\n",
					"    \"deleted_count\": 0,\n",
					"    \"cancelled_deleted_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"try:\n",
					"    # Set legacy time parser for compatibility\n",
					"    logInfo(\"Setting legacy time parser policy\")\n",
					"    spark.sql(\"\"\"SET spark.sql.legacy.timeParserPolicy = LEGACY\"\"\")\n",
					"    logInfo(\"Legacy time parser policy set successfully\")\n",
					"    \n",
					"    # Step 1: Create staging view with parsed AbsType data\n",
					"    logInfo(\"Creating staging view with parsed AbsType data\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW staging_absence AS\n",
					"    SELECT  \n",
					"        StaffNumber,\n",
					"        COALESCE(AbsType, '') AS AbsType,\n",
					"        COALESCE(SicknessGroup, '') AS SicknessGroup,\n",
					"        CAST(TO_DATE(StartDate,'dd/MM/yyyy') AS DATE) AS StartDate,\n",
					"        CAST(TO_DATE(EndDate,'dd/MM/yyyy') AS DATE) AS EndDate,\n",
					"        AttendanceorAbsenceType,\n",
					"        REPLACE(Days, ',', '') AS Days,\n",
					"        REPLACE(Hrs, ',', '') AS Hrs,\n",
					"        TO_DATE('31/12/1899','dd/MM/yyyy') AS Start,\n",
					"        TO_DATE('31/12/1899','dd/MM/yyyy') AS Endtime,  \n",
					"        Caldays,\n",
					"        WorkScheduleRule,\n",
					"        TRY_CAST(REPLACE(Wkhrs, ',', '') AS DOUBLE) AS Wkhrs,  \n",
					"        TRY_CAST(REPLACE(HrsDay, ',', '') AS DOUBLE) AS HrsDay,  \n",
					"        TRY_CAST(REPLACE(WkDys, ',', '') AS DOUBLE) AS WkDys,  \n",
					"        TO_DATE(AnnualLeaveStart,'dd/MM/yyyy') AS AnnualLeaveStart,\n",
					"        'saphr' AS SourceSystemID,\n",
					"        CURRENT_DATE() AS IngestionDate,\n",
					"        CURRENT_TIMESTAMP() AS ValidTo,\n",
					"        \n",
					"        CASE \n",
					"            WHEN AbsType LIKE '%-%' THEN TRIM(SPLIT(AbsType, '-')[0])\n",
					"            ELSE 'UNKNOWN'\n",
					"        END AS ApprovalStatus,\n",
					"        \n",
					"        CASE \n",
					"            WHEN AbsType RLIKE '^[A-Z_]+-[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}[.][0-9]+-(.+)$' THEN\n",
					"                REGEXP_EXTRACT(AbsType, '^[A-Z_]+-[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}[.][0-9]+-(.+)$', 1)\n",
					"            WHEN AbsType RLIKE '[a-f0-9]{32}' THEN\n",
					"                REGEXP_EXTRACT(AbsType, '([a-f0-9]{32})', 1)\n",
					"            WHEN AbsType RLIKE '.*-([A-Za-z0-9_]{4,})$' THEN\n",
					"                REGEXP_EXTRACT(AbsType, '.*-([A-Za-z0-9_]{4,})$', 1)\n",
					"            ELSE MD5(AbsType)\n",
					"        END AS RecordUUID,\n",
					"        \n",
					"        CASE \n",
					"            WHEN AbsType RLIKE '^[A-Z_]+-[0-9]{4}-[0-9]{2}-[0-9]{2}' THEN\n",
					"                TRY_CAST(\n",
					"                    REGEXP_EXTRACT(AbsType, '^[A-Z_]+-([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2})', 1)\n",
					"                    AS TIMESTAMP\n",
					"                )\n",
					"            ELSE CURRENT_TIMESTAMP()\n",
					"        END AS LastModifiedDate,\n",
					"        \n",
					"        MD5(CONCAT_WS('|',\n",
					"            StaffNumber,            \n",
					"            COALESCE(AbsType, ''),                \n",
					"            COALESCE(SicknessGroup, ''),          \n",
					"            TO_DATE(StartDate,'dd/MM/yyyy'),              \n",
					"            TO_DATE(EndDate,'dd/MM/yyyy'),                \n",
					"            AttendanceorAbsenceType,\n",
					"            REPLACE(Days, ',', ''),                   \n",
					"            REPLACE(Hrs, ',', ''),                    \n",
					"            Caldays,                \n",
					"            WorkScheduleRule,       \n",
					"            REPLACE(Wkhrs, ',', ''),                  \n",
					"            REPLACE(HrsDay, ',', ''),                 \n",
					"            REPLACE(WkDys, ',', '')\n",
					"        )) AS RowID,\n",
					"        'Y' AS IsActive\n",
					"        \n",
					"    FROM odw_standardised_db.hr_absence_monthly\n",
					"    WHERE \n",
					"        StaffNumber IS NOT NULL \n",
					"        AND StartDate IS NOT NULL \n",
					"        AND EndDate IS NOT NULL\n",
					"        AND AbsType IS NOT NULL\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step 2: Create view of latest records per UUID (handling duplicates)\n",
					"    logInfo(\"Creating view of latest records per UUID\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW latest_staging_records AS\n",
					"    SELECT *\n",
					"    FROM (\n",
					"        SELECT *,\n",
					"            ROW_NUMBER() OVER (\n",
					"                PARTITION BY RecordUUID \n",
					"                ORDER BY LastModifiedDate DESC, IngestionDate DESC\n",
					"            ) AS rn\n",
					"        FROM staging_absence\n",
					"        WHERE RecordUUID IS NOT NULL\n",
					"    ) ranked\n",
					"    WHERE rn = 1\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step 3: Create a snapshot of current target table\n",
					"    logInfo(\"Creating snapshot of current target table\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW current_target AS\n",
					"    SELECT * FROM odw_harmonised_db.sap_hr_absence_all\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step 4: Identify outdated records\n",
					"    logInfo(\"Identifying outdated records to replace\")\n",
					"    records_to_update = spark.sql(\"\"\"\n",
					"    SELECT DISTINCT t.RowID as old_rowid\n",
					"    FROM current_target t\n",
					"    INNER JOIN latest_staging_records s\n",
					"    ON t.RowID = s.RowID \n",
					"    WHERE s.LastModifiedDate > t.ValidTo\n",
					"    \"\"\")\n",
					"    \n",
					"    deleted_count = records_to_update.count()\n",
					"    logInfo(f\"Found {deleted_count} outdated records to replace\")\n",
					"    \n",
					"    # Step 5: Create view of records to keep (excluding outdated ones)\n",
					"    if deleted_count > 0:\n",
					"        old_rowids_list = [f\"'{row.old_rowid}'\" for row in records_to_update.collect()]\n",
					"        old_rowids_str = \",\".join(old_rowids_list)\n",
					"        \n",
					"        logInfo(f\"Creating view excluding {len(old_rowids_list)} outdated records\")\n",
					"        \n",
					"        spark.sql(f\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW records_to_keep AS\n",
					"        SELECT * FROM current_target\n",
					"        WHERE RowID NOT IN ({old_rowids_str})\n",
					"        \"\"\")\n",
					"    else:\n",
					"        logInfo(\"No outdated records found - keeping all current records\")\n",
					"        spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW records_to_keep AS\n",
					"        SELECT * FROM current_target\n",
					"        \"\"\")\n",
					"    \n",
					"    # Step 6: Get new records from staging (those not in current target)\n",
					"    logInfo(\"Identifying new records to insert\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW new_records AS\n",
					"    SELECT \n",
					"        StaffNumber, AbsType, SicknessGroup, StartDate, EndDate,\n",
					"        AttendanceorAbsenceType, Days, Hrs, Start, Endtime,\n",
					"        Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys,\n",
					"        AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo,\n",
					"        RowID, IsActive\n",
					"    FROM latest_staging_records s\n",
					"    WHERE NOT EXISTS (\n",
					"        SELECT 1 FROM current_target t\n",
					"        WHERE t.RowID = s.RowID\n",
					"    )\n",
					"    \"\"\")\n",
					"    \n",
					"    new_records_count = spark.sql(\"SELECT COUNT(*) as count FROM new_records\").collect()[0]['count']\n",
					"    logInfo(f\"Found {new_records_count} new records to add\")\n",
					"    \n",
					"    # Step 7: Create final dataset combining kept records + new records\n",
					"    logInfo(\"Creating final dataset combining existing and new records\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW final_dataset AS\n",
					"    SELECT * FROM records_to_keep\n",
					"    UNION ALL\n",
					"    SELECT * FROM new_records\n",
					"    \"\"\")\n",
					"    \n",
					"    # Step 8: Remove cancelled/rejected records from final dataset\n",
					"    logInfo(\"Filtering out cancelled/rejected records\")\n",
					"    \n",
					"    total_before_filter = spark.sql(\"SELECT COUNT(*) as count FROM final_dataset\").collect()[0]['count']\n",
					"    \n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW final_approved_dataset AS\n",
					"    SELECT * FROM final_dataset\n",
					"    WHERE NOT (\n",
					"        UPPER(CASE \n",
					"            WHEN AbsType LIKE '%-%' THEN TRIM(SPLIT(AbsType, '-')[0])\n",
					"            ELSE 'UNKNOWN'\n",
					"        END) IN ('CANCELLED', 'PENDING_CANCELLATION', 'REJECTED')\n",
					"    )\n",
					"    \"\"\")\n",
					"    \n",
					"    total_after_filter = spark.sql(\"SELECT COUNT(*) as count FROM final_approved_dataset\").collect()[0]['count']\n",
					"    cancelled_count = total_before_filter - total_after_filter\n",
					"    \n",
					"    logInfo(f\"Filtered out {cancelled_count} cancelled/rejected records\")\n",
					"    \n",
					"    # Step 9: TRUNCATE and INSERT - No overwrites!\n",
					"    logInfo(\"Refreshing target table with final approved dataset\")\n",
					"    \n",
					"    # Clear target table\n",
					"    spark.sql(\"DELETE FROM odw_harmonised_db.sap_hr_absence_all\")\n",
					"    logInfo(\"Truncated target table\")\n",
					"    \n",
					"    # Insert final approved dataset\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.sap_hr_absence_all (\n",
					"        StaffNumber, AbsType, SicknessGroup, StartDate, EndDate,\n",
					"        AttendanceorAbsenceType, Days, Hrs, Start, Endtime,\n",
					"        Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys,\n",
					"        AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo,\n",
					"        RowID, IsActive\n",
					"    )\n",
					"    SELECT \n",
					"        StaffNumber, AbsType, SicknessGroup, StartDate, EndDate,\n",
					"        AttendanceorAbsenceType, Days, Hrs, Start, Endtime,\n",
					"        Caldays, WorkScheduleRule, Wkhrs, HrsDay, WkDys,\n",
					"        AnnualLeaveStart, SourceSystemID, IngestionDate, ValidTo,\n",
					"        RowID, IsActive\n",
					"    FROM final_approved_dataset\n",
					"    \"\"\")\n",
					"    \n",
					"    final_record_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.sap_hr_absence_all\").collect()[0]['count']\n",
					"    logInfo(f\"Successfully loaded {final_record_count} total records into target table\")\n",
					"    \n",
					"    # Step 10: Calculate final counts for reporting\n",
					"    source_record_count = spark.sql(\"SELECT COUNT(*) as count FROM latest_staging_records\").collect()[0]['count']\n",
					"    \n",
					"    result[\"record_count\"] = final_record_count\n",
					"    result[\"inserted_count\"] = new_records_count\n",
					"    result[\"updated_count\"] = deleted_count  # Records that were replaced\n",
					"    result[\"deleted_count\"] = deleted_count\n",
					"    result[\"cancelled_deleted_count\"] = cancelled_count\n",
					"    \n",
					"    logInfo(f\"Incremental load completed successfully:\")\n",
					"    logInfo(f\"- Source records processed: {source_record_count}\")\n",
					"    logInfo(f\"- New records inserted: {new_records_count}\")\n",
					"    logInfo(f\"- Outdated records replaced: {deleted_count}\")\n",
					"    logInfo(f\"- Cancelled/rejected records removed: {cancelled_count}\")\n",
					"    logInfo(f\"- Total records in target: {final_record_count}\")\n",
					"    \n",
					"    # Clean up temporary views\n",
					"    spark.sql(\"DROP VIEW IF EXISTS staging_absence\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS latest_staging_records\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS current_target\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS records_to_keep\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS new_records\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS final_dataset\")\n",
					"    spark.sql(\"DROP VIEW IF EXISTS final_approved_dataset\")\n",
					"    \n",
					"    logInfo(\"Successfully completed incremental load process\")\n",
					"\n",
					"except Exception as e:\n",
					"    error_msg = f\"Error in status-based incremental absence data processing: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logException(e)\n",
					"    \n",
					"    truncated_error = (error_msg[:297] + '...') if len(error_msg) > 300 else error_msg\n",
					"    \n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = truncated_error\n",
					"    result[\"record_count\"] = -1\n",
					"    result[\"inserted_count\"] = -1\n",
					"    result[\"updated_count\"] = -1\n",
					"    result[\"deleted_count\"] = -1\n",
					"    result[\"cancelled_deleted_count\"] = -1\n",
					"    \n",
					"    # Clean up on error\n",
					"    try:\n",
					"        spark.sql(\"DROP VIEW IF EXISTS staging_absence\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS latest_staging_records\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS current_target\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS records_to_keep\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS new_records\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS final_dataset\")\n",
					"        spark.sql(\"DROP VIEW IF EXISTS final_approved_dataset\")\n",
					"    except:\n",
					"        pass\n",
					"    \n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					" SELECT count(*) FROM odw_harmonised_db.sap_hr_absence_all"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"   %%sql\n",
					"\n",
					" SELECT count(*) FROM odw_standardised_db.hr_absence_monthly"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"  %%sql\n",
					"\n",
					"SELECT COUNT(*) as total_source_records\n",
					"FROM odw_standardised_db.hr_absence_monthly\n",
					"WHERE StaffNumber IS NOT NULL \n",
					"  AND StartDate IS NOT NULL \n",
					"  AND EndDate IS NOT NULL\n",
					"  AND AbsType IS NOT NULL;"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"SELECT \n",
					"    CASE \n",
					"        WHEN AbsType LIKE '%-%' THEN TRIM(SPLIT(AbsType, '-')[0])\n",
					"        ELSE 'UNKNOWN'\n",
					"    END AS approval_status,\n",
					"    COUNT(*) as record_count\n",
					"FROM odw_standardised_db.hr_absence_monthly\n",
					"WHERE StaffNumber IS NOT NULL \n",
					"  AND StartDate IS NOT NULL \n",
					"  AND EndDate IS NOT NULL\n",
					"  AND AbsType IS NOT NULL\n",
					"GROUP BY CASE \n",
					"    WHEN AbsType LIKE '%-%' THEN TRIM(SPLIT(AbsType, '-')[0])\n",
					"    ELSE 'UNKNOWN'\n",
					"END\n",
					"ORDER BY record_count DESC;"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"WITH uuid_counts AS (\n",
					"    SELECT \n",
					"        CASE \n",
					"            WHEN AbsType RLIKE '.*-([a-f0-9]{30,})$' THEN \n",
					"                REGEXP_EXTRACT(AbsType, '.*-([a-f0-9]{30,})$', 1)\n",
					"            ELSE NULL\n",
					"        END AS RecordUUID,\n",
					"        COUNT(*) as uuid_count\n",
					"    FROM odw_standardised_db.hr_absence_monthly\n",
					"    WHERE StaffNumber IS NOT NULL \n",
					"      AND StartDate IS NOT NULL \n",
					"      AND EndDate IS NOT NULL\n",
					"      AND AbsType IS NOT NULL\n",
					"      AND AbsType RLIKE '.*-([a-f0-9]{30,})$'\n",
					"    GROUP BY CASE \n",
					"        WHEN AbsType RLIKE '.*-([a-f0-9]{30,})$' THEN \n",
					"            REGEXP_EXTRACT(AbsType, '.*-([a-f0-9]{30,})$', 1)\n",
					"        ELSE NULL\n",
					"    END\n",
					")\n",
					"SELECT \n",
					"    'Total UUIDs' as metric,\n",
					"    COUNT(*) as count\n",
					"FROM uuid_counts\n",
					"UNION ALL\n",
					"SELECT \n",
					"    'Duplicate UUIDs' as metric,\n",
					"    COUNT(*) as count\n",
					"FROM uuid_counts \n",
					"WHERE uuid_count > 1\n",
					"UNION ALL\n",
					"SELECT \n",
					"    'Total duplicate records' as metric,\n",
					"    SUM(uuid_count - 1) as count\n",
					"FROM uuid_counts \n",
					"WHERE uuid_count > 1;"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"SELECT COUNT(*) as final_target_records\n",
					"FROM odw_harmonised_db.sap_hr_absence_all;\n",
					"\n",
					"-- Query 5: Breakdown of what was filtered out\n",
					"SELECT \n",
					"    'APPROVED' as status_category,\n",
					"    COUNT(*) as record_count\n",
					"FROM odw_standardised_db.hr_absence_monthly\n",
					"WHERE UPPER(CASE \n",
					"    WHEN AbsType LIKE '%-%' THEN TRIM(SPLIT(AbsType, '-')[0])\n",
					"    ELSE 'UNKNOWN'\n",
					"END) = 'APPROVED'\n",
					"  AND StaffNumber IS NOT NULL \n",
					"  AND StartDate IS NOT NULL \n",
					"  AND EndDate IS NOT NULL\n",
					"  AND AbsType IS NOT NULL\n",
					"\n",
					"UNION ALL\n",
					"\n",
					"SELECT \n",
					"    'CANCELLED/REJECTED' as status_category,\n",
					"    COUNT(*) as record_count\n",
					"FROM odw_standardised_db.hr_absence_monthly\n",
					"WHERE UPPER(CASE \n",
					"    WHEN AbsType LIKE '%-%' THEN TRIM(SPLIT(AbsType, '-')[0])\n",
					"    ELSE 'UNKNOWN'\n",
					"END) IN ('CANCELLED', 'PENDING_CANCELLATION', 'REJECTED')\n",
					"  AND StaffNumber IS NOT NULL \n",
					"  AND StartDate IS NOT NULL \n",
					"  AND EndDate IS NOT NULL\n",
					"  AND AbsType IS NOT NULL\n",
					"\n",
					"UNION ALL\n",
					"\n",
					"SELECT \n",
					"    'OTHER STATUSES' as status_category,\n",
					"    COUNT(*) as record_count\n",
					"FROM odw_standardised_db.hr_absence_monthly\n",
					"WHERE UPPER(CASE \n",
					"    WHEN AbsType LIKE '%-%' THEN TRIM(SPLIT(AbsType, '-')[0])\n",
					"    ELSE 'UNKNOWN'\n",
					"END) NOT IN ('APPROVED', 'CANCELLED', 'PENDING_CANCELLATION', 'REJECTED')\n",
					"  AND StaffNumber IS NOT NULL \n",
					"  AND StartDate IS NOT NULL \n",
					"  AND EndDate IS NOT NULL\n",
					"  AND AbsType IS NOT NULL;"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"   \n",
					"    CREATE OR REPLACE TEMPORARY VIEW staging_analysis AS\n",
					"SELECT  \n",
					"    StaffNumber,\n",
					"    COALESCE(AbsType, '') AS AbsType,\n",
					"    COALESCE(SicknessGroup, '') AS SicknessGroup,\n",
					"    CAST(TO_DATE(StartDate,'dd/MM/yyyy') AS DATE) AS StartDate,\n",
					"    CAST(TO_DATE(EndDate,'dd/MM/yyyy') AS DATE) AS EndDate,\n",
					"    AttendanceorAbsenceType,\n",
					"    \n",
					"    -- Extract status from AbsType (everything before first hyphen)\n",
					"    CASE \n",
					"        WHEN AbsType LIKE '%-%' THEN TRIM(SPLIT(AbsType, '-')[0])\n",
					"        ELSE 'UNKNOWN'\n",
					"    END AS ApprovalStatus,\n",
					"    \n",
					"    -- Extract UUID (last part after final hyphen, assuming 30+ characters)\n",
					"    CASE \n",
					"        WHEN AbsType RLIKE '.*-([a-f0-9]{30,})$' THEN \n",
					"            REGEXP_EXTRACT(AbsType, '.*-([a-f0-9]{30,})$', 1)\n",
					"        ELSE NULL\n",
					"    END AS RecordUUID,\n",
					"    \n",
					"    -- Extract last modified datetime\n",
					"    CASE \n",
					"        WHEN AbsType RLIKE '^[A-Z_]+-[0-9]{4}-[0-9]{2}-[0-9]{2}' THEN\n",
					"            TRY_CAST(\n",
					"                REGEXP_EXTRACT(AbsType, '^[A-Z_]+-([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2})', 1)\n",
					"                AS TIMESTAMP\n",
					"            )\n",
					"        ELSE CURRENT_TIMESTAMP()\n",
					"    END AS LastModifiedDate,\n",
					"    \n",
					"    -- Original AbsType for debugging\n",
					"    AbsType as Original_AbsType\n",
					"    \n",
					"FROM odw_standardised_db.hr_absence_monthly\n",
					"WHERE \n",
					"    StaffNumber IS NOT NULL \n",
					"    AND StartDate IS NOT NULL \n",
					"    AND EndDate IS NOT NULL\n",
					"    AND AbsType IS NOT NULL;"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"SELECT 'NULL UUID Records' as filter_step, COUNT(*) as record_count\n",
					"FROM staging_analysis\n",
					"WHERE RecordUUID IS NULL;"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"SELECT \n",
					"    'Sample AbsType causing NULL UUID' as analysis_type,\n",
					"    AbsType,\n",
					"    LENGTH(AbsType) as abstype_length,\n",
					"    COUNT(*) as record_count\n",
					"FROM odw_standardised_db.hr_absence_monthly\n",
					"WHERE AbsType IS NOT NULL\n",
					"  AND NOT (AbsType RLIKE '.*-([a-f0-9]{30,})$')\n",
					"GROUP BY AbsType, LENGTH(AbsType)\n",
					"ORDER BY record_count DESC\n",
					"LIMIT 20;"
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"SELECT \n",
					"    'Sample AbsType with valid UUID' as analysis_type,\n",
					"    AbsType,\n",
					"    LENGTH(AbsType) as abstype_length,\n",
					"    REGEXP_EXTRACT(AbsType, '.*-([a-f0-9]{30,})$', 1) as extracted_uuid,\n",
					"    LENGTH(REGEXP_EXTRACT(AbsType, '.*-([a-f0-9]{30,})$', 1)) as uuid_length,\n",
					"    COUNT(*) as record_count\n",
					"FROM odw_standardised_db.hr_absence_monthly\n",
					"WHERE AbsType IS NOT NULL\n",
					"  AND AbsType RLIKE '.*-([a-f0-9]{30,})$'\n",
					"GROUP BY AbsType, LENGTH(AbsType), REGEXP_EXTRACT(AbsType, '.*-([a-f0-9]{30,})$', 1)\n",
					"ORDER BY record_count DESC\n",
					"LIMIT 10;"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"SELECT \n",
					"    'UUID Length Distribution' as analysis_type,\n",
					"    LENGTH(REGEXP_EXTRACT(AbsType, '.*-([a-f0-9]{30,})$', 1)) as uuid_length,\n",
					"    COUNT(*) as record_count\n",
					"FROM odw_standardised_db.hr_absence_monthly\n",
					"WHERE AbsType IS NOT NULL\n",
					"  AND AbsType RLIKE '.*-([a-f0-9]{30,})$'\n",
					"GROUP BY LENGTH(REGEXP_EXTRACT(AbsType, '.*-([a-f0-9]{30,})$', 1))\n",
					"ORDER BY record_count DESC;"
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"SELECT \n",
					"    'Alternative UUID Patterns' as analysis_type,\n",
					"    -- Check for UUIDs anywhere in the string (not just at end)\n",
					"    SUM(CASE WHEN AbsType RLIKE '[a-f0-9]{30,}' THEN 1 ELSE 0 END) as uuid_anywhere,\n",
					"    \n",
					"    -- Check for standard UUID format (8-4-4-4-12)\n",
					"    SUM(CASE WHEN AbsType RLIKE '[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}' THEN 1 ELSE 0 END) as standard_uuid_format,\n",
					"    \n",
					"    -- Check for continuous hex strings of different lengths\n",
					"    SUM(CASE WHEN AbsType RLIKE '[a-f0-9]{32}' THEN 1 ELSE 0 END) as hex_32_chars,\n",
					"    SUM(CASE WHEN AbsType RLIKE '[a-f0-9]{36}' THEN 1 ELSE 0 END) as hex_36_chars,\n",
					"    \n",
					"    COUNT(*) as total_records\n",
					"FROM odw_standardised_db.hr_absence_monthly\n",
					"WHERE AbsType IS NOT NULL;"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"SELECT \n",
					"    'Problematic AbsType Patterns' as analysis_type,\n",
					"    SUBSTRING(AbsType, 1, 50) as abstype_start,\n",
					"    SUBSTRING(AbsType, -50) as abstype_end,\n",
					"    LENGTH(AbsType) as total_length,\n",
					"    COUNT(*) as record_count\n",
					"FROM odw_standardised_db.hr_absence_monthly\n",
					"WHERE AbsType IS NOT NULL\n",
					"  AND NOT (AbsType RLIKE '.*-([a-f0-9]{30,})$')\n",
					"GROUP BY SUBSTRING(AbsType, 1, 50), SUBSTRING(AbsType, -50), LENGTH(AbsType)\n",
					"ORDER BY record_count DESC\n",
					"LIMIT 15;"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW staging_absence_fixed AS\n",
					"SELECT  \n",
					"    StaffNumber,\n",
					"    COALESCE(AbsType, '') AS AbsType,\n",
					"    COALESCE(SicknessGroup, '') AS SicknessGroup,\n",
					"    CAST(TO_DATE(StartDate,'dd/MM/yyyy') AS DATE) AS StartDate,\n",
					"    CAST(TO_DATE(EndDate,'dd/MM/yyyy') AS DATE) AS EndDate,\n",
					"    AttendanceorAbsenceType,\n",
					"    REPLACE(Days, ',', '') AS Days,\n",
					"    REPLACE(Hrs, ',', '') AS Hrs,\n",
					"    TO_DATE('31/12/1899','dd/MM/yyyy') AS Start,\n",
					"    TO_DATE('31/12/1899','dd/MM/yyyy') AS Endtime,  \n",
					"    Caldays,\n",
					"    WorkScheduleRule,\n",
					"    TRY_CAST(REPLACE(Wkhrs, ',', '') AS DOUBLE) AS Wkhrs,  \n",
					"    TRY_CAST(REPLACE(HrsDay, ',', '') AS DOUBLE) AS HrsDay,  \n",
					"    TRY_CAST(REPLACE(WkDys, ',', '') AS DOUBLE) AS WkDys,  \n",
					"    TO_DATE(AnnualLeaveStart,'dd/MM/yyyy') AS AnnualLeaveStart,\n",
					"    'saphr' AS SourceSystemID,\n",
					"    CURRENT_DATE() AS IngestionDate,\n",
					"    CURRENT_TIMESTAMP() AS ValidTo,\n",
					"    \n",
					"    -- Extract status from AbsType (everything before first hyphen)\n",
					"    CASE \n",
					"        WHEN AbsType LIKE '%-%' THEN TRIM(SPLIT(AbsType, '-')[0])\n",
					"        ELSE 'UNKNOWN'\n",
					"    END AS ApprovalStatus,\n",
					"    \n",
					"    -- FIXED: Extract UUID/ID from the pattern after timestamp\n",
					"    -- Pattern: STATUS-YYYY-MM-DD HH:MM:SS.sssssss-IDENTIFIER\n",
					"    CASE \n",
					"        -- First try: Extract anything after the timestamp pattern (most common)\n",
					"        WHEN AbsType RLIKE '^[A-Z_]+-[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}\\\\.[0-9]+-(.+)$' THEN\n",
					"            REGEXP_EXTRACT(AbsType, '^[A-Z_]+-[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}\\\\.[0-9]+-(.+)$', 1)\n",
					"        \n",
					"        -- Second try: Extract 32-character hex strings anywhere in the string\n",
					"        WHEN AbsType RLIKE '[a-f0-9]{32}' THEN\n",
					"            REGEXP_EXTRACT(AbsType, '([a-f0-9]{32})', 1)\n",
					"        \n",
					"        -- Third try: Extract any identifier after last hyphen (fallback)\n",
					"        WHEN AbsType RLIKE '.*-([A-Za-z0-9_]{4,})$' THEN\n",
					"            REGEXP_EXTRACT(AbsType, '.*-([A-Za-z0-9_]{4,})$', 1)\n",
					"            \n",
					"        -- Final fallback: Use MD5 hash of entire AbsType as unique identifier\n",
					"        ELSE MD5(AbsType)\n",
					"    END AS RecordUUID,\n",
					"    \n",
					"    -- Extract last modified datetime (same as before)\n",
					"    CASE \n",
					"        WHEN AbsType RLIKE '^[A-Z_]+-[0-9]{4}-[0-9]{2}-[0-9]{2}' THEN\n",
					"            TRY_CAST(\n",
					"                REGEXP_EXTRACT(AbsType, '^[A-Z_]+-([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2})', 1)\n",
					"                AS TIMESTAMP\n",
					"            )\n",
					"        ELSE CURRENT_TIMESTAMP()\n",
					"    END AS LastModifiedDate,\n",
					"    \n",
					"    -- Create row hash for change detection\n",
					"    MD5(CONCAT_WS('|',\n",
					"        StaffNumber,            \n",
					"        COALESCE(AbsType, ''),                \n",
					"        COALESCE(SicknessGroup, ''),          \n",
					"        TO_DATE(StartDate,'dd/MM/yyyy'),              \n",
					"        TO_DATE(EndDate,'dd/MM/yyyy'),                \n",
					"        AttendanceorAbsenceType,\n",
					"        REPLACE(Days, ',', ''),                   \n",
					"        REPLACE(Hrs, ',', ''),                    \n",
					"        Caldays,                \n",
					"        WorkScheduleRule,       \n",
					"        REPLACE(Wkhrs, ',', ''),                  \n",
					"        REPLACE(HrsDay, ',', ''),                 \n",
					"        REPLACE(WkDys, ',', '')\n",
					"    )) AS RowID,\n",
					"    'Y' AS IsActive\n",
					"    \n",
					"FROM odw_standardised_db.hr_absence_monthly\n",
					"WHERE \n",
					"    StaffNumber IS NOT NULL \n",
					"    AND StartDate IS NOT NULL \n",
					"    AND EndDate IS NOT NULL\n",
					"    AND AbsType IS NOT NULL;\n",
					"\n",
					"-- Test the fixed extraction\n",
					"SELECT \n",
					"    'Fixed UUID Extraction Results' as test_name,\n",
					"    COUNT(*) as total_records,\n",
					"    COUNT(CASE WHEN RecordUUID IS NOT NULL THEN 1 END) as records_with_uuid,\n",
					"    COUNT(CASE WHEN RecordUUID IS NULL THEN 1 END) as records_without_uuid,\n",
					"    ROUND(COUNT(CASE WHEN RecordUUID IS NOT NULL THEN 1 END) * 100.0 / COUNT(*), 2) as uuid_success_rate\n",
					"FROM staging_absence_fixed;\n",
					"\n",
					"-- Show sample of successfully extracted UUIDs\n",
					"SELECT \n",
					"    'Sample Fixed Extractions' as test_name,\n",
					"    ApprovalStatus,\n",
					"    SUBSTRING(AbsType, 1, 60) as abstype_sample,\n",
					"    RecordUUID,\n",
					"    LENGTH(RecordUUID) as uuid_length,\n",
					"    COUNT(*) as record_count\n",
					"FROM staging_absence_fixed\n",
					"WHERE RecordUUID IS NOT NULL\n",
					"GROUP BY ApprovalStatus, SUBSTRING(AbsType, 1, 60), RecordUUID, LENGTH(RecordUUID)\n",
					"ORDER BY record_count DESC\n",
					"LIMIT 10;"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}