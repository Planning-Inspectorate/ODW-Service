{
	"name": "zendesk_schema_validation",
	"properties": {
		"description": "this validates the multijson from export files by compared to a premade schmea which was crete from all the files and can be applied as a checking methid to any new json file which has to be added in the table. if the json does not respect the schema then it wont be ingested and will have to be investigated ",
		"folder": {
			"name": "odw-harmonised/Zendesk"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "7d1cc6bf-2b87-4637-9471-2cb27afaadb0"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from collections.abc import Mapping\r\n",
					"from itertools import chain\r\n",
					"from operator import add\r\n",
					"#ignore FutureWarning messages \r\n",
					"import warnings\r\n",
					"warnings.filterwarnings(\"ignore\", message=\"iteritems is deprecated\")\r\n",
					"from notebookutils import mssparkutils\r\n",
					"from pprint import pprint as pp\r\n",
					"import json\r\n",
					"import pyspark.sql.functions as F \r\n",
					"import pandas as pd\r\n",
					"from pyspark.sql.types import *"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run /utils/py_mount_storage"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from notebookutils import mssparkutils\r\n",
					"### mount the data lake storage in Synapse to the Synapse File Mount API\r\n",
					"jobId = mssparkutils.env.getJobId()\r\n",
					"storage_acc_name = spark.sparkContext.environment.get('dataLakeAccountName', 'get')\r\n",
					"mount_storage(path=\"abfss://odw-raw@\"+storage_acc_name+\".dfs.core.windows.net/ZenDesk/Export/\", mount_point=\"/zendesk_items\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from notebookutils import mssparkutils\r\n",
					"### mount the data lake storage in Synapse to the Synapse File Mount API\r\n",
					"jobId = mssparkutils.env.getJobId()\r\n",
					"storage_acc_name = spark.sparkContext.environment.get('dataLakeAccountName', 'get')\r\n",
					"mssparkutils.fs.unmount(\"/zendesk_schema\") \r\n",
					"mssparkutils.fs.mount( \r\n",
					"'abfss://odw-raw@'+storage_acc_name+'.dfs.core.windows.net/ZenDesk/Schema_Validation/',\r\n",
					"\"/zendesk_schema\", \r\n",
					"{\"linkedService\":\"ls_storage\"} \r\n",
					")   "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def read_schema():\r\n",
					"\r\n",
					"    with open(f\"/synfs/{jobId}/zendesk_schema/output_schema.json\", 'r') as schema:\r\n",
					"\r\n",
					"        schema_dict = json.load(schema)\r\n",
					"\r\n",
					"    return schema_dict\r\n",
					"schema_dict = read_schema()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import json\r\n",
					"import jsonschema\r\n",
					"from jsonschema import validate\r\n",
					"list_with_all_zendesk_dict = []\r\n",
					"def read_zendesk(schema_dict):\r\n",
					"    for i in range(2918):\r\n",
					"        if i == 2802:\r\n",
					"            i=i+1\r\n",
					"        else:\r\n",
					"            with open(f\"/synfs/{jobId}/zendesk_items/output_{i}.json\", \"r\",encoding='utf-8') as big_json:\r\n",
					"                zendesk_dict = json.load(big_json)\r\n",
					"                try:\r\n",
					"                    validate(instance=zendesk_dict, schema=schema_dict)\r\n",
					"                except jsonschema.exceptions.ValidationError as err:\r\n",
					"                    print(\"Schema could not validate file number:\")\r\n",
					"                    print(i)\r\n",
					"                list_with_all_zendesk_dict.append(zendesk_dict)\r\n",
					"    return list_with_all_zendesk_dict\r\n",
					"zendesk_dict = read_zendesk(schema_dict)"
				],
				"execution_count": null
			}
		]
	}
}