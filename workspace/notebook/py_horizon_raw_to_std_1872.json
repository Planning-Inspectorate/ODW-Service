{
	"name": "py_horizon_raw_to_std_1872",
	"properties": {
		"folder": {
			"name": "1-odw-raw-to-standardised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "09ca5bd8-13b5-4e96-9323-66b4f60ac1b3"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Prerequisites\n",
					"1. Make sure the new raw file's entry has been added to the Orchestration i.e `/infrastructure/configuration/data-lake/orchestration/orchestration.json`\n",
					"2. Make sure the standardised table's schema is present on the path specified in the entry added in step 1.\n",
					"3. Only if the raw file is huge (several GBs), the spark pool might need some upscaling. Hence the following cell"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%%configure -f\n",
					"{\n",
					"    \"conf\": {\n",
					"        \"spark.kryoserializer.buffer.max\": \"2047m\",\n",
					"        \"spark.driver.maxResultSize\": \"10g\",\n",
					"        \"spark.rpc.message.maxSize\": \"1280\"\n",
					"    }\n",
					"}"
				],
				"execution_count": 44
			},
			{
				"cell_type": "code",
				"source": [
					"%run /utils/py_mount_storage"
				],
				"execution_count": 45
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					" %run utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Initialise Application Insight Logging functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# %run utils/py_utils_common_logging_output"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_application_Insite"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Get the Storage Account"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import asyncio\n",
					"import nest_asyncio\n",
					"import tracemalloc\n",
					"tracemalloc.start()\n",
					"from pyspark.sql import SparkSession\n",
					"from notebookutils import mssparkutils\n",
					"import json\n",
					"import calendar\n",
					"from datetime import datetime, timedelta, date\n",
					"import pandas as pd\n",
					"import os\n",
					"import re\n",
					"from pyspark.sql.functions import col, lit, input_file_name, current_timestamp, sha2, concat,max as spark_max\n",
					"from pyspark.sql.types import StringType, DateType, TimestampType, IntegerType, FloatType, StructType, StructField\n",
					"from delta import DeltaTable"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Initialise the parameters"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"date_folder=''\n",
					"source_folder='Horizon' #need change\n",
					"specific_file='' # if not provided, it will ingest all files in the date_folder\n",
					"delete_existing_table=False\n",
					"dataAttribute = \"\"\n",
					"start_exec_time = datetime.now()\n",
					"insert_count = 0\n",
					"update_count = 0\n",
					"delete_count = 0\n",
					"error_message=''"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Initialise Logging and Tracking Metrics"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# This cell is generated from runtime parameters. Learn more: https://go.microsoft.com/fwlink/?linkid=2161015\n",
					"PipelineName = \"pln_horizon_to_odw_1872\"\n",
					"PipelineRunID = \"ae4d7397-72e7-4147-9152-e8711c6d6dab\"\n",
					"PipelineTriggerID = \"e41e5d48-981a-4c76-8f0e-7079be79686f\"\n",
					"PipelineTriggerName = \"e41e5d48-981a-4c76-8f0e-7079be79686f\"\n",
					"PipelineTriggerType = \"PipelineActivity\"\n",
					"PipelineTriggeredbyPipelineName = \"pln_all_horizon_data\"\n",
					"PipelineTriggeredbyPipelineRunID = \"7cd26982-eaab-4646-b3fd-afd12b4bee73\"\n",
					"instrumentation_key='a1609846-e819-4880-976a-de745a22203f'"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"spark = SparkSession.builder.getOrCreate()\n",
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"source_path: str = f\"abfss://odw-raw@{storage_account}{source_folder}/\"\n",
					"standardised_container = f\"abfss://odw-standardised@{storage_account}\"\n",
					"print(standardised_container)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Retrieve and Display Latest Modified Folder in Directory"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# List all items in the directory\n",
					"items = mssparkutils.fs.ls(source_path)\n",
					"# Filter for directories and get their names and modification times\n",
					"folders = [(item.name, item.modifyTime) for item in items if item.isDir]\n",
					"# Sort folders by modification time in descending order\n",
					"sorted_folders = sorted(folders, key=lambda x: x[1], reverse=True)\n",
					"# Get the name of the latest modified folder\n",
					"if sorted_folders:\n",
					"    latest_folder = sorted_folders[0][0]\n",
					"    source_path=f\"{source_path}{latest_folder}\"\n",
					"    print(f\"Latest modified folder: {latest_folder}\")\n",
					"    print(source_path)\n",
					"else:\n",
					"    print(\"No folders found in the specified directory.\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### List and Retrieve File Names from Directory\""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"files = mssparkutils.fs.ls(source_path)\n",
					"horizon_files = [file.name for file in files]\n",
					"horizon_files= horizon_files[:1]\n",
					"horizon_files\n",
					" # comment this during the full execution"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Create Delta Table from JSON Schema with Spark"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"@logging_to_appins\n",
					"def create_table_from_schema(jsonschema: str, db_name: str, table_name: str, target_container: str, target_folder: str, change_data_feed=False):\n",
					"   \n",
					"    spark = SparkSession.builder.getOrCreate()\n",
					"    target_loc = f\"{target_container}/{target_folder}\".rstrip(\"/\")\n",
					"\n",
					"    # Convert array fields to string in schema definition\n",
					"    schema_dict = json.loads(jsonschema)\n",
					"    for field in schema_dict.get(\"fields\", []):\n",
					"        if field.get(\"type\") == \"array\":\n",
					"            field[\"type\"] = \"string\"\n",
					"\n",
					"    # Create empty DataFrame with adjusted schema\n",
					"    schema = StructType.fromJson(schema_dict)\n",
					"    df = spark.createDataFrame([], schema)\n",
					"\n",
					"    # Verify database existence\n",
					"    if db_name not in [db.name for db in spark.catalog.listDatabases()]:\n",
					"        raise NameError(f\"Database '{db_name}' does not exist!\")\n",
					"\n",
					"    # Verify table existence\n",
					"    if table_name in [table.name for table in spark.catalog.listTables(db_name)]:\n",
					"        table_details = spark.sql(f\"DESCRIBE DETAIL {db_name}.{table_name}\").toPandas()\n",
					"        \n",
					"        if len(table_details) > 1:\n",
					"            raise logError(\"Multiple parquet files detected. Please investigate!\")\n",
					"        \n",
					"        existing_location = table_details['location'][0].rstrip(\"/\")\n",
					"        if existing_location == target_loc:\n",
					"            logInfo(f\"Table '{db_name}.{table_name}' already exists at correct location.\")\n",
					"        else:\n",
					"            raise logError(f\"Table exists but at a different location: {existing_location}. Expected: {target_loc}\")\n",
					"        return\n",
					"\n",
					"    # Create table if not exists\n",
					"    if not DeltaTable.isDeltaTable(spark, target_loc):\n",
					"        df.write.option(\"mergeSchema\", \"false\").format(\"delta\").save(target_loc)\n",
					"    \n",
					"    logInfo(f\"Creating table {db_name}.{table_name}\")\n",
					"    spark.sql(f\"CREATE TABLE {db_name}.{table_name} USING DELTA LOCATION '{target_loc}'\")\n",
					"    \n",
					"    if change_data_feed:\n",
					"        spark.sql(f\"ALTER TABLE {db_name}.{table_name} SET TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')\")\n",
					"    \n",
					"    logInfo(f\"Table '{db_name}.{table_name}' created successfully!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Ingest the data from the raw/source into the standardised table. \n",
					"If the table doesn't already exist, this will create the table first and ingest the data."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"@logging_to_appins\n",
					"def ingest_adhoc(storage_account, definition, folder_path, filename, expected_from, expected_to):\n",
					"    spark = SparkSession.builder.getOrCreate()\n",
					"    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
					"    ingestion_failure: bool = False\n",
					"    \n",
					"    standardised_container = f\"abfss://odw-standardised@{storage_account}\"\n",
					"    standardised_path = f\"{definition['Standardised_Path']}\" + \"/\" #need remove text\n",
					"    standardised_table_name = f\"{definition['Standardised_Table_Name']}\" #need remove text \n",
					"    source_filename_start = definition['Source_Filename_Start'] \n",
					"    process_name = mssparkutils.runtime.context['currentNotebookName']\n",
					"    \n",
					"    logging_container = f\"abfss://logging@{storage_account}\"\n",
					"    logging_table_name = 'tables_logs'\n",
					"    ingestion_log_table_location = logging_container + logging_table_name\n",
					"    \n",
					"    if 'Standardised_Table_Definition' in definition:\n",
					"        standardised_table_loc = f\"abfss://odw-config@{storage_account}\" + definition['Standardised_Table_Definition']\n",
					"        standardised_table_def_json = spark.read.text(standardised_table_loc, wholetext=True).first().value\n",
					"    else:\n",
					"        standardised_table_def_json = mssparkutils.notebook.run('/py_get_schema_from_url', 30, {'db_name': 'odw_standardised_db', 'entity_name': definition['Source_Frequency_Folder']})\n",
					"    \n",
					"    if not any([table.name.lower() == standardised_table_name.lower() for table in spark.catalog.listTables('odw_standardised_db')]):\n",
					"        create_table_from_schema(standardised_table_def_json, \"odw_standardised_db\", standardised_table_name, standardised_container, standardised_path + standardised_table_name)\n",
					"    \n",
					"    table_metadata = spark.sql(f\"DESCRIBE EXTENDED odw_standardised_db.{standardised_table_name}\")\n",
					"    data_format = table_metadata.filter(table_metadata.col_name == \"Provider\").collect()[0].data_type\n",
					"    if data_format == \"parquet\":\n",
					"        replace = spark.sql(f\"SELECT * FROM odw_standardised_db.{standardised_table_name}\")\n",
					"        replace.write.option(\"mergeSchema\", \"true\").format(\"delta\").mode(\"overwrite\").saveAsTable(f\"odw_standardised_db.{standardised_table_name}_new\")\n",
					"        spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.{standardised_table_name}\")\n",
					"        spark.sql(f\"ALTER TABLE odw_standardised_db.{standardised_table_name}_new RENAME TO odw_standardised_db.{standardised_table_name}\")\n",
					"    \n",
					"    try:\n",
					"        standardised_table_location = spark.sql(f\"DESCRIBE FORMATTED odw_standardised_db.{standardised_table_name}\") \\\n",
					"            .filter(\"col_name = 'Location'\") \\\n",
					"            .select(\"data_type\") \\\n",
					"            .collect()[0][0]\n",
					"    except:\n",
					"        standardised_table_location = standardised_container + standardised_path + standardised_table_name\n",
					"    \n",
					"    standardised_table_df = spark.read.format(\"delta\").load(standardised_table_location)\n",
					"    rows = standardised_table_df.filter((standardised_table_df.expected_from == expected_from) & (standardised_table_df.expected_to == expected_to)).count()\n",
					"    \n",
					"    last_load_date = standardised_table_df.agg(spark_max(\"ingested_datetime\")).collect()[0][0]\n",
					"\n",
					"    jobId = mssparkutils.env.getJobId()\n",
					"    \n",
					"    if '.csv' in filename.lower():\n",
					"        df = spark.read.options(quote='\"', escape='\\\\', encoding='utf8', header=True, multiLine=True, columnNameOfCorruptRecord='corrupted_records', mode=\"PERMISSIVE\").csv(f\"{folder_path}/{filename}\")\n",
					"        if \"corrupted_records\" in df.columns:\n",
					"            print(f\"Corrupted Records detected from CSV ingestion in {filename}\")\n",
					"            ingestion_failure = True\n",
					"    else:\n",
					"        raise RuntimeError(f\"This file type for {filename} is unsupported\")\n",
					"    \n",
					"    sparkDF = df.select([col for col in df.columns if not col.startswith('Unnamed')])\n",
					"    rows_raw = sparkDF.count()\n",
					"    \n",
					"    sparkDF = sparkDF.withColumn(\"ingested_datetime\", current_timestamp())\n",
					"    sparkDF = sparkDF.withColumn(\"ingested_by_process_name\", lit(process_name))\n",
					"    sparkDF = sparkDF.withColumn(\"expected_from\", lit(expected_from))\n",
					"    sparkDF = sparkDF.withColumn(\"expected_to\", lit(expected_to))\n",
					"    sparkDF = sparkDF.withColumn(\"input_file\", input_file_name())\n",
					"    sparkDF = sparkDF.withColumn(\"modified_datetime\", current_timestamp())\n",
					"    sparkDF = sparkDF.withColumn(\"modified_by_process_name\", lit(process_name))\n",
					"    sparkDF = sparkDF.withColumn(\"entity_name\", lit(source_filename_start))\n",
					"    sparkDF = sparkDF.withColumn(\"file_ID\", sha2(concat(lit(input_file_name()), current_timestamp().cast(\"string\")), 256))\n",
					"    \n",
					"    schema = json.loads(standardised_table_def_json)\n",
					"    for field in schema['fields']:\n",
					"        if field['type'] == 'array':\n",
					"            field['type'] = 'string'\n",
					"    schema = StructType.fromJson(schema)\n",
					"    \n",
					"    cols_orig = sparkDF.schema.names\n",
					"    cols = [re.sub('[^0-9a-zA-Z]+', '_', i).lower() for i in cols_orig]\n",
					"    cols = [colm.rstrip('_') for colm in cols]\n",
					"    newlist = []\n",
					"    for i, v in enumerate(cols):\n",
					"        totalcount = cols.count(v)\n",
					"        count = cols[:i].count(v)\n",
					"        newlist.append(v + str(count + 1) if totalcount > 1 else v)\n",
					"    for colix in range(len(cols_orig)):\n",
					"        sparkDF = sparkDF.toDF(*newlist)\n",
					"    \n",
					"    for field in sparkDF.schema:\n",
					"        table_field = next((f for f in schema if f.name.lower() == field.name.lower()), None)\n",
					"        if table_field is not None and field.dataType != table_field.dataType:\n",
					"            sparkDF = sparkDF.withColumn(field.name, col(field.name).cast(table_field.dataType))\n",
					"    \n",
					"    logInfo(f\"Writing data to odw_standardised_db.{standardised_table_name}\")\n",
					"    sparkDF.write.option(\"mergeSchema\", \"true\").format(\"delta\").mode(\"append\").saveAsTable(f\"odw_standardised_db.{standardised_table_name}\")\n",
					"    logInfo(f\"Written data to odw_standardised_db.{standardised_table_name}\")\n",
					"    end_exec_time = datetime.now()\n",
					"    \n",
					"    standardised_table_df_new = spark.read.format(\"delta\").load(standardised_table_location)\n",
					"    rows_new = standardised_table_df.filter((standardised_table_df.expected_from == expected_from) & (standardised_table_df.expected_to == expected_to)).count()\n",
					"    \n",
					"    #logging and Tracking\n",
					"    table_name = f\"odw_standardised_db.{definition['Standardised_Table_Name']}\"\n",
					"    insert_count = rows_new\n",
					"\n",
					"    try:\n",
					"        ingestion_log_schema_loc = f\"abfss://odw-config@{storage_account}tables_logs.json\"\n",
					"        ingestion_log_schema = spark.read.text(ingestion_log_schema_loc, wholetext=True).first().value\n",
					"        \n",
					"        try:\n",
					"            ingestion_log_df = spark.read.format(\"delta\").load(ingestion_log_table_location)\n",
					"            table_exists = True\n",
					"        except Exception as e:\n",
					"            logInfo(f\"Ingestion log table not found at {ingestion_log_table_location}. Creating a new one.\")\n",
					"            table_exists = False\n",
					"        \n",
					"        new_log_entry = sparkDF.select(\n",
					"            \"file_ID\",\n",
					"            \"ingested_datetime\",\n",
					"            \"ingested_by_process_name\",\n",
					"            \"input_file\",\n",
					"            \"modified_datetime\",\n",
					"            \"modified_by_process_name\",\n",
					"            \"entity_name\"\n",
					"        ).limit(1)\n",
					"        # new_log_entry = new_log_entry.withColumn(\"rows_raw\", lit(sparkDF.count()))\n",
					"        # new_log_entry = new_log_entry.withColumn(\"rows_new\", lit(standardised_table_df.filter((standardised_table_df.expected_from == expected_from) & (standardised_table_df.expected_to == expected_to)).count()))\n",
					"        rows_raw_count = sparkDF.count()\n",
					"        rows_new_count = standardised_table_df.filter((col(\"expected_from\") == expected_from) &(col(\"expected_to\") == expected_to) & (col(\"ingested_datetime\") > last_load_date)).count()\n",
					"        new_log_entry = new_log_entry.withColumn(\"rows_raw\", lit(rows_raw_count))\n",
					"        new_log_entry = new_log_entry.withColumn(\"rows_new\", lit(rows_new_count))\n",
					"\n",
					"        if not table_exists:\n",
					"            new_log_entry.write.format(\"delta\").option(\"path\", ingestion_log_table_location).saveAsTable(f\"logging.{logging_table_name}\")\n",
					"            logInfo(f\"Updating ingestion logging table {logging_table_name} with first entry.\")\n",
					"        else:\n",
					"            new_log_entry.write.format(\"delta\").mode(\"append\").saveAsTable(f\"logging.{logging_table_name}\")\n",
					"            logInfo(\"Appended to existing ingestion logging table with new entry\")\n",
					"    \n",
					"    except Exception as e:\n",
					"        logInfo('Logging to tables_logs failed')\n",
					"    \n",
					"    if rows_raw <= rows_new:\n",
					"        logInfo('All rows have successfully been written')\n",
					"    else:\n",
					"        logError(f\"All rows have NOT been successfully written. Expected {rows_raw} but {rows_new} written\")\n",
					"        ingestion_failure = True\n",
					"    \n",
					"    return ingestion_failure, rows_raw,standardised_table_name,end_exec_time"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@logging_to_appins\n",
					"async def ingest_horizon(date_folder: str, file: str):\n",
					"    row_count = 0\n",
					"    error_message = \"\"\n",
					"    start_exec_time = datetime.now()\n",
					"    try:\n",
					"        spark = SparkSession.builder.getOrCreate()\n",
					"\n",
					"        if date_folder == '':\n",
					"            date_folder = datetime.now().date()\n",
					"        else:\n",
					"            date_folder = datetime.strptime(date_folder, \"%Y-%m-%d\")\n",
					"\n",
					"        date_folder_str = date_folder.strftime('%Y-%m-%d')\n",
					"\n",
					"        # READ ORCHESTRATION DATA\n",
					"        path_to_orchestration_file = f\"abfss://odw-config@{storage_account}/orchestration/orchestration.json\"\n",
					"        df = spark.read.option(\"multiline\", \"true\").json(path_to_orchestration_file)\n",
					"        definitions = json.loads(df.toJSON().first())['definitions']\n",
					"\n",
					"        definition = next((d for d in definitions \n",
					"                          if (specific_file == '' or d['Source_Filename_Start'] == specific_file) \n",
					"                          and file.startswith(d['Source_Filename_Start']) \n",
					"                          and d['Load_Enable_status'] == 'True'), None)\n",
					"\n",
					"        if definition:\n",
					"            expected_from = date_folder - timedelta(days=1)\n",
					"            expected_from = datetime.combine(expected_from, datetime.min.time())\n",
					"            expected_to = expected_from + timedelta(days=definition['Expected_Within_Weekdays'])\n",
					"\n",
					"            if delete_existing_table:\n",
					"                print(f\"Deleting existing table if exists odw_standardised_db.{definition['Standardised_Table_Name']}\")\n",
					"                mssparkutils.notebook.run('/utils/py_delete_table', 300, arguments={'db_name': 'odw_standardised_db', 'table_name': definition['Standardised_Table_Name']})\n",
					"\n",
					"            ingestion_failure, row_count,standardised_table_name,end_exec_time = ingest_adhoc(storage_account, definition, source_path, file, expected_from, expected_to)\n",
					"\n",
					"            if ingestion_failure:\n",
					"                print(\"Errors reported during Ingestion!!\")\n",
					"                error_message = \"Errors reported during Ingestion!!\"\n",
					"            else:\n",
					"                print(\"No Errors reported during Ingestion\")\n",
					"        else:\n",
					"            if specific_file != '':\n",
					"                raise logError(f\"No definition found for {file}\")\n",
					"            else:\n",
					"                print(f\"Condition Not Satisfied for Load {file} File\")\n",
					"    except Exception as e:\n",
					"        print(f\"Failed to process {file}: {e}\")\n",
					"        error_message = f\"Failed to process {file}: {str(e)}\"\n",
					"\n",
					"    return row_count, error_message,standardised_table_name,start_exec_time,end_exec_time"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"import asyncio\n",
					"import nest_asyncio\n",
					"from datetime import datetime\n",
					"\n",
					"nest_asyncio.apply()\n",
					"\n",
					"async def process_file(file):\n",
					"    \n",
					"    # Ingest file, returning insert count, error message, and table name\n",
					"    insert_count, error_message, standardised_table_name,start_exec_time,end_exec_time = await ingest_horizon(date_folder=latest_folder, file=file)\n",
					"\n",
					"    \n",
					"    DurationSeconds = (end_exec_time - start_exec_time).total_seconds()\n",
					"    ActivityType = mssparkutils.runtime.context['currentNotebookName']\n",
					"    StatusMessage = f\"Data load happened from {file} file to {standardised_table_name} table\"\n",
					"\n",
					"    # Stage assignment: Success if error_message empty, Failed if not\n",
					"    Stage = \"Success\" if not error_message else \"Failed\"\n",
					"\n",
					"    # Ensure update_count and delete_count defined if needed (replace with correct logic)\n",
					"    update_count = 0\n",
					"    delete_count = 0\n",
					"\n",
					"    params = {\n",
					"        \"Stage\": Stage,\n",
					"        \"PipelineName\": PipelineName,\n",
					"        \"PipelineRunID\": PipelineRunID,\n",
					"        \"StartTime\": start_exec_time.isoformat(),\n",
					"        \"EndTime\": end_exec_time.isoformat(),\n",
					"        \"Inserts\": insert_count,\n",
					"        \"Updates\": update_count,\n",
					"        \"Deletes\": delete_count,\n",
					"        \"ErrorMessage\": error_message,\n",
					"        \"StatusMessage\": StatusMessage,\n",
					"        \"PipelineTriggerID\": PipelineTriggerID,\n",
					"        \"PipelineTriggerName\": PipelineTriggerName,\n",
					"        \"PipelineTriggerType\": PipelineTriggerType,\n",
					"        \"PipelineTriggeredbyPipelineName\": PipelineTriggeredbyPipelineName,\n",
					"        \"PipelineTriggeredbyPipelineRunID\": PipelineTriggeredbyPipelineRunID,\n",
					"        \"PipelineExecutionTimeInSec\": DurationSeconds,\n",
					"        \"ActivityType\": ActivityType,\n",
					"        \"DurationSeconds\": DurationSeconds,\n",
					"        \"StatusCode\": \"200\" if Stage == \"Success\" else \"500\",\n",
					"        \"AppInsCustomEventName\": \"ODW_Master_Pipeline_Logs\"\n",
					"    }\n",
					"\n",
					"    # Send telemetry\n",
					"    await asyncio.to_thread(send_telemetry_to_app_insights, params,instrumentation_key)\n",
					"    # Optional: print or raise error on failure\n",
					"    if error_message:\n",
					"        print(f\"Notebook failed for file {file}: {error_message}\")\n",
					"\n",
					"async def load_horizon_async():\n",
					"    # Process all files in parallel (ensure horizon_files is populated)\n",
					"    tasks = [process_file(file) for file in horizon_files]\n",
					"    await asyncio.gather(*tasks)\n",
					"\n",
					"# Main entry point\n",
					"loop = asyncio.get_event_loop()\n",
					"loop.run_until_complete(load_horizon_async())"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# @logging_to_appins\n",
					"# async def ingest_horizon(date_folder: str, file: str):\n",
					"#     try:\n",
					"#         spark = SparkSession.builder.getOrCreate()\n",
					"\n",
					"#         if date_folder == '':\n",
					"#             date_folder = datetime.now().date()\n",
					"#         else:\n",
					"#             date_folder = datetime.strptime(date_folder, \"%Y-%m-%d\")\n",
					"\n",
					"#         date_folder_str = date_folder.strftime('%Y-%m-%d')\n",
					"\n",
					"#         # READ ORCHESTRATION DATA\n",
					"#         path_to_orchestration_file = f\"abfss://odw-config@{storage_account}/orchestration/orchestration.json\"\n",
					"#         df = spark.read.option(\"multiline\", \"true\").json(path_to_orchestration_file)\n",
					"#         definitions = json.loads(df.toJSON().first())['definitions']\n",
					"\n",
					"#         definition = next((d for d in definitions \n",
					"#                           if (specific_file == '' or d['Source_Filename_Start'] == specific_file) \n",
					"#                           and file.startswith(d['Source_Filename_Start']) \n",
					"#                           and d['Load_Enable_status'] == 'True'), None)\n",
					"\n",
					"#         if definition:\n",
					"#             expected_from = date_folder - timedelta(days=1)\n",
					"#             expected_from = datetime.combine(expected_from, datetime.min.time())\n",
					"#             expected_to = expected_from + timedelta(days=definition['Expected_Within_Weekdays'])\n",
					"\n",
					"#             if delete_existing_table:\n",
					"#                 print(f\"Deleting existing table if exists odw_standardised_db.{definition['Standardised_Table_Name']}\")\n",
					"#                 mssparkutils.notebook.run('/utils/py_delete_table', 300, arguments={'db_name': 'odw_standardised_db', 'table_name': definition['Standardised_Table_Name']})\n",
					"\n",
					"#             ingestion_failure, row_count = ingest_adhoc(storage_account, definition, source_path, file, expected_from, expected_to)\n",
					"\n",
					"#             if ingestion_failure:\n",
					"#                 print(\"Errors reported during Ingestion!!\")\n",
					"#                 error_message = \"Errors reported during Ingestion!!\"\n",
					"#             else:\n",
					"#                 print(\"No Errors reported during Ingestion\")\n",
					"#         else:\n",
					"#             if specific_file != '':\n",
					"#                 raise logError(f\"No definition found for {file}\")\n",
					"#             else:\n",
					"#                 print(f\"Condition Not Satisfied for Load {file} File\")\n",
					"#     except Exception as e:\n",
					"#         print(f\"Failed to process {file}: {e}\")\n",
					"#         error_message = f\"Failed to process {file}: {str(e)}\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# import asyncio\n",
					"# import nest_asyncio\n",
					"# from datetime import datetime\n",
					"\n",
					"# nest_asyncio.apply()\n",
					"\n",
					"# async def load_horizon_async():\n",
					"#     async def process_file(file):\n",
					"#         await ingest_horizon(date_folder=latest_folder, file=file)\n",
					"\n",
					"#         end_exec_time = datetime.now()\n",
					"#         DurationSeconds = (end_exec_time - start_exec_time).total_seconds()  # Correctly indented inside the function\n",
					"#         ActivityType= mssparkutils.runtime.context['currentNotebookName']\n",
					"#         StatusMessage=f\"Data load happened from {file} file\"\n",
					"#         # Prepare telemetry params\n",
					"#         params = {\n",
					"#             \"Stage\": \"Completed\",\n",
					"#             \"PipelineName\": PipelineName,\n",
					"#             \"PipelineRunID\": PipelineRunID,\n",
					"#             \"StartTime\": start_exec_time.isoformat(),\n",
					"#             \"EndTime\": end_exec_time.isoformat(),\n",
					"#             \"Inserts\": insert_count,\n",
					"#             \"Updates\": update_count,\n",
					"#             \"Deletes\": delete_count,\n",
					"#             \"ErrorMessage\": error_message,\n",
					"#             \"StatusMessage\": StatusMessage,\n",
					"#             \"PipelineTriggerID\": PipelineTriggerID,\n",
					"#             \"PipelineTriggerName\": PipelineTriggerName,\n",
					"#             \"PipelineTriggerType\": PipelineTriggerType,\n",
					"#             \"PipelineTriggeredbyPipelineName\": PipelineTriggeredbyPipelineName,\n",
					"#             \"PipelineTriggeredbyPipelineRunID\": PipelineTriggeredbyPipelineRunID,\n",
					"#             \"PipelineExecutionTimeInSec\": DurationSeconds,\n",
					"#             \"ActivityType\": ActivityType,\n",
					"#             \"DurationSeconds\": DurationSeconds,\n",
					"#             \"StatusCode\": \"200\",\n",
					"#             \"AppInsCustomEventName\": \"ODW_Master_Pipeline_Logs\"\n",
					"#         }\n",
					"\n",
					"#         # Send telemetry (wrap in asyncio if not async)\n",
					"#         await asyncio.to_thread(send_telemetry_to_app_insights, params)\n",
					"\n",
					"#     tasks = [process_file(file) for file in horizon_files]\n",
					"#     await asyncio.gather(*tasks)\n",
					"\n",
					"# loop = asyncio.get_event_loop()\n",
					"# loop.run_until_complete(load_horizon_async())"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# async def load_horizon_async():\n",
					"#     tasks: list = [\n",
					"#         ingest_horizon(date_folder=latest_folder, file=file) for file in horizon_files\n",
					"#     ]\n",
					"#     await asyncio.gather(*tasks)\n",
					"\n",
					"# nest_asyncio.apply()\n",
					"# loop = asyncio.get_event_loop()\n",
					"# loop.run_until_complete(load_horizon_async())"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Produce Json formatted output"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# # Exit with the JSON result\n",
					"# mssparkutils.notebook.exit(app_insight_logger.generate_processing_results())"
				],
				"execution_count": null
			}
		]
	}
}