{
	"name": "delta_back_odw_validate",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "2e0df118-079a-4c82-a16e-425c5eb10887"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###### <b> This notebook performs delta backup between UKS and UKW, exits with a dictionary of non-delta files to be processed by copy activity </b>. \n",
					"###### Identifying Delta files\n",
					"----------------------------\n",
					" * Recursively checks the whole container,directories,sub-directories for _delta_log if true then it identifies as delta directory.\n",
					" * Recursively processes each delta directory to fetch the delta files, compares source to target, the difference is a set of delta files, that are backed-up.\n",
					" * This notebook handles both insert, deletes, updates of records as it also compares _delta_log files that contains versions between Source and target as well as the files within the directory. \n",
					"###### Identifying Non-Delta files (ex: CSV, JSON, PARQUET, Excel etc)\n",
					"---------------------------\n",
					" * The function list_non_delta_file_folders recusively checks for directories and sub-directories for _delta_log and excludes directories that have _delta_log.\n",
					" * All the non-delta files are listed for full backup meaning the destination directory is overwritten everytime.\n",
					"###### Cost-Value benefit\n",
					"-----\n",
					"* Percentage savings based on number of files being backed-up daily is calculated in the end\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import pprint\n",
					"from pyspark.sql.types import *\n",
					"import json\n",
					"import re\n",
					"from datetime import datetime\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"source_storage_account_path = re.search('url=https://(.+?);', mssparkutils.credentials.getFullConnectionString('ls_backup_source')).group(1)\n",
					"container            = 'odw-standardised'\n",
					"full_storage_path    = f'abfss://{container}@{source_storage_account_path}'"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"backup_storage_account_path=re.search('url=https://(.+?);', mssparkutils.credentials.getFullConnectionString('ls_backup_destination')).group(1)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"storage_account: str = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def list_all_delta_files(delta_table_path, relative_path=\"\"):\n",
					"    files = []\n",
					"    full_path = f\"{delta_table_path}/{relative_path}\" if relative_path else delta_table_path\n",
					"    \n",
					"    try:\n",
					"        items = mssparkutils.fs.ls(full_path)\n",
					"    except Exception as e:\n",
					"        print(f\"Warning: Could not access path '{full_path}': {str(e)}\")\n",
					"        return full_path  # Skip this path and continue\n",
					"\n",
					"    for item in items:\n",
					"        item_path = f\"{relative_path}/{item.name}\".rstrip(\"/\")\n",
					"        if item.isDir:\n",
					"            files.extend(list_all_delta_files(delta_table_path, item_path))\n",
					"        else:\n",
					"            files.append(item_path)\n",
					"\n",
					"    return set(files)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					}
				},
				"source": [
					"# new process to identify DELTA files \n",
					"def list_all_paths(path):\n",
					"    \"\"\"Recursively list all paths under the given ABFS path.\"\"\"\n",
					"    try:\n",
					"        files = mssparkutils.fs.ls(path)\n",
					"    except Exception as e:\n",
					"        print(f\"Error accessing {path}: {e}\")\n",
					"        return []\n",
					"\n",
					"    all_paths = []\n",
					"    for f in files:\n",
					"        if f.isDir:\n",
					"            all_paths.append(f.path)\n",
					"            all_paths.extend(list_all_paths(f.path))\n",
					"    return all_paths\n",
					"\n",
					"def is_delta_table(path):\n",
					"    \"\"\"Check if the given path is a Delta table by looking for the _delta_log directory.\"\"\"\n",
					"    try:\n",
					"        delta_log_path = path.rstrip(\"/\") + \"/_delta_log\"\n",
					"        mssparkutils.fs.ls(delta_log_path)\n",
					"        return True\n",
					"    except:\n",
					"        return False\n",
					"\n",
					"def count_files_in_path(path):\n",
					"    \"\"\"Recursively count all files (excluding directories) under the given path.\"\"\"\n",
					"    try:\n",
					"        files = mssparkutils.fs.ls(path)\n",
					"    except Exception as e:\n",
					"        print(f\"Error accessing {path}: {e}\")\n",
					"        return 0\n",
					"\n",
					"    count = 0\n",
					"    for f in files:\n",
					"        if f.isDir:\n",
					"            count += count_files_in_path(f.path)\n",
					"        else:\n",
					"            count += 1\n",
					"    return count\n",
					"\n",
					"# List all directories under full_storage_path\n",
					"all_dirs = list_all_paths(full_storage_path)\n",
					"\n",
					"# Filter for Delta tables\n",
					"delta_tables = [p for p in all_dirs if is_delta_table(p)]\n",
					"\n",
					"# Count files and compute cumulative total\n",
					"cumulative_total = 0\n",
					"delta_table_counts = {}\n",
					"\n",
					"for table_path in delta_tables:\n",
					"    count = count_files_in_path(table_path)\n",
					"    delta_table_counts[table_path] = count\n",
					"    cumulative_total += count\n",
					"    print(f\"{table_path}: {count} files\")\n",
					"\n",
					"print(f\"Total number of files across all Delta tables: {cumulative_total}\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					}
				},
				"source": [
					"# new process to copy DELTA files from source to target \n",
					"# Initialize grand total counter\n",
					"grand_total_changes = 0\n",
					"\n",
					"for delta_table in delta_tables:\n",
					"    try:\n",
					"        # print(f\"Processing: {delta_table}\")\n",
					"        \n",
					"        # Extract container name\n",
					"        try:\n",
					"            container_name = delta_table.split(\"@\")[0].split(\"//\")[1]\n",
					"        except Exception as e:\n",
					"            print(f\"Error extracting container name from {delta_table}: {e}\")\n",
					"            continue\n",
					"        \n",
					"        # Construct backup path\n",
					"        try:\n",
					"            backup_path = delta_table.replace(source_storage_account_path, backup_storage_account_path).replace(container_name, \"delta-backup-container\")\n",
					"        except Exception as e:\n",
					"            print(f\"Error constructing backup path for {delta_table}: {e}\")\n",
					"            continue\n",
					"        \n",
					"        # List source files\n",
					"        try:\n",
					"            if mssparkutils.fs.exists(delta_table):\n",
					"                source_file = list_all_delta_files(delta_table)\n",
					"            else:\n",
					"                print(f\"Source path does not exist: {delta_table}\")\n",
					"                continue\n",
					"        except Exception as e:\n",
					"            print(f\"Error listing files in source {delta_table}: {e}\")\n",
					"            continue\n",
					"        \n",
					"        # List backup files, handle if backup_path doesn't exist\n",
					"        try:\n",
					"            if mssparkutils.fs.exists(backup_path):\n",
					"                backup_file = list_all_delta_files(backup_path)\n",
					"            else:\n",
					"                print(f\"Backup path does not exist (assuming no backup yet): {backup_path}\")\n",
					"                mssparkutils.fs.mkdirs(backup_path)\n",
					"                backup_file = list_all_delta_files(backup_path) # Assume no backup files yet\n",
					"        except Exception as e:\n",
					"            print(f\"Error listing files in backup {backup_path}: {e}\")\n",
					"            backup_file = []  # Treat as empty\n",
					"        \n",
					"        # Determine changed files\n",
					"        try:\n",
					"            change_file = set(source_file) - set(backup_file)\n",
					"            change_count = len(change_file)\n",
					"            grand_total_changes += change_count\n",
					"            print(f\"{change_count} new/changed files in {delta_table}\")\n",
					"        except Exception as e:\n",
					"            print(f\"Error comparing files for {delta_table}: {e}\")\n",
					"            continue\n",
					"        \n",
					"        # Copy changed files\n",
					"        for file_name in change_file:\n",
					"            try:\n",
					"                source_path = delta_table + file_name\n",
					"                dest_path = backup_path + file_name\n",
					"                print(f\"Copying {source_path} to {dest_path}\")\n",
					"                \n",
					"                mssparkutils.fs.cp(source_path, dest_path)\n",
					"            except Exception as e:\n",
					"                print(f\"Error copying {source_path} to {dest_path}: {e}\")\n",
					"                continue\n",
					"    \n",
					"    except Exception as e:\n",
					"        print(f\"Unexpected error processing {delta_table}: {e}\")\n",
					"        continue\n",
					"\n",
					"print(f\"Grand total of new/changed files across all Delta tables: {grand_total_changes}\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# new Non delta process \n",
					"def list_non_delta_file_folders(base_path):\n",
					"    file_folders = []\n",
					"\n",
					"    def process_path(path):\n",
					"        items = mssparkutils.fs.ls(path)\n",
					"        has_delta_log = False\n",
					"        has_files = False\n",
					"        has_subfolders = False\n",
					"        \n",
					"        for item in items:\n",
					"            name = item.name\n",
					"            if item.isDir:\n",
					"                if name.strip('/') == '_delta_log':\n",
					"                    has_delta_log = True\n",
					"                else:\n",
					"                    has_subfolders = True\n",
					"                    process_path(item.path)  # Recursive call for subfolders\n",
					"            else:\n",
					"                has_files = True\n",
					"        \n",
					"        # Folder has files directly, no subfolders, no delta log folder\n",
					"        if has_files and not has_subfolders and not has_delta_log and \"_delta_log\" not in path:\n",
					"            print(path)\n",
					"            file_folders.append(path)\n",
					"\n",
					"    process_path(base_path)\n",
					"    return file_folders\n",
					"\n",
					"Non_delta_files_list = list_non_delta_file_folders(full_storage_path)\n",
					"full_storage_path\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"total_delta_files = grand_total_changes\n",
					"total_container_files = cumulative_total\n",
					"date_now = datetime.now().strftime(\"%Y-%m-%d\")\n",
					"\n",
					"percentage_Savings = ((total_container_files-total_delta_files)/total_container_files)*100\n",
					"\n",
					"print(f\"The total storage files on {date_now} are: {total_container_files} and total delta file are:{total_delta_files}\")\n",
					"print(f\"The percentage_Savings are: {percentage_Savings}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"non_delta_file_dict = {\n",
					"    \"backup_storage_name\":backup_storage_account_path.split(\".\")[0],\n",
					"    \"non_delta_file_list\":list(Non_delta_files_list)\n",
					"}"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					}
				},
				"source": [
					"mssparkutils.notebook.exit(non_delta_file_dict)"
				],
				"execution_count": null
			}
		],
		"folder": {
			"name": "archive/"
		}
	}
}