{
	"name": "delta_back_odw_validate",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "ac3d558e-be54-4f24-bb70-8fa199e05c81"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import pprint\n",
					"from pyspark.sql.types import *\n",
					"import json\n",
					"import re\n",
					"from pyspark.sql.functions import col, explode"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"source_storage_account_path = re.search('url=https://(.+?);', mssparkutils.credentials.getFullConnectionString('ls_backup_source')).group(1)\n",
					"container            = 'odw-standardised'\n",
					"full_storage_path    = f'abfss://{container}@{source_storage_account_path}'"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"backup_storage_account_path=re.search('url=https://(.+?);', mssparkutils.credentials.getFullConnectionString('ls_backup_destination')).group(1)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"storage_account: str = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"metadata_path: str = \"abfss://odw-config@\"+storage_account+\"existing-tables-metadata.json\"\n",
					"df_metadata  = spark.read.json(metadata_path, multiLine=True)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def list_all_delta_files(delta_table_path, relative_path=\"\"):\n",
					"    files = []\n",
					"    full_path = f\"{delta_table_path}/{relative_path}\" if relative_path else delta_table_path\n",
					"    \n",
					"    try:\n",
					"        items = mssparkutils.fs.ls(full_path)\n",
					"    except Exception as e:\n",
					"        print(f\"Warning: Could not access path '{full_path}': {str(e)}\")\n",
					"        return full_path  # Skip this path and continue\n",
					"\n",
					"    for item in items:\n",
					"        item_path = f\"{relative_path}/{item.name}\".rstrip(\"/\")\n",
					"        if item.isDir:\n",
					"            files.extend(list_all_files(delta_table_path, item_path))\n",
					"        else:\n",
					"            files.append(item_path)\n",
					"\n",
					"    return set(files)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false
					}
				},
				"source": [
					"# new process to identify DELTA files \n",
					"\n",
					"\n",
					"def list_all_paths(path):\n",
					"    \"\"\"Recursively list all paths under the given ABFS path.\"\"\"\n",
					"    try:\n",
					"        files = mssparkutils.fs.ls(path)\n",
					"    except Exception as e:\n",
					"        print(f\"Error accessing {path}: {e}\")\n",
					"        return []\n",
					"\n",
					"    all_paths = []\n",
					"    for f in files:\n",
					"        if f.isDir:\n",
					"            all_paths.append(f.path)\n",
					"            all_paths.extend(list_all_paths(f.path))\n",
					"    return all_paths\n",
					"\n",
					"def is_delta_table(path):\n",
					"    \"\"\"Check if the given path is a Delta table by looking for the _delta_log directory.\"\"\"\n",
					"    try:\n",
					"        delta_log_path = path.rstrip(\"/\") + \"/_delta_log\"\n",
					"        mssparkutils.fs.ls(delta_log_path) \n",
					"        return True\n",
					"    except:\n",
					"        return False\n",
					"\n",
					"# List all directories\n",
					"all_dirs = list_all_paths(full_storage_path)\n",
					"\n",
					"# # Filter for Delta tables\n",
					"delta_tables = [p for p in all_dirs if is_delta_table(p)]\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"for delta_table in delta_tables:\n",
					"    container_name = delta_table.split(\"@\")[0].split(\"//\")[1]\n",
					"    backup_path = delta_table.replace(source_storage_account_path,backup_storage_account_path).replace(container_name, \"delta-backup-container\")\n",
					"    source_file = list_all_delta_files(delta_table)\n",
					"    backup_file = list_all_delta_files(backup_path)\n",
					"    change_file = set(source_file)-set(backup_file)\n",
					"    for file_name in change_file:\n",
					"        mssparkutils.fs.cp(delta_table + file_name, backup_path + file_name)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# new Non delta process \n",
					"def list_non_delta_file_folders(base_path):\n",
					"    file_folders = []\n",
					"\n",
					"    def process_path(path):\n",
					"        items = mssparkutils.fs.ls(path)\n",
					"        has_delta_log = False\n",
					"        has_files = False\n",
					"        has_subfolders = False\n",
					"        \n",
					"        for item in items:\n",
					"            name = item.name\n",
					"            if item.isDir:\n",
					"                if name.strip('/') == '_delta_log':\n",
					"                    has_delta_log = True\n",
					"                else:\n",
					"                    has_subfolders = True\n",
					"                    process_path(item.path)  # Recursive call for subfolders\n",
					"            else:\n",
					"                has_files = True\n",
					"        \n",
					"        # Folder has files directly, no subfolders, no delta log folder\n",
					"        if has_files and not has_subfolders and not has_delta_log and \"_delta_log\" not in path:\n",
					"            file_folders.append(path)\n",
					"\n",
					"    process_path(base_path)\n",
					"    return file_folders\n",
					"\n",
					"Non_delta_files_list = list_non_delta_file_folders(full_storage_path)\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# log_list = []\n",
					"\n",
					"# for table_path in delta_file_list:\n",
					"#     source_table_path = table_path.split(\"'\")[1]\n",
					"#     container_name = table_path.split(\"@\")[0].split(\"//\")[1]\n",
					"#     backup_table_path = source_table_path.replace(container_name, \"delta-backup-container\") \\\n",
					"#                                          .replace(source_storage_account_path, backup_storage_account_path)\n",
					"\n",
					"#     table_name = table_path.split(\"/\")[-1]\n",
					"#     log_entry = {'table_name': table_name, 'source_count': 0, 'backup_count': 0, 'error': None}\n",
					"\n",
					"#     try:\n",
					"#         log_entry['source_count'] = spark.read.format(\"delta\").load(source_table_path).count()\n",
					"#         log_entry['backup_count'] = spark.read.format(\"delta\").load(backup_table_path).count()\n",
					"#     except Exception as e:\n",
					"#         log_entry['error'] = str(e)\n",
					"\n",
					"#     print(log_entry)\n",
					"#     log_list.append(log_entry)\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"non_delta_file_dict = {\n",
					"    \"backup_storage_name\":backup_storage_account_path.split(\".\")[0],\n",
					"    \"non_delta_file_list\":list(Non_delta_files_list)\n",
					"}"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"std_total_size = 2000\n",
					"std_delta_size = 20\n",
					"std_diff % = 99%"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					}
				},
				"source": [
					"mssparkutils.notebook.exit(non_delta_file_dict)"
				],
				"execution_count": null
			}
		]
	}
}