{
	"name": "py_saphr_src_data_anonymisation",
	"properties": {
		"folder": {
			"name": "0-odw-source-to-raw/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "c9fc246f-3605-4768-8615-397d0e552471"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this pyspark notebook is to reads all recent files from the given odw pinsdatalab folder path and anonymise the data into the source folder before it gets to odw-raw ADLS2. \n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Rohit Shukla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;19-Jan-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The functionality of this notebook is to anonymised certain columns of SAP HR data if the environment is dev or test then data will be anonymised.\n",
					"\n",
					"**Spark Cluster Configuration** -> Apache Spark Version- 3.4, Python Version \t\t- 3.10, Delta Lake Version \t- 2.4\n",
					"\n",
					"\n",
					"##### The input parameters are:\n",
					"###### Param_Env_Type => This is a mandatory parameter which refers to the environment where the data needs anonymisation."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Input parameters"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#Define Input Parameters to get values from the pipleline\n",
					"Param_Env_Type = 'dev'\n",
					"Param_File_Load_Type = 'MONTHLY'\n",
					""
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Import all required Python Libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#import all libraries and initialise Spark Session\n",
					"import json\n",
					"import traceback\n",
					"import calendar\n",
					"import time\n",
					"import csv\n",
					"import requests\n",
					"import pyspark.sql.functions as F \n",
					"import os\n",
					"import re\n",
					"import hashlib\n",
					"from io import StringIO\n",
					"from datetime import datetime, timedelta, date\n",
					"from azure.storage.fileshare import ShareDirectoryClient,ShareFileClient\n",
					"from operator import add\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql.functions import regexp_replace,lit, current_timestamp, to_date ,expr, md5, col, date_format, when, length, lpad,input_file_name,sha2,concat\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.sql import SparkSession\n",
					"spark = SparkSession.builder.getOrCreate()\n",
					"from delta.tables import DeltaTable\n",
					"#ignore FutureWarning messages \n",
					"import warnings\n",
					"warnings.filterwarnings(\"ignore\", message=\"iteritems is deprecated\")"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Enable message logging"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"source": [
					"import logging\n",
					"import csv\n",
					"import random\n",
					"import string\n",
					"from io import StringIO\n",
					"from datetime import datetime, timedelta\n",
					"from azure.storage.fileshare import ShareDirectoryClient, ShareFileClient\n",
					"\n",
					"# Disable Azure Storage SDK logging\n",
					"logging.getLogger('azure.storage').setLevel(logging.WARNING)\n",
					"logging.getLogger('azure.core').setLevel(logging.WARNING)\n",
					"\n",
					"# Global email mapping dictionary to ensure consistency across all files\n",
					"email_mapping = {}\n",
					"\n",
					"def generate_random_salary(min_salary=20000, max_salary=150000):\n",
					"    \"\"\"Generate random salary within realistic range\"\"\"\n",
					"    return random.randint(min_salary, max_salary)\n",
					"\n",
					"def generate_random_dob(min_age=18, max_age=65):\n",
					"    \"\"\"Generate random date of birth based on age range\"\"\"\n",
					"    today = datetime.now()\n",
					"    # Calculate birth year range\n",
					"    min_birth_year = today.year - max_age\n",
					"    max_birth_year = today.year - min_age\n",
					"    \n",
					"    # Generate random date\n",
					"    year = random.randint(min_birth_year, max_birth_year)\n",
					"    month = random.randint(1, 12)\n",
					"    day = random.randint(1, 28)  # Use 28 to avoid month-specific day issues\n",
					"    \n",
					"    return f\"{day:02d}/{month:02d}/{year}\"\n",
					"\n",
					"def generate_random_ni_number():\n",
					"    \"\"\"Generate random NI number in correct format (XX123456X)\"\"\"\n",
					"    # First two letters (avoid certain combinations)\n",
					"    valid_first_letters = 'ABCDEFGHJKLMNPQRSTUVWXYZ'  # Exclude I, O\n",
					"    valid_second_letters = 'ABCDEFGHJKLMNPQRSTUVWXYZ'\n",
					"    \n",
					"    first_letter = random.choice(valid_first_letters)\n",
					"    second_letter = random.choice(valid_second_letters)\n",
					"    \n",
					"    # Six digits\n",
					"    digits = ''.join([str(random.randint(0, 9)) for _ in range(6)])\n",
					"    \n",
					"    # Final letter\n",
					"    final_letter = random.choice('ABCD')\n",
					"    \n",
					"    return f\"{first_letter}{second_letter}{digits}{final_letter}\"\n",
					"\n",
					"def mask_email_simple(email):\n",
					"    \"\"\"Email masking with specific logic:\n",
					"    1. Remove any '.' before '@'\n",
					"    2. Replace last 3 characters of first name with '*'\n",
					"    3. Replace last 2 characters of last name with '*'\n",
					"    4. Replace domain with @xyz.com\n",
					"    \"\"\"\n",
					"    global email_mapping\n",
					"    \n",
					"    # Return consistent mapping if email already processed\n",
					"    if email in email_mapping:\n",
					"        return email_mapping[email]\n",
					"    \n",
					"    try:\n",
					"        if '@' not in email or not email.strip():\n",
					"            masked = email\n",
					"        else:\n",
					"            local_part, domain_part = email.split('@', 1)\n",
					"            \n",
					"            # Step 1: Remove any '.' from local part\n",
					"            local_clean = local_part.replace('.', '')\n",
					"            \n",
					"            # Convert to uppercase for processing (assuming names are in uppercase)\n",
					"            local_clean = local_clean.upper()\n",
					"            \n",
					"            # Check if we have enough characters to apply the logic\n",
					"            if len(local_clean) < 5:  # Need at least 5 characters for meaningful masking\n",
					"                # For short emails, use simpler masking\n",
					"                masked_local = local_clean[:2] + '*' * (len(local_clean) - 2) if len(local_clean) > 2 else local_clean\n",
					"            else:\n",
					"                # Assume the local part contains firstname + lastname\n",
					"                # Split roughly in the middle, or use a heuristic\n",
					"                # Assuming first half is firstname, second half is lastname\n",
					"                mid_point = len(local_clean) // 2\n",
					"                \n",
					"                # Adjust mid_point to ensure we have at least 3 chars for first name and 2 for last name\n",
					"                if mid_point < 3:\n",
					"                    mid_point = 3\n",
					"                elif len(local_clean) - mid_point < 2:\n",
					"                    mid_point = len(local_clean) - 2\n",
					"                \n",
					"                first_name = local_clean[:mid_point]\n",
					"                last_name = local_clean[mid_point:]\n",
					"                \n",
					"                # Step 2: Replace last 3 characters of first name with '*'\n",
					"                if len(first_name) >= 3:\n",
					"                    masked_first = first_name[:-3] + '***'\n",
					"                else:\n",
					"                    masked_first = '*' * len(first_name)\n",
					"                \n",
					"                # Step 3: Replace last 2 characters of last name with '*'  \n",
					"                if len(last_name) >= 2:\n",
					"                    masked_last = last_name[:-2] + '**'\n",
					"                else:\n",
					"                    masked_last = '*' * len(last_name)\n",
					"                \n",
					"                masked_local = masked_first + masked_last\n",
					"            \n",
					"            # Step 4: Replace domain with @xyz.com\n",
					"            masked = f\"{masked_local}@xyz.com\"\n",
					"    \n",
					"    except Exception as e:\n",
					"        # If any error occurs, return masked version\n",
					"        masked = '***@xyz.com'\n",
					"    \n",
					"    # Store in mapping for consistency\n",
					"    email_mapping[email] = masked\n",
					"    return masked\n",
					"\n",
					"def calculate_age_from_dob(dob_str):\n",
					"    \"\"\"Calculate age from date of birth string (DD/MM/YYYY format)\"\"\"\n",
					"    try:\n",
					"        # Parse the date string\n",
					"        birth_date = datetime.strptime(dob_str, \"%d/%m/%Y\")\n",
					"        today = datetime.now()\n",
					"        \n",
					"        # Calculate age\n",
					"        age = today.year - birth_date.year\n",
					"        if today.month < birth_date.month or (today.month == birth_date.month and today.day < birth_date.day):\n",
					"            age -= 1\n",
					"            \n",
					"        return str(age)\n",
					"    except:\n",
					"        # If parsing fails, return random age\n",
					"        return str(random.randint(18, 65))\n",
					"\n",
					"def find_column_indices(headers):\n",
					"    \"\"\"Find indices of columns to randomize (case-insensitive)\"\"\"\n",
					"    indices = {}\n",
					"    \n",
					"    # Define column name variations\n",
					"    salary_variations = ['annual salary', 'salary', 'annual_salary', 'annualsalary']\n",
					"    dob_variations = ['date of birth', 'dob', 'date_of_birth', 'dateofbirth', 'birth date', 'birthdate']\n",
					"    ni_variations = ['ni number', 'ni_number', 'ninumber', 'national insurance', 'national_insurance']\n",
					"    age_variations = ['age']\n",
					"    email_variations = ['email', 'email address', 'email_address', 'emailaddress', 'e-mail', 'e_mail']\n",
					"    \n",
					"    # Convert headers to lowercase for comparison\n",
					"    lower_headers = [header.lower().strip() for header in headers]\n",
					"    \n",
					"    # Store multiple email columns\n",
					"    email_columns = []\n",
					"    \n",
					"    for i, header in enumerate(lower_headers):\n",
					"        if any(var in header for var in salary_variations):\n",
					"            indices['salary'] = i\n",
					"        elif any(var in header for var in dob_variations):\n",
					"            indices['dob'] = i\n",
					"        elif any(var in header for var in ni_variations):\n",
					"            indices['ni'] = i\n",
					"        elif any(var in header for var in age_variations):\n",
					"            indices['age'] = i\n",
					"        elif any(var in header for var in email_variations):\n",
					"            email_columns.append(i)\n",
					"    \n",
					"    # Store all email columns\n",
					"    if email_columns:\n",
					"        indices['email_columns'] = email_columns\n",
					"    \n",
					"    return indices\n",
					"\n",
					"def randomize_row_data(row, column_indices, new_dob=None):\n",
					"    \"\"\"Randomize sensitive data in a row\"\"\"\n",
					"    if 'salary' in column_indices:\n",
					"        row[column_indices['salary']] = str(generate_random_salary())\n",
					"    \n",
					"    if 'dob' in column_indices:\n",
					"        if new_dob is None:\n",
					"            new_dob = generate_random_dob()\n",
					"        row[column_indices['dob']] = new_dob\n",
					"    \n",
					"    if 'ni' in column_indices:\n",
					"        row[column_indices['ni']] = generate_random_ni_number()\n",
					"    \n",
					"    # Handle multiple email columns\n",
					"    if 'email_columns' in column_indices:\n",
					"        for email_col_idx in column_indices['email_columns']:\n",
					"            if email_col_idx < len(row):  # Ensure column exists in this row\n",
					"                original_email = row[email_col_idx]\n",
					"                row[email_col_idx] = mask_email_simple(original_email)\n",
					"    \n",
					"    if 'age' in column_indices and new_dob:\n",
					"        row[column_indices['age']] = calculate_age_from_dob(new_dob)\n",
					"    elif 'age' in column_indices:\n",
					"        row[column_indices['age']] = str(random.randint(18, 65))\n",
					"    \n",
					"    return row\n",
					"\n",
					"def write_csv_to_azure(file_client, data):\n",
					"    \"\"\"Write CSV data back to Azure File Storage\"\"\"\n",
					"    # Convert data back to CSV string\n",
					"    output = StringIO()\n",
					"    csv_writer = csv.writer(output)\n",
					"    csv_writer.writerows(data)\n",
					"    csv_content = output.getvalue().encode('utf-8')\n",
					"    \n",
					"    try:\n",
					"        # Method 1: Try with overwrite parameter (newer SDK versions)\n",
					"        file_client.upload_file(csv_content, overwrite=True)\n",
					"    except TypeError:\n",
					"        try:\n",
					"            # Method 2: Delete and recreate (older SDK versions)\n",
					"            file_client.delete_file()\n",
					"            file_client.create_file(len(csv_content))\n",
					"            file_client.upload_range(csv_content, offset=0, length=len(csv_content))\n",
					"        except Exception:\n",
					"            try:\n",
					"                # Method 3: Create and upload in chunks\n",
					"                file_client.create_file(len(csv_content))\n",
					"                file_client.upload_range(csv_content, offset=0, length=len(csv_content))\n",
					"            except Exception as e:\n",
					"                # Method 4: Simple upload (last resort)\n",
					"                file_client.upload_file(csv_content)\n",
					"\n",
					"# Define connection parameters\n",
					"secret_name = \"datalab-connectionstring\"\n",
					"kv_linked_service = \"ls_kv\"\n",
					"share_name = \"datalab\"\n",
					"directory_path = f\"ODW/HR/SAPHR_SharepointData/{Param_Env_Type}/MONTHLY/\"\n",
					"\n",
					"# Retrieve connection string securely from Key Vault\n",
					"akv_name = spark.sparkContext.environment.get('keyVaultName', 'get')\n",
					"creds = mssparkutils.credentials.getSecret(akv_name, secret_name, kv_linked_service)\n",
					"\n",
					"# Create ShareDirectoryClient instance\n",
					"parent_dir = ShareDirectoryClient.from_connection_string(\n",
					"    conn_str=creds,\n",
					"    share_name=share_name,\n",
					"    directory_path=directory_path\n",
					")\n",
					"\n",
					"# Get files and directories\n",
					"files_and_dirs = list(parent_dir.list_directories_and_files())\n",
					"file_metadata = []\n",
					"processed_files = []\n",
					"\n",
					"print(\"Starting CSV anonymization process...\")\n",
					"print(\"-\" * 50)\n",
					"\n",
					"for item in files_and_dirs:\n",
					"    if not item['is_directory'] and item['name'].endswith('.csv'):\n",
					"        print(f\"\\nProcessing file: {item['name']}\")\n",
					"        \n",
					"        file_client = ShareFileClient.from_connection_string(\n",
					"            conn_str=creds,\n",
					"            share_name=share_name,\n",
					"            file_path=f\"{directory_path}{item['name']}\"\n",
					"        )\n",
					"        \n",
					"        try:\n",
					"            # Download and read the file\n",
					"            download_stream = file_client.download_file()\n",
					"            content = download_stream.readall().decode('utf-8')\n",
					"            csv_reader = csv.reader(StringIO(content))\n",
					"            data = list(csv_reader)\n",
					"            \n",
					"            if not data:\n",
					"                print(f\"  - Skipped: Empty file\")\n",
					"                continue\n",
					"            \n",
					"            # Get headers and find column indices\n",
					"            headers = data[0]\n",
					"            column_indices = find_column_indices(headers)\n",
					"            \n",
					"            print(f\"  - Original rows: {len(data)}\")\n",
					"            print(f\"  - Columns: {len(headers)}\")\n",
					"            print(f\"  - Found sensitive columns: {list(column_indices.keys())}\")\n",
					"            \n",
					"            # Show email columns specifically\n",
					"            if 'email_columns' in column_indices:\n",
					"                email_headers = [headers[i] for i in column_indices['email_columns']]\n",
					"                print(f\"  - Email columns found: {email_headers}\")\n",
					"            \n",
					"            if not column_indices:\n",
					"                print(f\"  - No sensitive columns found, skipping randomization\")\n",
					"                continue\n",
					"            \n",
					"            # Process each data row (skip header)\n",
					"            randomized_count = 0\n",
					"            for i in range(1, len(data)):\n",
					"                if len(data[i]) == len(headers):  # Ensure row has correct number of columns\n",
					"                    # Generate consistent DOB for age calculation\n",
					"                    new_dob = generate_random_dob() if 'dob' in column_indices else None\n",
					"                    data[i] = randomize_row_data(data[i], column_indices, new_dob)\n",
					"                    randomized_count += 1\n",
					"            \n",
					"            # Write back to Azure\n",
					"            write_csv_to_azure(file_client, data)\n",
					"            \n",
					"            print(f\"  - Randomized {randomized_count} rows\")\n",
					"            print(f\"  - Successfully uploaded anonymized file\")\n",
					"            processed_files.append(item['name'])\n",
					"            \n",
					"            # Get file metadata\n",
					"            props = file_client.get_file_properties()\n",
					"            last_modified_date = props['last_modified'].date()\n",
					"            file_metadata.append((item['name'], last_modified_date))\n",
					"            \n",
					"        except Exception as e:\n",
					"            print(f\"  - Error processing {item['name']}: {str(e)}\")\n",
					"            continue\n",
					"\n",
					"print(\"\\n\" + \"=\" * 50)\n",
					"print(\"ANONYMIZATION SUMMARY\")\n",
					"print(\"=\" * 50)\n",
					"print(f\"Total files processed: {len(processed_files)}\")\n",
					"print(f\"Successfully anonymized files:\")\n",
					"for filename in processed_files:\n",
					"    print(f\"  - {filename}\")\n",
					"\n",
					"if file_metadata:\n",
					"    most_recent_date = max(date for name, date in file_metadata)\n",
					"    print(f\"\\nMost recent file date: {most_recent_date}\")\n",
					"else:\n",
					"    print(\"\\nNo CSV files found.\")\n",
					"\n",
					"print(\"\\nAnonymization process completed!\")"
				],
				"execution_count": 7
			}
		]
	}
}