{
	"name": "py_saphr_src_data_anonymisation",
	"properties": {
		"folder": {
			"name": "0-odw-source-to-raw/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "7831fe77-6c1d-4f79-b15e-2f3db326a0f8"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this pyspark notebook is to reads all recent files from the given odw pinsdatalab folder path and anonymise the data into the source folder before it gets to odw-raw ADLS2. \n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Rohit Shukla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;19-Jan-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The functionality of this notebook is to anonymised certain columns of SAP HR data if the environment is dev or test then data will be anonymised.\n",
					"\n",
					"**Spark Cluster Configuration** -> Apache Spark Version- 3.4, Python Version \t\t- 3.10, Delta Lake Version \t- 2.4\n",
					"\n",
					"\n",
					"##### The input parameters are:\n",
					"###### Param_Env_Type => This is a mandatory parameter which refers to the environment where the data needs anonymisation."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Input parameters"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#Define Input Parameters to get values from the pipleline\n",
					"Param_Env_Type = 'dev'\n",
					"Param_File_Load_Type = 'MONTHLY'\n",
					""
				],
				"execution_count": 46
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Import all required Python Libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#import all libraries and initialise Spark Session\n",
					"import json\n",
					"import traceback\n",
					"import calendar\n",
					"import time\n",
					"import csv\n",
					"import requests\n",
					"import pyspark.sql.functions as F \n",
					"import os\n",
					"import re\n",
					"import hashlib\n",
					"from io import StringIO\n",
					"from datetime import datetime, timedelta, date\n",
					"from azure.storage.fileshare import ShareDirectoryClient,ShareFileClient\n",
					"from operator import add\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql.functions import regexp_replace,lit, current_timestamp, to_date ,expr, md5, col, date_format, when, length, lpad,input_file_name,sha2,concat\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.sql import SparkSession\n",
					"spark = SparkSession.builder.getOrCreate()\n",
					"from delta.tables import DeltaTable\n",
					"#ignore FutureWarning messages \n",
					"import warnings\n",
					"warnings.filterwarnings(\"ignore\", message=\"iteritems is deprecated\")"
				],
				"execution_count": 47
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Get Storage account"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#Get Storage account name\n",
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"#print(storage_account)"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Enable message logging"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"source": [
					"import logging\n",
					"import csv\n",
					"import random\n",
					"import string\n",
					"import hashlib\n",
					"from io import StringIO\n",
					"from datetime import datetime, timedelta\n",
					"from azure.storage.fileshare import ShareDirectoryClient, ShareFileClient\n",
					"\n",
					"# Disable Azure Storage SDK logging\n",
					"logging.getLogger('azure.storage').setLevel(logging.WARNING)\n",
					"logging.getLogger('azure.core').setLevel(logging.WARNING)\n",
					"\n",
					"# Global email mapping dictionary to ensure consistency across all files\n",
					"email_mapping = {}\n",
					"\n",
					"def generate_random_salary(min_salary=20000, max_salary=150000):\n",
					"  \"\"\"Generate random salary within realistic range\"\"\"\n",
					"  return random.randint(min_salary, max_salary)\n",
					"\n",
					"def generate_random_dob(min_age=18, max_age=65):\n",
					"  \"\"\"Generate random date of birth based on age range\"\"\"\n",
					"  today = datetime.now()\n",
					"  # Calculate birth year range\n",
					"  min_birth_year = today.year - max_age\n",
					"  max_birth_year = today.year - min_age\n",
					"  \n",
					"  # Generate random date\n",
					"  year = random.randint(min_birth_year, max_birth_year)\n",
					"  month = random.randint(1, 12)\n",
					"  day = random.randint(1, 28) # Use 28 to avoid month-specific day issues\n",
					"  \n",
					"  return f\"{day:02d}/{month:02d}/{year}\"\n",
					"\n",
					"def generate_random_ni_number():\n",
					"  \"\"\"Generate random NI number in correct format (XX123456X)\"\"\"\n",
					"  # First two letters (avoid certain combinations)\n",
					"  valid_first_letters = 'ABCDEFGHJKLMNPQRSTUVWXYZ' # Exclude I, O\n",
					"  valid_second_letters = 'ABCDEFGHJKLMNPQRSTUVWXYZ'\n",
					"  \n",
					"  first_letter = random.choice(valid_first_letters)\n",
					"  second_letter = random.choice(valid_second_letters)\n",
					"  \n",
					"  # Six digits\n",
					"  digits = ''.join([str(random.randint(0, 9)) for _ in range(6)])\n",
					"  \n",
					"  # Final letter\n",
					"  final_letter = random.choice('ABCD')\n",
					"  \n",
					"  return f\"{first_letter}{second_letter}{digits}{final_letter}\"\n",
					"\n",
					"def scramble_text_deterministic(text, seed=None):\n",
					"  \"\"\"\n",
					"  Scramble text deterministically - same input always produces same output\n",
					"  Uses the text itself as seed for consistency\n",
					"  \"\"\"\n",
					"  if not text or len(text) <= 1:\n",
					"    return text\n",
					"  \n",
					"  # Use the text itself as seed for deterministic randomization\n",
					"  if seed is None:\n",
					"    seed = text\n",
					"  \n",
					"  # Create a hash of the seed to ensure consistent randomization\n",
					"  hash_object = hashlib.md5(seed.encode())\n",
					"  hash_hex = hash_object.hexdigest()\n",
					"  \n",
					"  # Use the hash to seed the random number generator\n",
					"  random_gen = random.Random(int(hash_hex, 16))\n",
					"  \n",
					"  # Convert text to list for scrambling\n",
					"  chars = list(text.lower()) # Convert to lowercase for consistency\n",
					"  \n",
					"  # Scramble the characters\n",
					"  random_gen.shuffle(chars)\n",
					"  \n",
					"  return ''.join(chars)\n",
					"\n",
					"def anonymize_email_consistent(email):\n",
					"  \"\"\"\n",
					"  Anonymize email with consistent randomization logic\n",
					"  Example: OLIVER.MUNN@PLANNINGINSPECTORATE.GOV.UK -> OMLUINVERU@xyz.com\n",
					"  Same email will always generate the same anonymized version\n",
					"  \"\"\"\n",
					"  global email_mapping\n",
					"  \n",
					"  # Return consistent mapping if email already processed\n",
					"  if email in email_mapping:\n",
					"    return email_mapping[email]\n",
					"  \n",
					"  try:\n",
					"    if '@' not in email or not email.strip():\n",
					"      anonymized = email\n",
					"    else:\n",
					"      local_part, domain_part = email.split('@', 1)\n",
					"      \n",
					"      # Remove dots and other special characters from local part for scrambling\n",
					"      clean_local = ''.join(char for char in local_part if char.isalnum())\n",
					"      \n",
					"      if not clean_local:\n",
					"        # If no alphanumeric characters, use fallback\n",
					"        scrambled_local = 'user' + str(hash(email) % 10000)\n",
					"      else:\n",
					"        # Scramble the clean local part deterministically\n",
					"        scrambled_local = scramble_text_deterministic(clean_local, email)\n",
					"      \n",
					"      # Use a generic domain\n",
					"      anonymized_domain = \"xyz.com\"\n",
					"      \n",
					"      anonymized = f\"{scrambled_local}@{anonymized_domain}\"\n",
					"  \n",
					"  except Exception as e:\n",
					"    # If any error occurs, create a deterministic fallback\n",
					"    hash_value = hash(email) % 1000000\n",
					"    anonymized = f\"user{hash_value}@xyz.com\"\n",
					"  \n",
					"  # Store in mapping for consistency\n",
					"  email_mapping[email] = anonymized\n",
					"  return anonymized\n",
					"\n",
					"def calculate_age_from_dob(dob_str):\n",
					"  \"\"\"Calculate age from date of birth string (DD/MM/YYYY format)\"\"\"\n",
					"  try:\n",
					"    # Parse the date string\n",
					"    birth_date = datetime.strptime(dob_str, \"%d/%m/%Y\")\n",
					"    today = datetime.now()\n",
					"    \n",
					"    # Calculate age\n",
					"    age = today.year - birth_date.year\n",
					"    if today.month < birth_date.month or (today.month == birth_date.month and today.day < birth_date.day):\n",
					"      age -= 1\n",
					"      \n",
					"    return str(age)\n",
					"  except:\n",
					"    # If parsing fails, return random age\n",
					"    return str(random.randint(18, 65))\n",
					"\n",
					"def find_column_indices(headers):\n",
					"  \"\"\"Find indices of columns to randomize (case-insensitive)\"\"\"\n",
					"  indices = {}\n",
					"  \n",
					"  # Define column name variations\n",
					"  salary_variations = ['annual salary', 'salary', 'annual_salary', 'annualsalary']\n",
					"  dob_variations = ['date of birth', 'dob', 'date_of_birth', 'dateofbirth', 'birth date', 'birthdate']\n",
					"  ni_variations = ['ni number', 'ni_number', 'ninumber', 'national insurance', 'national_insurance']\n",
					"  age_variations = ['age']\n",
					"  email_variations = ['email', 'email address', 'email_address', 'emailaddress', 'e-mail', 'e_mail']\n",
					"  \n",
					"  # Convert headers to lowercase for comparison\n",
					"  lower_headers = [header.lower().strip() for header in headers]\n",
					"  \n",
					"  # Store multiple email columns\n",
					"  email_columns = []\n",
					"  \n",
					"  for i, header in enumerate(lower_headers):\n",
					"    if any(var in header for var in salary_variations):\n",
					"      indices['salary'] = i\n",
					"    elif any(var in header for var in dob_variations):\n",
					"      indices['dob'] = i\n",
					"    elif any(var in header for var in ni_variations):\n",
					"      indices['ni'] = i\n",
					"    elif any(var in header for var in age_variations):\n",
					"      indices['age'] = i\n",
					"    elif any(var in header for var in email_variations):\n",
					"      email_columns.append(i)\n",
					"  \n",
					"  # Store all email columns\n",
					"  if email_columns:\n",
					"    indices['email_columns'] = email_columns\n",
					"  \n",
					"  return indices\n",
					"\n",
					"def randomize_row_data(row, column_indices, new_dob=None):\n",
					"  \"\"\"Randomize sensitive data in a row\"\"\"\n",
					"  if 'salary' in column_indices:\n",
					"    row[column_indices['salary']] = str(generate_random_salary())\n",
					"  \n",
					"  if 'dob' in column_indices:\n",
					"    if new_dob is None:\n",
					"      new_dob = generate_random_dob()\n",
					"    row[column_indices['dob']] = new_dob\n",
					"  \n",
					"  if 'ni' in column_indices:\n",
					"    row[column_indices['ni']] = generate_random_ni_number()\n",
					"  \n",
					"  # Handle multiple email columns with new anonymization logic\n",
					"  if 'email_columns' in column_indices:\n",
					"    for email_col_idx in column_indices['email_columns']:\n",
					"      if email_col_idx < len(row): # Ensure column exists in this row\n",
					"        original_email = row[email_col_idx]\n",
					"        row[email_col_idx] = anonymize_email_consistent(original_email)\n",
					"  \n",
					"  if 'age' in column_indices and new_dob:\n",
					"    row[column_indices['age']] = calculate_age_from_dob(new_dob)\n",
					"  elif 'age' in column_indices:\n",
					"    row[column_indices['age']] = str(random.randint(18, 65))\n",
					"  \n",
					"  return row\n",
					"\n",
					"def write_csv_to_azure(file_client, data):\n",
					"  \"\"\"Write CSV data back to Azure File Storage\"\"\"\n",
					"  # Convert data back to CSV string\n",
					"  output = StringIO()\n",
					"  csv_writer = csv.writer(output)\n",
					"  csv_writer.writerows(data)\n",
					"  csv_content = output.getvalue().encode('utf-8')\n",
					"  \n",
					"  try:\n",
					"    # Method 1: Try with overwrite parameter (newer SDK versions)\n",
					"    file_client.upload_file(csv_content, overwrite=True)\n",
					"  except TypeError:\n",
					"    try:\n",
					"      # Method 2: Delete and recreate (older SDK versions)\n",
					"      file_client.delete_file()\n",
					"      file_client.create_file(len(csv_content))\n",
					"      file_client.upload_range(csv_content, offset=0, length=len(csv_content))\n",
					"    except Exception:\n",
					"      try:\n",
					"        # Method 3: Create and upload in chunks\n",
					"        file_client.create_file(len(csv_content))\n",
					"        file_client.upload_range(csv_content, offset=0, length=len(csv_content))\n",
					"      except Exception as e:\n",
					"        # Method 4: Simple upload (last resort)\n",
					"        file_client.upload_file(csv_content)\n",
					"\n",
					"# Define connection parameters\n",
					"secret_name = \"datalab-connectionstring\"\n",
					"kv_linked_service = \"ls_kv\"\n",
					"share_name = \"datalab\"\n",
					"directory_path = f\"ODW/HR/SAPHR_SharepointData/{Param_Env_Type}/MONTHLY/\"\n",
					"\n",
					"# Retrieve connection string securely from Key Vault\n",
					"akv_name = spark.sparkContext.environment.get('keyVaultName', 'get')\n",
					"creds = mssparkutils.credentials.getSecret(akv_name, secret_name, kv_linked_service)\n",
					"\n",
					"# Create ShareDirectoryClient instance\n",
					"parent_dir = ShareDirectoryClient.from_connection_string(\n",
					"  conn_str=creds,\n",
					"  share_name=share_name,\n",
					"  directory_path=directory_path\n",
					")\n",
					"\n",
					"# Get files and directories\n",
					"files_and_dirs = list(parent_dir.list_directories_and_files())\n",
					"file_metadata = []\n",
					"processed_files = []\n",
					"\n",
					"print(\"Starting CSV anonymization process...\")\n",
					"print(\"-\" * 50)\n",
					"\n",
					"for item in files_and_dirs:\n",
					"  if not item['is_directory'] and item['name'].endswith('.csv'):\n",
					"    print(f\"\\nProcessing file: {item['name']}\")\n",
					"    \n",
					"    file_client = ShareFileClient.from_connection_string(\n",
					"      conn_str=creds,\n",
					"      share_name=share_name,\n",
					"      file_path=f\"{directory_path}{item['name']}\"\n",
					"    )\n",
					"    \n",
					"    try:\n",
					"      # Download and read the file\n",
					"      download_stream = file_client.download_file()\n",
					"      content = download_stream.readall().decode('utf-8')\n",
					"      csv_reader = csv.reader(StringIO(content))\n",
					"      data = list(csv_reader)\n",
					"      \n",
					"      if not data:\n",
					"        print(f\" - Skipped: Empty file\")\n",
					"        continue\n",
					"      \n",
					"      # Get headers and find column indices\n",
					"      headers = data[0]\n",
					"      column_indices = find_column_indices(headers)\n",
					"      \n",
					"      print(f\" - Original rows: {len(data)}\")\n",
					"      print(f\" - Columns: {len(headers)}\")\n",
					"      print(f\" - Found sensitive columns: {list(column_indices.keys())}\")\n",
					"      \n",
					"      # Show email columns specifically\n",
					"      if 'email_columns' in column_indices:\n",
					"        email_headers = [headers[i] for i in column_indices['email_columns']]\n",
					"        print(f\" - Email columns found: {email_headers}\")\n",
					"      \n",
					"      if not column_indices:\n",
					"        print(f\" - No sensitive columns found, skipping randomization\")\n",
					"        continue\n",
					"      \n",
					"      # Process each data row (skip header)\n",
					"      randomized_count = 0\n",
					"      for i in range(1, len(data)):\n",
					"        if len(data[i]) == len(headers): # Ensure row has correct number of columns\n",
					"          # Generate consistent DOB for age calculation\n",
					"          new_dob = generate_random_dob() if 'dob' in column_indices else None\n",
					"          data[i] = randomize_row_data(data[i], column_indices, new_dob)\n",
					"          randomized_count += 1\n",
					"      \n",
					"      # Write back to Azure\n",
					"      write_csv_to_azure(file_client, data)\n",
					"      \n",
					"      print(f\" - Randomized {randomized_count} rows\")\n",
					"      print(f\" - Successfully uploaded anonymized file\")\n",
					"      processed_files.append(item['name'])\n",
					"      \n",
					"      # Get file metadata\n",
					"      props = file_client.get_file_properties()\n",
					"      last_modified_date = props['last_modified'].date()\n",
					"      file_metadata.append((item['name'], last_modified_date))\n",
					"      \n",
					"    except Exception as e:\n",
					"      print(f\" - Error processing {item['name']}: {str(e)}\")\n",
					"      continue\n",
					"\n",
					"print(\"\\n\" + \"=\" * 50)\n",
					"print(\"ANONYMIZATION SUMMARY\")\n",
					"print(\"=\" * 50)\n",
					"print(f\"Total files processed: {len(processed_files)}\")\n",
					"print(f\"Successfully anonymized files:\")\n",
					"for filename in processed_files:\n",
					"  print(f\" - {filename}\")\n",
					"\n",
					"print(f\"\\nTotal unique emails anonymized: {len(email_mapping)}\")\n",
					"if len(email_mapping) > 0:\n",
					"  print(\"Sample email mappings:\")\n",
					"  sample_count = min(5, len(email_mapping))\n",
					"  for i, (original, anonymized) in enumerate(list(email_mapping.items())[:sample_count]):\n",
					"    print(f\" {original} -> {anonymized}\")\n",
					"\n",
					"if file_metadata:\n",
					"  most_recent_date = max(date for name, date in file_metadata)\n",
					"  print(f\"\\nMost recent file date: {most_recent_date}\")\n",
					"else:\n",
					"  print(\"\\nNo CSV files found.\")\n",
					"\n",
					"print(\"\\nAnonymization process completed!\")\n",
					"\n",
					"# Test the email anonymization function\n",
					"print(\"\\n\" + \"=\" * 50)\n",
					"print(\"EMAIL ANONYMIZATION TEST\")\n",
					"print(\"=\" * 50)\n",
					"test_emails = [\n",
					"  \"OLIVER.MUNN@PLANNINGINSPECTORATE.GOV.UK\",\n",
					"  \"john.doe@company.com\",\n",
					"  \"jane.smith@example.org\",\n",
					"  \"test.user@domain.co.uk\"\n",
					"]\n",
					"\n",
					"for email in test_emails:\n",
					"  anonymized = anonymize_email_consistent(email)\n",
					"  print(f\"{email} -> {anonymized}\")\n",
					"\n",
					"# Test consistency - run the same emails again\n",
					"print(\"\\nConsistency Test (same emails should produce same results):\")\n",
					"for email in test_emails:\n",
					"  anonymized = anonymize_email_consistent(email)\n",
					"  print(f\"{email} -> {anonymized}\")\n",
					""
				],
				"execution_count": 49
			}
		]
	}
}