{
	"name": "py_saphr_src_data_anonymisation",
	"properties": {
		"folder": {
			"name": "0-odw-source-to-raw/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "013628ba-3c89-463f-9422-6075e8c4c744"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this pyspark notebook is to reads all recent files from the given odw pinsdatalab folder path and anonymise the data into the source folder before it gets to odw-raw ADLS2. \n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Rohit Shukla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;19-Jan-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The functionality of this notebook is to anonymised certain columns of SAP HR data if the environment is dev or test then data will be anonymised.\n",
					"\n",
					"**Spark Cluster Configuration** -> Apache Spark Version- 3.4, Python Version \t\t- 3.10, Delta Lake Version \t- 2.4\n",
					"\n",
					"\n",
					"##### The input parameters are:\n",
					"###### Param_Env_Type => This is a mandatory parameter which refers to the environment where the data needs anonymisation."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Input parameters"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#Define Input Parameters to get values from the pipleline\n",
					"Param_Env_Type = 'dev'\n",
					"Param_File_Load_Type = 'MONTHLY'\n",
					""
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Import all required Python Libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#import all libraries and initialise Spark Session\n",
					"import json\n",
					"import traceback\n",
					"import calendar\n",
					"import time\n",
					"import csv\n",
					"import requests\n",
					"import pyspark.sql.functions as F \n",
					"import os\n",
					"import re\n",
					"import logging\n",
					"import csv\n",
					"import random\n",
					"import string\n",
					"from io import StringIO\n",
					"from azure.storage.fileshare import ShareDirectoryClient, ShareFileClient\n",
					"from datetime import datetime, timedelta, date\n",
					"from azure.storage.fileshare import ShareDirectoryClient,ShareFileClient\n",
					"from operator import add\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql.functions import regexp_replace,lit, current_timestamp, to_date ,expr, md5, col, date_format, when, length, lpad,input_file_name,sha2,concat\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.sql import SparkSession\n",
					"spark = SparkSession.builder.getOrCreate()\n",
					"from delta.tables import DeltaTable\n",
					"#ignore FutureWarning messages \n",
					"import warnings\n",
					"warnings.filterwarnings(\"ignore\", message=\"iteritems is deprecated\")"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Enable message logging"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 37
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define all data anonymisation related functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import logging\n",
					"import csv\n",
					"import random\n",
					"import string\n",
					"from io import StringIO\n",
					"from datetime import datetime, timedelta\n",
					"from azure.storage.fileshare import ShareDirectoryClient, ShareFileClient\n",
					"\n",
					"# Disable Azure Storage SDK logging\n",
					"logging.getLogger('azure.storage').setLevel(logging.WARNING)\n",
					"logging.getLogger('azure.core').setLevel(logging.WARNING)\n",
					"\n",
					"# Global email mapping dictionary to ensure consistency across all files\n",
					"email_mapping = {}\n",
					"\n",
					"def generate_random_salary(min_salary=20000, max_salary=150000):\n",
					"    \"\"\"Generate random salary within realistic range\"\"\"\n",
					"    return random.randint(min_salary, max_salary)\n",
					"\n",
					"def generate_random_dob(min_age=18, max_age=65):\n",
					"    \"\"\"Generate random date of birth based on age range\"\"\"\n",
					"    today = datetime.now()\n",
					"    # Calculate birth year range\n",
					"    min_birth_year = today.year - max_age\n",
					"    max_birth_year = today.year - min_age\n",
					"    \n",
					"    # Generate random date\n",
					"    year = random.randint(min_birth_year, max_birth_year)\n",
					"    month = random.randint(1, 12)\n",
					"    day = random.randint(1, 28)  # Use 28 to avoid month-specific day issues\n",
					"    \n",
					"    return f\"{day:02d}/{month:02d}/{year}\"\n",
					"\n",
					"def generate_random_ni_number():\n",
					"    \"\"\"Generate random NI number in correct format (XX123456X)\"\"\"\n",
					"    # First two letters (avoid certain combinations)\n",
					"    valid_first_letters = 'ABCDEFGHJKLMNPQRSTUVWXYZ'  # Exclude I, O\n",
					"    valid_second_letters = 'ABCDEFGHJKLMNPQRSTUVWXYZ'\n",
					"    \n",
					"    first_letter = random.choice(valid_first_letters)\n",
					"    second_letter = random.choice(valid_second_letters)\n",
					"    \n",
					"    # Six digits\n",
					"    digits = ''.join([str(random.randint(0, 9)) for _ in range(6)])\n",
					"    \n",
					"    # Final letter\n",
					"    final_letter = random.choice('ABCD')\n",
					"    \n",
					"    return f\"{first_letter}{second_letter}{digits}{final_letter}\"\n",
					"\n",
					"def mask_email_with_pers_number(email, pers_no, is_lm_email=False):\n",
					"    \"\"\"\n",
					"    Email masking with PersNo/StaffNumber:\n",
					"    - Regular email: [PersNo]@PINS.com\n",
					"    - LM E-mail: [PersNo]_LM@PINS.com\n",
					"    - Use consistent mapping for same email+personnel combination\n",
					"    \"\"\"\n",
					"    global email_mapping\n",
					"    \n",
					"    # Create unique key for mapping (includes LM flag AND personnel number)\n",
					"    # This ensures same email with different personnel numbers gets different masks\n",
					"    mapping_key = f\"{email}|{pers_no}|{is_lm_email}\"\n",
					"    \n",
					"    # Return consistent mapping if this exact combination already processed\n",
					"    if mapping_key in email_mapping:\n",
					"        return email_mapping[mapping_key]\n",
					"    \n",
					"    try:\n",
					"        if '@' not in email or not email.strip():\n",
					"            masked = email\n",
					"        else:\n",
					"            # Clean and validate personnel number\n",
					"            base_identifier = \"UNKNOWN\"\n",
					"            if pers_no:\n",
					"                pers_no_cleaned = str(pers_no).strip()\n",
					"                if pers_no_cleaned and pers_no_cleaned.lower() not in ['', 'none', 'null', 'n/a', 'nan']:\n",
					"                    base_identifier = pers_no_cleaned\n",
					"                    print(f\"    DEBUG: Using PersNo '{base_identifier}' for email '{email}'\")\n",
					"                else:\n",
					"                    print(f\"    DEBUG: PersNo '{pers_no}' considered invalid/empty\")\n",
					"            else:\n",
					"                print(f\"    DEBUG: No PersNo provided for email '{email}'\")\n",
					"            \n",
					"            # Add _LM suffix for LM emails\n",
					"            if is_lm_email:\n",
					"                masked = f\"{base_identifier}_LM@PINS.com\"\n",
					"            else:\n",
					"                masked = f\"{base_identifier}@PINS.com\"\n",
					"    \n",
					"    except Exception as e:\n",
					"        print(f\"    DEBUG: Exception in email masking: {str(e)}\")\n",
					"        # If any error occurs, return masked version\n",
					"        if is_lm_email:\n",
					"            masked = 'UNKNOWN_LM@PINS.com'\n",
					"        else:\n",
					"            masked = 'UNKNOWN@PINS.com'\n",
					"    \n",
					"    # Store in mapping for consistency\n",
					"    email_mapping[mapping_key] = masked\n",
					"    return masked\n",
					"\n",
					"def mask_name_field(name):\n",
					"    \"\"\"\n",
					"    Mask name fields keeping first two characters intact\n",
					"    Example: OLIVER -> OL****\n",
					"             MUNN -> MU**\n",
					"    \"\"\"\n",
					"    if not name or not name.strip():\n",
					"        return name\n",
					"    \n",
					"    name = name.strip()\n",
					"    if len(name) <= 2:\n",
					"        return name  # Don't mask very short names\n",
					"    else:\n",
					"        return name[:2] + '*' * (len(name) - 2)\n",
					"\n",
					"def mask_full_name(full_name):\n",
					"    \"\"\"\n",
					"    Mask full name field that contains multiple names separated by spaces\n",
					"    Example: \"JOHN SMITH\" -> \"JO** SM***\"\n",
					"             \"MARY JANE DOE\" -> \"MA** JA** DO*\"\n",
					"    \"\"\"\n",
					"    if not full_name or not full_name.strip():\n",
					"        return full_name\n",
					"    \n",
					"    # Split the full name by spaces\n",
					"    name_parts = full_name.strip().split()\n",
					"    \n",
					"    # Apply masking to each part\n",
					"    masked_parts = []\n",
					"    for part in name_parts:\n",
					"        if part:  # Skip empty parts\n",
					"            masked_parts.append(mask_name_field(part))\n",
					"    \n",
					"    # Join back with spaces\n",
					"    return ' '.join(masked_parts)\n",
					"\n",
					"def calculate_age_from_dob(dob_str):\n",
					"    \"\"\"Calculate age from date of birth string (DD/MM/YYYY format)\"\"\"\n",
					"    try:\n",
					"        # Parse the date string\n",
					"        birth_date = datetime.strptime(dob_str, \"%d/%m/%Y\")\n",
					"        today = datetime.now()\n",
					"        \n",
					"        # Calculate age\n",
					"        age = today.year - birth_date.year\n",
					"        if today.month < birth_date.month or (today.month == birth_date.month and today.day < birth_date.day):\n",
					"            age -= 1\n",
					"            \n",
					"        return str(age)\n",
					"    except:\n",
					"        # If parsing fails, return random age\n",
					"        return str(random.randint(18, 65))\n",
					"\n",
					"def find_column_indices(headers):\n",
					"    \"\"\"Find indices of columns to randomize (case-insensitive)\"\"\"\n",
					"    indices = {}\n",
					"    \n",
					"    # Define column name variations\n",
					"    salary_variations = ['annual salary', 'salary', 'annual_salary', 'annualsalary']\n",
					"    dob_variations = ['date of birth', 'dob', 'date_of_birth', 'dateofbirth', 'birth date', 'birthdate']\n",
					"    ni_variations = ['ni number', 'ni_number', 'ninumber', 'national insurance', 'national_insurance']\n",
					"    age_variations = ['age', 'age of employee', 'employee age', 'emp age', 'age of emp', 'age_of_employee', 'employee_age']\n",
					"    email_variations = ['email', 'email address', 'email_address', 'emailaddress', 'e-mail', 'e_mail']\n",
					"    # Updated to match your CSV exactly - including variations with dots and spaces\n",
					"    pers_no_variations = ['pers.no.', 'pers.no', 'persno', 'pers no', 'pers_no', 'staffnumber', 'staff number', 'staff_number', 'personnel number', 'personnel_number', 'employee no.', 'employee no', 'employee_no']\n",
					"    name_variations = ['first name', 'firstname', 'first_name', 'last name', 'lastname', 'last_name', 'forename', 'surname']\n",
					"    # Updated to match your CSV exactly\n",
					"    manager_name_variations = ['name of manager (om)', 'name of manager', 'manager name', 'managername', 'manager_name']\n",
					"    \n",
					"    # Convert headers to lowercase for comparison\n",
					"    lower_headers = [header.lower().strip() for header in headers]\n",
					"    \n",
					"    # Store multiple email columns and LM email columns separately\n",
					"    email_columns = []\n",
					"    lm_email_columns = []\n",
					"    name_columns = []\n",
					"    manager_name_columns = []\n",
					"    personnel_number_columns = []  # Store all personnel number columns\n",
					"    \n",
					"    print(f\"  - DEBUG: Age variations being checked: {age_variations}\")\n",
					"    print(f\"  - DEBUG: Manager name variations being checked: {manager_name_variations}\")\n",
					"    print(f\"  - DEBUG: Searching through headers: {headers}\")\n",
					"    \n",
					"    for i, header in enumerate(lower_headers):\n",
					"        original_header = headers[i]  # Keep original case for display\n",
					"        \n",
					"        print(f\"  - DEBUG: Checking header '{original_header}' (lowercase: '{header}')\")\n",
					"        \n",
					"        # Check more specific patterns first to avoid false matches\n",
					"        if any(var == header for var in manager_name_variations):  # Exact match for manager names\n",
					"            manager_name_columns.append(i)\n",
					"            print(f\"  - DEBUG: Found manager name column: '{original_header}' at index {i}\")\n",
					"            matching_variation = next(var for var in manager_name_variations if var == header)\n",
					"            print(f\"  - DEBUG: Matched manager name variation: '{matching_variation}'\")\n",
					"        elif any(var == header for var in age_variations):  # Exact match for age\n",
					"            indices['age'] = i\n",
					"            print(f\"  - DEBUG: Found age column: '{original_header}' at index {i}\")\n",
					"            matching_variation = next(var for var in age_variations if var == header)\n",
					"            print(f\"  - DEBUG: Matched age variation: '{matching_variation}'\")\n",
					"        elif any(var in header for var in salary_variations):\n",
					"            indices['salary'] = i\n",
					"            print(f\"  - DEBUG: Found salary column: '{original_header}' at index {i}\")\n",
					"        elif any(var in header for var in dob_variations):\n",
					"            indices['dob'] = i\n",
					"            print(f\"  - DEBUG: Found DOB column: '{original_header}' at index {i}\")\n",
					"        elif any(var in header for var in ni_variations):\n",
					"            indices['ni'] = i\n",
					"            print(f\"  - DEBUG: Found NI column: '{original_header}' at index {i}\")\n",
					"        elif any(var in header for var in pers_no_variations):\n",
					"            personnel_number_columns.append((i, original_header))\n",
					"            print(f\"  - DEBUG: Found personnel number column: '{original_header}' at index {i}\")\n",
					"            # Note: Personnel number columns are used for email anonymization but not anonymized themselves\n",
					"        elif any(var in header for var in name_variations):\n",
					"            name_columns.append(i)\n",
					"            print(f\"  - DEBUG: Found name column: '{original_header}' at index {i}\")\n",
					"        elif 'lm e-mail' in header or 'lm_e-mail' in header or 'lm email' in header:\n",
					"            lm_email_columns.append(i)\n",
					"            print(f\"  - DEBUG: Found LM email column: '{original_header}' at index {i}\")\n",
					"        elif any(var in header for var in email_variations):\n",
					"            email_columns.append(i)\n",
					"            print(f\"  - DEBUG: Found email column: '{original_header}' at index {i}\")\n",
					"        else:\n",
					"            print(f\"  - DEBUG: No match for header: '{original_header}'\")\n",
					"    \n",
					"    # Handle multiple personnel number columns - prefer the first one found\n",
					"    if personnel_number_columns:\n",
					"        # Sort by preference: staffnumber > pers_no > others\n",
					"        personnel_number_columns.sort(key=lambda x: (\n",
					"            0 if 'staffnumber' in x[1].lower() else\n",
					"            1 if 'pers' in x[1].lower() else 2\n",
					"        ))\n",
					"        indices['pers_no'] = personnel_number_columns[0][0]\n",
					"        indices['all_pers_no_columns'] = personnel_number_columns\n",
					"        print(f\"  - DEBUG: Using personnel number column: '{personnel_number_columns[0][1]}' at index {personnel_number_columns[0][0]}\")\n",
					"        if len(personnel_number_columns) > 1:\n",
					"            print(f\"  - DEBUG: Other personnel number columns found: {[(col[1], col[0]) for col in personnel_number_columns[1:]]}\")\n",
					"    \n",
					"    # Store all email columns\n",
					"    if email_columns:\n",
					"        indices['email_columns'] = email_columns\n",
					"    if lm_email_columns:\n",
					"        indices['lm_email_columns'] = lm_email_columns\n",
					"    if name_columns:\n",
					"        indices['name_columns'] = name_columns\n",
					"    if manager_name_columns:\n",
					"        indices['manager_name_columns'] = manager_name_columns\n",
					"    \n",
					"    return indices\n",
					"\n",
					"def randomize_row_data(row, column_indices, new_dob=None):\n",
					"    \"\"\"Randomize sensitive data in a row\"\"\"\n",
					"    # Get PersNo for email masking with comprehensive debugging\n",
					"    pers_no = None\n",
					"    pers_no_column_name = \"Not Found\"\n",
					"    \n",
					"    if 'pers_no' in column_indices and column_indices['pers_no'] < len(row):\n",
					"        pers_no_index = column_indices['pers_no']\n",
					"        pers_no = row[pers_no_index]\n",
					"        \n",
					"        # Get column name for debugging\n",
					"        if 'all_pers_no_columns' in column_indices:\n",
					"            for col_index, col_name in column_indices['all_pers_no_columns']:\n",
					"                if col_index == pers_no_index:\n",
					"                    pers_no_column_name = col_name\n",
					"                    break\n",
					"        \n",
					"        print(f\"    DEBUG: Raw personnel number from column '{pers_no_column_name}' (index {pers_no_index}): '{pers_no}' (type: {type(pers_no)})\")\n",
					"        \n",
					"        # Clean and validate the personnel number\n",
					"        if pers_no is not None:\n",
					"            pers_no_str = str(pers_no).strip()\n",
					"            print(f\"    DEBUG: After str() and strip(): '{pers_no_str}'\")\n",
					"            \n",
					"            if pers_no_str and pers_no_str.lower() not in ['', 'none', 'null', 'n/a', 'nan']:\n",
					"                pers_no = pers_no_str\n",
					"                print(f\"    DEBUG: Final valid personnel number: '{pers_no}'\")\n",
					"            else:\n",
					"                print(f\"    DEBUG: Personnel number '{pers_no_str}' considered invalid/empty\")\n",
					"                pers_no = None\n",
					"        else:\n",
					"            print(f\"    DEBUG: Personnel number is None/empty\")\n",
					"            pers_no = None\n",
					"    else:\n",
					"        print(f\"    DEBUG: Personnel number column not found or row too short\")\n",
					"        # Show all available columns in this row for debugging\n",
					"        if len(row) > 0:\n",
					"            print(f\"    DEBUG: Row has {len(row)} columns: {row[:min(10, len(row))]}\")  # Show first 10 columns\n",
					"    \n",
					"    if 'salary' in column_indices:\n",
					"        row[column_indices['salary']] = str(generate_random_salary())\n",
					"    \n",
					"    if 'dob' in column_indices:\n",
					"        if new_dob is None:\n",
					"            new_dob = generate_random_dob()\n",
					"        row[column_indices['dob']] = new_dob\n",
					"    \n",
					"    if 'ni' in column_indices:\n",
					"        row[column_indices['ni']] = generate_random_ni_number()\n",
					"    \n",
					"    # Handle regular email columns\n",
					"    if 'email_columns' in column_indices:\n",
					"        for email_col_idx in column_indices['email_columns']:\n",
					"            if email_col_idx < len(row):  # Ensure column exists in this row\n",
					"                original_email = row[email_col_idx]\n",
					"                row[email_col_idx] = mask_email_with_pers_number(original_email, pers_no, is_lm_email=False)\n",
					"    \n",
					"    # Handle LM email columns\n",
					"    if 'lm_email_columns' in column_indices:\n",
					"        for lm_email_col_idx in column_indices['lm_email_columns']:\n",
					"            if lm_email_col_idx < len(row):  # Ensure column exists in this row\n",
					"                original_email = row[lm_email_col_idx]\n",
					"                print(f\"    DEBUG: Processing LM email '{original_email}' with PersNo '{pers_no}' from column '{pers_no_column_name}'\")\n",
					"                masked_email = mask_email_with_pers_number(original_email, pers_no, is_lm_email=True)\n",
					"                row[lm_email_col_idx] = masked_email\n",
					"                print(f\"    DEBUG: LM Email {original_email} -> {masked_email} (PersNo: {pers_no})\")\n",
					"    \n",
					"    # Handle name columns (individual names like First Name, Last Name)\n",
					"    if 'name_columns' in column_indices:\n",
					"        for name_col_idx in column_indices['name_columns']:\n",
					"            if name_col_idx < len(row):  # Ensure column exists in this row\n",
					"                original_name = row[name_col_idx]\n",
					"                row[name_col_idx] = mask_name_field(original_name)\n",
					"    \n",
					"    # Handle manager name columns (full names like \"John Smith\")\n",
					"    if 'manager_name_columns' in column_indices:\n",
					"        print(f\"    DEBUG: Processing {len(column_indices['manager_name_columns'])} manager name columns\")\n",
					"        for manager_name_col_idx in column_indices['manager_name_columns']:\n",
					"            if manager_name_col_idx < len(row):  # Ensure column exists in this row\n",
					"                original_manager_name = row[manager_name_col_idx]\n",
					"                print(f\"    DEBUG: Processing manager name '{original_manager_name}' at column {manager_name_col_idx}\")\n",
					"                masked_manager_name = mask_full_name(original_manager_name)\n",
					"                row[manager_name_col_idx] = masked_manager_name\n",
					"                print(f\"    DEBUG: Manager name '{original_manager_name}' -> '{masked_manager_name}'\")\n",
					"            else:\n",
					"                print(f\"    DEBUG: Manager name column {manager_name_col_idx} not found in row (row length: {len(row)})\")\n",
					"    \n",
					"    if 'age' in column_indices and new_dob:\n",
					"        age_col_idx = column_indices['age']\n",
					"        if age_col_idx < len(row):\n",
					"            original_age = row[age_col_idx]\n",
					"            new_age = calculate_age_from_dob(new_dob)\n",
					"            row[age_col_idx] = new_age\n",
					"            print(f\"    DEBUG: Age column updated from '{original_age}' to '{new_age}' (calculated from DOB)\")\n",
					"        else:\n",
					"            print(f\"    DEBUG: Age column {age_col_idx} not found in row (row length: {len(row)})\")\n",
					"    elif 'age' in column_indices:\n",
					"        age_col_idx = column_indices['age']\n",
					"        if age_col_idx < len(row):\n",
					"            original_age = row[age_col_idx]\n",
					"            new_age = str(random.randint(18, 65))\n",
					"            row[age_col_idx] = new_age\n",
					"            print(f\"    DEBUG: Age column updated from '{original_age}' to '{new_age}' (random)\")\n",
					"        else:\n",
					"            print(f\"    DEBUG: Age column {age_col_idx} not found in row (row length: {len(row)})\")\n",
					"    \n",
					"    return row\n",
					"\n",
					"def write_csv_to_azure(file_client, data):\n",
					"    \"\"\"Write CSV data back to Azure File Storage\"\"\"\n",
					"    # Convert data back to CSV string\n",
					"    output = StringIO()\n",
					"    csv_writer = csv.writer(output)\n",
					"    csv_writer.writerows(data)\n",
					"    csv_content = output.getvalue().encode('utf-8')\n",
					"    \n",
					"    try:\n",
					"        # Method 1: Try with overwrite parameter (newer SDK versions)\n",
					"        file_client.upload_file(csv_content, overwrite=True)\n",
					"    except TypeError:\n",
					"        try:\n",
					"            # Method 2: Delete and recreate (older SDK versions)\n",
					"            file_client.delete_file()\n",
					"            file_client.create_file(len(csv_content))\n",
					"            file_client.upload_range(csv_content, offset=0, length=len(csv_content))\n",
					"        except Exception:\n",
					"            try:\n",
					"                # Method 3: Create and upload in chunks\n",
					"                file_client.create_file(len(csv_content))\n",
					"                file_client.upload_range(csv_content, offset=0, length=len(csv_content))\n",
					"            except Exception as e:\n",
					"                # Method 4: Simple upload (last resort)\n",
					"                file_client.upload_file(csv_content)\n",
					"\n",
					"# Define connection parameters\n",
					"secret_name = \"datalab-connectionstring\"\n",
					"kv_linked_service = \"ls_kv\"\n",
					"share_name = \"datalab\"\n",
					"directory_path = f\"ODW/HR/SAPHR_SharepointData/{Param_Env_Type}/MONTHLY/\"\n",
					"\n",
					"# Retrieve connection string securely from Key Vault\n",
					"akv_name = spark.sparkContext.environment.get('keyVaultName', 'get')\n",
					"creds = mssparkutils.credentials.getSecret(akv_name, secret_name, kv_linked_service)\n",
					"\n",
					"# Create ShareDirectoryClient instance\n",
					"parent_dir = ShareDirectoryClient.from_connection_string(\n",
					"    conn_str=creds,\n",
					"    share_name=share_name,\n",
					"    directory_path=directory_path\n",
					")\n",
					"\n",
					"# Get files and directories\n",
					"files_and_dirs = list(parent_dir.list_directories_and_files())\n",
					"file_metadata = []\n",
					"processed_files = []\n",
					"skipped_files = []\n",
					"\n",
					"print(\"Starting CSV anonymization process...\")\n",
					"print(\"-\" * 50)\n",
					"\n",
					"for item in files_and_dirs:\n",
					"    if not item['is_directory'] and item['name'].endswith('.csv'):\n",
					"        print(f\"\\nProcessing file: {item['name']}\")\n",
					"        \n",
					"        file_client = ShareFileClient.from_connection_string(\n",
					"            conn_str=creds,\n",
					"            share_name=share_name,\n",
					"            file_path=f\"{directory_path}{item['name']}\"\n",
					"        )\n",
					"        \n",
					"        try:\n",
					"            # Download and read the file\n",
					"            download_stream = file_client.download_file()\n",
					"            content = download_stream.readall().decode('utf-8')\n",
					"            csv_reader = csv.reader(StringIO(content))\n",
					"            data = list(csv_reader)\n",
					"            \n",
					"            if not data:\n",
					"                print(f\"  - Skipped: Empty file\")\n",
					"                skipped_files.append((item['name'], \"Empty file\"))\n",
					"                continue\n",
					"            \n",
					"            # Get headers and find column indices\n",
					"            headers = data[0]\n",
					"            column_indices = find_column_indices(headers)\n",
					"            \n",
					"            print(f\"  - Original rows: {len(data)}\")\n",
					"            print(f\"  - Columns: {len(headers)}\")\n",
					"            print(f\"  - Found sensitive columns: {list(column_indices.keys())}\")\n",
					"            \n",
					"            # Show column detection details\n",
					"            if 'email_columns' in column_indices:\n",
					"                email_headers = [headers[i] for i in column_indices['email_columns']]\n",
					"                print(f\"  - Email columns found: {email_headers}\")\n",
					"            \n",
					"            if 'lm_email_columns' in column_indices:\n",
					"                lm_email_headers = [headers[i] for i in column_indices['lm_email_columns']]\n",
					"                print(f\"  - LM Email columns found: {lm_email_headers}\")\n",
					"            \n",
					"            if 'name_columns' in column_indices:\n",
					"                name_headers = [headers[i] for i in column_indices['name_columns']]\n",
					"                print(f\"  - Name columns found: {name_headers}\")\n",
					"            \n",
					"            if 'manager_name_columns' in column_indices:\n",
					"                manager_name_headers = [headers[i] for i in column_indices['manager_name_columns']]\n",
					"                print(f\"  - Manager Name columns found: {manager_name_headers}\")\n",
					"            \n",
					"            if 'pers_no' in column_indices:\n",
					"                pers_no_header = \"Unknown\"\n",
					"                pers_no_index = column_indices['pers_no']\n",
					"                if 'all_pers_no_columns' in column_indices:\n",
					"                    for col_index, col_name in column_indices['all_pers_no_columns']:\n",
					"                        if col_index == pers_no_index:\n",
					"                            pers_no_header = col_name\n",
					"                            break\n",
					"                print(f\"  - Personnel Number column: {pers_no_header} (index: {pers_no_index})\")\n",
					"                \n",
					"                # Show all personnel number columns found\n",
					"                if 'all_pers_no_columns' in column_indices and len(column_indices['all_pers_no_columns']) > 1:\n",
					"                    all_pers_cols = [(name, idx) for idx, name in column_indices['all_pers_no_columns']]\n",
					"                    print(f\"  - All personnel number columns found: {all_pers_cols}\")\n",
					"            else:\n",
					"                print(f\"  - WARNING: No Personnel Number column found!\")\n",
					"                print(f\"  - Available headers: {headers}\")\n",
					"                # Show which headers were checked\n",
					"                pers_no_variations = ['persno', 'pers no', 'pers_no', 'staffnumber', 'staff number', 'staff_number', 'personnel number', 'personnel_number']\n",
					"                print(f\"  - Looking for variations: {pers_no_variations}\")\n",
					"            \n",
					"            if not column_indices:\n",
					"                print(f\"  - No sensitive columns found, skipping randomization\")\n",
					"                skipped_files.append((item['name'], \"No sensitive columns found\"))\n",
					"                continue\n",
					"            \n",
					"            # Skip files that don't have any data to anonymize\n",
					"            has_anonymizable_data = any(key in column_indices for key in \n",
					"                ['salary', 'dob', 'ni', 'age', 'email_columns', 'lm_email_columns', 'name_columns', 'manager_name_columns'])\n",
					"            \n",
					"            if not has_anonymizable_data:\n",
					"                print(f\"  - No anonymizable data columns found, skipping file\")\n",
					"                skipped_files.append((item['name'], \"No anonymizable data columns\"))\n",
					"                continue\n",
					"            \n",
					"            # Process each data row (skip header)\n",
					"            randomized_count = 0\n",
					"            for i in range(1, len(data)):\n",
					"                if len(data[i]) == len(headers):  # Ensure row has correct number of columns\n",
					"                    # Generate consistent DOB for age calculation\n",
					"                    new_dob = generate_random_dob() if 'dob' in column_indices else None\n",
					"                    data[i] = randomize_row_data(data[i], column_indices, new_dob)\n",
					"                    randomized_count += 1\n",
					"            \n",
					"            # Write back to Azure\n",
					"            write_csv_to_azure(file_client, data)\n",
					"            \n",
					"            print(f\"  - Randomized {randomized_count} rows\")\n",
					"            print(f\"  - Successfully uploaded anonymized file\")\n",
					"            processed_files.append(item['name'])\n",
					"            \n",
					"            # Get file metadata\n",
					"            props = file_client.get_file_properties()\n",
					"            last_modified_date = props['last_modified'].date()\n",
					"            file_metadata.append((item['name'], last_modified_date))\n",
					"            \n",
					"        except Exception as e:\n",
					"            print(f\"  - Error processing {item['name']}: {str(e)}\")\n",
					"            skipped_files.append((item['name'], f\"Error: {str(e)}\"))\n",
					"            continue\n",
					"\n",
					"print(\"\\n\" + \"=\" * 50)\n",
					"print(\"ANONYMIZATION SUMMARY\")\n",
					"print(\"=\" * 50)\n",
					"print(f\"Total files processed: {len(processed_files)}\")\n",
					"print(f\"Total files skipped: {len(skipped_files)}\")\n",
					"\n",
					"if processed_files:\n",
					"    print(f\"\\nSuccessfully anonymized files:\")\n",
					"    for filename in processed_files:\n",
					"        print(f\"  - {filename}\")\n",
					"\n",
					"if skipped_files:\n",
					"    print(f\"\\nSkipped files:\")\n",
					"    for filename, reason in skipped_files:\n",
					"        print(f\"  - {filename}: {reason}\")\n",
					"\n",
					"print(f\"\\nTotal unique emails anonymized: {len(email_mapping)}\")\n",
					"if len(email_mapping) > 0:\n",
					"    print(\"Sample email mappings:\")\n",
					"    sample_count = min(3, len(email_mapping))\n",
					"    for i, (original, masked) in enumerate(list(email_mapping.items())[:sample_count]):\n",
					"        print(f\"  {original} -> {masked}\")\n",
					"\n",
					"if file_metadata:\n",
					"    most_recent_date = max(date for name, date in file_metadata)\n",
					"    print(f\"\\nMost recent file date: {most_recent_date}\")\n",
					"else:\n",
					"    print(\"\\nNo CSV files found.\")\n",
					"\n",
					"print(\"\\nAnonymization process completed!\")"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Anonymised all csv files present in datalab"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": false,
					"run_control": {
						"frozen": true
					}
				},
				"source": [
					"\n",
					"def write_csv_to_odw(file_client, data):\n",
					"    \"\"\"Write CSV data back to Azure File Storage\"\"\"\n",
					"    # Convert data back to CSV string\n",
					"    output = StringIO()\n",
					"    csv_writer = csv.writer(output)\n",
					"    csv_writer.writerows(data)\n",
					"    csv_content = output.getvalue().encode('utf-8')\n",
					"    \n",
					"    try:\n",
					"        # Method 1: Try with overwrite parameter (newer SDK versions)\n",
					"        file_client.upload_file(csv_content)\n",
					"    except Exception as e:\n",
					"        print(f\"  Error occurred while uploading csv files : {str(e)}\")\n",
					"\n",
					"# Define connection parameters\n",
					"secret_name = \"datalab-connectionstring\"\n",
					"kv_linked_service = \"ls_kv\"\n",
					"share_name = \"datalab\"\n",
					"directory_path = f\"ODW/HR/SAPHR_SharepointData/{Param_Env_Type}/MONTHLY/\"\n",
					"\n",
					"# Retrieve connection string securely from Key Vault\n",
					"akv_name = spark.sparkContext.environment.get('keyVaultName', 'get')\n",
					"creds = mssparkutils.credentials.getSecret(akv_name, secret_name, kv_linked_service)\n",
					"\n",
					"# Create ShareDirectoryClient instance\n",
					"parent_dir = ShareDirectoryClient.from_connection_string(\n",
					"    conn_str=creds,\n",
					"    share_name=share_name,\n",
					"    directory_path=directory_path\n",
					")\n",
					"\n",
					"# Get files and directories\n",
					"files_and_dirs = list(parent_dir.list_directories_and_files())\n",
					"file_metadata = []\n",
					"processed_files = []\n",
					"\n",
					"print(\"Starting CSV anonymization process...\")\n",
					"print(\"-\" * 50)\n",
					"\n",
					"for item in files_and_dirs:\n",
					"    if not item['is_directory'] and item['name'].endswith('.csv'):\n",
					"        print(f\"\\nProcessing file: {item['name']}\")\n",
					"        \n",
					"        file_client = ShareFileClient.from_connection_string(\n",
					"            conn_str=creds,\n",
					"            share_name=share_name,\n",
					"            file_path=f\"{directory_path}{item['name']}\"\n",
					"        )\n",
					"        \n",
					"        try:\n",
					"            # Download and read the file\n",
					"            download_stream = file_client.download_file()\n",
					"            content = download_stream.readall().decode('utf-8')\n",
					"            csv_reader = csv.reader(StringIO(content))\n",
					"            data = list(csv_reader)\n",
					"            \n",
					"            if not data:\n",
					"                print(f\"  - Skipped: Empty file\")\n",
					"                continue\n",
					"            \n",
					"            # Get headers and find column indices\n",
					"            headers = data[0]\n",
					"            column_indices = find_column_indices(headers)\n",
					"            \n",
					"            print(f\"  - Original rows: {len(data)}\")\n",
					"            print(f\"  - Columns: {len(headers)}\")\n",
					"            print(f\"  - Found sensitive columns: {list(column_indices.keys())}\")\n",
					"            \n",
					"            # Show email columns specifically\n",
					"            if 'email_columns' in column_indices:\n",
					"                email_headers = [headers[i] for i in column_indices['email_columns']]\n",
					"                print(f\"  - Email columns found: {email_headers}\")\n",
					"            \n",
					"            if not column_indices:\n",
					"                print(f\"  - No sensitive columns found, skipping randomization\")\n",
					"                continue\n",
					"            \n",
					"            # Process each data row (skip header)\n",
					"            randomized_count = 0\n",
					"            for i in range(1, len(data)):\n",
					"                if len(data[i]) == len(headers):  # Ensure row has correct number of columns\n",
					"                    # Generate consistent DOB for age calculation\n",
					"                    new_dob = generate_random_dob() if 'dob' in column_indices else None\n",
					"                    data[i] = randomize_row_data(data[i], column_indices, new_dob)\n",
					"                    randomized_count += 1\n",
					"            \n",
					"            # Write back to Azure\n",
					"            write_csv_to_odw(file_client, data)\n",
					"            \n",
					"            print(f\"  - Randomized {randomized_count} rows\")\n",
					"            print(f\"  - Successfully uploaded anonymized file\")\n",
					"            processed_files.append(item['name'])\n",
					"            \n",
					"            # Get file metadata\n",
					"            props = file_client.get_file_properties()\n",
					"            last_modified_date = props['last_modified'].date()\n",
					"            file_metadata.append((item['name'], last_modified_date))\n",
					"            \n",
					"        except Exception as e:\n",
					"            print(f\"  - Error processing {item['name']}: {str(e)}\")\n",
					"            continue\n",
					"\n",
					"print(\"\\n\" + \"=\" * 50)\n",
					"print(\"ANONYMIZATION SUMMARY\")\n",
					"print(\"=\" * 50)\n",
					"print(f\"Total files processed: {len(processed_files)}\")\n",
					"print(f\"Successfully anonymized files:\")\n",
					"for filename in processed_files:\n",
					"    print(f\"  - {filename}\")\n",
					"\n",
					"if file_metadata:\n",
					"    most_recent_date = max(date for name, date in file_metadata)\n",
					"    print(f\"\\nMost recent file date: {most_recent_date}\")\n",
					"else:\n",
					"    print(\"\\nNo CSV files found.\")\n",
					"\n",
					"print(\"\\nAnonymization process completed!\")"
				],
				"execution_count": 12
			}
		]
	}
}