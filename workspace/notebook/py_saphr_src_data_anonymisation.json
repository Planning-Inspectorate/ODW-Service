{
	"name": "py_saphr_src_data_anonymisation",
	"properties": {
		"folder": {
			"name": "0-odw-source-to-raw/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "a5cf221e-5da1-4f7f-960e-cfe5455b0d0c"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this pyspark notebook is to reads all recent files from the given odw pinsdatalab folder path and anonymise the data into the source folder before it gets to odw-raw ADLS2. \n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Rohit Shukla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;19-Jan-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The functionality of this notebook is to anonymised certain columns of SAP HR data if the environment is dev or test then data will be anonymised.\n",
					"\n",
					"**Spark Cluster Configuration** -> Apache Spark Version- 3.4, Python Version \t\t- 3.10, Delta Lake Version \t- 2.4\n",
					"\n",
					"\n",
					"##### The input parameters are:\n",
					"###### Param_Env_Type => This is a mandatory parameter which refers to the environment where the data needs anonymisation."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Input parameters"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#Define Input Parameters to get values from the pipleline\n",
					"Param_Env_Type = 'dev'"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Import all required Python Libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#import all libraries and initialise Spark Session\n",
					"import json\n",
					"import traceback\n",
					"import calendar\n",
					"import time\n",
					"import csv\n",
					"import requests\n",
					"import pyspark.sql.functions as F \n",
					"import os\n",
					"import re\n",
					"from io import StringIO\n",
					"from datetime import datetime, timedelta, date\n",
					"from azure.storage.fileshare import ShareDirectoryClient,ShareFileClient\n",
					"from operator import add\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql.functions import regexp_replace,lit, current_timestamp, to_date ,expr, md5, col, date_format, when, length, lpad,input_file_name,sha2,concat\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.sql import SparkSession\n",
					"spark = SparkSession.builder.getOrCreate()\n",
					"from delta.tables import DeltaTable\n",
					"#ignore FutureWarning messages \n",
					"import warnings\n",
					"warnings.filterwarnings(\"ignore\", message=\"iteritems is deprecated\")"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Get Storage account"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#Get Storage account name\n",
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"#print(storage_account)"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Enable message logging"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define all required folder paths"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Define all Folder paths used in the notebook\n",
					"\n",
					"Param_Json_SchemaFolder_Name = Param_FileFolder_Path.lower()\n",
					"\n",
					"odw_raw_base_folder_path = f\"abfss://odw-raw@{storage_account}{Param_FileFolder_Path}/\"\n",
					"delta_table_base_path = f\"abfss://odw-standardised@{storage_account}{Param_Json_SchemaFolder_Name}\"\n",
					"#schema_file_path = f\"abfss://odw-config@{storage_account}/schema_creation/{Param_Json_SchemaFolder_Name}/create_schema.json\"\n",
					"json_schema_file_path = f\"abfss://odw-config@{storage_account}/orchestration/orchestration_saphr.json\"\n",
					"\n",
					"database_name = \"odw_standardised_db\"\n",
					"process_name = 'py_raw_to_std'\n",
					"\n",
					"logging_container = f\"abfss://logging@{storage_account}\"\n",
					"logging_table_name = 'tables_logs'\n",
					"ingestion_log_table_location = logging_container + logging_table_name\n",
					"\n",
					"# json result result dump list\n",
					"processing_results = []"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"import logging\n",
					"\n",
					"# Disable Azure Storage SDK logging\n",
					"logging.getLogger('azure.storage').setLevel(logging.WARNING)\n",
					"logging.getLogger('azure.core').setLevel(logging.WARNING)\n",
					"\n",
					"\n",
					"# Define connection string\n",
					"secret_name = \"datalab-connectionstring\"\n",
					"kv_linked_service = \"ls_kv\"\n",
					"share_name = \"datalab\"\n",
					"directory_path = f\"ODW/HR/SAPHR_SharepointData/{Param_Env_Type}/MONTHLY/\"\n",
					"\n",
					"# Retrieve connection string securely from Key Vault\n",
					"akv_name = spark.sparkContext.environment.get('keyVaultName', 'get')\n",
					"creds = mssparkutils.credentials.getSecret(akv_name, secret_name, kv_linked_service)\n",
					"\n",
					"# Create ShareDirectoryClient instance\n",
					"parent_dir = ShareDirectoryClient.from_connection_string(\n",
					"    conn_str=creds,\n",
					"    share_name=share_name,\n",
					"    directory_path=directory_path\n",
					")\n",
					"\n",
					"# Initialize list for storing (filename, last_modified_date)\n",
					"files_and_dirs = list(parent_dir.list_directories_and_files())\n",
					"file_metadata = []\n",
					"\n",
					"for item in files_and_dirs:\n",
					"    if not item['is_directory'] and item['name'].endswith('.csv'):\n",
					"        file_client = ShareFileClient.from_connection_string(\n",
					"            conn_str=creds,\n",
					"            share_name=share_name,\n",
					"            file_path=f\"{directory_path}{item['name']}\"\n",
					"        )\n",
					"        \n",
					"        download_stream = file_client.download_file()\n",
					"        \n",
					"        # Read CSV files in UTF-8 format\n",
					"        content = download_stream.readall().decode('utf-8')\n",
					"        csv_reader = csv.reader(StringIO(content))\n",
					"        data = list(csv_reader)\n",
					"        \n",
					"        print(f\"File: {item['name']}\")\n",
					"        print(f\"Rows: {len(data)}\")\n",
					"        print(f\"Columns: {len(data[0]) if data else 0}\")\n",
					"        \n",
					"        props = file_client.get_file_properties()\n",
					"        last_modified_date = props['last_modified'].date()\n",
					"        file_metadata.append((item['name'], last_modified_date))\n",
					"\n",
					"if file_metadata:\n",
					"    most_recent_date = max(date for name, date in file_metadata)\n",
					"    print(f\"\\nMost Recent File Date: {most_recent_date}\")\n",
					"else:\n",
					"    print(\"No CSV files found.\")"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"source": [
					"import logging\n",
					"import csv\n",
					"import random\n",
					"import string\n",
					"from io import StringIO\n",
					"from datetime import datetime, timedelta\n",
					"from azure.storage.fileshare import ShareDirectoryClient, ShareFileClient\n",
					"\n",
					"# Disable Azure Storage SDK logging\n",
					"logging.getLogger('azure.storage').setLevel(logging.WARNING)\n",
					"logging.getLogger('azure.core').setLevel(logging.WARNING)\n",
					"\n",
					"def generate_random_salary(min_salary=20000, max_salary=150000):\n",
					"    \"\"\"Generate random salary within realistic range\"\"\"\n",
					"    return random.randint(min_salary, max_salary)\n",
					"\n",
					"def generate_random_dob(min_age=18, max_age=65):\n",
					"    \"\"\"Generate random date of birth based on age range\"\"\"\n",
					"    today = datetime.now()\n",
					"    # Calculate birth year range\n",
					"    min_birth_year = today.year - max_age\n",
					"    max_birth_year = today.year - min_age\n",
					"\n",
					"    # Generate random date\n",
					"    year = random.randint(min_birth_year, max_birth_year)\n",
					"    month = random.randint(1, 12)\n",
					"    day = random.randint(1, 28)  # Use 28 to avoid month-specific day issues\n",
					"\n",
					"    return f\"{day:02d}/{month:02d}/{year}\"\n",
					"\n",
					"def generate_random_ni_number():\n",
					"    \"\"\"Generate random NI number in correct format (XX123456X)\"\"\"\n",
					"    # First two letters (avoid certain combinations)\n",
					"    valid_first_letters = 'ABCDEFGHJKLMNPQRSTUVWXYZ'  # Exclude I, O\n",
					"    valid_second_letters = 'ABCDEFGHJKLMNPQRSTUVWXYZ'\n",
					"\n",
					"    first_letter = random.choice(valid_first_letters)\n",
					"    second_letter = random.choice(valid_second_letters)\n",
					"\n",
					"    # Six digits\n",
					"    digits = ''.join([str(random.randint(0, 9)) for _ in range(6)])\n",
					"\n",
					"    # Final letter\n",
					"    final_letter = random.choice('ABCD')\n",
					"\n",
					"    return f\"{first_letter}{second_letter}{digits}{final_letter}\"\n",
					"\n",
					"def generate_anonymous_email():\n",
					"    \"\"\"Generate completely anonymous email address\"\"\"\n",
					"    # Random username components\n",
					"    prefixes = ['user', 'emp', 'staff', 'person', 'member', 'team', 'dept']\n",
					"\n",
					"    # Generate random components\n",
					"    prefix = random.choice(prefixes)\n",
					"    random_number = random.randint(1000, 9999)\n",
					"    random_suffix = ''.join(random.choices(string.ascii_lowercase, k=3))\n",
					"\n",
					"    # Common business domains\n",
					"    domains = ['company.com', 'business.co.uk', 'corp.com', 'enterprise.org', \n",
					"                'group.com', 'solutions.com', 'services.co.uk', 'consulting.com']\n",
					"\n",
					"    domain = random.choice(domains)\n",
					"\n",
					"    return f\"{prefix}{random_number}{random_suffix}@{domain}\"\n",
					"\n",
					"def mask_email_advanced(original_email):\n",
					"     \"\"\"\n",
					"    Advanced email masking that makes it very difficult to guess original\n",
					"    Creates completely new anonymous email while preserving format\n",
					"    \"\"\"\n",
					"    try:\n",
					"        if '@' not in original_email:\n",
					"            return generate_anonymous_email()\n",
					"\n",
					"        # Split email into parts\n",
					"        local_part, domain_part = original_email.split('@', 1)\n",
					"\n",
					"        # Strategy 1: Complete replacement with anonymous email (70% of time)\n",
					"        if random.random() < 0.7:\n",
					"            return generate_anonymous_email()\n",
					"\n",
					"        # Strategy 2: Partial structure preservation but heavily masked (30% of time)\n",
					"        # Create completely new local part\n",
					"        masked_local = ''.join(random.choices(string.ascii_lowercase + string.digits, k=random.randint(6, 10)))\n",
					"\n",
					"        # Mask domain while keeping structure\n",
					"        if '.' in domain_part:\n",
					"            domain_parts = domain_part.split('.')\n",
					"            # Replace domain name but keep TLD structure\n",
					"            masked_domain_name = ''.join(random.choices(string.ascii_lowercase, k=random.randint(5, 8)))\n",
					"            # Keep original TLD structure or randomize it\n",
					"            if random.random() < 0.5:\n",
					"                masked_domain = f\"{masked_domain_name}.{domain_parts[-1]}\"\n",
					"            else:\n",
					"                tlds = ['com', 'co.uk', 'org', 'net', 'gov.uk', 'edu']\n",
					"                masked_domain = f\"{masked_domain_name}.{random.choice(tlds)}\"\n",
					"        else:\n",
					"            masked_domain = ''.join(random.choices(string.ascii_lowercase, k=random.randint(5, 8))) + '.com'\n",
					"\n",
					"        return f\"{masked_local}@{masked_domain}\"\n",
					"\n",
					"    except:\n",
					"        # If any error occurs, return completely anonymous email\n",
					"        return generate_anonymous_email()\n",
					"\n",
					"def calculate_age_from_dob(dob_str):\n",
					"    \"\"\"Calculate age from date of birth string (DD/MM/YYYY format)\"\"\"\n",
					"    try:\n",
					"        # Parse the date string\n",
					"        birth_date = datetime.strptime(dob_str, \"%d/%m/%Y\")\n",
					"        today = datetime.now()\n",
					"\n",
					"        # Calculate age\n",
					"        age = today.year - birth_date.year\n",
					"        if today.month < birth_date.month or (today.month == birth_date.month and today.day < birth_date.day):\n",
					"            age -= 1\n",
					"\n",
					"        return str(age)\n",
					"    except:\n",
					"        # If parsing fails, return random age\n",
					"        return str(random.randint(18, 65))\n",
					"\n",
					"def find_column_indices(headers):\n",
					"    \"\"\"Find indices of columns to randomize (case-insensitive)\"\"\"\n",
					"    indices = {}\n",
					"\n",
					"    # Define column name variations\n",
					"    salary_variations = ['annual salary', 'salary', 'annual_salary', 'annualsalary']\n",
					"    dob_variations = ['date of birth', 'dob', 'date_of_birth', 'dateofbirth', 'birth date', 'birthdate']\n",
					"    ni_variations = ['ni number', 'ni_number', 'ninumber', 'national insurance', 'national_insurance']\n",
					"    age_variations = ['age']\n",
					"    email_variations = ['email', 'email address', 'email_address', 'emailaddress', 'e-mail', 'e_mail']\n",
					"\n",
					"    # Convert headers to lowercase for comparison\n",
					"    lower_headers = [header.lower().strip() for header in headers]\n",
					"\n",
					"    for i, header in enumerate(lower_headers):\n",
					"        if any(var in header for var in salary_variations):\n",
					"            indices['salary'] = i\n",
					"        elif any(var in header for var in dob_variations):\n",
					"             indices['dob'] = i\n",
					"        elif any(var in header for var in ni_variations):\n",
					"             indices['ni'] = i\n",
					"        elif any(var in header for var in age_variations):\n",
					"             indices['age'] = i\n",
					"        elif any(var in header for var in email_variations):\n",
					"             indices['email'] = i\n",
					"\n",
					"    return indices\n",
					"\n",
					"def randomize_row_data(row, column_indices, new_dob=None):\n",
					"    \"\"\"Randomize sensitive data in a row\"\"\"\n",
					"    if 'salary' in column_indices:\n",
					"        row[column_indices['salary']] = str(generate_random_salary())\n",
					"\n",
					"    if 'dob' in column_indices:\n",
					"        if new_dob is None:\n",
					"            new_dob = generate_random_dob()\n",
					"        row[column_indices['dob']] = new_dob\n",
					"\n",
					"    if 'ni' in column_indices:\n",
					"        row[column_indices['ni']] = generate_random_ni_number()\n",
					"\n",
					"    if 'email' in column_indices:\n",
					"        original_email = row[column_indices['email']]\n",
					"        row[column_indices['email']] = mask_email_advanced(original_email)\n",
					"\n",
					"    if 'age' in column_indices and new_dob:\n",
					"        row[column_indices['age']] = calculate_age_from_dob(new_dob)\n",
					"    elif 'age' in column_indices:\n",
					"         row[column_indices['age']] = str(random.randint(18, 65))\n",
					"\n",
					"    return row\n",
					"\n",
					"def write_csv_to_azure(file_client, data):\n",
					"    \"\"\"Write CSV data back to Azure File Storage\"\"\"\n",
					"    # Convert data back to CSV string\n",
					"    output = StringIO()\n",
					"    csv_writer = csv.writer(output)\n",
					"    csv_writer.writerows(data)\n",
					"    csv_content = output.getvalue()\n",
					"\n",
					"    # Upload the file\n",
					"    file_client.upload_file(csv_content.encode('utf-8'), overwrite=True)\n",
					"\n",
					"# Define connection parameters\n",
					"secret_name = \"datalab-connectionstring\"\n",
					"kv_linked_service = \"ls_kv\"\n",
					"share_name = \"datalab\"\n",
					"directory_path = f\"ODW/HR/SAPHR_SharepointData/{Param_Env_Type}/MONTHLY/\"\n",
					"\n",
					"# Retrieve connection string securely from Key Vault\n",
					"akv_name = spark.sparkContext.environment.get('keyVaultName', 'get')\n",
					"creds = mssparkutils.credentials.getSecret(akv_name, secret_name, kv_linked_service)\n",
					"\n",
					"# Create ShareDirectoryClient instance\n",
					"parent_dir = ShareDirectoryClient.from_connection_string(\n",
					"    conn_str=creds,\n",
					"    share_name=share_name,\n",
					"    directory_path=directory_path\n",
					")\n",
					"\n",
					"# Get files and directories\n",
					"files_and_dirs = list(parent_dir.list_directories_and_files())\n",
					"file_metadata = []\n",
					"processed_files = []\n",
					"\n",
					"print(\"Starting CSV anonymization process...\")\n",
					"print(\"-\" * 50)\n",
					"\n",
					"for item in files_and_dirs:\n",
					"    if not item['is_directory'] and item['name'].endswith('.csv'):\n",
					"        print(f\"\\nProcessing file: {item['name']}\")\n",
					"\n",
					"        file_client = ShareFileClient.from_connection_string(\n",
					"            conn_str=creds,\n",
					"            share_name=share_name,\n",
					"            file_path=f\"{directory_path}{item['name']}\"\n",
					"        )\n",
					"        \n",
					"        try:\n",
					"            # Download and read the file\n",
					"            download_stream = file_client.download_file()\n",
					"            content = download_stream.readall().decode('utf-8')\n",
					"            csv_reader = csv.reader(StringIO(content))\n",
					"            data = list(csv_reader)\n",
					"            \n",
					"            if not data:\n",
					"                print(f\"  - Skipped: Empty file\")\n",
					"                continue\n",
					"            \n",
					"            # Get headers and find column indices\n",
					"            headers = data[0]\n",
					"            column_indices = find_column_indices(headers)\n",
					"            \n",
					"            print(f\"  - Original rows: {len(data)}\")\n",
					"            print(f\"  - Columns: {len(headers)}\")\n",
					"            print(f\"  - Found sensitive columns: {list(column_indices.keys())}\")\n",
					"            \n",
					"            if not column_indices:\n",
					"                print(f\"  - No sensitive columns found, skipping randomization\")\n",
					"                continue\n",
					"            \n",
					"            # Process each data row (skip header)\n",
					"            randomized_count = 0\n",
					"            for i in range(1, len(data)):\n",
					"                if len(data[i]) == len(headers):  # Ensure row has correct number of columns\n",
					"                    # Generate consistent DOB for age calculation\n",
					"                    new_dob = generate_random_dob() if 'dob' in column_indices else None\n",
					"                    data[i] = randomize_row_data(data[i], column_indices, new_dob)\n",
					"                    randomized_count += 1\n",
					"            \n",
					"            # Write back to Azure\n",
					"            write_csv_to_azure(file_client, data)\n",
					"            \n",
					"            print(f\"  - Randomized {randomized_count} rows\")\n",
					"            print(f\"  - Successfully uploaded anonymized file\")\n",
					"            processed_files.append(item['name'])\n",
					"            \n",
					"            # Get file metadata\n",
					"            props = file_client.get_file_properties()\n",
					"            last_modified_date = props['last_modified'].date()\n",
					"            file_metadata.append((item['name'], last_modified_date))\n",
					"            \n",
					"        except Exception as e:\n",
					"            print(f\"  - Error processing {item['name']}: {str(e)}\")\n",
					"            continue\n",
					"\n",
					"print(\"\\n\" + \"=\" * 50)\n",
					"print(\"ANONYMIZATION SUMMARY\")\n",
					"print(\"=\" * 50)\n",
					"print(f\"Total files processed: {len(processed_files)}\")\n",
					"print(f\"Successfully anonymized files:\")\n",
					"for filename in processed_files:\n",
					"    print(f\"  - {filename}\")\n",
					"\n",
					"if file_metadata:\n",
					"    most_recent_date = max(date for name, date in file_metadata)\n",
					"    print(f\"\\nMost recent file date: {most_recent_date}\")\n",
					"else:\n",
					"    print(\"\\nNo CSV files found.\")\n",
					"\n",
					"print(\"\\nAnonymization process completed!\")"
				],
				"execution_count": null
			}
		]
	}
}