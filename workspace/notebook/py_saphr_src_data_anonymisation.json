{
	"name": "py_saphr_src_data_anonymisation",
	"properties": {
		"folder": {
			"name": "0-odw-source-to-raw/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "8b14e9ec-9226-4a3a-b583-d2858f44ceb0"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this pyspark notebook is to reads all recent files from the given odw pinsdatalab folder path and anonymise the data into the source folder before it gets to odw-raw ADLS2. \n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Rohit Shukla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;19-Jan-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The functionality of this notebook is to anonymised certain columns of SAP HR data if the environment is dev or test then data will be anonymised.\n",
					"\n",
					"**Spark Cluster Configuration** -> Apache Spark Version- 3.4, Python Version \t\t- 3.10, Delta Lake Version \t- 2.4\n",
					"\n",
					"\n",
					"##### The input parameters are:\n",
					"###### Param_Env_Type => This is a mandatory parameter which refers to the environment where the data needs anonymisation."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Input parameters"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#Define Input Parameters to get values from the pipleline\n",
					"Param_Env_Type = 'dev'\n",
					"Param_File_Load_Type = 'MONTHLY'\n",
					""
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Import all required Python Libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#import all libraries and initialise Spark Session\n",
					"import json\n",
					"import traceback\n",
					"import calendar\n",
					"import time\n",
					"import csv\n",
					"import requests\n",
					"import pyspark.sql.functions as F \n",
					"import os\n",
					"import re\n",
					"import logging\n",
					"import csv\n",
					"import random\n",
					"import string\n",
					"from io import StringIO\n",
					"from azure.storage.fileshare import ShareDirectoryClient, ShareFileClient\n",
					"from datetime import datetime, timedelta, date\n",
					"from azure.storage.fileshare import ShareDirectoryClient,ShareFileClient\n",
					"from operator import add\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql.functions import regexp_replace,lit, current_timestamp, to_date ,expr, md5, col, date_format, when, length, lpad,input_file_name,sha2,concat\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.sql import SparkSession\n",
					"spark = SparkSession.builder.getOrCreate()\n",
					"from delta.tables import DeltaTable\n",
					"#ignore FutureWarning messages \n",
					"import warnings\n",
					"warnings.filterwarnings(\"ignore\", message=\"iteritems is deprecated\")"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Enable message logging"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define all data anonymisation related functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"# Disable Azure Storage SDK logging\n",
					"logging.getLogger('azure.storage').setLevel(logging.WARNING)\n",
					"logging.getLogger('azure.core').setLevel(logging.WARNING)\n",
					"\n",
					"# Global email mapping dictionary to ensure consistency across all files\n",
					"email_mapping = {}\n",
					"\n",
					"def generate_random_salary(min_salary=20000, max_salary=150000):\n",
					"    \"\"\"Generate random salary within realistic range\"\"\"\n",
					"    return random.randint(min_salary, max_salary)\n",
					"\n",
					"def generate_random_dob(min_age=18, max_age=70):\n",
					"    \"\"\"Generate random date of birth based on age range\"\"\"\n",
					"    today = datetime.now()\n",
					"    # Calculate birth year range\n",
					"    min_birth_year = today.year - max_age\n",
					"    max_birth_year = today.year - min_age\n",
					"    \n",
					"    # Generate random date\n",
					"    year = random.randint(min_birth_year, max_birth_year)\n",
					"    month = random.randint(1, 12)\n",
					"    day = random.randint(1, 28)  # Use 28 to avoid month-specific day issues\n",
					"    \n",
					"    return f\"{day:02d}/{month:02d}/{year}\"\n",
					"\n",
					"def generate_random_ni_number():\n",
					"    \"\"\"Generate random NI number in correct format (XX123456X)\"\"\"\n",
					"    # First two letters (avoid certain combinations)\n",
					"    valid_first_letters = 'ABCDEFGHJKLMNPQRSTUVWXYZ'  # Exclude I, O\n",
					"    valid_second_letters = 'ABCDEFGHJKLMNPQRSTUVWXYZ'\n",
					"    \n",
					"    first_letter = random.choice(valid_first_letters)\n",
					"    second_letter = random.choice(valid_second_letters)\n",
					"    \n",
					"    # Six digits\n",
					"    digits = ''.join([str(random.randint(0, 9)) for _ in range(6)])\n",
					"    \n",
					"    # Final letter\n",
					"    final_letter = random.choice('ABCD')\n",
					"    \n",
					"    return f\"{first_letter}{second_letter}{digits}{final_letter}\"\n",
					"\n",
					"def mask_email_with_persno(email, pers_no, is_lm_email=False):\n",
					"    \"\"\"\n",
					"    Email masking with PersNo/StaffNumber:\n",
					"    1. Regular email: [PersNo]@PINS.com\n",
					"    2. LM E-mail: [PersNo]_LM@PINS.com\n",
					"    3. Use consistent mapping for same email+personnel combination\n",
					"    \"\"\"\n",
					"    global email_mapping\n",
					"    \n",
					"    # Create unique key for mapping (includes LM flag AND personnel number)\n",
					"    # This ensures same email with different personnel numbers gets different masks\n",
					"    mapping_key = f\"{email}|{pers_no}|{is_lm_email}\"\n",
					"    \n",
					"    # Return consistent mapping if this exact combination already processed\n",
					"    if mapping_key in email_mapping:\n",
					"        return email_mapping[mapping_key]\n",
					"    \n",
					"    try:\n",
					"        if '@' not in email or not email.strip():\n",
					"            masked = email\n",
					"        else:\n",
					"            # Clean and validate personnel number\n",
					"            base_identifier = \"UNKNOWN\"\n",
					"            if pers_no:\n",
					"                pers_no_cleaned = str(pers_no).strip()\n",
					"                if pers_no_cleaned and pers_no_cleaned.lower() not in ['', 'none', 'null', 'n/a', 'nan']:\n",
					"                    base_identifier = pers_no_cleaned\n",
					"            \n",
					"            # Add _LM suffix for LM emails\n",
					"            if is_lm_email:\n",
					"                masked = f\"{base_identifier}_LM@PINS.com\"\n",
					"            else:\n",
					"                masked = f\"{base_identifier}@PINS.com\"\n",
					"    \n",
					"    except Exception as e:\n",
					"        # If any error occurs, return masked version\n",
					"        if is_lm_email:\n",
					"            masked = 'UNKNOWN_LM@PINS.com'\n",
					"        else:\n",
					"            masked = 'UNKNOWN@PINS.com'\n",
					"    \n",
					"    # Store in mapping for consistency\n",
					"    email_mapping[mapping_key] = masked\n",
					"    return masked\n",
					"\n",
					"def mask_name_field(name):\n",
					"    \"\"\"\n",
					"    Mask name fields using first two characters as first and last positions\n",
					"    Example: OLIVER -> O***l (using first 'O' and second 'l')\n",
					"             MUNN -> M**u (using first 'M' and second 'u')\n",
					"             AL -> AL (don't mask very short names)\n",
					"    \"\"\"\n",
					"    if not name or not name.strip():\n",
					"        return name\n",
					"    \n",
					"    name = name.strip()\n",
					"    if len(name) <= 2:\n",
					"        return name  # Don't mask very short names\n",
					"    else:\n",
					"        # Use first character as first position, second character as last position\n",
					"        # Fill middle with asterisks\n",
					"        first_char = name[0]\n",
					"        second_char = name[1]\n",
					"        asterisks_count = len(name) - 2\n",
					"        return first_char + '*' * asterisks_count + second_char\n",
					"\n",
					"def mask_full_name(full_name):\n",
					"    \"\"\"\n",
					"    Anonymise full name that contains multiple names separated by spaces\n",
					"    Example: \"JOHN SMITH\" -> \"JO** SM***\"\n",
					"             \"MARY JANE DOE\" -> \"MA** JA** DO*\"\n",
					"    \"\"\"\n",
					"    if not full_name or not full_name.strip():\n",
					"        return full_name\n",
					"    \n",
					"    # Split the full name by spaces\n",
					"    name_parts = full_name.strip().split()\n",
					"    \n",
					"    # Apply masking to each part\n",
					"    masked_parts = []\n",
					"    for part in name_parts:\n",
					"        if part:  # Skip empty parts\n",
					"            masked_parts.append(mask_name_field(part))\n",
					"    \n",
					"    # Join back with spaces\n",
					"    return ' '.join(masked_parts)\n",
					"\n",
					"def calculate_age_from_dob(dob_str):\n",
					"    \"\"\"Calculate age from date of birth string (DD/MM/YYYY format)\"\"\"\n",
					"    try:\n",
					"        # Parse the date string\n",
					"        birth_date = datetime.strptime(dob_str, \"%d/%m/%Y\")\n",
					"        today = datetime.now()\n",
					"        \n",
					"        # Calculate age\n",
					"        age = today.year - birth_date.year\n",
					"        if today.month < birth_date.month or (today.month == birth_date.month and today.day < birth_date.day):\n",
					"            age -= 1\n",
					"            \n",
					"        return str(age)\n",
					"    except:\n",
					"        # If parsing fails, return random age\n",
					"        return str(random.randint(18, 65))\n",
					"\n",
					"def find_column_indices(headers):\n",
					"    \"\"\"Find indices of columns to randomize (case-insensitive)\"\"\"\n",
					"    indices = {}\n",
					"    \n",
					"    # Define column name variations\n",
					"    salary_variations = ['annual salary', 'salary', 'annual_salary', 'annualsalary']\n",
					"    dob_variations = ['date of birth', 'dob', 'date_of_birth', 'dateofbirth', 'birth date', 'birthdate']\n",
					"    ni_variations = ['ni number', 'ni_number', 'ninumber', 'national insurance', 'national_insurance']\n",
					"    age_variations = ['age', 'age of employee', 'employee age', 'emp age', 'age of emp', 'age_of_employee', 'employee_age']\n",
					"    email_variations = ['email', 'email address', 'email_address', 'emailaddress', 'e-mail', 'e_mail']\n",
					"    pers_no_variations = ['pers.no.', 'pers.no', 'persno', 'pers no', 'pers_no', 'staffnumber', 'staff number', 'staff_number', 'personnel number', 'personnel_number', 'employee no.', 'employee no', 'employee_no']\n",
					"    name_variations = ['first name', 'firstname', 'first_name', 'last name', 'lastname', 'last_name', 'forename', 'surname']\n",
					"    # Updated to match CSV files exactly\n",
					"    manager_name_variations = ['text: line manager','name of manager (om)', 'name of manager', 'manager name', 'managername', 'manager_name']\n",
					"    \n",
					"    # Convert csv column names to lowercase for comparison\n",
					"    lower_headers = [header.lower().strip() for header in headers]\n",
					"    \n",
					"    # Store multiple email columns and LM email columns separately\n",
					"    email_columns = []\n",
					"    lm_email_columns = []\n",
					"    name_columns = []\n",
					"    manager_name_columns = []\n",
					"    personnel_number_columns = []\n",
					"    \n",
					"    for i, header in enumerate(lower_headers):\n",
					"        original_header = headers[i]\n",
					"        \n",
					"        # Check more specific patterns first to avoid false matches\n",
					"        if any(var == header for var in manager_name_variations):\n",
					"            manager_name_columns.append(i)\n",
					"        elif any(var == header for var in age_variations):\n",
					"            indices['age'] = i\n",
					"        elif any(var in header for var in salary_variations):\n",
					"            indices['salary'] = i\n",
					"        elif any(var in header for var in dob_variations):\n",
					"            indices['dob'] = i\n",
					"        elif any(var in header for var in ni_variations):\n",
					"            indices['ni'] = i\n",
					"        elif any(var in header for var in pers_no_variations):\n",
					"            personnel_number_columns.append((i, original_header))\n",
					"        elif any(var in header for var in name_variations):\n",
					"            name_columns.append(i)\n",
					"        elif 'lm e-mail' in header or 'lm_e-mail' in header or 'lm email' in header:\n",
					"            lm_email_columns.append(i)\n",
					"        elif any(var in header for var in email_variations):\n",
					"            email_columns.append(i)\n",
					"    \n",
					"    # Handle multiple personnel number columns - prefer the first one found\n",
					"    if personnel_number_columns:\n",
					"        \n",
					"        # Sort by preference: staffnumber > pers_no > others\n",
					"        personnel_number_columns.sort(key=lambda x: (\n",
					"            0 if 'staffnumber' in x[1].lower() else\n",
					"            1 if 'pers' in x[1].lower() else 2\n",
					"        ))\n",
					"        indices['pers_no'] = personnel_number_columns[0][0]\n",
					"        indices['all_pers_no_columns'] = personnel_number_columns\n",
					"    \n",
					"    # Store all email columns\n",
					"    if email_columns:\n",
					"        indices['email_columns'] = email_columns\n",
					"    if lm_email_columns:\n",
					"        indices['lm_email_columns'] = lm_email_columns\n",
					"    if name_columns:\n",
					"        indices['name_columns'] = name_columns\n",
					"    if manager_name_columns:\n",
					"        indices['manager_name_columns'] = manager_name_columns\n",
					"    \n",
					"    return indices\n",
					"\n",
					"def randomize_row_data(row, column_indices, new_dob=None):\n",
					"    \"\"\"Randomize sensitive data in a row\"\"\"\n",
					"    # Get PersNo for email anonymisation\n",
					"    pers_no = None\n",
					"    pers_no_column_name = \"Not Found\"\n",
					"    \n",
					"    if 'pers_no' in column_indices and column_indices['pers_no'] < len(row):\n",
					"        pers_no_index = column_indices['pers_no']\n",
					"        pers_no = row[pers_no_index]\n",
					"        \n",
					"        # Get column name for debugging\n",
					"        if 'all_pers_no_columns' in column_indices:\n",
					"            for col_index, col_name in column_indices['all_pers_no_columns']:\n",
					"                if col_index == pers_no_index:\n",
					"                    pers_no_column_name = col_name\n",
					"                    break\n",
					"        \n",
					"        # Clean and validate the personnel number\n",
					"        if pers_no is not None:\n",
					"            pers_no_str = str(pers_no).strip()\n",
					"            \n",
					"            if pers_no_str and pers_no_str.lower() not in ['', 'none', 'null', 'n/a', 'nan']:\n",
					"                pers_no = pers_no_str\n",
					"            else:\n",
					"                pers_no = None\n",
					"        else:\n",
					"            pers_no = None\n",
					"    \n",
					"    if 'salary' in column_indices:\n",
					"        row[column_indices['salary']] = str(generate_random_salary())\n",
					"    \n",
					"    if 'dob' in column_indices:\n",
					"        if new_dob is None:\n",
					"            new_dob = generate_random_dob()\n",
					"        row[column_indices['dob']] = new_dob\n",
					"    \n",
					"    if 'ni' in column_indices:\n",
					"        row[column_indices['ni']] = generate_random_ni_number()\n",
					"    \n",
					"    # Handle regular email columns\n",
					"    if 'email_columns' in column_indices:\n",
					"        for email_col_idx in column_indices['email_columns']:\n",
					"            if email_col_idx < len(row):  # Ensure column exists in this row\n",
					"                original_email = row[email_col_idx]\n",
					"                row[email_col_idx] = mask_email_with_persno(original_email, pers_no, is_lm_email=False)\n",
					"    \n",
					"    # Handle LM email columns\n",
					"    if 'lm_email_columns' in column_indices:\n",
					"        for lm_email_col_idx in column_indices['lm_email_columns']:\n",
					"            if lm_email_col_idx < len(row):  # Ensure column exists in this row\n",
					"                original_email = row[lm_email_col_idx]\n",
					"                masked_email = mask_email_with_persno(original_email, pers_no, is_lm_email=True)\n",
					"                row[lm_email_col_idx] = masked_email\n",
					"    \n",
					"    # Handle name columns (individual names like First Name, Last Name)\n",
					"    if 'name_columns' in column_indices:\n",
					"        for name_col_idx in column_indices['name_columns']:\n",
					"            if name_col_idx < len(row):  # Ensure column exists in this row\n",
					"                original_name = row[name_col_idx]\n",
					"                row[name_col_idx] = mask_name_field(original_name)\n",
					"    \n",
					"    # Handle manager full name columns\n",
					"    if 'manager_name_columns' in column_indices:\n",
					"        for manager_name_col_idx in column_indices['manager_name_columns']:\n",
					"            if manager_name_col_idx < len(row):  # Ensure column exists in this row\n",
					"                original_manager_name = row[manager_name_col_idx]\n",
					"                masked_manager_name = mask_full_name(original_manager_name)\n",
					"                row[manager_name_col_idx] = masked_manager_name\n",
					"    \n",
					"    if 'age' in column_indices and new_dob:\n",
					"        age_col_idx = column_indices['age']\n",
					"        if age_col_idx < len(row):\n",
					"            new_age = calculate_age_from_dob(new_dob)\n",
					"            row[age_col_idx] = new_age\n",
					"    elif 'age' in column_indices:\n",
					"        age_col_idx = column_indices['age']\n",
					"        if age_col_idx < len(row):\n",
					"            new_age = str(random.randint(18, 65))\n",
					"            row[age_col_idx] = new_age\n",
					"    \n",
					"    return row\n",
					"\n",
					"def write_csv_to_datalab(file_client, data):\n",
					"    \"\"\"Write CSV data back to datalab/ODW Azure File Storage\"\"\"\n",
					"    # Convert data back to CSV string\n",
					"    output = StringIO()\n",
					"    csv_writer = csv.writer(output)\n",
					"    csv_writer.writerows(data)\n",
					"    csv_content = output.getvalue().encode('utf-8')\n",
					"    \n",
					"    try:\n",
					"        # Upload csv file without overwrite parameter\n",
					"                file_client.upload_file(csv_content)\n",
					"    except Exception as e:\n",
					"                print(f\"Error uploading file to datalab/ODW : {str(e)}\")\n",
					"\n",
					"def time_diff_seconds(start, end):\n",
					"    try:\n",
					"        if not start or not end:\n",
					"            return 0\n",
					"\n",
					"        # Parse strings into datetime objects if needed\n",
					"        if isinstance(start, str):\n",
					"            start = datetime.strptime(start, \"%Y-%m-%d %H:%M:%S.%f\")\n",
					"        if isinstance(end, str):\n",
					"            end = datetime.strptime(end, \"%Y-%m-%d %H:%M:%S.%f\")\n",
					"\n",
					"        diff_seconds = int((end - start).total_seconds())\n",
					"        return diff_seconds if diff_seconds > 0 else 0\n",
					"\n",
					"    except Exception as e:\n",
					"        return 0\n",
					"\n",
					"#This funtion handles datetime object and covert into string\n",
					"def datetime_handler(obj):\n",
					"    if isinstance(obj, datetime):\n",
					"        return obj.isoformat()\n",
					"    raise TypeError(f\"Object of type {obj.__class__.__name__} is not JSON serializable\")\n",
					""
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Anonymised all csv files present in datalab"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"\n",
					"# Define connection parameters\n",
					"secret_name = \"datalab-connectionstring\"\n",
					"kv_linked_service = \"ls_kv\"\n",
					"share_name = \"datalab\"\n",
					"\n",
					"directory_path = f\"ODW/HR/SAPHR_SharepointData/{Param_Env_Type}/\"\n",
					"        \n",
					"if Param_File_Load_Type:            \n",
					"    directory_path += f\"{Param_File_Load_Type}/\"\n",
					"\n",
					"# Retrieve connection string securely from Key Vault\n",
					"akv_name = spark.sparkContext.environment.get('keyVaultName', 'get')\n",
					"creds = mssparkutils.credentials.getSecret(akv_name, secret_name, kv_linked_service)\n",
					"\n",
					"# Create ShareDirectoryClient instance\n",
					"parent_dir = ShareDirectoryClient.from_connection_string(\n",
					"    conn_str=creds,\n",
					"    share_name=share_name,\n",
					"    directory_path=directory_path\n",
					")\n",
					"\n",
					"# Get files and directories\n",
					"files_and_dirs = list(parent_dir.list_directories_and_files())\n",
					"file_metadata = []\n",
					"processed_files = []\n",
					"skipped_files = []\n",
					"\n",
					"print(\"Starting CSV anonymization process...\")\n",
					"\n",
					"for item in files_and_dirs:\n",
					"    if not item['is_directory'] and item['name'].endswith('.csv'):\n",
					"        print(f\"\\nProcessing file: {item['name']}\")\n",
					"        \n",
					"        file_client = ShareFileClient.from_connection_string(\n",
					"            conn_str=creds,\n",
					"            share_name=share_name,\n",
					"            file_path=f\"{directory_path}{item['name']}\"\n",
					"        )\n",
					"        \n",
					"        try:\n",
					"            # Download and read the file\n",
					"            download_stream = file_client.download_file()\n",
					"            content = download_stream.readall().decode('utf-8')\n",
					"            csv_reader = csv.reader(StringIO(content))\n",
					"            data = list(csv_reader)\n",
					"            \n",
					"            if not data:\n",
					"                print(f\"  - Skipped: Empty file\")\n",
					"                skipped_files.append((item['name'], \"Empty file\"))\n",
					"                continue\n",
					"            \n",
					"            # Get headers and find column indices\n",
					"            headers = data[0]\n",
					"            column_indices = find_column_indices(headers)\n",
					"            \n",
					"            print(f\"Original rows: {len(data)}\")\n",
					"            print(f\"Columns: {len(headers)}\")\n",
					"            print(f\"Found sensitive columns: {list(column_indices.keys())}\")\n",
					"            \n",
					"            # Show column detection details\n",
					"            if 'email_columns' in column_indices:\n",
					"                email_headers = [headers[i] for i in column_indices['email_columns']]\n",
					"                print(f\"  - Email columns found: {email_headers}\")\n",
					"            \n",
					"            if 'lm_email_columns' in column_indices:\n",
					"                lm_email_headers = [headers[i] for i in column_indices['lm_email_columns']]\n",
					"                print(f\"  - LM Email columns found: {lm_email_headers}\")\n",
					"            \n",
					"            if 'name_columns' in column_indices:\n",
					"                name_headers = [headers[i] for i in column_indices['name_columns']]\n",
					"                print(f\"  - Name columns found: {name_headers}\")\n",
					"            \n",
					"            if 'manager_name_columns' in column_indices:\n",
					"                manager_name_headers = [headers[i] for i in column_indices['manager_name_columns']]\n",
					"                print(f\"  - Manager Name columns found: {manager_name_headers}\")\n",
					"            \n",
					"            if 'pers_no' in column_indices:\n",
					"                pers_no_header = \"Unknown\"\n",
					"                pers_no_index = column_indices['pers_no']\n",
					"                if 'all_pers_no_columns' in column_indices:\n",
					"                    for col_index, col_name in column_indices['all_pers_no_columns']:\n",
					"                        if col_index == pers_no_index:\n",
					"                            pers_no_header = col_name\n",
					"                            break\n",
					"                print(f\"Pers.No. column: {pers_no_header} (index: {pers_no_index})\")\n",
					"            else:\n",
					"                print(f\"No Pers. No. column found!\")\n",
					"                print(f\"Column Names: {headers}\")\n",
					"                # Show which headers were checked\n",
					"                pers_no_variations = ['persno', 'pers no', 'pers_no', 'staffnumber', 'staff number', 'staff_number', 'personnel number', 'personnel_number']\n",
					"                print(f\"Looking for variations: {pers_no_variations}\")\n",
					"            \n",
					"            if not column_indices:\n",
					"                print(f\"No sensitive columns found, skipping anonymization\")\n",
					"                skipped_files.append((item['name'], \"No sensitive columns found\"))\n",
					"                continue\n",
					"            \n",
					"            # Skip files that don't have any data to anonymize\n",
					"            has_anonymizable_data = any(key in column_indices for key in \n",
					"                ['salary', 'dob', 'ni', 'age', 'email_columns', 'lm_email_columns', 'name_columns', 'manager_name_columns'])\n",
					"            \n",
					"            if not has_anonymizable_data:\n",
					"                print(f\"No anonymizable data columns found, skipping file\")\n",
					"                skipped_files.append((item['name'], \"No anonymizable data columns\"))\n",
					"                continue\n",
					"            \n",
					"            # Process each data row (skip header)\n",
					"            randomized_count = 0\n",
					"            for i in range(1, len(data)):\n",
					"                if len(data[i]) == len(headers):  # Ensure row has correct number of columns\n",
					"                    # Generate consistent DOB for age calculation\n",
					"                    new_dob = generate_random_dob() if 'dob' in column_indices else None\n",
					"                    data[i] = randomize_row_data(data[i], column_indices, new_dob)\n",
					"                    randomized_count += 1\n",
					"            \n",
					"            # Write back to Azure File Storage i.e. datalab/ODW\n",
					"            write_csv_to_datalab(file_client, data)\n",
					"            \n",
					"            print(f\"Randomized {randomized_count} rows\")\n",
					"            print(f\"Successfully uploaded anonymized file\")\n",
					"            processed_files.append(item['name'])\n",
					"            \n",
					"            # Get file metadata\n",
					"            props = file_client.get_file_properties()\n",
					"            last_modified_date = props['last_modified'].date()\n",
					"            file_metadata.append((item['name'], last_modified_date))\n",
					"            \n",
					"        except Exception as e:\n",
					"            print(f\"Error processing {item['name']}: {str(e)}\")\n",
					"            skipped_files.append((item['name'], f\"Error: {str(e)}\"))\n",
					"            continue\n",
					"\n",
					"print(f\"files processed details\")\n",
					"print(f\"Total files processed: {len(processed_files)}\")\n",
					"print(f\"Total files skipped: {len(skipped_files)}\")\n",
					"\n",
					"if processed_files:\n",
					"    print(f\"\\nSuccessfully anonymized files:\")\n",
					"    for filename in processed_files:\n",
					"        print(f\"  - {filename}\")\n",
					"\n",
					"if skipped_files:\n",
					"    print(f\"\\nSkipped files:\")\n",
					"    for filename, reason in skipped_files:\n",
					"        print(f\"  - {filename}: {reason}\")\n",
					"\n",
					"if file_metadata:\n",
					"    most_recent_date = max(date for name, date in file_metadata)\n",
					"    print(f\"\\nMost recent file date: {most_recent_date}\")\n",
					"else:\n",
					"    print(\"\\nNo CSV files found.\")\n",
					"\n",
					"print(\"\\nAnonymization process completed!\")"
				],
				"execution_count": 6
			}
		]
	}
}