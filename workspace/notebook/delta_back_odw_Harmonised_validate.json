{
	"name": "delta_back_odw_Harmonised_validate",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "142df98b-7dfb-4d68-a24e-8fc3e09a5c9f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###### <b> This notebook performs delta backup between UKS and UKW, exits with a dictionary of non-delta files to be processed by copy activity </b>. \n",
					"###### Identifying Delta files\n",
					"----------------------------\n",
					" * Recursively checks the whole container,directories,sub-directories for _delta_log if true then it identifies as delta directory.\n",
					" * Recursively processes each delta directory to fetch the delta files, compares source to target, the difference is a set of delta files, that are backed-up.\n",
					" * This notebook handles both insert, deletes, updates of records as it also compares _delta_log files that contains versions between Source and target as well as the files within the directory. \n",
					"###### Identifying Non-Delta files (ex: CSV, JSON, PARQUET, Excel etc)\n",
					"---------------------------\n",
					" * The function list_non_delta_file_folders recusively checks for directories and sub-directories for _delta_log and excludes directories that have _delta_log.\n",
					" * All the non-delta files are listed for full backup meaning the destination directory is overwritten everytime.\n",
					"###### Cost-Value benefit\n",
					"-----\n",
					"* Percentage savings based on number of files being backed-up daily is calculated in the end\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# pipeline parameter override at runtime based on input container\n",
					"\n",
					"container = 'odw-curated'\n",
					"target_container= 'delta-backup-curated'"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"source": [
					"import pprint\n",
					"from pyspark.sql.types import *\n",
					"import json\n",
					"import re\n",
					"from datetime import datetime\n",
					"from pyspark.sql import Row\n",
					"import traceback"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"source": [
					"source_storage_account_path = re.search('url=https://(.+?);', mssparkutils.credentials.getFullConnectionString('ls_backup_source')).group(1)\n",
					"\n",
					"full_storage_path    = f'abfss://{container}@{source_storage_account_path}'"
				],
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"source": [
					"backup_storage_account_path=re.search('url=https://(.+?);', mssparkutils.credentials.getFullConnectionString('ls_backup_destination')).group(1)"
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"source": [
					"storage_account: str = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')"
				],
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"source": [
					"# list all the directory files including files within sub-directories and delta logs\n",
					"@logging_to_appins\n",
					"def list_all_delta_files(delta_table_path, relative_path=\"\"):\n",
					"    files = []\n",
					"    full_path = f\"{delta_table_path}/{relative_path}\" if relative_path else delta_table_path\n",
					"    \n",
					"    try:\n",
					"        items = mssparkutils.fs.ls(full_path)\n",
					"    except Exception as e:\n",
					"        logInfo(f\"Warning: Could not access path '{full_path}': {str(e)}\")\n",
					"        return full_path  # Skip this path and continue\n",
					"\n",
					"    for item in items:\n",
					"        item_path = f\"{relative_path}/{item.name}\".rstrip(\"/\")\n",
					"        if item.isDir:\n",
					"            files.extend(list_all_delta_files(delta_table_path, item_path))\n",
					"        else:\n",
					"            files.append(item_path)\n",
					"\n",
					"    return set(files)"
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					}
				},
				"source": [
					"# new process to identify DELTA files -- Get list of all delta file paths \n",
					"# This function recursively lists all the 'folders' within a parent directory if not empty list \n",
					"@logging_to_appins\n",
					"def list_all_paths(path):\n",
					"    \"\"\"Recursively list all paths under the given ABFS path.\"\"\"\n",
					"    try:        \n",
					"        files = mssparkutils.fs.ls(path)\n",
					"    except Exception as e:\n",
					"        logError(f\"Error accessing {path}: {e}\")\n",
					"        return []\n",
					"\n",
					"    all_paths = []\n",
					"    for f in files:\n",
					"        if f.isDir:\n",
					"            all_paths.append(f.path)\n",
					"            all_paths.extend(list_all_paths(f.path))\n",
					"    return all_paths\n",
					"\n",
					"# function returns the True if the directory contains _delta_log\n",
					"def is_delta_table(path):\n",
					"    \"\"\"Check if the given path is a Delta table by looking for the _delta_log directory.\"\"\"\n",
					"    try:\n",
					"        delta_log_path = path.rstrip(\"/\") + \"/_delta_log\"\n",
					"        mssparkutils.fs.ls(delta_log_path)\n",
					"        return True\n",
					"    except:\n",
					"        return False\n",
					"\n",
					"# count no of files in a directory(path) including all files within sub-directories'\n",
					"@logging_to_appins\n",
					"def count_files_in_path(path):\n",
					"    \"\"\"Recursively count all files (excluding directories) under the given path.\"\"\"\n",
					"    try:\n",
					"        files = mssparkutils.fs.ls(path)\n",
					"    except Exception as e:\n",
					"        logInfo(f\"Error accessing {path}: {e}\")\n",
					"        return 0\n",
					"\n",
					"    count = 0\n",
					"    for f in files:\n",
					"        if f.isDir:\n",
					"            count += count_files_in_path(f.path)\n",
					"        else:\n",
					"            count += 1\n",
					"    return count\n",
					"\n",
					"# List all directories under full_storage_path\n",
					"all_dirs = list_all_paths(full_storage_path)\n",
					"\n",
					"# Filter for Delta tables\n",
					"delta_tables = [p for p in all_dirs if is_delta_table(p)]\n",
					"\n",
					"\n",
					"# Count files and compute cumulative total\n",
					"cumulative_total = 0\n",
					"delta_table_counts = {}\n",
					"\n",
					"for table_path in delta_tables:\n",
					"    count = count_files_in_path(table_path)\n",
					"    delta_table_counts[table_path] = count\n",
					"    cumulative_total += count\n",
					"\n",
					"\n",
					"print(f\"Total number of files across all Delta tables: {cumulative_total}\")\n",
					""
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					}
				},
				"source": [
					"# new process to copy DELTA files from source to target \n",
					"# Initialize grand total counter\n",
					"grand_total_changes = 0\n",
					"error = []\n",
					"\n",
					"for delta_table in delta_tables:\n",
					"\n",
					"    try:\n",
					"        # Extract container name\n",
					"        container_name = delta_table.split(\"@\")[0].split(\"//\")[1]\n",
					"\n",
					"        # Construct backup path\n",
					"        backup_path = delta_table.replace(source_storage_account_path, backup_storage_account_path).replace(container_name, target_container)\n",
					"        \n",
					"        \n",
					"        # List source files\n",
					"        try:\n",
					"            if mssparkutils.fs.exists(delta_table):\n",
					"                source_file = list_all_delta_files(delta_table)\n",
					"            else:\n",
					"                logInfo(f\"Source path does not exist: {delta_table}\")\n",
					"                continue\n",
					"        except FileNotFoundError as e:\n",
					"            logError(f\"Error listing files in source {delta_table}: {e}\")\n",
					"            continue\n",
					"        \n",
					"        # List backup files, handle if backup_path doesn't exist\n",
					"        try:\n",
					"            if mssparkutils.fs.exists(backup_path):\n",
					"                backup_file = list_all_delta_files(backup_path)\n",
					"            else:\n",
					"                print(f\"Backup path does not exist (assuming no backup yet): {backup_path}\")\n",
					"                mssparkutils.fs.mkdirs(backup_path)\n",
					"                backup_file = list_all_delta_files(backup_path) # Assume no backup files yet\n",
					"        except Exception as e:\n",
					"            logError(f\"Error listing files in backup {backup_path}: {e}\")\n",
					"            backup_file = []  # Treat as empty\n",
					"        \n",
					"        # Determine changed files\n",
					"        try:\n",
					"            change_file = set(source_file) - set(backup_file)\n",
					"            change_count = len(change_file)\n",
					"            grand_total_changes += change_count\n",
					"            print(f\"{change_count} new/changed files in {delta_table}\")\n",
					"        except Exception as e:\n",
					"            logError(f\"Error comparing changed files for {delta_table}: {e}\")\n",
					"            continue\n",
					"        \n",
					"        # Copy changed files\n",
					"        for file_name in change_file:\n",
					"            try:\n",
					"                source_path = delta_table + file_name\n",
					"                dest_path = backup_path + file_name\n",
					"                # print(f\"Copying delta files from 'UKS' {source_path} to '===> UKW' {dest_path}\")\n",
					"                # Copy changed files from source to target\n",
					"                mssparkutils.fs.cp(source_path, dest_path)\n",
					"            except Exception as e:\n",
					"                logError(f\"Error copying files from source {source_path} to '\\n' target {dest_path}: {e}\")\n",
					"                continue\n",
					"    \n",
					"    except Exception as e:\n",
					"        print(f\"Unexpected error processing {delta_table}: {e}\")\n",
					"        continue\n",
					"\n",
					"print(f\"Grand total of new/changed files across all Delta tables: {grand_total_changes}\")\n",
					""
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"source": [
					"# This function traverses each directory untill it find a non-delta folder and lists them\n",
					"# new Non delta process - modified 10Jun25 \"\n",
					"@logging_to_appins\n",
					"def list_non_delta_file_folders(base_path):\n",
					"    non_delta_folders = []\n",
					"\n",
					"    def process_path(path):\n",
					"        try:\n",
					"            items = mssparkutils.fs.ls(path)\n",
					"        except Exception as e:\n",
					"            logError(f\"Failed to access path: {path}, error: {e}\")\n",
					"            return\n",
					"\n",
					"        # Check if current directory contains _delta_log → it's a Delta table root\n",
					"        contains_delta_log = any(item.isDir and item.name.strip('/') == \"_delta_log\" for item in items)\n",
					"\n",
					"        if contains_delta_log:\n",
					"            # This is a Delta table root folder, skip it and its children\n",
					"            logInfo(f\"Delta table found at: {path}\")\n",
					"            return\n",
					"\n",
					"        has_files = any(not item.isDir for item in items)\n",
					"        if has_files:\n",
					"            logInfo(f\"Non-Delta folder with files: {path}\")\n",
					"            non_delta_folders.append(path)\n",
					"\n",
					"        # Recurse into subfolders (only if not a Delta root)\n",
					"        for item in items:\n",
					"            if item.isDir:\n",
					"                process_path(item.path)\n",
					"\n",
					"    process_path(base_path)\n",
					"    return non_delta_folders\n",
					"\n",
					"\n",
					"\n",
					"Non_delta_files_list = list_non_delta_file_folders(full_storage_path)\n",
					"\n",
					"Non_delta_files_list_filtered = [path for path in Non_delta_files_list if \".net\" not in path]"
				],
				"execution_count": 36
			},
			{
				"cell_type": "code",
				"source": [
					"total_delta_files = grand_total_changes\n",
					"total_container_files = cumulative_total\n",
					"date_now = datetime.now().strftime(\"%Y-%m-%d\")\n",
					"\n",
					"percentage_Savings = ((total_container_files-total_delta_files)/total_container_files)*100\n",
					"\n",
					"print(f\"The total storage files on {date_now} are: {total_container_files} and total delta file are:{total_delta_files}\")\n",
					"print(f\"The percentage_Savings are: {percentage_Savings}\")"
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"source": [
					"# Define categorised delta table lists for testing each layer \n",
					"delta_tables_std = [\"AIEDocumentData/aie_document_data\"\n",
					"                    ,\"entraid\"\n",
					"                    ,\"listed_building\"\n",
					"                    ,\"horizon_appeal_s78_final\"\n",
					"                    ,\"Horizon/document_meta_data\"\n",
					"                    ,\"sb_appeal_has\"\n",
					"                    ,\"sb_service_user\"]\n",
					"\n",
					"delta_tables_hrm = ['nsip_document'\n",
					"                    ,'appeal_s78 '\n",
					"                    ,'appeal_document'\n",
					"                    ,'nsip_s51_advice'\n",
					"                    ,'nsip_project'\n",
					"                    ,'aie_document_data'\n",
					"                    ,'horizon_appeals_event']\n",
					"\n",
					"delta_tables_cur = ['s51_advice'\n",
					"                    ,'nsip_representation'\n",
					"                    ,'listed_building'\n",
					"                    ,'appeal_service_user_curated_mipins'\n",
					"                    ,'appeal_event_estimate'\n",
					"                    ,'appeals_has_curated_mipins'\n",
					"                    ,'appeals_s78_curated_mipins']\n",
					"\n",
					"delta_tables_config = [\"main_pipeline_config\"\n",
					"                       ,\"orchestration\"\n",
					"                       ,\"standardised_table_definitions\"]\n",
					"                       \n",
					"delta_tables_all = delta_tables_hrm + delta_tables_cur + delta_tables_std + delta_tables_config\n",
					"\n",
					"run_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
					"cumulative_total = 0\n",
					"delta_table_counts = {}\n",
					"rows = []\n",
					"\n",
					"# Define container-wise table mapping\n",
					"container_table_map = {\n",
					"    \"odw-harmonised\": delta_tables_hrm,\n",
					"    \"odw-curated\": delta_tables_cur,\n",
					"    \"odw-standardised\": delta_tables_std,\n",
					"    \"odw-config\": delta_tables_config\n",
					"}\n",
					"\n",
					"# Loop through each container\n",
					"for container_name, delta_tables_list in container_table_map.items():\n",
					"    if container_name == container:\n",
					"        for table_name in delta_tables_list:\n",
					"            error_message = \"\"\n",
					"            source_file_count = 0\n",
					"            backup_file_count = 0\n",
					"            source_df_count = 0\n",
					"            backup_df_count = 0\n",
					"\n",
					"            try:\n",
					"                source_table_path = f\"abfss://{container_name}@{storage_account}{table_name}\"\n",
					"                \n",
					"                # Skip paths that contain `.net/` in table_name\n",
					"                if \".net/\" in table_name:\n",
					"                    print(f\"Skipping table due to .net/ pattern: {source_table_path}\")\n",
					"                    continue\n",
					"\n",
					"                    backup_table_path = source_table_path.replace(\n",
					"                    source_storage_account_path,\n",
					"                    backup_storage_account_path\n",
					"                ).replace(container_name, target_container)\n",
					"\n",
					"                # Count files\n",
					"                try:\n",
					"                    source_file_count = count_files_in_path(source_table_path)\n",
					"                except Exception as e:\n",
					"                    error_message += f\"[SourceFileCountError] {str(e)}; \"\n",
					"\n",
					"                try:\n",
					"                    backup_file_count = count_files_in_path(backup_table_path)\n",
					"                except Exception as e:\n",
					"                    error_message += f\"[BackupFileCountError] {str(e)}; \"\n",
					"\n",
					"                delta_table_counts[source_table_path] = source_file_count\n",
					"                delta_table_counts[backup_table_path] = backup_file_count\n",
					"                cumulative_total += source_file_count + backup_file_count\n",
					"\n",
					"                print(source_file_count, backup_file_count)\n",
					"\n",
					"                # Row counts\n",
					"                try:\n",
					"                    source_df_count = spark.read.format(\"delta\").load(source_table_path).count()\n",
					"                except Exception as e:\n",
					"                    error_message += f\"[SourceDFCountError] {str(e)}; \"\n",
					"\n",
					"                try:\n",
					"                    backup_df_count = spark.read.format(\"delta\").load(backup_table_path).count()\n",
					"                except Exception as e:\n",
					"                    error_message += f\"[BackupDFCountError] {str(e)}; \"\n",
					"\n",
					"            except Exception as e:\n",
					"                error_message += f\"[GeneralError] {str(e)}; \"\n",
					"\n",
					"            rows.append(Row(\n",
					"                Date_Run=run_date,\n",
					"                table_name=table_name,\n",
					"                container_name=container_name,\n",
					"                pipeline_name='pln_delta_backup_odw',\n",
					"                source_file_name = source_table_path,\n",
					"                source_file_counts=source_file_count,\n",
					"                backup_file_counts=backup_file_count,\n",
					"                source_DF_counts=source_df_count,\n",
					"                backup_DF_counts=backup_df_count,\n",
					"                File_diff=source_file_count - backup_file_count,\n",
					"                Data_diff=source_df_count - backup_df_count,\n",
					"                Error=error_message if error_message else 'na'\n",
					"            ))\n",
					"\n",
					"# Create and display final DataFrame\n",
					"df = spark.createDataFrame(rows)\n",
					"print(f\"Total number of files across all Delta tables: {cumulative_total}\")\n",
					""
				],
				"execution_count": 40
			},
			{
				"cell_type": "code",
				"source": [
					"df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"logging.delta_backup_logs\")"
				],
				"execution_count": 41
			},
			{
				"cell_type": "code",
				"source": [
					"non_delta_file_dict = {\n",
					"    \"backup_storage_name\":backup_storage_account_path.split(\".\")[0],\n",
					"    \"non_delta_file_list\":list(Non_delta_files_list_filtered)\n",
					"}"
				],
				"execution_count": 42
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					}
				},
				"source": [
					"mssparkutils.notebook.exit(non_delta_file_dict)"
				],
				"execution_count": 43
			}
		]
	}
}