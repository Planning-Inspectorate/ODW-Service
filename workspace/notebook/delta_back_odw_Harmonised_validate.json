{
	"name": "delta_back_odw_Harmonised_validate",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "e7f5cd8d-f6b5-4f8e-b57e-0f51433fc5b8"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###### <b> This notebook performs delta backup between UKS and UKW, exits with a dictionary of non-delta files to be processed by copy activity </b>. \n",
					"###### Identifying Delta files\n",
					"----------------------------\n",
					" * Recursively checks the whole container,directories,sub-directories for _delta_log if true then it identifies as delta directory.\n",
					" * Recursively processes each delta directory to fetch the delta files, compares source to target, the difference is a set of delta files, that are backed-up.\n",
					" * This notebook handles both insert, deletes, updates of records as it also compares _delta_log files that contains versions between Source and target as well as the files within the directory. \n",
					"###### Identifying Non-Delta files (ex: CSV, JSON, PARQUET, Excel etc)\n",
					"---------------------------\n",
					" * The function list_non_delta_file_folders recusively checks for directories and sub-directories for _delta_log and excludes directories that have _delta_log.\n",
					" * All the non-delta files are listed for full backup meaning the destination directory is overwritten everytime.\n",
					"###### Cost-Value benefit\n",
					"-----\n",
					"* Percentage savings based on number of files being backed-up daily is calculated in the end\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# pipeline parameter override at runtime based on input container\n",
					"\n",
					"container = ''\n",
					"target_container= ''"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"import pprint\n",
					"from pyspark.sql.types import *\n",
					"import json\n",
					"import re\n",
					"from datetime import datetime\n",
					"from pyspark.sql import Row\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"source_storage_account_path = re.search('url=https://(.+?);', mssparkutils.credentials.getFullConnectionString('ls_backup_source')).group(1)\n",
					"\n",
					"full_storage_path    = f'abfss://{container}@{source_storage_account_path}'"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"backup_storage_account_path=re.search('url=https://(.+?);', mssparkutils.credentials.getFullConnectionString('ls_backup_destination')).group(1)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"storage_account: str = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# list all the directory files including files within sub-directories and delta logs\n",
					"@logging_to_appins\n",
					"def list_all_delta_files(delta_table_path, relative_path=\"\"):\n",
					"    files = []\n",
					"    full_path = f\"{delta_table_path}/{relative_path}\" if relative_path else delta_table_path\n",
					"    \n",
					"    try:\n",
					"        items = mssparkutils.fs.ls(full_path)\n",
					"    except Exception as e:\n",
					"        logInfo(f\"Warning: Could not access path '{full_path}': {str(e)}\")\n",
					"        return full_path  # Skip this path and continue\n",
					"\n",
					"    for item in items:\n",
					"        item_path = f\"{relative_path}/{item.name}\".rstrip(\"/\")\n",
					"        if item.isDir:\n",
					"            files.extend(list_all_delta_files(delta_table_path, item_path))\n",
					"        else:\n",
					"            files.append(item_path)\n",
					"\n",
					"    return set(files)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					}
				},
				"source": [
					"# new process to identify DELTA files -- Get list of all delta file paths \n",
					"# This function recursively lists all the 'folders' within a parent directory if not empty list \n",
					"@logging_to_appins\n",
					"def list_all_paths(path):\n",
					"    \"\"\"Recursively list all paths under the given ABFS path.\"\"\"\n",
					"    try:\n",
					"        files = mssparkutils.fs.ls(path)\n",
					"    except Exception as e:\n",
					"        logError(f\"Error accessing {path}: {e}\")\n",
					"        return []\n",
					"\n",
					"    all_paths = []\n",
					"    for f in files:\n",
					"        if f.isDir:\n",
					"            all_paths.append(f.path)\n",
					"            all_paths.extend(list_all_paths(f.path))\n",
					"    return all_paths\n",
					"\n",
					"# function returns the True if the directory contains _delta_log\n",
					"def is_delta_table(path):\n",
					"    \"\"\"Check if the given path is a Delta table by looking for the _delta_log directory.\"\"\"\n",
					"    try:\n",
					"        delta_log_path = path.rstrip(\"/\") + \"/_delta_log\"\n",
					"        mssparkutils.fs.ls(delta_log_path)\n",
					"        return True\n",
					"    except:\n",
					"        return False\n",
					"\n",
					"# count no of files in a directory(path) including all files within sub-directories'\n",
					"@logging_to_appins\n",
					"def count_files_in_path(path):\n",
					"    \"\"\"Recursively count all files (excluding directories) under the given path.\"\"\"\n",
					"    try:\n",
					"        files = mssparkutils.fs.ls(path)\n",
					"    except Exception as e:\n",
					"        logInfo(f\"Error accessing {path}: {e}\")\n",
					"        return 0\n",
					"\n",
					"    count = 0\n",
					"    for f in files:\n",
					"        if f.isDir:\n",
					"            count += count_files_in_path(f.path)\n",
					"        else:\n",
					"            count += 1\n",
					"    return count\n",
					"\n",
					"# List all directories under full_storage_path\n",
					"all_dirs = list_all_paths(full_storage_path)\n",
					"\n",
					"# Filter for Delta tables\n",
					"delta_tables = [p for p in all_dirs if is_delta_table(p)]\n",
					"\n",
					"\n",
					"# Count files and compute cumulative total\n",
					"cumulative_total = 0\n",
					"delta_table_counts = {}\n",
					"\n",
					"for table_path in delta_tables:\n",
					"    count = count_files_in_path(table_path)\n",
					"    delta_table_counts[table_path] = count\n",
					"    cumulative_total += count\n",
					"\n",
					"\n",
					"print(f\"Total number of files across all Delta tables: {cumulative_total}\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					}
				},
				"source": [
					"# new process to copy DELTA files from source to target \n",
					"# Initialize grand total counter\n",
					"grand_total_changes = 0\n",
					"error = []\n",
					"\n",
					"for delta_table in delta_tables:\n",
					"\n",
					"    try:\n",
					"        # Extract container name\n",
					"        container_name = delta_table.split(\"@\")[0].split(\"//\")[1]\n",
					"\n",
					"        # Construct backup path\n",
					"        backup_path = delta_table.replace(source_storage_account_path, backup_storage_account_path).replace(container_name, target_container)\n",
					"        \n",
					"        \n",
					"        # List source files\n",
					"        try:\n",
					"            if mssparkutils.fs.exists(delta_table):\n",
					"                source_file = list_all_delta_files(delta_table)\n",
					"            else:\n",
					"                logInfo(f\"Source path does not exist: {delta_table}\")\n",
					"                continue\n",
					"        except FileNotFoundError as e:\n",
					"            logError(f\"Error listing files in source {delta_table}: {e}\")\n",
					"            continue\n",
					"        \n",
					"        # List backup files, handle if backup_path doesn't exist\n",
					"        try:\n",
					"            if mssparkutils.fs.exists(backup_path):\n",
					"                backup_file = list_all_delta_files(backup_path)\n",
					"            else:\n",
					"                print(f\"Backup path does not exist (assuming no backup yet): {backup_path}\")\n",
					"                mssparkutils.fs.mkdirs(backup_path)\n",
					"                backup_file = list_all_delta_files(backup_path) # Assume no backup files yet\n",
					"        except Exception as e:\n",
					"            logError(f\"Error listing files in backup {backup_path}: {e}\")\n",
					"            backup_file = []  # Treat as empty\n",
					"        \n",
					"        # Determine changed files\n",
					"        try:\n",
					"            change_file = set(source_file) - set(backup_file)\n",
					"            change_count = len(change_file)\n",
					"            grand_total_changes += change_count\n",
					"            print(f\"{change_count} new/changed files in {delta_table}\")\n",
					"        except Exception as e:\n",
					"            logError(f\"Error comparing changed files for {delta_table}: {e}\")\n",
					"            continue\n",
					"        \n",
					"        # Copy changed files\n",
					"        for file_name in change_file:\n",
					"            try:\n",
					"                source_path = delta_table + file_name\n",
					"                dest_path = backup_path + file_name\n",
					"                # print(f\"Copying delta files from 'UKS' {source_path} to '===> UKW' {dest_path}\")\n",
					"                # Copy changed files from source to target\n",
					"                mssparkutils.fs.cp(source_path, dest_path)\n",
					"            except Exception as e:\n",
					"                logError(f\"Error copying files from source {source_path} to '\\n' target {dest_path}: {e}\")\n",
					"                continue\n",
					"    \n",
					"    except Exception as e:\n",
					"        print(f\"Unexpected error processing {delta_table}: {e}\")\n",
					"        continue\n",
					"\n",
					"print(f\"Grand total of new/changed files across all Delta tables: {grand_total_changes}\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# This function traverses each directory untill it find a non-delta folder and lists them\n",
					"# new Non delta process - modified 10Jun25 \"\n",
					"@logging_to_appins\n",
					"def list_non_delta_file_folders(base_path):\n",
					"    non_delta_folders = []\n",
					"\n",
					"    def process_path(path):\n",
					"        try:\n",
					"            items = mssparkutils.fs.ls(path)\n",
					"        except Exception as e:\n",
					"            logError(f\"Failed to access path: {path}, error: {e}\")\n",
					"            return\n",
					"\n",
					"        # Check if current directory contains _delta_log â†’ it's a Delta table root\n",
					"        contains_delta_log = any(item.isDir and item.name.strip('/') == \"_delta_log\" for item in items)\n",
					"\n",
					"        if contains_delta_log:\n",
					"            # This is a Delta table root folder, skip it and its children\n",
					"            logInfo(f\"Delta table found at: {path}\")\n",
					"            return\n",
					"\n",
					"        has_files = any(not item.isDir for item in items)\n",
					"        if has_files:\n",
					"            logInfo(f\"Non-Delta folder with files: {path}\")\n",
					"            non_delta_folders.append(path)\n",
					"\n",
					"        # Recurse into subfolders (only if not a Delta root)\n",
					"        for item in items:\n",
					"            if item.isDir:\n",
					"                process_path(item.path)\n",
					"\n",
					"    process_path(base_path)\n",
					"    return non_delta_folders\n",
					"\n",
					"\n",
					"\n",
					"Non_delta_files_list = list_non_delta_file_folders(full_storage_path)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"total_delta_files = grand_total_changes\n",
					"total_container_files = cumulative_total\n",
					"date_now = datetime.now().strftime(\"%Y-%m-%d\")\n",
					"\n",
					"percentage_Savings = ((total_container_files-total_delta_files)/total_container_files)*100\n",
					"\n",
					"print(f\"The total storage files on {date_now} are: {total_container_files} and total delta file are:{total_delta_files}\")\n",
					"print(f\"The percentage_Savings are: {percentage_Savings}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# data check tables for few delta tables derives the file_count, data_count between source and target\n",
					"delta_tables = [\"aie_document_data\",\"entraid\", \"sb_appeal_s78\"]\n",
					"\n",
					"run_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
					"# Count files and compute cumulative total\n",
					"cumulative_total = 0\n",
					"delta_table_counts = {}\n",
					"rows = []\n",
					"\n",
					"for table_path in delta_tables:\n",
					"    container_name = full_storage_path.split(\"@\")[0].split(\"//\")[1]\n",
					"    source_table_path = full_storage_path +table_path\n",
					"    backup_table_path = source_table_path.replace(source_storage_account_path, backup_storage_account_path).replace(container_name, \"delta-backup-harmonised\")\n",
					"\n",
					"    #get the source and delta counts \n",
					"    source_file_count = count_files_in_path(source_table_path)\n",
					"    delta_table_counts[source_table_path] = source_file_count\n",
					"    cumulative_total += source_file_count\n",
					"\n",
					"    backup_file_count = count_files_in_path(backup_table_path)\n",
					"    delta_table_counts[backup_table_path] = backup_file_count\n",
					"    cumulative_total += backup_file_count\n",
					"    print(source_file_count,backup_file_count)\n",
					"\n",
					"    source_df_count = spark.read.format(\"delta\").load(source_table_path).count()\n",
					"    backup_df_count = spark.read.format(\"delta\").load(backup_table_path).count()\n",
					"\n",
					"    # print(table_path, source_df_count,backup_df_count)\n",
					"\n",
					"    file_diff = source_file_count - backup_file_count\n",
					"    data_diff = source_df_count - backup_df_count\n",
					"\n",
					"    rows.append(Row(\n",
					"        Date_Run= run_date,\n",
					"        table_name = table_path,\n",
					"        container_name = container_name,\n",
					"        pipeline_name= 'pln_delta_backup_odw',\n",
					"        source_file_counts= source_file_count,\n",
					"        backup_file_counts= backup_file_count,\n",
					"        source_DF_counts= source_df_count,\n",
					"        backup_DF_counts= backup_df_count,\n",
					"        File_diff= file_diff,\n",
					"        Data_diff= data_diff,\n",
					"        Error= 'na'))\n",
					"\n",
					"    df = spark.createDataFrame(rows)\n",
					"\n",
					"\n",
					"print(f\"Total number of files across all Delta tables: {cumulative_total}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"logging.delta_backup_logs\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"non_delta_file_dict = {\n",
					"    \"backup_storage_name\":backup_storage_account_path.split(\".\")[0],\n",
					"    \"non_delta_file_list\":list(Non_delta_files_list)\n",
					"}"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false
					}
				},
				"source": [
					"mssparkutils.notebook.exit(non_delta_file_dict)"
				],
				"execution_count": null
			}
		]
	}
}