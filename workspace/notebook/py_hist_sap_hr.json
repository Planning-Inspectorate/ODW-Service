{
	"name": "py_hist_sap_hr",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "2b533833-78f0-4ed8-8e4f-730be2b45182"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Standardised layer and build a table for Curated Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;25-Feb-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This Notebook is designed to facilitate the monthly processing and harmonization of SAP HR data. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The Notebook ensures that HR data is accurately transformed, stored, and made available for reporting and analysis."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Intialisations\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from odw.core.util.logging_util import LoggingUtil\n",
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Historic Data Load"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"import json\n",
					"\n",
					"# Initialize result dictionary\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"try:\n",
					"    LoggingUtil().log_info(\"Starting historical SAP HR data processing\")\n",
					"    \n",
					"    # Step 2: Create a temporary table to store Report_MonthEnd_Date values\n",
					"    LoggingUtil().log_info(\"Creating temporary view of month end dates\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW MonthEndDates AS\n",
					"    SELECT date_format(to_date(Report_MonthEnd_Date,'dd/MM/yyyy'), 'yyyy-MM-dd') AS Report_MonthEnd_Date\n",
					"    FROM odw_harmonised_db.load_sap_hr_monthly\n",
					"    GROUP BY Report_MonthEnd_Date\n",
					"    \"\"\")\n",
					"    LoggingUtil().log_info(\"Month end dates view created successfully\")\n",
					"    \n",
					"    # Delete matching records\n",
					"    LoggingUtil().log_info(\"Deleting existing records that match new month end dates\")\n",
					"    delete_result = spark.sql(\"\"\"\n",
					"    MERGE INTO odw_harmonised_db.hist_SAP_HR AS target\n",
					"    USING MonthEndDates AS source\n",
					"    ON target.Report_MonthEnd_Date = source.Report_MonthEnd_Date\n",
					"    WHEN MATCHED THEN DELETE\n",
					"    \"\"\")\n",
					"    LoggingUtil().log_info(\"Existing records deleted successfully\")\n",
					"    \n",
					"    # Step 3: Extract file date from the source file name\n",
					"    LoggingUtil().log_info(\"Creating file date view\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW FileDate AS\n",
					"    SELECT \n",
					"        to_date(substring(file_name, 19, 8), 'yyyyMMdd') AS file_date\n",
					"    FROM \n",
					"        odw_standardised_db.leave_entitlement\n",
					"    LIMIT 1\n",
					"    \"\"\")\n",
					"    LoggingUtil().log_info(\"File date view created successfully\")\n",
					"    \n",
					"    # Step 4: Calculate staff cost\n",
					"    LoggingUtil().log_info(\"Calculating staff costs\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW StaffCost AS\n",
					"    SELECT DISTINCT\n",
					"        h.PersNo AS staff_number,\n",
					"        h.PSgroup AS grade,\n",
					"        h.Annualsalary AS salary,\n",
					"        CAST(h.Annualsalary AS DOUBLE) / (365.0 * 7.4) AS hourly_rate,\n",
					"        CASE \n",
					"            WHEN datediff(le.Dedfrom, fd.file_date) >= 0 THEN le.Dedfrom\n",
					"            ELSE date_add(le.Dedfrom, -365)\n",
					"        END AS leave_start_date,\n",
					"        to_date(substring(le.file_name, 19, 8), 'yyyyMMdd') AS date_from_file_name,\n",
					"        COALESCE(le.Number, 0) * ABS((datediff(\n",
					"            CASE \n",
					"                WHEN datediff(le.Dedfrom, fd.file_date) >= 0 THEN le.Dedfrom\n",
					"                ELSE date_add(le.Dedfrom, -365)\n",
					"            END,\n",
					"            to_date(substring(le.file_name, 19, 8), 'yyyyMMdd')\n",
					"        ) / 364.25) % 1) AS expected_leave_taken_hrs,\n",
					"        COALESCE(le.Number, 0) * (1 - ABS((datediff(\n",
					"            CASE \n",
					"                WHEN datediff(le.Dedfrom, fd.file_date) >= 0 THEN le.Dedfrom\n",
					"                ELSE date_add(le.Dedfrom, -365)\n",
					"            END,\n",
					"            to_date(substring(le.file_name, 19, 8), 'yyyyMMdd')\n",
					"        ) / 364.25) % 1)) AS expected_leave_hours_remaining,\n",
					"        COALESCE(le.Number, 0) AS leave_allowance,\n",
					"        ABS((datediff(\n",
					"            CASE \n",
					"                WHEN datediff(le.Dedfrom, fd.file_date) >= 0 THEN le.Dedfrom\n",
					"                ELSE date_add(le.Dedfrom, -365)\n",
					"            END,\n",
					"            to_date(substring(le.file_name, 19, 8), 'yyyyMMdd')\n",
					"        ) / 364.25) % 1) AS percent_leave_year_passed,\n",
					"        (1 - ABS((datediff(\n",
					"            CASE \n",
					"                WHEN datediff(le.Dedfrom, fd.file_date) >= 0 THEN le.Dedfrom\n",
					"                ELSE date_add(le.Dedfrom, -365)\n",
					"            END,\n",
					"            to_date(substring(le.file_name, 19, 8), 'yyyyMMdd')\n",
					"        ) / 364.25) % 1)) AS percent_leave_year_remaining,\n",
					"        h.ContractType\n",
					"    FROM \n",
					"        odw_harmonised_db.load_sap_hr_monthly h\n",
					"    LEFT JOIN \n",
					"        odw_standardised_db.leave_entitlement le \n",
					"    ON \n",
					"        h.PersNo = CASE \n",
					"            WHEN LEFT(CAST(le.StaffNumber AS STRING), 1) = '4' THEN CONCAT('50', le.StaffNumber)\n",
					"            WHEN LEFT(CAST(le.StaffNumber AS STRING), 1) = '5' THEN CONCAT('00', le.StaffNumber)\n",
					"            ELSE CAST(le.StaffNumber AS STRING)\n",
					"        END\n",
					"        AND le.AbsenceQuotaType = 'Annual Leave / P&P'\n",
					"    CROSS JOIN \n",
					"        FileDate fd\n",
					"    \"\"\")\n",
					"    LoggingUtil().log_info(\"Staff cost calculations completed successfully\")\n",
					"    \n",
					"    # Step 5: Calculate carried-over hours\n",
					"    LoggingUtil().log_info(\"Calculating carried-over hours\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW CarriedOver AS\n",
					"    SELECT DISTINCT\n",
					"        h.PersNo AS staff_number,\n",
					"        CASE \n",
					"            WHEN COALESCE(le.Number, 0) - 148 >= 0 THEN 148\n",
					"            ELSE COALESCE(le.Number, 0)\n",
					"        END AS carried_over_hours\n",
					"    FROM \n",
					"        odw_harmonised_db.load_sap_hr_monthly h\n",
					"    LEFT JOIN \n",
					"        odw_standardised_db.leave_entitlement le \n",
					"    ON \n",
					"        h.PersNo = CASE \n",
					"            WHEN LEFT(CAST(le.StaffNumber AS STRING), 1) = '4' THEN CONCAT('50', le.StaffNumber)\n",
					"            WHEN LEFT(CAST(le.StaffNumber AS STRING), 1) = '5' THEN CONCAT('00', le.StaffNumber)\n",
					"            ELSE CAST(le.StaffNumber AS STRING)\n",
					"        END\n",
					"        AND le.AbsenceQuotaType = 'Brought Forward'\n",
					"    \"\"\")\n",
					"    LoggingUtil().log_info(\"Carried-over hours calculations completed successfully\")\n",
					"    \n",
					"    # Step 6: Calculate leave taken - FIXED\n",
					"    LoggingUtil().log_info(\"Calculating leave taken hours\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW LeaveTaken AS\n",
					"    SELECT\n",
					"        h.PersNo AS staff_number,\n",
					"        COALESCE(SUM(f.absencehours), 0) AS leave_hours_taken\n",
					"    FROM \n",
					"        odw_harmonised_db.load_sap_hr_monthly h\n",
					"    LEFT JOIN \n",
					"        StaffCost l \n",
					"    ON \n",
					"        h.PersNo = l.staff_number\n",
					"    LEFT JOIN \n",
					"        FileDate fd\n",
					"    ON 1=1\n",
					"    LEFT JOIN \n",
					"        odw_harmonised_db.sap_hr_fact_absence_all f\n",
					"    ON \n",
					"        h.PersNo = cast(f.staffnumber as int)\n",
					"        AND f.AttendanceorAbsenceType IN ('PT Annual / P&P Leave', 'FT Annual / Priv Leave')\n",
					"        AND (f.absencedate BETWEEN l.leave_start_date AND fd.file_date)\n",
					"    GROUP BY \n",
					"        h.PersNo\n",
					"    \"\"\")\n",
					"    LoggingUtil().log_info(\"Leave taken calculations completed successfully\")\n",
					"    \n",
					"    # Step 7: Insert data into the historical table\n",
					"    LoggingUtil().log_info(\"Starting data insertion into hist_SAP_HR\")\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.hist_SAP_HR\n",
					"    SELECT \n",
					"        h.PersNo,\n",
					"        h.Firstname,\n",
					"        h.Lastname,\n",
					"        h.EmployeeNo,\n",
					"        h.CoCd,\n",
					"        h.CompanyCode,\n",
					"        h.PA,\n",
					"        h.PersonnelArea,\n",
					"        h.PSubarea,\n",
					"        h.PersonnelSubarea,\n",
					"        h.Orgunit,\n",
					"        h.OrganizationalUnit,\n",
					"        h.Organizationalkey,\n",
					"        h.OrganizationalKey1,\n",
					"        h.WorkC,\n",
					"        h.WorkContract,\n",
					"        h.CT,\n",
					"        h.ContractType,\n",
					"        h.PSgroup,\n",
					"        h.PayBandDescription,\n",
					"        CAST(ROUND(h.FTE, 3) AS DECIMAL(10,3)) as FTE,\n",
					"        CAST(ROUND(h.Wkhrs, 2) AS DECIMAL(10,2)) as Wkhrs,\n",
					"        h.IndicatorPartTimeEmployee,\n",
					"        h.S,\n",
					"        h.EmploymentStatus,\n",
					"        h.GenderKey,\n",
					"        h.TRAStartDate,\n",
					"        h.TRAEndDate,\n",
					"        h.TRAStatus,\n",
					"        h.TRAGrade,\n",
					"        h.PrevPersNo,\n",
					"        h.ActR,\n",
					"        h.ReasonforAction,\n",
					"        h.Position,\n",
					"        h.Position1,\n",
					"        h.CostCtr,\n",
					"        h.CostCentre,\n",
					"        h.CivilServiceStart,\n",
					"        h.DatetoCurrentJob,\n",
					"        h.SeniorityDate,\n",
					"        h.DatetoSubstGrade,\n",
					"        h.PersNo1,\n",
					"        h.NameofManagerOM,\n",
					"        h.ManagerPosition,\n",
					"        h.ManagerPositionText,\n",
					"        h.CounterSignManager,\n",
					"        h.Loc,\n",
					"        h.Location,\n",
					"        h.OrgStartDate,\n",
					"        CASE\n",
					"        WHEN h.FixTermEndDate IS NULL THEN NULL\n",
					"        ELSE DATE_FORMAT(TO_DATE(h.FixTermEndDate, 'dd/MM/yyyy'), 'yyyy-MM-dd')\n",
					"    END AS FixTermEndDate,\n",
					"        h.LoanStartDate,\n",
					"        h.LoanEndDate,\n",
					"        h.EEGrp,\n",
					"        h.EmployeeGroup,\n",
					"        h.Annualsalary,\n",
					"        h.Curr,\n",
					"        h.NInumber,\n",
					"        h.Birthdate,\n",
					"        h.Ageofemployee,\n",
					"        h.EO,\n",
					"        h.Ethnicorigin,\n",
					"        h.NID,\n",
					"        h.Rel,\n",
					"        h.ReligiousDenominationKey,\n",
					"        h.SxO,\n",
					"        h.WageType,\n",
					"        h.EmployeeSubgroup,\n",
					"        h.LOAAbsType,\n",
					"        h.LOAAbsenceTypeText,\n",
					"        h.Schemereference,\n",
					"        h.PensionSchemeName,\n",
					"        h.DisabilityCode,\n",
					"        h.DisabilityText,\n",
					"        h.DisabilityCodeDescription,\n",
					"        h.PArea,\n",
					"        h.PayrollArea,\n",
					"        h.AssignmentNumber,\n",
					"        CASE \n",
					"            WHEN TRIM(h.FTE2) = '' OR TRIM(h.FTE2) = 'NULL' THEN NULL \n",
					"            ELSE FORMAT_NUMBER(TRY_CAST(h.FTE2 AS FLOAT), 4)\n",
					"        END AS FTE2,\n",
					"        h.Report_MonthEnd_Date,\n",
					"        CURRENT_DATE() AS PDAC_ETL_Date,\n",
					"        h.SourceSystemID,\n",
					"        h.IngestionDate,\n",
					"        h.ValidTo,\n",
					"        h.RowID,\n",
					"        h.IsActive,\n",
					"        CAST(ROUND(COALESCE(sc.leave_allowance, 0) + COALESCE(co.carried_over_hours, 0), 4) AS DECIMAL(18,4)) AS leave_entitlement_hrs,\n",
					"        CAST(ROUND(COALESCE(lt.leave_hours_taken, 0), 4) AS DECIMAL(18,4)) AS leave_taken_hrs,\n",
					"        CAST(ROUND((COALESCE(sc.leave_allowance, 0) + COALESCE(co.carried_over_hours, 0) - \n",
					"                   COALESCE(sc.expected_leave_hours_remaining, 0) - COALESCE(lt.leave_hours_taken, 0)), 4) AS DECIMAL(18,4)) AS leave_remaining_hours,\n",
					"        CAST(ROUND(COALESCE(sc.expected_leave_hours_remaining, 0), 4) AS DECIMAL(18,4)) AS leave_remaining_prorata_hours\n",
					"    FROM \n",
					"        odw_harmonised_db.load_sap_hr_monthly h\n",
					"    LEFT JOIN \n",
					"        StaffCost sc \n",
					"    ON \n",
					"        h.PersNo = sc.staff_number\n",
					"    LEFT JOIN \n",
					"        CarriedOver co \n",
					"    ON \n",
					"        h.PersNo = co.staff_number\n",
					"    LEFT JOIN \n",
					"        LeaveTaken lt \n",
					"    ON \n",
					"        h.PersNo = lt.staff_number\n",
					"    LEFT JOIN \n",
					"        odw_standardised_db.pension_ernic_rates p\n",
					"    ON \n",
					"        sc.grade = p.Grade\n",
					"    WHERE \n",
					"       right(date_format(to_date(Report_MonthEnd_Date,'dd/MM/yyyy'), 'yyyy-MM-dd'),5) IN ('01-31', '02-28', '02-29', '03-31', '04-30', '05-31', '06-30', '07-31', '08-31', '09-30', '10-31', '11-30', '12-31')\n",
					"    \"\"\")\n",
					"    \n",
					"    # Get count of inserted records\n",
					"    inserted_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count FROM odw_harmonised_db.hist_SAP_HR\n",
					"    WHERE PDAC_ETL_Date = CURRENT_DATE()\n",
					"    \"\"\").collect()[0]['count']\n",
					"    result[\"record_count\"] = inserted_count\n",
					"    LoggingUtil().log_info(f\"Successfully inserted {inserted_count} records into hist_SAP_HR\")\n",
					"    \n",
					"    LoggingUtil().log_info(\"Historical SAP HR data processing completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Capture error information and limit to 300 characters\n",
					"    error_msg = f\"Error in historical SAP HR data processing: {str(e)}\"\n",
					"    LoggingUtil().log_error(error_msg)\n",
					"    LoggingUtil().log_exception(e)\n",
					"    \n",
					"    # Truncate error message if it exceeds 300 characters\n",
					"    truncated_error = (error_msg[:297] + '...') if len(error_msg) > 300 else error_msg\n",
					"    \n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = truncated_error\n",
					"    result[\"record_count\"] = -1  # Indicate failure with -1 count\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    LoggingUtil().log_info(\"Flushing logs\")\n",
					"    flushLogging()\n",
					"    \n",
					"    # Output the result as JSON for ADF to capture\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": null
			}
		]
	}
}