{
	"name": "py_hist_sap_hr",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "72a87477-db75-4387-8600-654b0c211660"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Standardised layer and build a table for Curated Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;25-Feb-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This Notebook is designed to facilitate the monthly processing and harmonization of SAP HR data. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The Notebook ensures that HR data is accurately transformed, stored, and made available for reporting and analysis."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Intialisations\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Historic Data Load"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"\n",
					"try:\n",
					"    logInfo(\"Starting historical SAP HR data processing\")\n",
					"    \n",
					"    # Step 2: Create a temporary table to store Report_MonthEnd_Date values\n",
					"    logInfo(\"Creating temporary view of month end dates\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW MonthEndDates AS\n",
					"    SELECT date_format(to_date(Report_MonthEnd_Date,'dd/MM/yyyy'), 'yyyy-MM-dd') AS Report_MonthEnd_Date\n",
					"    FROM odw_harmonised_db.load_sap_hr_monthly\n",
					"    GROUP BY Report_MonthEnd_Date\n",
					"    \"\"\")\n",
					"    logInfo(\"Month end dates view created successfully\")\n",
					"    \n",
					"    # Delete matching records\n",
					"    logInfo(\"Deleting existing records that match new month end dates\")\n",
					"    delete_result = spark.sql(\"\"\"\n",
					"    MERGE INTO odw_harmonised_db.hist_SAP_HR AS target\n",
					"    USING MonthEndDates AS source\n",
					"    ON target.Report_MonthEnd_Date = source.Report_MonthEnd_Date\n",
					"    WHEN MATCHED THEN DELETE\n",
					"    \"\"\")\n",
					"    logInfo(\"Existing records deleted successfully\")\n",
					"    \n",
					"    # Step 3: Extract file date from the source file name\n",
					"    logInfo(\"Creating file date view\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW FileDate AS\n",
					"    SELECT \n",
					"        to_date(substring(file_name, 19, 8), 'yyyyMMdd') AS file_date\n",
					"    FROM \n",
					"        odw_standardised_db.leave_entitlement\n",
					"    LIMIT 1\n",
					"    \"\"\")\n",
					"    logInfo(\"File date view created successfully\")\n",
					"    \n",
					"    # Step 4: Calculate staff cost\n",
					"    logInfo(\"Calculating staff costs\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW StaffCost AS\n",
					"    SELECT DISTINCT\n",
					"        h.PersNo AS staff_number,\n",
					"        h.PSgroup AS grade,\n",
					"        h.Annualsalary AS salary,\n",
					"        CAST(h.Annualsalary AS DOUBLE) / (365.0 * 7.4) AS hourly_rate,\n",
					"        CASE \n",
					"            WHEN datediff(le.Dedfrom, fd.file_date) >= 0 THEN le.Dedfrom\n",
					"            ELSE date_add(le.Dedfrom, -365)\n",
					"        END AS leave_start_date,\n",
					"        to_date(substring(le.file_name, 19, 8), 'yyyyMMdd') AS date_from_file_name,\n",
					"        COALESCE(le.Number, 0) * ABS((datediff(\n",
					"            CASE \n",
					"                WHEN datediff(le.Dedfrom, fd.file_date) >= 0 THEN le.Dedfrom\n",
					"                ELSE date_add(le.Dedfrom, -365)\n",
					"            END,\n",
					"            to_date(substring(le.file_name, 19, 8), 'yyyyMMdd')\n",
					"        ) / 364.25) % 1) AS expected_leave_taken_hrs,\n",
					"        COALESCE(le.Number, 0) * (1 - ABS((datediff(\n",
					"            CASE \n",
					"                WHEN datediff(le.Dedfrom, fd.file_date) >= 0 THEN le.Dedfrom\n",
					"                ELSE date_add(le.Dedfrom, -365)\n",
					"            END,\n",
					"            to_date(substring(le.file_name, 19, 8), 'yyyyMMdd')\n",
					"        ) / 364.25) % 1)) AS expected_leave_hours_remaining,\n",
					"        COALESCE(le.Number, 0) AS leave_allowance,\n",
					"        ABS((datediff(\n",
					"            CASE \n",
					"                WHEN datediff(le.Dedfrom, fd.file_date) >= 0 THEN le.Dedfrom\n",
					"                ELSE date_add(le.Dedfrom, -365)\n",
					"            END,\n",
					"            to_date(substring(le.file_name, 19, 8), 'yyyyMMdd')\n",
					"        ) / 364.25) % 1) AS percent_leave_year_passed,\n",
					"        (1 - ABS((datediff(\n",
					"            CASE \n",
					"                WHEN datediff(le.Dedfrom, fd.file_date) >= 0 THEN le.Dedfrom\n",
					"                ELSE date_add(le.Dedfrom, -365)\n",
					"            END,\n",
					"            to_date(substring(le.file_name, 19, 8), 'yyyyMMdd')\n",
					"        ) / 364.25) % 1)) AS percent_leave_year_remaining,\n",
					"        h.ContractType\n",
					"    FROM \n",
					"        odw_harmonised_db.load_sap_hr_monthly h\n",
					"    LEFT JOIN \n",
					"        odw_standardised_db.leave_entitlement le \n",
					"    ON \n",
					"        h.PersNo = CASE \n",
					"            WHEN LEFT(CAST(le.StaffNumber AS STRING), 1) = '4' THEN CONCAT('50', le.StaffNumber)\n",
					"            WHEN LEFT(CAST(le.StaffNumber AS STRING), 1) = '5' THEN CONCAT('00', le.StaffNumber)\n",
					"            ELSE CAST(le.StaffNumber AS STRING)\n",
					"        END\n",
					"        AND le.AbsenceQuotaType = 'Annual Leave / P&P'\n",
					"    CROSS JOIN \n",
					"        FileDate fd\n",
					"    \"\"\")\n",
					"    logInfo(\"Staff cost calculations completed successfully\")\n",
					"    \n",
					"    # Step 5: Calculate carried-over hours\n",
					"    logInfo(\"Calculating carried-over hours\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW CarriedOver AS\n",
					"    SELECT DISTINCT\n",
					"        h.PersNo AS staff_number,\n",
					"        CASE \n",
					"            WHEN COALESCE(le.Number, 0) - 148 >= 0 THEN 148\n",
					"            ELSE COALESCE(le.Number, 0)\n",
					"        END AS carried_over_hours\n",
					"    FROM \n",
					"        odw_harmonised_db.load_sap_hr_monthly h\n",
					"    LEFT JOIN \n",
					"        odw_standardised_db.leave_entitlement le \n",
					"    ON \n",
					"        h.PersNo = CASE \n",
					"            WHEN LEFT(CAST(le.StaffNumber AS STRING), 1) = '4' THEN CONCAT('50', le.StaffNumber)\n",
					"            WHEN LEFT(CAST(le.StaffNumber AS STRING), 1) = '5' THEN CONCAT('00', le.StaffNumber)\n",
					"            ELSE CAST(le.StaffNumber AS STRING)\n",
					"        END\n",
					"        AND le.AbsenceQuotaType = 'Brought Forward'\n",
					"    \"\"\")\n",
					"    logInfo(\"Carried-over hours calculations completed successfully\")\n",
					"    \n",
					"    # Step 6: Calculate leave taken\n",
					"    logInfo(\"Calculating leave taken hours\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW LeaveTaken AS\n",
					"    SELECT\n",
					"        h.PersNo AS staff_number,\n",
					"        COALESCE(SUM(f.absence_hours), 0) AS leave_hours_taken\n",
					"    FROM \n",
					"        odw_harmonised_db.load_sap_hr_monthly h\n",
					"    LEFT JOIN \n",
					"        odw_harmonised_db.fact_absence_all f\n",
					"    ON \n",
					"        h.PersNo = cast(f.staffnumber as int)\n",
					"        AND f.AttendanceorAbsenceType IN ('PT Annual / P&P Leave', 'FT Annual / Priv Leave')\n",
					"    LEFT JOIN \n",
					"        StaffCost l \n",
					"    ON \n",
					"        h.PersNo = l.staff_number\n",
					"    LEFT JOIN \n",
					"        FileDate fd\n",
					"    ON 1=1\n",
					"    WHERE \n",
					"        (f.absence_date BETWEEN l.leave_start_date AND fd.file_date OR f.absence_date IS NULL)\n",
					"    GROUP BY \n",
					"        h.PersNo\n",
					"    \"\"\")\n",
					"    logInfo(\"Leave taken calculations completed successfully\")\n",
					"    \n",
					"    # Step 7: Insert data into the historical table\n",
					"    logInfo(\"Starting data insertion into hist_SAP_HR\")\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.hist_SAP_HR\n",
					"    SELECT \n",
					"        h.PersNo,\n",
					"        h.Firstname,\n",
					"        h.Lastname,\n",
					"        h.EmployeeNo,\n",
					"        h.CoCd,\n",
					"        h.CompanyCode,\n",
					"        h.PA,\n",
					"        h.PersonnelArea,\n",
					"        h.PSubarea,\n",
					"        h.PersonnelSubarea,\n",
					"        h.Orgunit,\n",
					"        h.OrganizationalUnit,\n",
					"        h.Organizationalkey,\n",
					"        h.OrganizationalKey1,\n",
					"        h.WorkC,\n",
					"        h.WorkContract,\n",
					"        h.CT,\n",
					"        h.ContractType,\n",
					"        h.PSgroup,\n",
					"        h.PayBandDescription,\n",
					"        CAST(ROUND(h.FTE, 3) AS DECIMAL(10,3)) as FTE ,\n",
					"        CAST(ROUND(h.Wkhrs, 2) AS DECIMAL(10,2)) as Wkhrs,\n",
					"        h.IndicatorPartTimeEmployee,\n",
					"        h.S,\n",
					"        h.EmploymentStatus,\n",
					"        h.GenderKey,\n",
					"        h.TRAStartDate,\n",
					"        h.TRAEndDate,\n",
					"        h.TRAStatus,\n",
					"        h.TRAGrade,\n",
					"        h.PrevPersNo,\n",
					"        h.ActR,\n",
					"        h.ReasonforAction,\n",
					"        h.Position,\n",
					"        h.Position1,\n",
					"        h.CostCtr,\n",
					"        h.CostCentre,\n",
					"        h.CivilServiceStart,\n",
					"        h.DatetoCurrentJob,\n",
					"        h.SeniorityDate,\n",
					"        h.DatetoSubstGrade,\n",
					"        h.PersNo1,\n",
					"        h.NameofManagerOM,\n",
					"        h.ManagerPosition,\n",
					"        h.ManagerPositionText,\n",
					"        h.CounterSignManager,\n",
					"        h.Loc,\n",
					"        h.Location,\n",
					"        h.OrgStartDate,\n",
					"        h.FixTermEndDate,\n",
					"        h.LoanStartDate,\n",
					"        h.LoanEndDate,\n",
					"        h.EEGrp,\n",
					"        h.EmployeeGroup,\n",
					"        h.Annualsalary,\n",
					"        h.Curr,\n",
					"        h.NInumber,\n",
					"        h.Birthdate,\n",
					"        h.Ageofemployee,\n",
					"        h.EO,\n",
					"        h.Ethnicorigin,\n",
					"        h.NID,\n",
					"        h.Rel,\n",
					"        h.ReligiousDenominationKey,\n",
					"        h.SxO,\n",
					"        h.WageType,\n",
					"        h.EmployeeSubgroup,\n",
					"        h.LOAAbsType,\n",
					"        h.LOAAbsenceTypeText,\n",
					"        h.Schemereference,\n",
					"        h.PensionSchemeName,\n",
					"        h.DisabilityCode,\n",
					"        h.DisabilityText,\n",
					"        h.DisabilityCodeDescription,\n",
					"        h.PArea,\n",
					"        h.PayrollArea,\n",
					"        h.AssignmentNumber,\n",
					"        CASE \n",
					"    WHEN TRIM(h.FTE2) = '' OR TRIM(h.FTE2) = 'NULL' THEN NULL \n",
					"    ELSE FORMAT_NUMBER(TRY_CAST(h.FTE2 AS FLOAT), 4)\n",
					"END AS FTE2,\n",
					"        h.Report_MonthEnd_Date,\n",
					"        CURRENT_DATE() AS PDAC_ETL_Date,\n",
					"        SourceSystemID,\n",
					"        IngestionDate,\n",
					"        ValidTo,\n",
					"        RowID,\n",
					"        IsActive,\n",
					"        COALESCE(sc.leave_allowance, 0) + COALESCE(co.carried_over_hours, 0) AS leave_entitlement_hrs,\n",
					"        COALESCE(lt.leave_hours_taken, 0) AS leave_taken_hrs,\n",
					"        ROUND((COALESCE(sc.leave_allowance, 0) + COALESCE(co.carried_over_hours, 0) - COALESCE(sc.expected_leave_hours_remaining, 0) - COALESCE(lt.leave_hours_taken, 0)), 2) AS leave_remaining_hours,\n",
					"        COALESCE(ROUND(sc.expected_leave_hours_remaining, 2), 0) AS leave_remaining_prorata_hours\n",
					"    FROM \n",
					"        odw_harmonised_db.load_sap_hr_monthly h\n",
					"    LEFT JOIN \n",
					"        StaffCost sc \n",
					"    ON \n",
					"        h.PersNo = sc.staff_number\n",
					"    LEFT JOIN \n",
					"        CarriedOver co \n",
					"    ON \n",
					"        h.PersNo = co.staff_number\n",
					"    LEFT JOIN \n",
					"        LeaveTaken lt \n",
					"    ON \n",
					"        h.PersNo = lt.staff_number\n",
					"    LEFT JOIN \n",
					"        odw_standardised_db.pension_ernic_rates p\n",
					"    ON \n",
					"        sc.grade = p.Grade\n",
					"    WHERE \n",
					"       right(date_format(to_date(Report_MonthEnd_Date,'dd/MM/yyyy'), 'yyyy-MM-dd'),5) IN ('01-31', '02-28', '02-29', '03-31', '04-30', '05-31', '06-30', '07-31', '08-31', '09-30', '10-31', '11-30', '12-31')\n",
					"    \"\"\")\n",
					"    \n",
					"    # Get count of inserted records\n",
					"    inserted_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count FROM odw_harmonised_db.hist_SAP_HR\n",
					"    WHERE PDAC_ETL_Date = CURRENT_DATE()\n",
					"    \"\"\").collect()[0]['count']\n",
					"    logInfo(f\"Successfully inserted {inserted_count} records into hist_SAP_HR\")\n",
					"    \n",
					"    logInfo(\"Historical SAP HR data processing completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail\n",
					"    logError(f\"Error in historical SAP HR data processing: {str(e)}\")\n",
					"    logException(e)\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()"
				],
				"execution_count": 10
			}
		]
	}
}