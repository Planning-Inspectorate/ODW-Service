{
	"name": "new_rebuild_tables",
	"properties": {
		"folder": {
			"name": "utils"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "6fbba078-ca25-423a-8839-82df3a18cecc"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import requests\r\n",
					"import json"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"existing_tables = \"https://raw.githubusercontent.com/Planning-Inspectorate/odw-config/refs/heads/main/data-lake/existing-tables-metadata.json\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def rebuild_table(db_name: str, table_name: str, table_location: str, table_format: str, table_type: str):\r\n",
					"    try:\r\n",
					"        # Validate parameters\r\n",
					"        if not db_name or db_name == \"Unknown\":\r\n",
					"            raise ValueError(\"Database name not provided or is of value 'Unknown'.\")\r\n",
					"        if not table_name or table_name == \"Unknown\":\r\n",
					"            raise ValueError(\"Table name not provided or is of value 'Unknown'.\")\r\n",
					"        if not table_location or table_location == \"Unknown\":\r\n",
					"            raise ValueError(\"Location not provided or is of value = 'Unknown'.\")\r\n",
					"        if not table_format or table_format == \"Unknown\":\r\n",
					"            raise ValueError(\"Table format not provided or is of value 'Unknown'.\")\r\n",
					"        if not table_type or table_type == \"Unknown\":\r\n",
					"            raise ValueError(\"Table type not provided or is of value 'Unknown'.\")\r\n",
					"\r\n",
					"        if table_format == 'hive':\r\n",
					"            table_format = 'delta'\r\n",
					"            \r\n",
					"        # Create or rebuild the table\r\n",
					"        spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {db_name}.{table_name}\r\n",
					"        USING {table_format}\r\n",
					"        LOCATION '{table_location}'\r\n",
					"        \"\"\")\r\n",
					"\r\n",
					"        print(f\"Table {db_name}.{table_name} rebuilt at location\\n{table_location}\")\r\n",
					"    except ValueError as ve:\r\n",
					"        print(f\"Validation error: {ve}\")\r\n",
					"    except Exception as e:\r\n",
					"        print(f\"Failed to rebuild table {db_name}.{table_name}: {e}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# List of metadata\r\n",
					"metadata_list = [\r\n",
					"    'standardised_metadata',\r\n",
					"    'harmonised_metadata',\r\n",
					"    'curated_metadata',\r\n",
					"    'curated_migration_metadata',\r\n",
					"    'config_metadata',\r\n",
					"    'logging_metadata'\r\n",
					"]"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"response = requests.get(existing_tables)\r\n",
					"exisitng_tables_metadata = response.json()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"for metadata_name in metadata_list:\r\n",
					"    print(f\"Rebuilding {metadata_name.rsplit('_', 1)[0]} tables\")\r\n",
					"    metadata = next((item[metadata_name] for item in exisitng_tables_metadata if metadata_name in item), [])\r\n",
					"    for table in metadata:\r\n",
					"        db_name = table.get('database_name')\r\n",
					"        table_name = table.get('table_name')\r\n",
					"        table_location = table.get('table_location')\r\n",
					"        table_format = table.get('table_format')\r\n",
					"        table_type = table.get('table_type')\r\n",
					"        rebuild_table(db_name=db_name, table_name=table_name, table_location=table_location, table_format=table_format, table_type=table_type)"
				],
				"execution_count": null
			}
		]
	}
}