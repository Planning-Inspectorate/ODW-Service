{
	"name": "py_service_bus_json_to_csv",
	"properties": {
		"folder": {
			"name": "archive"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "tempsparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "73847c7a-f732-4f59-b72b-770322d79e0f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/tempsparkpool",
				"name": "tempsparkpool",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/tempsparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Converts JSON files received from Service Bus to a CSV"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Set the notebook parameters"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"date_folder=''\n",
					"source_folder=''\n",
					"source_frequency_folder=''"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Get Storage Account"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from notebookutils import mssparkutils\n",
					"import json\n",
					"import calendar\n",
					"from datetime import datetime, timedelta, date\n",
					"import pandas as pd\n",
					"import os\n",
					"\n",
					"spark = SparkSession.builder.getOrCreate()\n",
					"\n",
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"raw_container = \"abfss://odw-raw@\" + storage_account"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Utils"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\n",
					"def convert_json_array_to_csv(json_obj):\n",
					"    def process_json(obj):\n",
					"        if isinstance(obj, dict):\n",
					"            for key, value in obj.items():\n",
					"                obj[key] = process_json(value)\n",
					"            return obj\n",
					"        elif isinstance(obj, list):\n",
					"            return ', '.join(obj)\n",
					"        else:\n",
					"            return obj\n",
					"\n",
					"    return process_json(json_obj)"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Read the JSON files"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if date_folder == '':    \n",
					"    date_folder = datetime.now().date().strftime('%Y-%m-%d')\n",
					"\n",
					"source_path = f\"{raw_container}{source_folder}/{source_frequency_folder}/{date_folder}\"\n",
					"\n",
					"try:\n",
					"    files = mssparkutils.fs.ls(source_path)\n",
					"except Exception as e:\n",
					"    logInfo(f\"No new data\")\n",
					"    mssparkutils.notebook.exit(False)\n",
					"\n",
					"files = [f for f in files if f.name.endswith('.json')]\n",
					"\n",
					"data = []\n",
					"\n",
					"for file in files:\n",
					"    logInfo(f\"Processing: {file.name}\")\n",
					"    df = spark.read.json(file.path)\n",
					"    try:\n",
					"        json_data = [json.loads(d) for d in df.toJSON().collect()]\n",
					"\n",
					"        # some entities have nested objects and have special requirements when converting to csv\n",
					"        if source_frequency_folder == 'nsip-exam-timetable':\n",
					"            json_data_new = []\n",
					"\n",
					"            for d in json_data:\n",
					"                for e in d['events']:\n",
					"                    for key, value in d.items():\n",
					"                        if not isinstance(value, dict) and not isinstance(value, list):\n",
					"                            e[key] = d[key]\n",
					"                    json_data_new.append(e)\n",
					"\n",
					"            json_data = json_data_new\n",
					"        # some entities have nested objects and have special requirements when converting to csv\n",
					"        if source_frequency_folder == 'appeal-has':\n",
					"            json_data_new = []\n",
					"\n",
					"            for d in json_data:\n",
					"                for e in d['neighbouringSiteAddresses']:\n",
					"                    for key, value in d.items():\n",
					"                        if not isinstance(value, dict) and not isinstance(value, list):\n",
					"                            e[key] = d[key]\n",
					"                    json_data_new.append(e)\n",
					"\n",
					"            json_data = json_data_new\n",
					"\n",
					"        json_data_parsed = [convert_json_array_to_csv(d) for d in json_data]\n",
					"        data.extend(json_data_parsed)\n",
					"    except Exception as e:\n",
					"        logError(f\"Unable to process {file.name}: {str(e)}\")\n",
					"\n",
					"if len(data) == 0:\n",
					"    logInfo('No new data')\n",
					"    mssparkutils.notebook.exit(False)"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write the data in a CSV file"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"output_folder_path = f\"{source_path}/output.csv/\"\n",
					"final_output_file_path = f\"{source_path}/{source_frequency_folder}.csv\"\n",
					"\n",
					"try:\n",
					"    df = spark.createDataFrame(data)\n",
					"    df.repartition(1).write.mode('overwrite').option(\"header\",True).csv(output_folder_path)\n",
					"\n",
					"    files = mssparkutils.fs.ls(output_folder_path)\n",
					"    output_files = [f for f in files if f.name.endswith('.csv')]\n",
					"\n",
					"    if len(output_files) == 0:\n",
					"        mssparkutils.notebook.exit(False)\n",
					"\n",
					"    output_file = output_files[0]\n",
					"    try:\n",
					"        mssparkutils.fs.rm(final_output_file_path, True)\n",
					"    except Exception as e:\n",
					"        logInfo(f\"File does not exist already. Continuing conversion.\")\n",
					"    mssparkutils.fs.mv(output_file.path, final_output_file_path, True)\n",
					"    mssparkutils.fs.rm(output_folder_path, True)\n",
					"    \n",
					"except Exception as e:\n",
					"    logError(f\"An error occurred while writing to CSV: {str(e)}\")\n",
					"    mssparkutils.notebook.exit(False)\n",
					"\n",
					"mssparkutils.notebook.exit(True)"
				],
				"execution_count": null
			}
		]
	}
}