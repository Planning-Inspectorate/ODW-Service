{
	"name": "data_validation_wip",
	"properties": {
		"folder": {
			"name": "utils"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodwpr",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "d486a735-e05f-4dd3-af63-dbc949ccd970"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodwpr",
				"name": "pinssynspodwpr",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodwpr",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Data validation notebook  \r\n",
					"\r\n",
					"This contains a table mapping of source (MiPINS) and target (ODW) tables and various data validation tests to run to compare each source with its corresponding target table."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Imports etc"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from notebookutils import mssparkutils\r\n",
					"from pyspark.sql import SparkSession \r\n",
					"from pyspark.sql.types import * \r\n",
					"import pyspark.sql.functions as f\r\n",
					"import pprint\r\n",
					"import json\r\n",
					"from tqdm import tqdm\r\n",
					"\r\n",
					"# update pandas to make use of pandas.compare \"result_names\" parameter\r\n",
					"# however, %pip magic command is disabled when running a notebook from a pipeline so this needs sorting first\r\n",
					"# %pip install -U pandas\r\n",
					"import pandas as pd"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Get MiPINS password from KeyVault"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"akv_name = 'pinskvsynwodwdevuks'\r\n",
					"secret_name = 'sql-mipins-password'\r\n",
					"kv_linked_service = 'ls_kv'\r\n",
					"password = mssparkutils.credentials.getSecret(akv_name, secret_name, kv_linked_service)\r\n",
					"print(\"Password retrieved\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Define MiPINS SQL connection"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"source_servername = \"jdbc:sqlserver://pins-prod-pdac-sql.database.windows.net:1433\"\r\n",
					"source_dbname = \"MiPINS-PRD-ISS\"\r\n",
					"source_url = source_servername + \";\" + \"databaseName=\" + source_dbname + \";\"\r\n",
					"source_user = \"kincarta\" \r\n",
					"source_password = password\r\n",
					"print(\"Connection created\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Set storage account global variables"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"account_name = 'pinsstodwdevuks9h80mb'\r\n",
					"container_name = 'odw-config'\r\n",
					"jobId = mssparkutils.env.getJobId()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Function to define target dataset"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def targetdataset(target_dbtable: str):\r\n",
					"    query = f\"SELECT * FROM `odw_curated_db`.`{target_dbtable}`\"\r\n",
					"    target_df = spark.sql(query)\r\n",
					"    target_df = target_df.toPandas()\r\n",
					"\r\n",
					"    return target_df"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Function to define source dataset"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def sourcedataset(source_dbtable: str):\r\n",
					"        sourceData = spark.read \\\r\n",
					"                .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
					"                .option(\"url\", source_url) \\\r\n",
					"                .option(\"dbtable\", source_dbtable) \\\r\n",
					"                .option(\"user\", source_user) \\\r\n",
					"                .option(\"password\", source_password).load()\r\n",
					"\r\n",
					"        source_df = sourceData.toPandas()\r\n",
					"\r\n",
					"        return source_df"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### test1_rows_in_source_not_in_target"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def test1_rows_in_source_not_in_target(test_source_df, test_target_df):\r\n",
					"    result_df = test_source_df[~test_source_df.index.isin(test_target_df.index)]\r\n",
					"    if len(result_df) > 0:\r\n",
					"        result = True\r\n",
					"    else:\r\n",
					"        result = False\r\n",
					"    return result"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### test1_export_values"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def test1_export_values(test_source_df, test_target_df):\r\n",
					"    result_df = test_source_df[~test_source_df.index.isin(test_target_df.index)]\r\n",
					"    if len(result_df) > 0:\r\n",
					"        return result_df\r\n",
					"    else:\r\n",
					"        pass"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### test2_rows_in_target_not_in_source"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def test2_rows_in_target_not_in_source(test_source_df, test_target_df):\r\n",
					"    result_df = test_target_df[~test_target_df.index.isin(test_source_df.index)]\r\n",
					"    if len(result_df) > 0:\r\n",
					"        result = True\r\n",
					"    else:\r\n",
					"        result = False\r\n",
					"    return result"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### test2_export_values"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def test2_export_values(test_source_df, test_target_df):\r\n",
					"    result_df = test_target_df[~test_target_df.index.isin(test_source_df.index)]\r\n",
					"    if len(result_df) > 0:\r\n",
					"        return result_df\r\n",
					"    else:\r\n",
					"        pass"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### test3_count_source_table_rows"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def test3_count_source_table_rows(test_source_df):\r\n",
					"    return len(test_source_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### test4_count_target_table_rows"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def test4_count_target_table_rows(test_target_df):\r\n",
					"    return len(test_target_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### test5_count_source_table_columns"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def test5_count_source_table_columns(test_source_df):\r\n",
					"    return len(test_source_df.columns)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### test6_count_target_table_columns"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def test6_count_target_table_columns(test_target_df):\r\n",
					"    return len(test_target_df.columns)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### test7_duplicates_in_odw"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def test7_duplicates_in_odw(test_target_df):\r\n",
					"    duplicates_count = test_target_df.duplicated().sum()\r\n",
					"    if duplicates_count > 0:\r\n",
					"        duplicates = True\r\n",
					"    else:\r\n",
					"        duplicates = False\r\n",
					"\r\n",
					"    return duplicates"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Function to add a new test to the test results dictionary\r\n",
					"\r\n",
					"Pass a test name into the function and it sets the test result initially to \"\" (empty string)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def add_new_test_to_results_dict(test_name: str) -> None:\r\n",
					"    for k, v in test_results_dict.items():\r\n",
					"        value_dict = v\r\n",
					"        value_dict[test_name] = \"\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# # add a new test\r\n",
					"# add_new_test_to_results_dict(\"test8_values_in_source_not_in_target\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Function to add a new source table to the test results dictionary\r\n",
					"\r\n",
					"Pass the name of the new source table into the function and it will get added to the test results dictionary with the same values as the other source tables"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# def add_new_source_table_to_test_results_dict(new_source_table: str) -> None:\r\n",
					"#     for k, v in test_results_dict.copy().items():\r\n",
					"#         if new_source_table not in test_results_dict.keys():\r\n",
					"#             test_results_dict[new_source_table] = v"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# # add a new source table\r\n",
					"# add_new_source_table_to_test_result_dict(\"{table name}\")\r\n",
					"# pprint.pprint(test_result_dict)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Loop through tables and run each test against each table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"for source, target in tablemapping.items():\r\n",
					"    source_dbtable = source\r\n",
					"    target_dbtable = target\r\n",
					"    test_source_df = sourcedataset(source_dbtable)\r\n",
					"    test_target_df = targetdataset(target_dbtable)\r\n",
					"    \r\n",
					"    # run test1_rows_in_source_not_in_target\r\n",
					"    result = test1_rows_in_source_not_in_target(test_source_df, test_target_df)\r\n",
					"    test_results_dict[source_dbtable]['target_table'] = target_dbtable\r\n",
					"    test_results_dict[source_dbtable]['test1_rows_in_source_not_in_target'] = result\r\n",
					"\r\n",
					"    # run test2_rows_in_target_not_in_source\r\n",
					"    result = test2_rows_in_target_not_in_source(test_source_df, test_target_df)\r\n",
					"    test_results_dict[source_dbtable]['test2_rows_in_target_not_in_source'] = result\r\n",
					"\r\n",
					"    # run test3_count_source_table_rows\r\n",
					"    result = test3_count_source_table_rows(test_source_df)\r\n",
					"    test_results_dict[source_dbtable]['test3_count_source_table_rows'] = result\r\n",
					"\r\n",
					"    # run test4_count_target_table_rows\r\n",
					"    result = test4_count_target_table_rows(test_target_df)\r\n",
					"    test_results_dict[source_dbtable]['test4_count_target_table_rows'] = result\r\n",
					"\r\n",
					"    # run test5_count_source_table_columns\r\n",
					"    result = test5_count_source_table_columns(test_source_df)\r\n",
					"    test_results_dict[source_dbtable]['test5_count_source_table_columns'] = result\r\n",
					"\r\n",
					"    # run test6_count_target_table_columns\r\n",
					"    result = test6_count_target_table_columns(test_source_df)\r\n",
					"    test_results_dict[source_dbtable]['test6_count_target_table_columns'] = result\r\n",
					"\r\n",
					"    # run test7_duplicates_in_odw\r\n",
					"    result = test7_duplicates_in_odw(test_target_df)\r\n",
					"    test_results_dict[source_dbtable]['test7_duplicates_in_odw'] = result\r\n",
					"\r\n",
					"print(\"All tests complete!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Show test results in a DataFrame"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"result_df = pd.DataFrame.from_dict(test_results_dict, orient=\"index\")\r\n",
					"result_df.index.name = \"Source Table\"\r\n",
					"result_df"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Set the exit value of the notebook to the test results dictionary"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# mssparkutils.notebook.exit(test_results_dict)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Function to create adls path"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def create_adls_path(account_name: str, container_name: str, relative_path: str):\r\n",
					"    file_path = f'abfss://{container_name}@{account_name}.dfs.core.windows.net/{relative_path}'\r\n",
					"    return file_path"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Write to storage account"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def write_to_storage(account_name: str, container_name: str, relative_path: str, data) -> None:\r\n",
					"    account_name = account_name\r\n",
					"    container_name = container_name\r\n",
					"    relative_path = relative_path\r\n",
					"    path = create_adls_path(account_name, container_name, relative_path)\r\n",
					"\r\n",
					"    # Write spark dataframe as a csv file \r\n",
					"    print(\"adls path is \" + path)\r\n",
					"    data.write.csv(path, mode = 'overwrite', header = 'true', sep = \",\")\r\n",
					"\r\n",
					"# # Write spark dataframe as a parquet file \r\n",
					"# parquet_path = adls_path + ' Your file name ' \r\n",
					"# data.write.parquet(parquet_path, mode = 'overwrite') \r\n",
					"\r\n",
					"# # Write spark dataframe as a json file \r\n",
					"# json_path = adls_path + 'Your file name ' \r\n",
					"# data.write.json(json_path, mode = 'overwrite')"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Function to rename csv files"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def rename_csv_files(old_path: str, new_path: str) -> None:\r\n",
					"    # rename the file to a sensible name using the mssparkutils mv function\r\n",
					"    files = list(mssparkutils.fs.ls(old_path))\r\n",
					"    csv_files = [f for f in files if f.name.endswith('.csv')]\r\n",
					"    file_path = csv_files[0].path\r\n",
					"    mssparkutils.fs.mv(file_path, new_path)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Function to remove _SUCCESS files"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def remove_success_files(old_path: str, new_path: str) -> None:\r\n",
					"    # delete the _SUCCESS file as it's not needed\r\n",
					"    files = list(mssparkutils.fs.ls(old_path))\r\n",
					"    success_files = [f for f in files if f.name.endswith('_SUCCESS')]\r\n",
					"    success_file_path = success_files[0].path\r\n",
					"    mssparkutils.fs.rm(success_file_path)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Write test results file to storage account"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# define relative path\r\n",
					"relative_path = 'data_validation/outputs/test_results'\r\n",
					"\r\n",
					"# Get full path\r\n",
					"path = create_adls_path(account_name, container_name, relative_path)\r\n",
					"\r\n",
					"# convert result_df to spark DataFrame\r\n",
					"data = spark.createDataFrame(result_df)\r\n",
					"\r\n",
					"# use repartition and set to 1 so all data is written to 1 file\r\n",
					"data = data.repartition(1)\r\n",
					"\r\n",
					"# call the write_to_storage function, passing in the variables above\r\n",
					"write_to_storage(account_name, container_name, relative_path, data)\r\n",
					"print(\"File written to storage\")\r\n",
					"\r\n",
					"# Rename the file\r\n",
					"print(\"Renaming file...\")\r\n",
					"relative_path_new = 'data_validation/outputs/test_results/test_results.csv'\r\n",
					"new_path = create_adls_path(account_name, container_name, relative_path_new)\r\n",
					"rename_csv_files(old_path=path, new_path=new_path)\r\n",
					"print(\"File renamed: \" + new_path)\r\n",
					"\r\n",
					"# Delete _SUCCESS file\r\n",
					"print(\"Deleting _SUCCESS file...\")\r\n",
					"remove_success_files(old_path=path, new_path=new_path)\r\n",
					"print(\"_SUCCESS file deleted\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Function to write records to csv"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def write_records_to_csv(export_values_test, folder: str):\r\n",
					"    # create a DataFrame of the mismatched records\r\n",
					"    export_values_test = export_values_test\r\n",
					"    values_df = export_values_test(test_source_df, test_target_df)\r\n",
					"\r\n",
					"    # pass in the source table name to the file path so the records for each source table are saved in their own folder\r\n",
					"    relative_path = f'data_validation/outputs/test_results_records/{folder}/{source}'\r\n",
					"    data = spark.createDataFrame(values_df)\r\n",
					"\r\n",
					"    # cast null columns to string to avoid errors\r\n",
					"    data = data.select([f.lit(None).cast('string').alias(i.name) if isinstance(i.dataType, NullType) else i.name for i in data.schema])\r\n",
					"\r\n",
					"    # repartition so all data is written to a single file\r\n",
					"    data = data.repartition(1)\r\n",
					"\r\n",
					"    # call the write_to_storage function, passing in the account, path and data variables\r\n",
					"    path = create_adls_path(account_name, container_name, relative_path)\r\n",
					"    write_to_storage(account_name, container_name, relative_path, data)\r\n",
					"\r\n",
					"    print(f'File {source} written to storage')\r\n",
					"    print(\"Renaming file...\")\r\n",
					"\r\n",
					"    # rename the file to a sensible name using the mssparkutils mv function\r\n",
					"    relative_path_new = f'data_validation/outputs/test_results_records/{folder}/{source}/{source}.csv'\r\n",
					"    new_path = create_adls_path(account_name, container_name, relative_path_new)\r\n",
					"    rename_csv_files(old_path=path, new_path=new_path)\r\n",
					"\r\n",
					"    print(f'File renamed - new file path: {new_path}')\r\n",
					"\r\n",
					"    print(\"Deleting _SUCCESS file\")\r\n",
					"\r\n",
					"    remove_success_files(old_path=path, new_path=new_path)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Write mismatched records to a csv file and send to storage account"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# loop through test results and set the source and target DataFrames\r\n",
					"for source, target in test_results_dict.items():\r\n",
					"    source_dbtable = source\r\n",
					"    target_dbtable = target['target_table']\r\n",
					"    test_source_df = sourcedataset(source_dbtable)\r\n",
					"    test_target_df = targetdataset(target_dbtable)\r\n",
					"\r\n",
					"    # if certain tests are True then the records need exporting to csv and saved in storage\r\n",
					"    if target['test1_rows_in_source_not_in_target'] == True:\r\n",
					"        test = test1_export_values\r\n",
					"        folder = 'test1_rows_in_source_not_in_target'\r\n",
					"        write_records_to_csv(test, folder)\r\n",
					"        print(\"test1 records saved to storage\")\r\n",
					"    elif target['test2_rows_in_target_not_in_source'] == True:\r\n",
					"        test = test2_export_values\r\n",
					"        folder = 'test2_rows_in_target_not_in_source'\r\n",
					"        write_records_to_csv(test, folder)\r\n",
					"        print(\"test2 records saved to storage\")\r\n",
					"    else:\r\n",
					"        print(f'Table {source}: No records saved to storage')\r\n",
					"\r\n",
					"print(\"All files written to storage\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Test reading a json config file  \r\n",
					"\r\n",
					"It seems best to mount the storage account and then use json.load to read the file to a dictionary variable to work with normally in python. The other option is load to a spark dataframe, convert to pandas dataframe and then work with that."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Mount storage to work with json files as python dictionaries rather than use spark or pandas DataFrames"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def mount_storage(relative_path: str, jobId):\r\n",
					"    path = create_adls_path(account_name, container_name, relative_path)\r\n",
					"    print(f\"adls path: {path}\")\r\n",
					"\r\n",
					"    # unmount path first\r\n",
					"    mssparkutils.fs.unmount(f\"/{relative_path}\")\r\n",
					"\r\n",
					"    # mount file path\r\n",
					"    mssparkutils.fs.mount( \r\n",
					"        f\"abfss://{container_name}@{account_name}.dfs.core.windows.net/{relative_path}\",\r\n",
					"        f\"/{relative_path}\", \r\n",
					"        {\"linkedService\":\"ls_storage\"} \r\n",
					"    )\r\n",
					"    print(\"Path mounted\")\r\n",
					"\r\n",
					"    spark_fs_path = f\"synfs:/{jobId}/{relative_path}\"\r\n",
					"    print(f\"File path (mssparkutils fs API): {spark_fs_path}\")\r\n",
					"    local_fs_path = f\"/synfs/{jobId}/{relative_path}\"\r\n",
					"    print(f\"File path (local file system path): {local_fs_path}\")\r\n",
					"    print(\"Listing files in path - size = 0 means it's a folder\")\r\n",
					"    \r\n",
					"    return mssparkutils.fs.ls(file_path)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Load json files to python dictionaries to work with"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def load_json_file_to_dict(json_file: str, jobId):\r\n",
					"    json_path = f\"/synfs/{jobId}/{relative_path}/{json_file}\"\r\n",
					"    print(f\"json path: {json_path}\")\r\n",
					"    with open(f'/{json_path}', \"r\", encoding=\"utf-8\") as data:\r\n",
					"        table_mapping = json.load(data)\r\n",
					"\r\n",
					"    return table_mapping"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Define variables for mounting storage and reading json files to dictionaries"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"relative_path = \"data_validation\"\r\n",
					"json_file = \"test_results_dict.json\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"mount_storage(relative_path, jobId)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"load_json_file_to_dict(json_file, jobId)"
				],
				"execution_count": null
			}
		]
	}
}