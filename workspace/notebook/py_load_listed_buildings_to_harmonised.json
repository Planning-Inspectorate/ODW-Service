{
	"name": "py_load_listed_buildings_to_harmonised",
	"properties": {
		"folder": {
			"name": "odw-harmonised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "e2020571-c7e9-49d1-bb4f-88ce3cd4f5cd"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Remove data that's changed in some way. We won't keep an explicit history as we have no requirements, this is master data, and if we need to debug changes we can use the delta timetravel history."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import asyncio\r\n",
					"import nest_asyncio\r\n",
					"import tracemalloc\r\n",
					"tracemalloc.start()\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from notebookutils import mssparkutils\r\n",
					"import json\r\n",
					"import calendar\r\n",
					"from datetime import datetime, timedelta, date\r\n",
					"import pandas as pd\r\n",
					"import os\r\n",
					"import re\r\n",
					"from pyspark.sql.functions import col, lit, input_file_name, current_timestamp, sha2, concat,max as spark_max\r\n",
					"from pyspark.sql.types import StringType, DateType, TimestampType, IntegerType, FloatType, StructType, StructField\r\n",
					"from delta import DeltaTable"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_utils_common_logging_output"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator\r\n",
					""
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"source": [
					"# Initialize Logging and Tracking Metrics\r\n",
					"start_exec_time = str(datetime.now())\r\n",
					"insert_count = 0\r\n",
					"update_count = 0\r\n",
					"delete_count = 0\r\n",
					"\r\n",
					"@logging_to_appins\r\n",
					"def process_listed_building():\r\n",
					"    \"\"\"\r\n",
					"    Process listed building data from standardised to harmonised layer\r\n",
					"    \"\"\"\r\n",
					"    global insert_count, update_count, delete_count\r\n",
					"    \r\n",
					"    try:\r\n",
					"        spark = SparkSession.builder.getOrCreate()\r\n",
					"        spark.sql(\"\"\"SET spark.sql.legacy.timeParserPolicy = LEGACY;\"\"\")\r\n",
					"        spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\r\n",
					"        \r\n",
					"        # Initialize result dictionary with only required fields\r\n",
					"        result = {\r\n",
					"            \"status\": \"success\",\r\n",
					"            \"record_count\": 0,\r\n",
					"            \"error_message\": None\r\n",
					"        }\r\n",
					"        \r\n",
					"        logInfo(\"Starting listed building data processing from standardised to harmonised\")\r\n",
					"        \r\n",
					"        # Get storage account\r\n",
					"        storage_account = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\r\n",
					"        standardised_container = f\"abfss://odw-standardised@{storage_account}\"\r\n",
					"        harmonised_container = f\"abfss://odw-harmonised@{storage_account}\"\r\n",
					"        \r\n",
					"        # Define table names and paths\r\n",
					"        source_table = \"odw_standardised_db.listed_building\"\r\n",
					"        target_table = \"odw_harmonised_db.listed_building\"\r\n",
					"        target_path = f\"{harmonised_container}/listed_building\"\r\n",
					"        \r\n",
					"        logInfo(f\"Source table: {source_table}\")\r\n",
					"        logInfo(f\"Target table: {target_table}\")\r\n",
					"        logInfo(f\"Target path: {target_path}\")\r\n",
					"        \r\n",
					"        # Check if source table exists\r\n",
					"        if not spark._jsparkSession.catalog().tableExists('odw_standardised_db', 'listed_building'):\r\n",
					"            error_msg = \"Source table 'odw_standardised_db.listed_building' does not exist\"\r\n",
					"            logError(error_msg)\r\n",
					"            result[\"status\"] = \"failed\"\r\n",
					"            result[\"error_message\"] = error_msg\r\n",
					"            result[\"record_count\"] = -1\r\n",
					"            \r\n",
					"            # Log failure to app insights\r\n",
					"            end_exec_time = str(datetime.now())\r\n",
					"            app_insight_logger.add_table_result(\r\n",
					"                delta_table_name=target_table,\r\n",
					"                insert_count=insert_count,\r\n",
					"                update_count=update_count,\r\n",
					"                delete_count=delete_count,\r\n",
					"                table_result=\"failed\",\r\n",
					"                start_exec_time=start_exec_time,\r\n",
					"                end_exec_time=end_exec_time,\r\n",
					"                total_exec_time=\"\",\r\n",
					"                error_message=error_msg\r\n",
					"            )\r\n",
					"            \r\n",
					"            mssparkutils.notebook.exit(json.dumps(result))\r\n",
					"            return\r\n",
					"        \r\n",
					"        # Get initial row count from source\r\n",
					"        source_count = spark.sql(f\"SELECT COUNT(*) as count FROM {source_table}\").collect()[0]['count']\r\n",
					"        logInfo(f\"Source table contains {source_count} records\")\r\n",
					"        \r\n",
					"        # Check if target table exists and perform appropriate data selection\r\n",
					"        if spark._jsparkSession.catalog().tableExists('odw_harmonised_db', 'listed_building'):\r\n",
					"            logInfo(\"Target table exists - performing incremental load with change detection\")\r\n",
					"            \r\n",
					"            # Get the current target table count\r\n",
					"            target_count_before = spark.sql(f\"SELECT COUNT(*) as count FROM {target_table}\").collect()[0]['count']\r\n",
					"            logInfo(f\"Target table currently contains {target_count_before} records\")\r\n",
					"            \r\n",
					"            df = spark.sql(\"\"\"\r\n",
					"            SELECT\r\n",
					"                    Listings.dataset\r\n",
					"                    ,Listings.`end-date` AS endDate\r\n",
					"                    ,Listings.entity AS entity\r\n",
					"                    ,Listings.`entry-date` AS entryDate\r\n",
					"                    ,Listings.geometry\r\n",
					"                    ,Listings.`listed-building-grade` AS listedBuildingGrade\r\n",
					"                    ,Listings.name\r\n",
					"                    ,Listings.`organisation-entity` AS organisationEntity\r\n",
					"                    ,Listings.point\r\n",
					"                    ,Listings.prefix\r\n",
					"                    ,Listings.reference\r\n",
					"                    ,Listings.`start-date` AS startDate\r\n",
					"                    ,Listings.typology\r\n",
					"                    ,Listings.`documentation-url` AS documentationUrl\r\n",
					"                    ,NOW() AS dateReceived\r\n",
					"                FROM\r\n",
					"                    odw_standardised_db.listed_building as Listings\r\n",
					"                    LEFT OUTER JOIN odw_harmonised_db.listed_building AS Target\r\n",
					"                        ON Listings.entity = Target.entity\r\n",
					"                        AND Listings.dataset = Target.dataset\r\n",
					"                        AND COALESCE(Listings.`end-date`, '') = COALESCE(Target.endDate, '')\r\n",
					"                        AND COALESCE(Listings.`entry-date`, '') = COALESCE(Target.entryDate, '')\r\n",
					"                        AND COALESCE(Listings.geometry, '') = COALESCE(Target.geometry, '')\r\n",
					"                        AND COALESCE(Listings.`listed-building-grade`, '') = COALESCE(Target.listedBuildingGrade, '')\r\n",
					"                        AND COALESCE(Listings.name, '') = COALESCE(Target.name, '')\r\n",
					"                        AND COALESCE(Listings.`organisation-entity`, '') = COALESCE(Target.organisationEntity, '')\r\n",
					"                        AND COALESCE(Listings.point, '') = COALESCE(Target.point, '')\r\n",
					"                        AND COALESCE(Listings.prefix, '') = COALESCE(Target.prefix, '')\r\n",
					"                        AND COALESCE(Listings.reference, '') = COALESCE(Target.reference, '')\r\n",
					"                        AND COALESCE(Listings.`start-date`, '') = COALESCE(Target.startDate, '')\r\n",
					"                        AND COALESCE(Listings.`documentation-url`, '') = COALESCE(Target.documentationUrl, '')\r\n",
					"                        AND COALESCE(Listings.typology, '') = COALESCE(Target.typology, '')\r\n",
					"                WHERE\r\n",
					"                    Target.entity IS NULL\r\n",
					"            UNION\r\n",
					"            SELECT\r\n",
					"                    Target.dataset\r\n",
					"                    ,Target.endDate\r\n",
					"                    ,Target.entity\r\n",
					"                    ,Target.entryDate\r\n",
					"                    ,Target.geometry\r\n",
					"                    ,Target.listedBuildingGrade\r\n",
					"                    ,Target.name\r\n",
					"                    ,Target.organisationEntity\r\n",
					"                    ,Target.point\r\n",
					"                    ,Target.prefix\r\n",
					"                    ,Target.reference\r\n",
					"                    ,Target.startDate\r\n",
					"                    ,Target.typology\r\n",
					"                    ,Target.documentationUrl\r\n",
					"                    ,Target.dateReceived\r\n",
					"                FROM\r\n",
					"                    odw_standardised_db.listed_building as Listings\r\n",
					"                    INNER JOIN odw_harmonised_db.listed_building AS Target\r\n",
					"                        ON Listings.entity = Target.entity\r\n",
					"                        AND Listings.dataset = Target.dataset\r\n",
					"                        AND COALESCE(Listings.`end-date`, '') = COALESCE(Target.endDate, '')\r\n",
					"                        AND COALESCE(Listings.`entry-date`, '') = COALESCE(Target.entryDate, '')\r\n",
					"                        AND COALESCE(Listings.geometry, '') = COALESCE(Target.geometry, '')\r\n",
					"                        AND COALESCE(Listings.`listed-building-grade`, '') = COALESCE(Target.listedBuildingGrade, '')\r\n",
					"                        AND COALESCE(Listings.name, '') = COALESCE(Target.name, '')\r\n",
					"                        AND COALESCE(Listings.`organisation-entity`, '') = COALESCE(Target.organisationEntity, '')\r\n",
					"                        AND COALESCE(Listings.point, '') = COALESCE(Target.point, '')\r\n",
					"                        AND COALESCE(Listings.prefix, '') = COALESCE(Target.prefix, '')\r\n",
					"                        AND COALESCE(Listings.reference, '') = COALESCE(Target.reference, '')\r\n",
					"                        AND COALESCE(Listings.`start-date`, '') = COALESCE(Target.startDate, '')\r\n",
					"                        AND COALESCE(Listings.`documentation-url`, '') = COALESCE(Target.documentationUrl, '')\r\n",
					"                        AND COALESCE(Listings.typology, '') = COALESCE(Target.typology, '')\r\n",
					"            \"\"\")\r\n",
					"        else:\r\n",
					"            logInfo(\"Target table does not exist - performing full load from source\")\r\n",
					"            df = spark.sql(\"\"\"\r\n",
					"            SELECT\r\n",
					"                    Listings.dataset\r\n",
					"                    ,Listings.`end-date` AS endDate\r\n",
					"                    ,Listings.entity AS entity\r\n",
					"                    ,Listings.`entry-date` AS entryDate\r\n",
					"                    ,Listings.geometry\r\n",
					"                    ,Listings.`listed-building-grade` AS listedBuildingGrade\r\n",
					"                    ,Listings.name\r\n",
					"                    ,Listings.`organisation-entity` AS organisationEntity\r\n",
					"                    ,Listings.point\r\n",
					"                    ,Listings.prefix\r\n",
					"                    ,Listings.reference\r\n",
					"                    ,Listings.`start-date` AS startDate\r\n",
					"                    ,Listings.typology\r\n",
					"                    ,Listings.`documentation-url` AS documentationUrl\r\n",
					"                    ,NOW() AS dateReceived\r\n",
					"                FROM\r\n",
					"                    odw_standardised_db.listed_building as Listings\"\"\")\r\n",
					"        \r\n",
					"        # Get record count before writing\r\n",
					"        record_count = df.count()\r\n",
					"        logInfo(f\"Processed {record_count} listed building records for write operation\")\r\n",
					"        \r\n",
					"        # Check if we have any data to process\r\n",
					"        if record_count == 0:\r\n",
					"            logInfo(\"No data to process - source table is empty or no changes detected\")\r\n",
					"            result[\"record_count\"] = 0\r\n",
					"            \r\n",
					"            # Log success for no-change scenario\r\n",
					"            end_exec_time = str(datetime.now())\r\n",
					"            app_insight_logger.add_table_result(\r\n",
					"                delta_table_name=target_table,\r\n",
					"                insert_count=0,\r\n",
					"                update_count=0,\r\n",
					"                delete_count=0,\r\n",
					"                table_result=\"success\",\r\n",
					"                start_exec_time=start_exec_time,\r\n",
					"                end_exec_time=end_exec_time,\r\n",
					"                total_exec_time=\"\",\r\n",
					"                error_message=\"\"\r\n",
					"            )\r\n",
					"        else:\r\n",
					"            # Write the DataFrame to Delta table with schema merging enabled\r\n",
					"            logInfo(\"Writing data to listed_building table\")\r\n",
					"            df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(target_table)\r\n",
					"            logInfo(\"Successfully written data to listed_building table\")\r\n",
					"            \r\n",
					"            # Verify the write was successful\r\n",
					"            loaded_count = spark.sql(f\"SELECT COUNT(*) as count FROM {target_table}\").collect()[0]['count']\r\n",
					"            result[\"record_count\"] = loaded_count\r\n",
					"            insert_count = loaded_count  # For app insights logging\r\n",
					"            \r\n",
					"            logInfo(f\"Successfully loaded {loaded_count} listed building records to target table\")\r\n",
					"            \r\n",
					"            # Log success to app insights BEFORE optional optimization steps\r\n",
					"            end_exec_time = str(datetime.now())\r\n",
					"            app_insight_logger.add_table_result(\r\n",
					"                delta_table_name=target_table,\r\n",
					"                insert_count=insert_count,\r\n",
					"                update_count=update_count,\r\n",
					"                delete_count=delete_count,\r\n",
					"                table_result=\"success\",\r\n",
					"                start_exec_time=start_exec_time,\r\n",
					"                end_exec_time=end_exec_time,\r\n",
					"                total_exec_time=\"\",\r\n",
					"                error_message=\"\"\r\n",
					"            )\r\n",
					"            \r\n",
					"            # Optional: Optimize the Delta table for better performance\r\n",
					"            try:\r\n",
					"                logInfo(\"Optimizing Delta table for better query performance\")\r\n",
					"                spark.sql(f\"OPTIMIZE {target_table}\")\r\n",
					"                logInfo(\"Delta table optimization completed\")\r\n",
					"            except Exception as opt_e:\r\n",
					"                logInfo(f\"Delta table optimization skipped: {str(opt_e)}\")\r\n",
					"            \r\n",
					"            # Optional: Update table statistics (Delta v2 tables don't support ANALYZE TABLE)\r\n",
					"            try:\r\n",
					"                logInfo(\"Updating table statistics\")\r\n",
					"                # Use Delta-specific statistics command instead\r\n",
					"                spark.sql(f\"DESCRIBE DETAIL {target_table}\")\r\n",
					"                logInfo(\"Table statistics updated\")\r\n",
					"            except Exception as stats_e:\r\n",
					"                logInfo(f\"Table statistics update skipped: {str(stats_e)}\")\r\n",
					"        \r\n",
					"        logInfo(\"Listed building data processing completed successfully\")\r\n",
					"        \r\n",
					"        logInfo(\"Listed building data processing completed successfully\")\r\n",
					"\r\n",
					"    except Exception as e:\r\n",
					"        # Capture error information\r\n",
					"        error_msg = f\"Error during listed building data processing: {str(e)}\"\r\n",
					"        logError(error_msg)\r\n",
					"        logException(e)\r\n",
					"        \r\n",
					"        result[\"status\"] = \"failed\"\r\n",
					"        result[\"error_message\"] = error_msg\r\n",
					"        result[\"record_count\"] = -1  # Indicate failure with -1 count\r\n",
					"        \r\n",
					"        # Log failure to app insights\r\n",
					"        end_exec_time = str(datetime.now())\r\n",
					"        app_insight_logger.add_table_result(\r\n",
					"            delta_table_name=target_table,\r\n",
					"            insert_count=insert_count,\r\n",
					"            update_count=update_count,\r\n",
					"            delete_count=delete_count,\r\n",
					"            table_result=\"failed\",\r\n",
					"            start_exec_time=start_exec_time,\r\n",
					"            end_exec_time=end_exec_time,\r\n",
					"            total_exec_time=\"\",\r\n",
					"            error_message=error_msg\r\n",
					"        )\r\n",
					"        \r\n",
					"        # Re-raise the exception to ensure the notebook fails properly\r\n",
					"        raise e\r\n",
					"    finally:\r\n",
					"        # Always flush logs regardless of success or failure\r\n",
					"        logInfo(\"Flushing logs\")\r\n",
					"        flushLogging()\r\n",
					"        \r\n",
					"        # Output the simplified result as JSON for ADF to capture\r\n",
					"        mssparkutils.notebook.exit(app_insight_logger.generate_processing_results())\r\n",
					"\r\n",
					"# Execute the main function\r\n",
					"if __name__ == \"__main__\":\r\n",
					"    process_listed_building()"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}