{
	"name": "py_horizon_raw_to_std",
	"properties": {
		"folder": {
			"name": "1-odw-raw-to-standardised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "7cc1a95e-b54e-4f7b-981e-1812c60b0d69"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Prerequisites\n",
					"1. Make sure the new raw file's entry has been added to the Orchestration i.e `/infrastructure/configuration/data-lake/orchestration/orchestration.json`\n",
					"2. Make sure the standardised table's schema is present on the path specified in the entry added in step 1.\n",
					"3. Only if the raw file is huge (several GBs), the spark pool might need some upscaling. Hence the following cell"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%%configure -f\n",
					"{\n",
					"    \"conf\": {\n",
					"        \"spark.kryoserializer.buffer.max\": \"2047m\",\n",
					"        \"spark.driver.maxResultSize\": \"10g\",\n",
					"        \"spark.rpc.message.maxSize\": \"1280\"\n",
					"    }\n",
					"}"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"source": [
					"%run /utils/py_mount_storage"
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					" %run utils/py_logging_decorator"
				],
				"execution_count": 21
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Get the Storage Account"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import asyncio\n",
					"import nest_asyncio\n",
					"import tracemalloc\n",
					"tracemalloc.start()\n",
					"from pyspark.sql import SparkSession\n",
					"from notebookutils import mssparkutils\n",
					"import json\n",
					"import calendar\n",
					"from datetime import datetime, timedelta, date\n",
					"import pandas as pd\n",
					"import os\n",
					"import re\n",
					"from pyspark.sql.functions import col, lit, input_file_name, current_timestamp, sha2, concat, count\n",
					"from pyspark.sql.types import StringType, DateType, TimestampType, IntegerType, FloatType, StructType, StructField\n",
					"from delta import DeltaTable"
				],
				"execution_count": 22
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Initialise the parameters"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"date_folder=''\n",
					"source_folder='Horizon_Loop_Test' #need change\n",
					"specific_file='' # if not provided, it will ingest all files in the date_folder\n",
					"delete_existing_table=False\n",
					"dataAttribute = \"\""
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"source": [
					"spark = SparkSession.builder.getOrCreate()\n",
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"source_path: str = f\"abfss://odw-raw@{storage_account}{source_folder}/\"\n",
					"standardised_container = f\"abfss://odw-standardised@{storage_account}\"\n",
					"print(standardised_container)"
				],
				"execution_count": 24
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Retrieve and Display Latest Modified Folder in Directory"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# List all items in the directory\n",
					"items = mssparkutils.fs.ls(source_path)\n",
					"# Filter for directories and get their names and modification times\n",
					"folders = [(item.name, item.modifyTime) for item in items if item.isDir]\n",
					"# Sort folders by modification time in descending order\n",
					"sorted_folders = sorted(folders, key=lambda x: x[1], reverse=True)\n",
					"# Get the name of the latest modified folder\n",
					"if sorted_folders:\n",
					"    latest_folder = sorted_folders[0][0]\n",
					"    source_path=f\"{source_path}{latest_folder}\"\n",
					"    print(f\"Latest modified folder: {latest_folder}\")\n",
					"    print(source_path)\n",
					"else:\n",
					"    print(\"No folders found in the specified directory.\")"
				],
				"execution_count": 25
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### List and Retrieve File Names from Directory\""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"files = mssparkutils.fs.ls(source_path)\n",
					"horizon_files = [file.name for file in files]\n",
					"# horizon_files= horizon_files[:1]\n",
					"horizon_files\n",
					" # comment this during the full execution"
				],
				"execution_count": 26
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Create Delta Table from JSON Schema with Spark"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"@logging_to_appins\n",
					"def create_table_from_schema(jsonschema: str, db_name: str, table_name: str, target_container: str, target_folder: str, change_data_feed=False):\n",
					"   \n",
					"    spark = SparkSession.builder.getOrCreate()\n",
					"    target_loc = f\"{target_container}/{target_folder}\".rstrip(\"/\")\n",
					"\n",
					"    # Convert array fields to string in schema definition\n",
					"    schema_dict = json.loads(jsonschema)\n",
					"    for field in schema_dict.get(\"fields\", []):\n",
					"        if field.get(\"type\") == \"array\":\n",
					"            field[\"type\"] = \"string\"\n",
					"\n",
					"    # Create empty DataFrame with adjusted schema\n",
					"    schema = StructType.fromJson(schema_dict)\n",
					"    df = spark.createDataFrame([], schema)\n",
					"\n",
					"    # Verify database existence\n",
					"    if db_name not in [db.name for db in spark.catalog.listDatabases()]:\n",
					"        raise NameError(f\"Database '{db_name}' does not exist!\")\n",
					"\n",
					"    # Verify table existence\n",
					"    if table_name in [table.name for table in spark.catalog.listTables(db_name)]:\n",
					"        table_details = spark.sql(f\"DESCRIBE DETAIL {db_name}.{table_name}\").toPandas()\n",
					"        \n",
					"        if len(table_details) > 1:\n",
					"            raise ValueError(\"Multiple parquet files detected. Please investigate!\")\n",
					"        \n",
					"        existing_location = table_details['location'][0].rstrip(\"/\")\n",
					"        if existing_location == target_loc:\n",
					"            logInfo(f\"Table '{db_name}.{table_name}' already exists at correct location.\")\n",
					"        else:\n",
					"            raise ValueError(f\"Table exists but at a different location: {existing_location}. Expected: {target_loc}\")\n",
					"        return\n",
					"\n",
					"    # Create table if not exists\n",
					"    if not DeltaTable.isDeltaTable(spark, target_loc):\n",
					"        df.write.option(\"mergeSchema\", \"false\").format(\"delta\").save(target_loc)\n",
					"    \n",
					"    logInfo(f\"Creating table {db_name}.{table_name}\")\n",
					"    spark.sql(f\"CREATE TABLE {db_name}.{table_name} USING DELTA LOCATION '{target_loc}'\")\n",
					"    \n",
					"    if change_data_feed:\n",
					"        spark.sql(f\"ALTER TABLE {db_name}.{table_name} SET TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')\")\n",
					"    \n",
					"    logInfo(f\"Table '{db_name}.{table_name}' created successfully!\")"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"source": [
					"# from notebookutils import mssparkutils\n",
					"\n",
					"def mount_storage(path):\n",
					"    try:\n",
					"        mssparkutils.fs.mount(\n",
					"            source=path,\n",
					"            mount_point=\"/mnt/temp_mount\",\n",
					"            extra_configs={\"fs.azure.account.key.<storage_account>.blob.core.windows.net\": \"<access_key>\"}\n",
					"        )\n",
					"        print(f\"Mounted {path} successfully.\")\n",
					"    except Exception as e:\n",
					"        print(f\"Failed to mount {path}: {e}\")\n",
					"\n",
					"def unmount_storage():\n",
					"    try:\n",
					"        mssparkutils.fs.unmount(\"/mnt/temp_mount\")\n",
					"        print(\"Unmounted storage successfully.\")\n",
					"    except Exception as e:\n",
					"        print(f\"Failed to unmount storage: {e}\")"
				],
				"execution_count": 28
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Ingest the data from the raw/source into the standardised table. \n",
					"If the table doesn't already exist, this will create the table first and ingest the data."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"@logging_to_appins\n",
					"def ingest_adhoc(storage_account, definition, folder_path, filename, expected_from, expected_to):\n",
					"    spark = SparkSession.builder.getOrCreate()\n",
					"    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
					"    ingestion_failure: bool = False\n",
					"    \n",
					"    standardised_container = f\"abfss://odw-standardised@{storage_account}\"\n",
					"    standardised_path = f\"{definition['Standardised_Path']}_Horizon_loop_test\" + \"/\" #need remove text\n",
					"    standardised_table_name = f\"loop_test_{definition['Standardised_Table_Name']}\" #need remove text \n",
					"    source_filename_start = definition['Source_Filename_Start'] \n",
					"    process_name = mssparkutils.runtime.context['currentNotebookName']\n",
					"    \n",
					"    logging_container = f\"abfss://logging@{storage_account}\"\n",
					"    logging_table_name = 'tables_logs'\n",
					"    ingestion_log_table_location = logging_container + logging_table_name\n",
					"    \n",
					"    if 'Standardised_Table_Definition' in definition:\n",
					"        standardised_table_loc = f\"abfss://odw-config@{storage_account}\" + definition['Standardised_Table_Definition']\n",
					"        standardised_table_def_json = spark.read.text(standardised_table_loc, wholetext=True).first().value\n",
					"    else:\n",
					"        standardised_table_def_json = mssparkutils.notebook.run('/py_get_schema_from_url', 30, {'db_name': 'odw_standardised_db', 'entity_name': definition['Source_Frequency_Folder']})\n",
					"    \n",
					"    if not any([table.name.lower() == standardised_table_name.lower() for table in spark.catalog.listTables('odw_standardised_db')]):\n",
					"        create_table_from_schema(standardised_table_def_json, \"odw_standardised_db\", standardised_table_name, standardised_container, standardised_path + standardised_table_name)\n",
					"    \n",
					"    table_metadata = spark.sql(f\"DESCRIBE EXTENDED odw_standardised_db.{standardised_table_name}\")\n",
					"    data_format = table_metadata.filter(table_metadata.col_name == \"Provider\").collect()[0].data_type\n",
					"    if data_format == \"parquet\":\n",
					"        replace = spark.sql(f\"SELECT * FROM odw_standardised_db.{standardised_table_name}\")\n",
					"        replace.write.option(\"mergeSchema\", \"true\").format(\"delta\").mode(\"overwrite\").saveAsTable(f\"odw_standardised_db.{standardised_table_name}_new\")\n",
					"        spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.{standardised_table_name}\")\n",
					"        spark.sql(f\"ALTER TABLE odw_standardised_db.{standardised_table_name}_new RENAME TO odw_standardised_db.{standardised_table_name}\")\n",
					"    \n",
					"    try:\n",
					"        standardised_table_location = spark.sql(f\"DESCRIBE FORMATTED odw_standardised_db.{standardised_table_name}\") \\\n",
					"            .filter(\"col_name = 'Location'\") \\\n",
					"            .select(\"data_type\") \\\n",
					"            .collect()[0][0]\n",
					"    except:\n",
					"        standardised_table_location = standardised_container + standardised_path + standardised_table_name\n",
					"    \n",
					"    standardised_table_df = spark.read.format(\"delta\").load(standardised_table_location)\n",
					"    rows = standardised_table_df.filter((standardised_table_df.expected_from == expected_from) & (standardised_table_df.expected_to == expected_to)).count()\n",
					"    \n",
					"    jobId = mssparkutils.env.getJobId()\n",
					"    \n",
					"    if '.csv' in filename.lower():\n",
					"        df = spark.read.options(quote='\"', escape='\\\\', encoding='utf8', header=True, multiLine=True, columnNameOfCorruptRecord='corrupted_records', mode=\"PERMISSIVE\").csv(f\"{folder_path}/{filename}\")\n",
					"        if \"corrupted_records\" in df.columns:\n",
					"            print(f\"Corrupted Records detected from CSV ingestion in {filename}\")\n",
					"            ingestion_failure = True\n",
					"    else:\n",
					"        raise RuntimeError(f\"This file type for {filename} is unsupported\")\n",
					"    \n",
					"    sparkDF = df.select([col for col in df.columns if not col.startswith('Unnamed')])\n",
					"    rows_raw = sparkDF.count()\n",
					"    \n",
					"    sparkDF = sparkDF.withColumn(\"ingested_datetime\", current_timestamp())\n",
					"    sparkDF = sparkDF.withColumn(\"ingested_by_process_name\", lit(process_name))\n",
					"    sparkDF = sparkDF.withColumn(\"expected_from\", lit(expected_from))\n",
					"    sparkDF = sparkDF.withColumn(\"expected_to\", lit(expected_to))\n",
					"    sparkDF = sparkDF.withColumn(\"input_file\", input_file_name())\n",
					"    sparkDF = sparkDF.withColumn(\"modified_datetime\", current_timestamp())\n",
					"    sparkDF = sparkDF.withColumn(\"modified_by_process_name\", lit(process_name))\n",
					"    sparkDF = sparkDF.withColumn(\"entity_name\", lit(source_filename_start))\n",
					"    sparkDF = sparkDF.withColumn(\"file_ID\", sha2(concat(lit(input_file_name()), current_timestamp().cast(\"string\")), 256))\n",
					"    \n",
					"    schema = json.loads(standardised_table_def_json)\n",
					"    for field in schema['fields']:\n",
					"        if field['type'] == 'array':\n",
					"            field['type'] = 'string'\n",
					"    schema = StructType.fromJson(schema)\n",
					"    \n",
					"    cols_orig = sparkDF.schema.names\n",
					"    cols = [re.sub('[^0-9a-zA-Z]+', '_', i).lower() for i in cols_orig]\n",
					"    cols = [colm.rstrip('_') for colm in cols]\n",
					"    newlist = []\n",
					"    for i, v in enumerate(cols):\n",
					"        totalcount = cols.count(v)\n",
					"        count = cols[:i].count(v)\n",
					"        newlist.append(v + str(count + 1) if totalcount > 1 else v)\n",
					"    for colix in range(len(cols_orig)):\n",
					"        sparkDF = sparkDF.toDF(*newlist)\n",
					"    \n",
					"    for field in sparkDF.schema:\n",
					"        table_field = next((f for f in schema if f.name.lower() == field.name.lower()), None)\n",
					"        if table_field is not None and field.dataType != table_field.dataType:\n",
					"            sparkDF = sparkDF.withColumn(field.name, col(field.name).cast(table_field.dataType))\n",
					"    \n",
					"    logInfo(f\"Writing data to odw_standardised_db.{standardised_table_name}\")\n",
					"    sparkDF.write.option(\"mergeSchema\", \"true\").format(\"delta\").mode(\"append\").saveAsTable(f\"odw_standardised_db.{standardised_table_name}\")\n",
					"    logInfo(f\"Written data to odw_standardised_db.{standardised_table_name}\")\n",
					"    \n",
					"    standardised_table_df_new = spark.read.format(\"delta\").load(standardised_table_location)\n",
					"    rows_new = standardised_table_df.filter((standardised_table_df.expected_from == expected_from) & (standardised_table_df.expected_to == expected_to)).count()\n",
					"    \n",
					"    try:\n",
					"        ingestion_log_schema_loc = f\"abfss://odw-config@{storage_account}tables_logs.json\"\n",
					"        ingestion_log_schema = spark.read.text(ingestion_log_schema_loc, wholetext=True).first().value\n",
					"        \n",
					"        try:\n",
					"            ingestion_log_df = spark.read.format(\"delta\").load(ingestion_log_table_location)\n",
					"            table_exists = True\n",
					"        except Exception as e:\n",
					"            logInfo(f\"Ingestion log table not found at {ingestion_log_table_location}. Creating a new one.\")\n",
					"            table_exists = False\n",
					"        \n",
					"        new_log_entry = sparkDF.select(\n",
					"            \"file_ID\",\n",
					"            \"ingested_datetime\",\n",
					"            \"ingested_by_process_name\",\n",
					"            \"input_file\",\n",
					"            \"modified_datetime\",\n",
					"            \"modified_by_process_name\",\n",
					"            \"entity_name\"\n",
					"        ).limit(1)\n",
					"        new_log_entry = new_log_entry.withColumn(\"rows_raw\", lit(sparkDF.count()))\n",
					"        new_log_entry = new_log_entry.withColumn(\"rows_new\", lit(standardised_table_df.filter((standardised_table_df.expected_from == expected_from) & (standardised_table_df.expected_to == expected_to)).count()))\n",
					"        \n",
					"        if not table_exists:\n",
					"            new_log_entry.write.format(\"delta\").option(\"path\", ingestion_log_table_location).saveAsTable(f\"logging.{logging_table_name}\")\n",
					"            logInfo(f\"Updating ingestion logging table {logging_table_name} with first entry.\")\n",
					"        else:\n",
					"            new_log_entry.write.format(\"delta\").mode(\"append\").saveAsTable(f\"logging.{logging_table_name}\")\n",
					"            logInfo(\"Appended to existing ingestion logging table with new entry\")\n",
					"    \n",
					"    except Exception as e:\n",
					"        logInfo('Logging to tables_logs failed')\n",
					"    \n",
					"    if rows_raw <= rows_new:\n",
					"        logInfo('All rows have successfully been written')\n",
					"    else:\n",
					"        logError(f\"All rows have NOT been successfully written. Expected {rows_raw} but {rows_new} written\")\n",
					"        ingestion_failure = True\n",
					"    \n",
					"    return ingestion_failure, rows_raw\n",
					""
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"source": [
					"async def ingest_horizon(date_folder: str, file: str):\n",
					"    try:\n",
					"        spark = SparkSession.builder.getOrCreate()\n",
					"        \n",
					"        if date_folder == '':\n",
					"            date_folder = datetime.now().date()\n",
					"        else:\n",
					"            date_folder = datetime.strptime(date_folder, \"%Y-%m-%d\")\n",
					"\n",
					"        date_folder_str = date_folder.strftime('%Y-%m-%d')\n",
					"        \n",
					"        # READ ORCHESTRATION DATA\n",
					"        path_to_orchestration_file = f\"abfss://odw-config@{storage_account}/orchestration_horizon/orchestration.json\"\n",
					"        df = spark.read.option(\"multiline\", \"true\").json(path_to_orchestration_file)\n",
					"        definitions = json.loads(df.toJSON().first())['definitions']\n",
					"        \n",
					"        definition = next((d for d in definitions \n",
					"                          if (specific_file == '' or d['Source_Filename_Start'] == specific_file) \n",
					"                          and file.startswith(d['Source_Filename_Start']) \n",
					"                          and d['Load_Enable_status'] == 'True'), None)\n",
					"        \n",
					"        if definition:\n",
					"            expected_from = date_folder - timedelta(days=1)\n",
					"            expected_from = datetime.combine(expected_from, datetime.min.time())\n",
					"            expected_to = expected_from + timedelta(days=definition['Expected_Within_Weekdays']) \n",
					"\n",
					"            if delete_existing_table:\n",
					"                print(f\"Deleting existing table if exists odw_standardised_db.{definition['Standardised_Table_Name']}\")\n",
					"                mssparkutils.notebook.run('/utils/py_delete_table', 300, arguments={'db_name': 'odw_standardised_db', 'table_name': definition['Standardised_Table_Name']})\n",
					"\n",
					"            print(f\"Ingesting {file}\")\n",
					"            ingestion_failure, row_count = ingest_adhoc(storage_account, definition, source_path, file, expected_from, expected_to)\n",
					"\n",
					"            print(f\"Ingested {row_count} rows\")\n",
					"\n",
					"            if ingestion_failure:\n",
					"                print(\"Errors reported during Ingestion!!\")\n",
					"                raise RuntimeError(\"Ingestion Failure\")\n",
					"            else:\n",
					"                print(\"No Errors reported during Ingestion\")\n",
					"        else:\n",
					"            if specific_file != '':\n",
					"                raise ValueError(f\"No definition found for {specific_file}\")\n",
					"            else:\n",
					"                print(\"No definition found\")\n",
					"    except Exception as e:\n",
					"        print(f\"Failed to process {file}: {e}\")"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"source": [
					"async def load_horizon_async():\n",
					"    tasks: list = [\n",
					"        ingest_horizon(date_folder=latest_folder, file=file) for file in horizon_files\n",
					"    ]\n",
					"    await asyncio.gather(*tasks)\n",
					"\n",
					"nest_asyncio.apply()\n",
					"loop = asyncio.get_event_loop()\n",
					"loop.run_until_complete(load_horizon_async())"
				],
				"execution_count": 36
			}
		]
	}
}