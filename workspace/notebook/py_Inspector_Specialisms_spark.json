{
	"name": "py_Inspector_Specialisms_spark",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "07951370-9768-4dd2-af2a-0bb9175df201"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Standardised layer and build a table for Curated Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;25-Feb-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This template is designed to facilitate the monthly processing and harmonization of Inspector Specialism. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The template ensures that Inspector Specialism data is accurately transformed, stored, and made available for reporting and analysis."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Intialisations"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator\n",
					""
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Entity Name : inspector_Specialisms"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"\n",
					"\n",
					"logInfo(\"Starting inspector specialisms data processing\")\n",
					"\n",
					"# Step 1: Delete all records from the transform table\n",
					"logInfo(\"Step 1: Deleting records from transform_inspector_Specialisms\")\n",
					"spark.sql(\"\"\"\n",
					"DELETE FROM odw_harmonised_db.transform_inspector_Specialisms\n",
					"\"\"\")\n",
					"logInfo(\"Records deleted from transform table\")\n",
					"\n",
					"# Step 2: Insert new records into the transform table\n",
					"logInfo(\"Step 2: Inserting records into transform_inspector_Specialisms\")\n",
					"spark.sql(\"\"\"\n",
					"INSERT INTO odw_harmonised_db.transform_inspector_Specialisms (\n",
					"    StaffNumber,\n",
					"    Firstname,\n",
					"    Lastname,\n",
					"    QualificationName,\n",
					"    Proficien,\n",
					"    SourceSystemID,\n",
					"    IngestionDate,\n",
					"    ValidTo,\n",
					"    RowID,\n",
					"    IsActive\n",
					")\n",
					"SELECT \n",
					"    -- Format StaffNumber based on length and prefix\n",
					"    CASE \n",
					"        WHEN LENGTH(StaffNumber) = 6 THEN\n",
					"            CASE \n",
					"                WHEN StaffNumber LIKE '50%' THEN CONCAT('00', StaffNumber)\n",
					"                WHEN StaffNumber LIKE '42%' THEN CONCAT('50', StaffNumber)\n",
					"                ELSE StaffNumber\n",
					"            END\n",
					"        WHEN LENGTH(StaffNumber) = 8 THEN StaffNumber\n",
					"        ELSE StaffNumber\n",
					"    END AS StaffNumber,\n",
					"    Firstname,\n",
					"    Lastname,\n",
					"    QualificationName,\n",
					"    Proficien,\n",
					"    'saphr' AS SourceSystemID,\n",
					"    CURRENT_DATE() AS IngestionDate,\n",
					"    CURRENT_DATE() AS ValidTo,\n",
					"    -- Generate RowID during insert instead of separate update\n",
					"    md5(concat_ws('|', \n",
					"        coalesce(cast(\n",
					"            CASE \n",
					"                WHEN LENGTH(StaffNumber) = 6 THEN\n",
					"                    CASE \n",
					"                        WHEN StaffNumber LIKE '50%' THEN CONCAT('00', StaffNumber)\n",
					"                        WHEN StaffNumber LIKE '42%' THEN CONCAT('50', StaffNumber)\n",
					"                        ELSE StaffNumber\n",
					"                    END\n",
					"                WHEN LENGTH(StaffNumber) = 8 THEN StaffNumber\n",
					"                ELSE StaffNumber\n",
					"            END as string), ''), \n",
					"        coalesce(cast(Firstname as string), ''), \n",
					"        coalesce(cast(Lastname as string), ''), \n",
					"        coalesce(cast(QualificationName as string), ''), \n",
					"        coalesce(cast(Proficien as string), '')\n",
					"    )) AS RowID,\n",
					"    'Y' AS IsActive\n",
					"FROM \n",
					"    odw_standardised_db.inspector_specialisms_monthly t1\n",
					"WHERE \n",
					"    StaffNumber IS NOT NULL\n",
					"\"\"\")\n",
					"\n",
					"# Get count of inserted records\n",
					"record_count = spark.sql(\"SELECT COUNT(*) as record_count FROM odw_harmonised_db.transform_inspector_Specialisms\").collect()[0]['record_count']\n",
					"logInfo(f\"Inserted {record_count} records into transform_inspector_Specialisms\")\n",
					"\n",
					"logInfo(\"Inspector specialisms data processing completed successfully\")\n",
					"\n",
					"# Ensure logs are flushed\n",
					"flushLogging()"
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Update Statements"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"try:\n",
					"    logInfo(\"Starting insertion of new inspector specialisms records\")\n",
					"    \n",
					"    # First get count of potential new records\n",
					"    new_records_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"    LEFT OUTER JOIN odw_harmonised_db.sap_hr_inspector_Specialisms tarSpe\n",
					"        ON souSpe.StaffNumber = tarSpe.StaffNumber\n",
					"        AND tarSpe.Current = 1\n",
					"        AND souSpe.QualificationName = tarSpe.QualificationName\n",
					"    WHERE tarSpe.StaffNumber IS NULL\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    logInfo(f\"Found {new_records_count} new inspector specialisms to insert\")\n",
					"    \n",
					"    # Insert new records that don't exist in the target table\n",
					"    logInfo(\"Inserting new records into sap_hr_inspector_Specialisms\")\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.sap_hr_inspector_Specialisms (\n",
					"        StaffNumber, \n",
					"        Firstname, \n",
					"        Lastname, \n",
					"        QualificationName, \n",
					"        Proficien, \n",
					"        SourceSystemID, \n",
					"        IngestionDate, \n",
					"        ValidFrom, \n",
					"        ValidTo, \n",
					"        Current, \n",
					"        RowID, \n",
					"        IsActive, \n",
					"        LastUpdated\n",
					"    )\n",
					"    SELECT \n",
					"        souSpe.StaffNumber, \n",
					"        souSpe.Firstname, \n",
					"        souSpe.Lastname, \n",
					"        souSpe.QualificationName, \n",
					"        souSpe.Proficien, \n",
					"        'saphr', \n",
					"        current_date(), \n",
					"        current_date(), \n",
					"        '9999-12-31', \n",
					"        1, \n",
					"        souSpe.RowID, \n",
					"        'Y', \n",
					"        current_date()\n",
					"    FROM odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"    LEFT OUTER JOIN odw_harmonised_db.sap_hr_inspector_Specialisms tarSpe\n",
					"        ON souSpe.StaffNumber = tarSpe.StaffNumber\n",
					"        AND tarSpe.Current = 1\n",
					"        AND souSpe.QualificationName = tarSpe.QualificationName\n",
					"    WHERE tarSpe.StaffNumber IS NULL\n",
					"    \"\"\")\n",
					"    \n",
					"    # Verify the insertion was successful\n",
					"    actual_inserted = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count \n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE IngestionDate = current_date()\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    logInfo(f\"Successfully inserted {actual_inserted} new inspector specialisms records\")\n",
					"    \n",
					"    if actual_inserted != new_records_count:\n",
					"        logInfo(f\"Note: Expected to insert {new_records_count} records but actually inserted {actual_inserted}\")\n",
					"    \n",
					"    logInfo(\"Inspector specialisms insertion completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail\n",
					"    logError(f\"Error inserting inspector specialisms records: {str(e)}\")\n",
					"    logException(e)\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"try:\n",
					"    logInfo(\"Starting update of obsolete inspector specialisms records\")\n",
					"    \n",
					"    # First get count of records to be updated\n",
					"    obsolete_records_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms oldSpe\n",
					"    LEFT OUTER JOIN odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"    ON \n",
					"        oldSpe.StaffNumber = souSpe.StaffNumber\n",
					"        AND oldSpe.QualificationName = souSpe.QualificationName\n",
					"    WHERE \n",
					"        souSpe.StaffNumber IS NULL\n",
					"        AND oldSpe.Current = 1\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    logInfo(f\"Found {obsolete_records_count} obsolete inspector specialisms to update\")\n",
					"    \n",
					"    # Update records that are no longer present in the source data\n",
					"    logInfo(\"Updating obsolete records in sap_hr_inspector_Specialisms\")\n",
					"    spark.sql(\"\"\"\n",
					"    MERGE INTO odw_harmonised_db.sap_hr_inspector_Specialisms AS target\n",
					"    USING (\n",
					"        SELECT \n",
					"            oldSpe.StaffNumber, \n",
					"            oldSpe.QualificationName\n",
					"        FROM \n",
					"            odw_harmonised_db.sap_hr_inspector_Specialisms oldSpe\n",
					"        LEFT OUTER JOIN \n",
					"            odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"        ON \n",
					"            oldSpe.StaffNumber = souSpe.StaffNumber\n",
					"            AND oldSpe.QualificationName = souSpe.QualificationName\n",
					"        WHERE \n",
					"            souSpe.StaffNumber IS NULL\n",
					"    ) AS source\n",
					"    ON \n",
					"        target.StaffNumber = source.StaffNumber\n",
					"        AND target.QualificationName = source.QualificationName\n",
					"        AND target.Current = 1\n",
					"    WHEN MATCHED THEN\n",
					"    UPDATE SET \n",
					"        target.Current = 0,\n",
					"        target.ValidTo = CURRENT_DATE()\n",
					"    \"\"\")\n",
					"    \n",
					"    # Verify the update was successful\n",
					"    updated_records = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count \n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE ValidTo = CURRENT_DATE() AND Current = 0\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    logInfo(f\"Successfully updated {updated_records} obsolete inspector specialisms records\")\n",
					"    \n",
					"    if updated_records != obsolete_records_count:\n",
					"        logInfo(f\"Note: Expected to update {obsolete_records_count} records but actually updated {updated_records}\")\n",
					"    \n",
					"    logInfo(\"Obsolete inspector specialisms update completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail\n",
					"    logError(f\"Error updating obsolete inspector specialisms records: {str(e)}\")\n",
					"    logException(e)\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"try:\n",
					"    logInfo(\"Starting insertion of changed inspector specialisms records\")\n",
					"    \n",
					"    # First get count of changed records to be inserted\n",
					"    changed_records_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"    INNER JOIN odw_harmonised_db.sap_hr_inspector_Specialisms tarSpe\n",
					"        ON souSpe.StaffNumber = tarSpe.StaffNumber\n",
					"        AND tarSpe.Current = 1\n",
					"        AND souSpe.QualificationName = tarSpe.QualificationName\n",
					"    WHERE tarSpe.RowID <> souSpe.RowID\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    logInfo(f\"Found {changed_records_count} changed inspector specialisms to insert\")\n",
					"    \n",
					"    # Insert new versions of records that have changed (different RowID)\n",
					"    logInfo(\"Inserting changed records into sap_hr_inspector_Specialisms\")\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.sap_hr_inspector_Specialisms (\n",
					"        StaffNumber, \n",
					"        Firstname, \n",
					"        Lastname, \n",
					"        QualificationName, \n",
					"        Proficien, \n",
					"        SourceSystemID, \n",
					"        IngestionDate, \n",
					"        ValidFrom, \n",
					"        ValidTo, \n",
					"        Current, \n",
					"        RowID, \n",
					"        IsActive, \n",
					"        LastUpdated\n",
					"    )\n",
					"    SELECT \n",
					"        souSpe.StaffNumber, \n",
					"        souSpe.Firstname, \n",
					"        souSpe.Lastname, \n",
					"        souSpe.QualificationName, \n",
					"        souSpe.Proficien, \n",
					"        'saphr', \n",
					"        current_date(), \n",
					"        current_date(), \n",
					"        '9999-12-31', \n",
					"        1, \n",
					"        souSpe.RowID, \n",
					"        'Y', \n",
					"        current_date()\n",
					"    FROM odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"    LEFT OUTER JOIN odw_harmonised_db.sap_hr_inspector_Specialisms tarSpe\n",
					"        ON souSpe.StaffNumber = tarSpe.StaffNumber\n",
					"        AND tarSpe.Current = 1\n",
					"        AND souSpe.QualificationName = tarSpe.QualificationName\n",
					"    WHERE tarSpe.RowID <> souSpe.RowID\n",
					"    \"\"\")\n",
					"    \n",
					"    # Verify the insertion was successful\n",
					"    inserted_records = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count \n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE IngestionDate = CURRENT_DATE() AND Current = 1\n",
					"    \"\"\").collect()[0]['count'] - spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count \n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE ValidFrom = CURRENT_DATE() AND Current = 1\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    logInfo(f\"Successfully inserted {inserted_records} changed inspector specialisms records\")\n",
					"    \n",
					"    if inserted_records != changed_records_count:\n",
					"        logInfo(f\"Note: Expected to insert {changed_records_count} records but actual count differs\")\n",
					"    \n",
					"    logInfo(\"Changed inspector specialisms insertion completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail\n",
					"    logError(f\"Error inserting changed inspector specialisms records: {str(e)}\")\n",
					"    logException(e)\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"logInfo(\"Use CTE to deduplicate and then MERGE in a single operation\")\n",
					"spark.sql(\"\"\"\n",
					"-- Step 1: Deduplicate the source table (newSpe)\n",
					"WITH DeduplicatedSource AS (\n",
					"    SELECT \n",
					"        StaffNumber,\n",
					"        QualificationName,\n",
					"        ValidFrom,\n",
					"        Current,\n",
					"        ROW_NUMBER() OVER (\n",
					"            PARTITION BY StaffNumber, QualificationName \n",
					"            ORDER BY ValidFrom DESC\n",
					"        ) AS row_num\n",
					"    FROM \n",
					"        odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE \n",
					"        Current = 1\n",
					"        AND ValidFrom = current_date()\n",
					")\n",
					"\n",
					"-- Step 2: Perform the MERGE operation using the deduplicated source\n",
					"MERGE INTO odw_harmonised_db.sap_hr_inspector_Specialisms AS oldSpe\n",
					"USING DeduplicatedSource AS newSpe\n",
					"ON \n",
					"    oldSpe.StaffNumber = newSpe.StaffNumber\n",
					"    AND oldSpe.QualificationName = newSpe.QualificationName\n",
					"    AND oldSpe.Current = 1\n",
					"    AND oldSpe.ValidFrom < current_date()\n",
					"    AND newSpe.row_num = 1 -- Ensure only one row per combination\n",
					"WHEN MATCHED THEN\n",
					"UPDATE SET \n",
					"    oldSpe.Current = 0,\n",
					"    oldSpe.ValidTo = current_date()\n",
					"\"\"\")"
				],
				"execution_count": 15
			}
		]
	}
}