{
	"name": "zendesk_standardised_and_harmonised",
	"properties": {
		"folder": {
			"name": "odw-raw"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "77c5b33a-6e37-4962-b140-cb4ad8e4ad58"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 32,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"The purpose of the notebook is to break the export file which holds 2819 zendesk tickets in separate instances\r\n",
					"The notebook also holds placeholder comments which can be used for table create. (this info has beeen moved into another notebook but is used as reference point)"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from collections.abc import Mapping\r\n",
					"from itertools import chain\r\n",
					"from operator import add\r\n",
					"#ignore FutureWarning messages \r\n",
					"import warnings\r\n",
					"warnings.filterwarnings(\"ignore\", message=\"iteritems is deprecated\")\r\n",
					"from notebookutils import mssparkutils\r\n",
					"from pprint import pprint as pp\r\n",
					"import json\r\n",
					"import pyspark.sql.functions as F \r\n",
					"import pandas as pd\r\n",
					"from pyspark.sql.types import *"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"_FLAG_FIRST = object()\r\n",
					"#flatteb dict function\r\n",
					"def flattenDict(d, join=add, lift=lambda x:(x,)):\r\n",
					"    results = []\r\n",
					"    def visit(subdict, results, partialKey):\r\n",
					"        for k,v in subdict.items():\r\n",
					"            newKey = lift(k) if partialKey==_FLAG_FIRST else join(partialKey,lift(k))\r\n",
					"            if isinstance(v,Mapping):\r\n",
					"                visit(v, results, newKey)\r\n",
					"            else:\r\n",
					"                results.append((newKey,v))\r\n",
					"    visit(d, results, _FLAG_FIRST)\r\n",
					"    return results"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run /utils/py_mount_storage"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"ms_comments": [
						{
							"threadId": "85770421-1f75-4461-b831-0cc4944d7cc9",
							"text": "parameterise for environment",
							"status": "resolved",
							"user": {
								"name": "Pakwashee, Abdullah",
								"idType": "aad"
							},
							"createdDateUTC": 1685110159242,
							"modifiedDateUTC": 1685544441197,
							"replies": []
						}
					]
				},
				"source": [
					"from notebookutils import mssparkutils\r\n",
					"### mount the data lake storage in Synapse to the Synapse File Mount API\r\n",
					"jobId = mssparkutils.env.getJobId()\r\n",
					"storage_acc_name = spark.sparkContext.environment.get('dataLakeAccountName', 'get')\r\n",
					"mount_storage(path=\"abfss://odw-raw@\"+storage_acc_name+\".dfs.core.windows.net/ZenDesk/Export/\", mount_point=\"/zendesk_items\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"ms_comments": [
						{
							"threadId": "357b88a3-d6dc-47c2-aa6f-99567564b7f8",
							"text": "comment out - single run",
							"status": "resolved",
							"user": {
								"name": "Pakwashee, Abdullah",
								"idType": "aad"
							},
							"createdDateUTC": 1685110200896,
							"modifiedDateUTC": 1685548270056,
							"replies": []
						},
						{
							"threadId": "3e53afea-ddf1-41a5-9671-4f69f1292c03",
							"text": "how do we get this file into test?",
							"status": "active",
							"user": {
								"name": "Pakwashee, Abdullah",
								"idType": "aad"
							},
							"createdDateUTC": 1685110215187,
							"modifiedDateUTC": 1685547915792,
							"replies": [
								{
									"replyId": "899dbea9-5a37-4e5f-af83-e50d6e30ba31",
									"text": "schema file too",
									"user": {
										"name": "Pakwashee, Abdullah",
										"idType": "aad"
									},
									"createdDateUTC": 1685110269134,
									"modifiedDateUTC": 1685110269134
								}
							]
						}
					],
					"ms_comment_ranges": {
						"357b88a3-d6dc-47c2-aa6f-99567564b7f8": {
							"text": "write_to_single_file():",
							"start": {
								"line": 1,
								"column": 5
							},
							"end": {
								"line": 1,
								"column": 28
							}
						},
						"3e53afea-ddf1-41a5-9671-4f69f1292c03": {
							"text": "zendesk_items/export-2023-03-15-1051-10932060-13758561992081e5c3.json\"",
							"start": {
								"line": 4,
								"column": 32
							},
							"end": {
								"line": 4,
								"column": 102
							}
						}
					}
				},
				"source": [
					"def write_to_single_file():\r\n",
					"    \r\n",
					"\r\n",
					"    with open(f\"/synfs/{jobId}/zendesk_items/export-2023-03-15-1051-10932060-13758561992081e5c3.json\", 'r',encoding=\"utf-8\") as zendesk_json_raw:\r\n",
					"\r\n",
					"        zendesk_json_decoded = zendesk_json_raw.read()\r\n",
					"\r\n",
					"        zendesk_json_array = zendesk_json_decoded.split(\"\\n\")\r\n",
					"\r\n",
					"        item_number = 0\r\n",
					"\r\n",
					"        for zendesk_json_item in zendesk_json_array:\r\n",
					"\r\n",
					"            if zendesk_json_item != \"\":\r\n",
					"\r\n",
					"                zendesk_json_dict = json.loads(zendesk_json_item)\r\n",
					"\r\n",
					"                with open(f\"/synfs/{jobId}/zendesk_items/output_{item_number}.json\", \"w\", encoding=\"utf-8\") as write_single_zendesk_json:\r\n",
					"\r\n",
					"                    json.dump(zendesk_json_dict, write_single_zendesk_json)\r\n",
					"\r\n",
					"                item_number += 1\r\n",
					"\r\n",
					"write_to_single_file()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# def read_schema():\r\n",
					"\r\n",
					"#     with open(f\"/synfs/{jobId}/zendesk_schema/output_schema.json\", 'r') as schema:\r\n",
					"\r\n",
					"#         schema_dict = json.load(schema)\r\n",
					"\r\n",
					"#     return schema_dict\r\n",
					"# schema_dict = read_schema()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"ms_comments": [
						{
							"threadId": "c95e30e1-6b8f-4606-9cee-e5ccbc004783",
							"text": "add logging to all of the functions",
							"status": "resolved",
							"user": {
								"name": "Pakwashee, Abdullah",
								"idType": "aad"
							},
							"createdDateUTC": 1685110443510,
							"modifiedDateUTC": 1685450500081,
							"replies": []
						},
						{
							"threadId": "45a7f6d4-05e7-4918-a6d5-0820c6fb600b",
							"text": "figure out why this one is breaking",
							"status": "active",
							"user": {
								"name": "Pakwashee, Abdullah",
								"idType": "aad"
							},
							"createdDateUTC": 1685110370334,
							"modifiedDateUTC": 1685110370334,
							"replies": []
						},
						{
							"threadId": "f0a638a9-885a-4769-bb2b-b06f4fc38d58",
							"text": "allow exceptions to occur - if we do keep this except, add detail to error that is raised",
							"status": "resolved",
							"user": {
								"name": "Pakwashee, Abdullah",
								"idType": "aad"
							},
							"createdDateUTC": 1685110416607,
							"modifiedDateUTC": 1685450008801,
							"replies": []
						}
					],
					"ms_comment_ranges": {
						"45a7f6d4-05e7-4918-a6d5-0820c6fb600b": {
							"text": "        if i == 2802:\n#             i=i+1",
							"start": {
								"line": 7,
								"column": 3
							},
							"end": {
								"line": 8,
								"column": 20
							}
						},
						"f0a638a9-885a-4769-bb2b-b06f4fc38d58": {
							"text": "                except jsonschema.exceptions.ValidationError as err:\n#                     print(\"Schema could not validate file number:\")\n#                     print(i)",
							"start": {
								"line": 14,
								"column": 3
							},
							"end": {
								"line": 16,
								"column": 31
							}
						},
						"c95e30e1-6b8f-4606-9cee-e5ccbc004783": {
							"text": "def read_zendesk(schema_dict):",
							"start": {
								"line": 5,
								"column": 3
							},
							"end": {
								"line": 5,
								"column": 33
							}
						}
					}
				},
				"source": [
					"# import json\r\n",
					"# import jsonschema\r\n",
					"# from jsonschema import validate\r\n",
					"# list_with_all_zendesk_dict = []\r\n",
					"# def read_zendesk(schema_dict):\r\n",
					"#     for i in range(2918):\r\n",
					"#         if i == 2802:\r\n",
					"#             i=i+1\r\n",
					"#         else:\r\n",
					"#             with open(f\"/synfs/{jobId}/zendesk_items/output_{i}.json\", \"r\",encoding='utf-8') as big_json:\r\n",
					"#                 zendesk_dict = json.load(big_json)\r\n",
					"#                 try:\r\n",
					"#                     validate(instance=zendesk_dict, schema=schema_dict)\r\n",
					"#                 except jsonschema.exceptions.ValidationError as err:\r\n",
					"#                     print(\"Schema could not validate file number:\")\r\n",
					"#                     print(i)\r\n",
					"#                 list_with_all_zendesk_dict.append(zendesk_dict)\r\n",
					"#     return list_with_all_zendesk_dict\r\n",
					"# zendesk_dict = read_zendesk(schema_dict)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"ms_comments": [
						{
							"threadId": "db0363b6-21d2-499d-bad3-b65a7e2a3130",
							"text": "parameterise storage account name",
							"status": "resolved",
							"user": {
								"name": "Pakwashee, Abdullah",
								"idType": "aad"
							},
							"createdDateUTC": 1685110539830,
							"modifiedDateUTC": 1685544462905,
							"replies": []
						},
						{
							"threadId": "63e0fe52-c87f-4e8b-b764-1d82e8584c45",
							"text": "rename standardised table to odw_standardised_db.zendesk_system_extract",
							"status": "resolved",
							"user": {
								"name": "Pakwashee, Abdullah",
								"idType": "aad"
							},
							"createdDateUTC": 1685110625195,
							"modifiedDateUTC": 1685436236858,
							"replies": []
						}
					],
					"ms_comment_ranges": {
						"db0363b6-21d2-499d-bad3-b65a7e2a3130": {
							"text": "shows = spark.read.json(\"abfss://odw-raw@\"+storage_acc_name+\".dfs.core.windows.net/ZenDesk/Export/output_*.json\",multiLine=True)\n",
							"start": {
								"line": 4,
								"column": 3
							},
							"end": {
								"line": 5,
								"column": 1
							}
						},
						"63e0fe52-c87f-4e8b-b764-1d82e8584c45": {
							"text": "zendesk_s",
							"start": {
								"line": 7,
								"column": 64
							},
							"end": {
								"line": 7,
								"column": 73
							}
						}
					}
				},
				"source": [
					"# from pyspark.sql import SparkSession \r\n",
					"# spark = SparkSession.builder.getOrCreate() \r\n",
					"# storage_acc_name = spark.sparkContext.environment.get('dataLakeAccountName', 'get')\r\n",
					"# shows = spark.read.json(\"abfss://odw-raw@\"+storage_acc_name+\".dfs.core.windows.net/ZenDesk/Export/output_*.json\",multiLine=True)\r\n",
					"# # Create mega table\r\n",
					"# spark.sql(f\"drop table if exists odw_standardised_db.zendesk_system_extract;\")\r\n",
					"# shows.write.format('delta').saveAsTable(\"odw_standardised_db.zendesk_system_extract\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# storage_acc_name = spark.sparkContext.environment.get('dataLakeAccountName', 'get')"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"ms_comments": []
				},
				"source": [
					"# import pyspark.sql.functions as F \r\n",
					"# def fields_table():\r\n",
					"#     fields = shows.select(\r\n",
					"#     \"id\", F.explode(\"fields\").alias(\"fields\")\r\n",
					"#     )\r\n",
					"\r\n",
					"#     fields_id_value = shows.select(F.col(\"id\").alias(\"ticket_id\"),\r\n",
					"#         F.map_from_arrays(\r\n",
					"#         F.col(\"fields.id\"), F.col(\"fields.value\")\r\n",
					"#         ).alias(\"ID_VALUE\")\r\n",
					"#         )\r\n",
					"#     # fields_id_value.show()\r\n",
					"#     fields_id_value = fields_id_value.select(\"ticket_id\",\r\n",
					"#         F.posexplode(\"ID_VALUE\").alias(\"position\", \"id\", \"value\")\r\n",
					"#         )\r\n",
					"#     # fields_id_value.show()\r\n",
					"#     fields_id_value.drop(\"position\")\r\n",
					"#     spark.sql(f\"drop table if exists odw_harmonised_db.zendesk_fields_id_value;\")\r\n",
					"#     fields_id_value.write.format('delta').saveAsTable(\"odw_harmonised_db.zendesk_fields_id_value\")\r\n",
					"# fields_table()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# import pyspark.sql.functions as F \r\n",
					"# def custom_fields_table():\r\n",
					"#     custom_fields = shows.select(\r\n",
					"#     \"id\", F.explode(\"custom_fields\").alias(\"custom_fields\")\r\n",
					"#     )\r\n",
					"\r\n",
					"#     custom_fields_id_value = shows.select(F.col(\"id\").alias(\"ticket_id\"),\r\n",
					"#         F.map_from_arrays(\r\n",
					"#         F.col(\"custom_fields.id\"), F.col(\"custom_fields.value\")\r\n",
					"#         ).alias(\"ID_VALUE\")\r\n",
					"#         )\r\n",
					"#     # custom_fields_id_value.show()\r\n",
					"#     custom_fields_id_value = custom_fields_id_value.select(\"ticket_id\",\r\n",
					"#         F.posexplode(\"ID_VALUE\").alias(\"position\", \"id\", \"value\")\r\n",
					"#         )\r\n",
					"#     # custom_fields_id_value.show()\r\n",
					"#     custom_fields_id_value.drop(\"position\")\r\n",
					"#     spark.sql(f\"drop table if exists odw_harmonised_db.zendesk_custom_fields_id_value;\")\r\n",
					"#     custom_fields_id_value.write.format('delta').saveAsTable(\"odw_harmonised_db.zendesk_custom_fields_id_value\")\r\n",
					"# custom_fields_table()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"ms_comments": [
						{
							"threadId": "437e3dbd-79c9-4db6-bfb3-2bce1e8ea056",
							"text": "let's review this once we fix the logic app",
							"status": "active",
							"user": {
								"name": "Pakwashee, Abdullah",
								"idType": "aad"
							},
							"createdDateUTC": 1685111080736,
							"modifiedDateUTC": 1685111080736,
							"replies": []
						},
						{
							"threadId": "3bdf619d-6e98-4047-8495-94d579792d6b",
							"text": "ticket_id",
							"status": "resolved",
							"user": {
								"name": "Pakwashee, Abdullah",
								"idType": "aad"
							},
							"createdDateUTC": 1685110934263,
							"modifiedDateUTC": 1685436289130,
							"replies": [
								{
									"replyId": "1d05f795-3ab9-43fa-b597-cb57357343c0",
									"text": "consistency in naming convention- use ticket everywhere",
									"user": {
										"name": "Pakwashee, Abdullah",
										"idType": "aad"
									},
									"createdDateUTC": 1685110952830,
									"modifiedDateUTC": 1685110952830
								}
							]
						},
						{
							"threadId": "99e88483-8d34-41b4-882e-7e79a22f8872",
							"text": "needs to be delta",
							"status": "active",
							"user": {
								"name": "Pakwashee, Abdullah",
								"idType": "aad"
							},
							"createdDateUTC": 1685111128426,
							"modifiedDateUTC": 1685111128426,
							"replies": [
								{
									"replyId": "5f78509d-d5f5-4035-86a1-97ffa3e8c34a",
									"text": "let's debug it if it can't be delta",
									"user": {
										"name": "Pakwashee, Abdullah",
										"idType": "aad"
									},
									"createdDateUTC": 1685111147234,
									"modifiedDateUTC": 1685111147234
								}
							]
						}
					],
					"ms_comment_ranges": {
						"3bdf619d-6e98-4047-8495-94d579792d6b": {
							"text": "tick",
							"start": {
								"line": 16,
								"column": 33
							},
							"end": {
								"line": 16,
								"column": 37
							}
						},
						"437e3dbd-79c9-4db6-bfb3-2bce1e8ea056": {
							"text": " comments_and_attachments():",
							"start": {
								"line": 1,
								"column": 6
							},
							"end": {
								"line": 1,
								"column": 34
							}
						},
						"99e88483-8d34-41b4-882e-7e79a22f8872": {
							"text": "sFlatCommentsDictList.write.saveAsTable(\"odw_harmonised_db.zendesk_comments\")",
							"start": {
								"line": 35,
								"column": 7
							},
							"end": {
								"line": 35,
								"column": 84
							}
						}
					}
				},
				"source": [
					"# def comments_and_attachments():\r\n",
					"#     #create empty list to store the values of the comments \r\n",
					"#     FlatCommentsDictList=[]\r\n",
					"#     #create empty list to store the value of the attachments array within the commments column\r\n",
					"#     FlatCommentsDictAttachmentsList=[]\r\n",
					"#     for i in range(2918):\r\n",
					"#         # print(i)\r\n",
					"#         if i == 2802:\r\n",
					"#             i=i+1\r\n",
					"#         else:\r\n",
					"#             with open(f\"/synfs/{jobId}/zendesk_items/output_{i}.json\", \"r\",encoding='utf-8') as big_json:\r\n",
					"#                 zendesk_dict = json.load(big_json)\r\n",
					"#             CommentsListOfDict = zendesk_dict['comments']\r\n",
					"#             for CommentsDict in CommentsListOfDict:\r\n",
					"#                 #adding the file id to the dictionary before converting into df and then table\r\n",
					"#                 CommentsDict['ticket_id']=zendesk_dict['id']\r\n",
					"#                 FlatCommentsDict = dict( flattenDict(CommentsDict, join=lambda a,b:a+'_'+b, lift=lambda x:x) )\r\n",
					"#                 #go through all attachments within the comments column \r\n",
					"#                 for attachments in CommentsDict['attachments']:\r\n",
					"#                     #add the comments id and file id so we can late join the tables on them\r\n",
					"#                     attachments['ticket_id']=zendesk_dict['id']\r\n",
					"#                     attachments['comment_id']=CommentsDict['id']\r\n",
					"#                     FlatCommentsDictAttachmentsList.append(attachments)\r\n",
					"#                 #delete the attachment column because we already stored it in a list which will be made in a table \r\n",
					"#                 del FlatCommentsDict['attachments']\r\n",
					"#                 #flatten the comments column so we can make a table out of it \r\n",
					"#                 FlatCommentsDictList.append(FlatCommentsDict)\r\n",
					"#     #create a dataframe out of all comments of a file to put them in a table \r\n",
					"#     dfFlatCommentsDictList =pd.DataFrame(FlatCommentsDictList)\r\n",
					"#     #create a dataframe out of all commects_attachmenst of a file to create and store them in a table \r\n",
					"#     dfFlatCommentsDictAttachmentsList = pd.DataFrame(FlatCommentsDictAttachmentsList)\r\n",
					"#     dfFlatCommentsDictList.via_source_to_email_ccs = dfFlatCommentsDictList.via_source_to_email_ccs.astype(str)\r\n",
					"#     sFlatCommentsDictList = spark.createDataFrame(dfFlatCommentsDictList)\r\n",
					"#     spark.sql(f\"drop table if exists odw_harmonised_db.zendesk_comments;\")\r\n",
					"#     sFlatCommentsDictList.write.saveAsTable(\"odw_harmonised_db.zendesk_comments\")\r\n",
					"#     spark.sql(f\"drop table if exists odw_harmonised_db.zendesk_comments_attachments;\")\r\n",
					"#     sparkDF=spark.createDataFrame(dfFlatCommentsDictAttachmentsList)   \r\n",
					"#     sparkDF1= sparkDF.withColumn(\"width\",F.col(\"width\").cast(\"string\")).withColumn(\"height\",F.col(\"height\").cast(\"string\")).withColumn(\"thumbnails\",F.col(\"thumbnails\").cast(ArrayType(StringType(),True)))\r\n",
					"#     sparkDF1.printSchema\r\n",
					"#     sparkDF1.write.format('delta').saveAsTable(\"odw_harmonised_db.zendesk_comments_attachments\")      \r\n",
					"# comments_and_attachments()\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"ms_comments": [
						{
							"threadId": "52e648f1-d6c1-4fdb-a508-ed450b8ca1cb",
							"text": "let's explore this",
							"status": "active",
							"user": {
								"name": "Pakwashee, Abdullah",
								"idType": "aad"
							},
							"createdDateUTC": 1685111258127,
							"modifiedDateUTC": 1685111258127,
							"replies": []
						}
					],
					"ms_comment_ranges": {
						"52e648f1-d6c1-4fdb-a508-ed450b8ca1cb": {
							"text": "schema = StructType([StructField(\"foo\", StringType(), True)])",
							"start": {
								"line": 23,
								"column": 7
							},
							"end": {
								"line": 23,
								"column": 68
							}
						}
					}
				},
				"source": [
					"# def assignee_table(list_with_all_zendesk_dict):\r\n",
					"#     assigneelist=[]\r\n",
					"#     for assigneecolumn in list_with_all_zendesk_dict:\r\n",
					"#         AssigneeDict= assigneecolumn['assignee']\r\n",
					"#         if AssigneeDict is None:\r\n",
					"#             continue\r\n",
					"#         else:\r\n",
					"#             AssigneeDict['ticket_id']=assigneecolumn['id']\r\n",
					"#             assigneelist.append(AssigneeDict)\r\n",
					"            \r\n",
					"    \r\n",
					"#     dfassigneelist=pd.DataFrame(assigneelist)\r\n",
					"#     dfassigneelist.id = dfassigneelist.id.astype(str)\r\n",
					"#     dfassigneelist.locale_id = dfassigneelist.locale_id.astype(str)\r\n",
					"#     dfassigneelist.organization_id = dfassigneelist.organization_id.astype(str)\r\n",
					"#     dfassigneelist.role_type = dfassigneelist.role_type.astype(str)\r\n",
					"#     dfassigneelist.custom_role_id = dfassigneelist.custom_role_id.astype(str)\r\n",
					"#     dfassigneelist.organization_id = dfassigneelist.organization_id.astype(str)\r\n",
					"#     dfassigneelist.default_group_id = dfassigneelist.default_group_id.astype(str)\r\n",
					"\r\n",
					"\r\n",
					"#     from pyspark.sql.types import StructType, StructField, StringType\r\n",
					"#     schema = StructType([StructField(\"foo\", StringType(), True)])\r\n",
					"#     dfassigneelist = dfassigneelist.dropna(axis='columns', how='all') # Drops columns with all NA values\r\n",
					"\r\n",
					"#     sassigneelist = spark.createDataFrame(dfassigneelist)\r\n",
					"#     spark.sql(f\"drop table if exists odw_harmonised_db.zendesk_assignee;\")\r\n",
					"#     sassigneelist.write.format('delta').saveAsTable(\"odw_harmonised_db.zendesk_assignee\")\r\n",
					"\r\n",
					"# assignee_table(list_with_all_zendesk_dict)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# def submitter_table(list_with_all_zendesk_dict):\r\n",
					"#     submitterlist=[]\r\n",
					"#     for submittercolumn in list_with_all_zendesk_dict:\r\n",
					"#         submitterDict= submittercolumn['submitter']\r\n",
					"#         if submitterDict is None:\r\n",
					"#             continue\r\n",
					"#         else:\r\n",
					"#             submitterDict['ticket_id']=submittercolumn['id']\r\n",
					"#             submitterlist.append(submitterDict)\r\n",
					"            \r\n",
					"    \r\n",
					"#     dfsubmitterlist=pd.DataFrame(submitterlist)\r\n",
					"#     dfsubmitterlist.id = dfsubmitterlist.id.astype(str)\r\n",
					"#     dfsubmitterlist.locale_id = dfsubmitterlist.locale_id.astype(str)\r\n",
					"#     dfsubmitterlist.organization_id = dfsubmitterlist.organization_id.astype(str)\r\n",
					"#     dfsubmitterlist.role_type = dfsubmitterlist.role_type.astype(str)\r\n",
					"#     dfsubmitterlist.custom_role_id = dfsubmitterlist.custom_role_id.astype(str)\r\n",
					"#     dfsubmitterlist.organization_id = dfsubmitterlist.organization_id.astype(str)\r\n",
					"#     dfsubmitterlist.default_group_id = dfsubmitterlist.default_group_id.astype(str)\r\n",
					"\r\n",
					"\r\n",
					"#     from pyspark.sql.types import StructType, StructField, StringType\r\n",
					"#     schema = StructType([StructField(\"foo\", StringType(), True)])\r\n",
					"#     dfsubmitterlist = dfsubmitterlist.dropna(axis='columns', how='all') # Drops columns with all NA values\r\n",
					"\r\n",
					"#     ssubmitterlist = spark.createDataFrame(dfsubmitterlist)\r\n",
					"#     spark.sql(f\"drop table if exists odw_harmonised_db.zendesk_submitter;\")\r\n",
					"#     ssubmitterlist.write.format('delta').saveAsTable(\"odw_harmonised_db.zendesk_submitter\")\r\n",
					"\r\n",
					"# submitter_table(list_with_all_zendesk_dict)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# def requester_table(list_with_all_zendesk_dict):\r\n",
					"#     requesterlist=[]\r\n",
					"#     for requestercolumn in list_with_all_zendesk_dict:\r\n",
					"#         requesterDict= requestercolumn['requester']\r\n",
					"#         if requesterDict is None:\r\n",
					"#             continue\r\n",
					"#         else:\r\n",
					"#             requesterDict['ticket_id']=requestercolumn['id']\r\n",
					"#             requesterlist.append(requesterDict)\r\n",
					"            \r\n",
					"    \r\n",
					"#     dfrequesterlist=pd.DataFrame(requesterlist)\r\n",
					"#     dfrequesterlist.id = dfrequesterlist.id.astype(str)\r\n",
					"#     dfrequesterlist.locale_id = dfrequesterlist.locale_id.astype(str)\r\n",
					"#     dfrequesterlist.organization_id = dfrequesterlist.organization_id.astype(str)\r\n",
					"#     dfrequesterlist.role_type = dfrequesterlist.role_type.astype(str)\r\n",
					"#     dfrequesterlist.custom_role_id = dfrequesterlist.custom_role_id.astype(str)\r\n",
					"#     dfrequesterlist.organization_id = dfrequesterlist.organization_id.astype(str)\r\n",
					"#     dfrequesterlist.default_group_id = dfrequesterlist.default_group_id.astype(str)\r\n",
					"\r\n",
					"\r\n",
					"#     from pyspark.sql.types import StructType, StructField, StringType\r\n",
					"#     schema = StructType([StructField(\"foo\", StringType(), True)])\r\n",
					"#     dfrequesterlist = dfrequesterlist.dropna(axis='columns', how='all') # Drops columns with all NA values\r\n",
					"\r\n",
					"#     srequesterlist = spark.createDataFrame(dfrequesterlist)\r\n",
					"#     spark.sql(f\"drop table if exists odw_harmonised_db.zendesk_requester;\")\r\n",
					"#     srequesterlist.write.format('delta').saveAsTable(\"odw_harmonised_db.zendesk_requester\")\r\n",
					"\r\n",
					"# requester_table(list_with_all_zendesk_dict)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# def collaborator_table(list_with_all_zendesk_dict):\r\n",
					"#     collaboratorlist=[]\r\n",
					"#     for collaboratorcolumn in list_with_all_zendesk_dict:\r\n",
					"#         collaboratorDict= collaboratorcolumn['collaborator']\r\n",
					"#         if collaboratorDict is None:\r\n",
					"#             continue\r\n",
					"#         else:\r\n",
					"#             for dictionary in collaboratorDict:\r\n",
					"#                 dictionary['ticket_id']=collaboratorcolumn['id']\r\n",
					"#                 collaboratorlist.append(dictionary)\r\n",
					"            \r\n",
					"    \r\n",
					"#     dfcollaboratorlist=pd.DataFrame(collaboratorlist)\r\n",
					"#     dfcollaboratorlist.id = dfcollaboratorlist.id.astype(str)\r\n",
					"#     dfcollaboratorlist.locale_id = dfcollaboratorlist.locale_id.astype(str)\r\n",
					"#     dfcollaboratorlist.organization_id = dfcollaboratorlist.organization_id.astype(str)\r\n",
					"#     dfcollaboratorlist.role_type = dfcollaboratorlist.role_type.astype(str)\r\n",
					"#     dfcollaboratorlist.custom_role_id = dfcollaboratorlist.custom_role_id.astype(str)\r\n",
					"#     dfcollaboratorlist.organization_id = dfcollaboratorlist.organization_id.astype(str)\r\n",
					"#     dfcollaboratorlist.default_group_id = dfcollaboratorlist.default_group_id.astype(str)\r\n",
					"\r\n",
					"\r\n",
					"#     from pyspark.sql.types import StructType, StructField, StringType\r\n",
					"#     schema = StructType([StructField(\"foo\", StringType(), True)])\r\n",
					"#     dfcollaboratorlist = dfcollaboratorlist.dropna(axis='columns', how='all') # Drops columns with all NA values\r\n",
					"\r\n",
					"#     scollaboratorlist = spark.createDataFrame(dfcollaboratorlist)\r\n",
					"#     spark.sql(f\"drop table if exists odw_harmonised_db.zendesk_collaborator;\")\r\n",
					"#     scollaboratorlist.write.format('delta').saveAsTable(\"odw_harmonised_db.zendesk_collaborator\")\r\n",
					"\r\n",
					"# collaborator_table(list_with_all_zendesk_dict)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"ms_comments": [
						{
							"threadId": "16e9b02f-2b18-4cbe-8e09-52277ee4425e",
							"text": "Add ingested_datetime",
							"status": "active",
							"user": {
								"name": "Pakwashee, Abdullah",
								"idType": "aad"
							},
							"createdDateUTC": 1685110739841,
							"modifiedDateUTC": 1685110739841,
							"replies": []
						},
						{
							"threadId": "92bcd456-8b3a-4d69-85ba-69d3624637e3",
							"text": "Change 7 tables to harmonised, delete the tables you don't need from standardised",
							"status": "resolved",
							"user": {
								"name": "Pakwashee, Abdullah",
								"idType": "aad"
							},
							"createdDateUTC": 1685110888188,
							"modifiedDateUTC": 1685435903349,
							"replies": []
						}
					],
					"ms_comment_ranges": {
						"16e9b02f-2b18-4cbe-8e09-52277ee4425e": {
							"text": "Comments",
							"start": {
								"line": 1,
								"column": 10
							},
							"end": {
								"line": 1,
								"column": 18
							}
						},
						"92bcd456-8b3a-4d69-85ba-69d3624637e3": {
							"text": "Comments",
							"start": {
								"line": 1,
								"column": 10
							},
							"end": {
								"line": 1,
								"column": 18
							}
						}
					}
				},
				"source": [
					"#General Comments\r\n",
					"# spark.sql(f\"drop table if exists odw_standardised_db.zendesk_collaborator;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_standardised_db.zendesk_requester;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_standardised_db.zendesk_submitter;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_standardised_db.zendesk_assignee;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_standardised_db.zendesk_comments_attachments;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_standardised_db.zendesk_comments;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_standardised_db.zendesk_fields_id_value;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_standardised_db.zendesk_custom_fields_id_value;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_standardised_db.zendesk_shows;\")"
				],
				"execution_count": null
			}
		]
	}
}