{
	"name": "dart_api",
	"properties": {
		"folder": {
			"name": "odw-curated"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "harrisonPool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "f1c14273-dc2a-4c3e-8152-cd03aee423eb"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/harrisonPool",
				"name": "harrisonPool",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/harrisonPool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from delta.tables import DeltaTable"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Create DataFrame of required data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"df = spark.sql(\r\n",
					"    \"\"\"\r\n",
					"    SELECT\r\n",
					"    h.caseId\r\n",
					"    ,h.caseReference\r\n",
					"    ,h.caseStatus\r\n",
					"    ,h.caseType\r\n",
					"    ,h.caseProcedure\r\n",
					"    ,h.lpaCode\r\n",
					"    ,l.lpaName\r\n",
					"    ,h.allocationLevel\r\n",
					"    ,h.allocationBand\r\n",
					"    ,h.caseSpecialisms\r\n",
					"    ,h.caseSubmittedDate\r\n",
					"    ,h.caseCreatedDate\r\n",
					"    ,h.caseUpdatedDate\r\n",
					"    ,h.caseValidDate\r\n",
					"    ,h.caseValidationDate\r\n",
					"    ,h.caseValidationOutcome\r\n",
					"    ,h.caseValidationInvalidDetails\r\n",
					"    ,h.caseValidationIncompleteDetails\r\n",
					"    ,h.caseExtensionDate\r\n",
					"    ,h.caseStartedDate\r\n",
					"    ,h.casePublishedDate\r\n",
					"    ,h.linkedCaseStatus\r\n",
					"    ,h.leadCaseReference\r\n",
					"    ,h.caseWithdrawnDate\r\n",
					"    ,h.caseTransferredDate\r\n",
					"    ,h.transferredCaseClosedDate\r\n",
					"    ,h.caseDecisionOutcomeDate\r\n",
					"    ,h.caseDecisionPublishedDate\r\n",
					"    ,h.caseDecisionOutcome\r\n",
					"    ,h.caseCompletedDate\r\n",
					"    ,h.enforcementNotice\r\n",
					"    ,h.applicationReference\r\n",
					"    ,h.applicationDate\r\n",
					"    ,h.applicationDecision\r\n",
					"    ,h.applicationDecisionDate as lpaDecisionDate\r\n",
					"    ,h.caseSubmissionDueDate\r\n",
					"    ,h.siteAddressLine1\r\n",
					"    ,h.siteAddressLine2\r\n",
					"    ,h.siteAddressTown\r\n",
					"    ,h.siteAddressCounty\r\n",
					"    ,h.siteAddressPostcode\r\n",
					"    ,h.isCorrectAppealType\r\n",
					"    ,h.originalDevelopmentDescription\r\n",
					"    ,h.changedDevelopmentDescription\r\n",
					"    ,h.newConditionDetails\r\n",
					"    ,h.nearbyCaseReferences\r\n",
					"    ,h.neighbouringSiteAddresses\r\n",
					"    ,h.affectedListedBuildingNumbers\r\n",
					"    ,h.appellantCostsAppliedFor\r\n",
					"    ,h.lpaCostsAppliedFor\r\n",
					"    ,su.firstName + ' ' + su.LastName as appellantName\r\n",
					"    ,e.eventType as typeOfEvent\r\n",
					"    ,e.eventStartDateTime as startDateOfTheEvent\r\n",
					"    ,ent_ins.givenName + ' ' +  ent_ins.surname as inspectorName\r\n",
					"    ,ent_co.givenName + ' ' +  ent_co.surname as caseOfficerName\r\n",
					"    ,i.qualifications as inspectorQualifications\r\n",
					"FROM \r\n",
					"    odw_harmonised_db.sb_appeal_has h\r\n",
					"        left join odw_harmonised_db.pins_lpa l on h.lpaCode = l.pinsLpaCode and l.isActive = 'Y'\r\n",
					"        left join odw_harmonised_db.sb_service_user su on h.caseReference = su.caseReference and su.serviceUserType = 'Appellant' and su.isActive = 'Y'\r\n",
					"        left join odw_harmonised_db.sb_appeal_event e on h.caseReference = e.caseReference and e.isActive = 'Y'\r\n",
					"        left join odw_harmonised_db.entraid ent_ins on h.inspectorId = ent_ins.id and ent_ins.isActive = 'Y'\r\n",
					"        left join odw_harmonised_db.entraid ent_co on h.caseOfficerId = ent_co.id and ent_co.isActive = 'Y'\r\n",
					"        left join odw_harmonised_db.pins_inspector i on ent_ins.userPrincipalName = i.email and i.isActive = 'Y'\r\n",
					"WHERE\r\n",
					"    h.IsActive = 'Y'\r\n",
					"\tand\r\n",
					"\th.lpaCode <> 'Q9999'\r\n",
					"    \"\"\"\r\n",
					"    )"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Write DataFrame to delta table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"table_name = \"odw_curated_db.dart_api\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Partition by `applicationReference` as this field only contains around 50 unique values and will be included in the search filters by users."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df.write.mode(\"overwrite\") \\\r\n",
					"    .partitionBy(\"applicationReference\") \\\r\n",
					"    .option(\"overwriteSchema\", \"true\") \\\r\n",
					"    .format(\"delta\") \\\r\n",
					"    .saveAsTable(table_name)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Optimise by adding `ZOrderBy` for the column `caseReference`. This column has too many values for partitioning to be useful but is one of the columns used in the query filters by users. This ensures the minimum number of files are read when querying for a `caseReference`."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"delta_table = DeltaTable.forName(spark, table_name)\r\n",
					"delta_table.optimize().executeZOrderBy(\"caseReference\")\r\n",
					"delta_table.optimize()"
				],
				"execution_count": null
			}
		]
	}
}