{
	"name": "find_unused_files",
	"properties": {
		"folder": {
			"name": "odw-harmonised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "efe55403-3f0c-4b4c-9de6-dd6a50f0f894"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql import SparkSession\n",
					"\n",
					"# Configuration - Different thresholds for different container types\n",
					"CONTAINER_THRESHOLDS = {\n",
					"    # Temporary data - aggressive cleanup\n",
					"    'temp-sap-hr-data': 30,\n",
					"    'odw-temp': 30,\n",
					"    \n",
					"    # Test environments - moderate cleanup\n",
					"    'test': 60,\n",
					"    'hbttestdbcontainer': 60,\n",
					"    'hbttestfilesystem': 60,\n",
					"    \n",
					"    # Log files - longer retention\n",
					"    'backup-logs': 90,\n",
					"    'logging': 90,\n",
					"    \n",
					"    # Production data - conservative cleanup\n",
					"    'odw-raw': 180,\n",
					"    'odw-curated': 180,\n",
					"    'odw-standardised': 180,\n",
					"    'odw-harmonised': 180,\n",
					"    'odw-config': 180,\n",
					"    'odw-config-db': 180,\n",
					"    'data-lake-config': 180,\n",
					"    \n",
					"    # Archive and backup - very conservative\n",
					"    's51-advice-backup': 365,\n",
					"    'saphrsdata-to-odw': 180,\n",
					"    'mipins-database': 365,\n",
					"    \n",
					"    # Other containers - default\n",
					"    'synapse': 120,\n",
					"    'dart': 120,\n",
					"    'ims-poc': 120,\n",
					"    'odw-standardised-delta': 120,\n",
					"    'insights-logs-builtinsqlreqsended': 90\n",
					"}\n",
					"\n",
					"def get_storage_account_info():\n",
					"    storage_account_url = mssparkutils.notebook.run('/utils/py_utils_get_storage_account').strip()\n",
					"    print(f\"Raw Storage Account URL: {storage_account_url}\")\n",
					"    \n",
					"    if '://' in storage_account_url:\n",
					"        domain_part = storage_account_url.split('://')[1]\n",
					"    else:\n",
					"        domain_part = storage_account_url\n",
					"    \n",
					"    domain_part = domain_part.rstrip('/')\n",
					"    storage_account_name = domain_part.split('.')[0]\n",
					"    \n",
					"    print(f\"Extracted Storage Account Name: {storage_account_name}\")\n",
					"    return storage_account_name, storage_account_url\n",
					"\n",
					"def analyze_containers_with_smart_thresholds(storage_account_name):\n",
					"    containers = [\n",
					"        'odw-temp', 'test', 'backup-logs', 'logging', \n",
					"        'odw-raw', 'odw-curated', 'temp-sap-hr-data',\n",
					"        'odw-standardised', 'odw-harmonised', 'synapse',\n",
					"        'dart', 'ims-poc', 's51-advice-backup', 'odw-config',\n",
					"        'data-lake-config', 'mipins-database', 'odw-config-db',\n",
					"        'odw-standardised-delta', 'saphrsdata-to-odw',\n",
					"        'hbttestdbcontainer', 'hbttestfilesystem',\n",
					"        'insights-logs-builtinsqlreqsended'\n",
					"    ]\n",
					"    \n",
					"    all_files = []\n",
					"    container_stats = {}\n",
					"    \n",
					"    print(f\"\\nAnalyzing files with container-specific thresholds:\")\n",
					"    print(\"=\" * 80)\n",
					"    \n",
					"    for container in containers:\n",
					"        container_threshold = CONTAINER_THRESHOLDS.get(container, 120)\n",
					"        cutoff_date = datetime.now() - timedelta(days=container_threshold)\n",
					"        \n",
					"        try:\n",
					"            path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\"\n",
					"            files = mssparkutils.fs.ls(path)\n",
					"            \n",
					"            container_files = 0\n",
					"            container_unused = 0\n",
					"            container_size = 0\n",
					"            \n",
					"            print(f\"\\nChecking: {container} (>{container_threshold} days, before {cutoff_date.strftime('%Y-%m-%d')})\")\n",
					"            \n",
					"            for file in files:\n",
					"                if not file.isDir and hasattr(file, 'modifyTime') and file.modifyTime:\n",
					"                    container_files += 1\n",
					"                    try:\n",
					"                        mod_time = pd.to_datetime(file.modifyTime, errors='coerce')\n",
					"                        if pd.isna(mod_time) or mod_time.year < 2000:\n",
					"                            print(f\"  Skipping {file.name} - invalid timestamp\")\n",
					"                            continue\n",
					"                        \n",
					"                        mod_time = mod_time.to_pydatetime().replace(tzinfo=None)\n",
					"                        file_size_mb = round(file.size / (1024*1024), 2) if hasattr(file, 'size') else 0\n",
					"                        days_old = (datetime.now() - mod_time).days\n",
					"                        \n",
					"                        if mod_time < cutoff_date:\n",
					"                            container_unused += 1\n",
					"                            container_size += file_size_mb\n",
					"                            \n",
					"                            file_ext = file.name.split('.')[-1].lower() if '.' in file.name else 'no_extension'\n",
					"                            \n",
					"                            all_files.append({\n",
					"                                'container': container,\n",
					"                                'file_name': file.name,\n",
					"                                'size_mb': file_size_mb,\n",
					"                                'days_old': days_old,\n",
					"                                'threshold_used': container_threshold,\n",
					"                                'last_modified': mod_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
					"                                'file_extension': file_ext\n",
					"                            })\n",
					"                    except Exception as e:\n",
					"                        continue\n",
					"            \n",
					"            container_stats[container] = {\n",
					"                'total_files': container_files,\n",
					"                'unused_files': container_unused,\n",
					"                'unused_size_mb': container_size,\n",
					"                'threshold_days': container_threshold\n",
					"            }\n",
					"            \n",
					"            if container_unused > 0:\n",
					"                print(f\"  ✅ Found {container_unused} unused files ({container_size:.1f} MB)\")\n",
					"            else:\n",
					"                print(f\"  ✅ No unused files found\")\n",
					"                \n",
					"        except Exception as e:\n",
					"            print(f\"❌ Error with {container}: {str(e)}\")\n",
					"            container_stats[container] = {\n",
					"                'total_files': 0, 'unused_files': 0, 'unused_size_mb': 0, \n",
					"                'threshold_days': container_threshold\n",
					"            }\n",
					"    \n",
					"    return all_files, container_stats\n",
					"\n",
					"def generate_report(unused_files, container_stats):\n",
					"    if not unused_files:\n",
					"        print(f\"\\n🎉 EXCELLENT! No unused files found with smart thresholds!\")\n",
					"        print(f\"\\nYour storage is exceptionally well-maintained!\")\n",
					"        return\n",
					"    \n",
					"    df = pd.DataFrame(unused_files)\n",
					"    total_files = len(df)\n",
					"    total_size_mb = df['size_mb'].sum()\n",
					"    \n",
					"    print(f\"\\n🧹 CLEANUP ANALYSIS REPORT\")\n",
					"    print(\"=\" * 60)\n",
					"    print(f\"📊 Total unused files: {total_files}\")\n",
					"    print(f\"💾 Total size: {total_size_mb:.1f} MB\")\n",
					"    \n",
					"    # By container\n",
					"    print(f\"\\n📁 BY CONTAINER:\")\n",
					"    container_summary = df.groupby('container').agg({\n",
					"        'file_name': 'count',\n",
					"        'size_mb': 'sum',\n",
					"        'threshold_used': 'first'\n",
					"    }).rename(columns={'file_name': 'file_count'})\n",
					"    \n",
					"    for container, row in container_summary.iterrows():\n",
					"        threshold = int(row['threshold_used'])\n",
					"        if container in ['test', 'temp-sap-hr-data', 'odw-temp']:\n",
					"            safety = \"🟢 SAFE\"\n",
					"        elif container in ['backup-logs', 'logging']:\n",
					"            safety = \"🟡 REVIEW\"\n",
					"        else:\n",
					"            safety = \"🟠 CAREFUL\"\n",
					"        \n",
					"        print(f\"  {safety} {container}: {row['file_count']} files, {row['size_mb']:.1f} MB (>{threshold} days)\")\n",
					"    \n",
					"    # Top files\n",
					"    print(f\"\\n🔍 LARGEST FILES:\")\n",
					"    top_files = df.nlargest(5, 'size_mb')\n",
					"    for i, (_, file) in enumerate(top_files.iterrows(), 1):\n",
					"        print(f\"{i}. {file['file_name']}: {file['size_mb']:.1f} MB ({file['days_old']} days)\")\n",
					"\n",
					"def main():\n",
					"    print(\"🔍 AZURE STORAGE CLEANUP WITH SMART THRESHOLDS\")\n",
					"    print(\"=\"*60)\n",
					"    \n",
					"    try:\n",
					"        storage_account_name, _ = get_storage_account_info()\n",
					"        unused_files, container_stats = analyze_containers_with_smart_thresholds(storage_account_name)\n",
					"        generate_report(unused_files, container_stats)\n",
					"        print(f\"\\n✅ Analysis complete!\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"❌ Error: {str(e)}\")\n",
					"\n",
					"if __name__ == \"__main__\":\n",
					"    main()"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql import SparkSession\n",
					"import json\n",
					"import re\n",
					"\n",
					"# Configuration - Different thresholds for different container types\n",
					"CONTAINER_THRESHOLDS = {\n",
					"    # Temporary data - aggressive cleanup\n",
					"    'temp-sap-hr-data': 30,\n",
					"    'odw-temp': 30,\n",
					"    \n",
					"    # Test environments - moderate cleanup\n",
					"    'test': 60,\n",
					"    'hbttestdbcontainer': 60,\n",
					"    'hbttestfilesystem': 60,\n",
					"    \n",
					"    # Log files - longer retention\n",
					"    'backup-logs': 90,\n",
					"    'logging': 90,\n",
					"    'insights-logs-builtinsqlreqsended': 90,\n",
					"    \n",
					"    # Production data - conservative cleanup\n",
					"    'odw-raw': 180,\n",
					"    'odw-curated': 180,\n",
					"    'odw-standardised': 180,\n",
					"    'odw-harmonised': 180,\n",
					"    'odw-config': 180,\n",
					"    'odw-config-db': 180,\n",
					"    'data-lake-config': 180,\n",
					"    \n",
					"    # Archive and backup - very conservative\n",
					"    's51-advice-backup': 365,\n",
					"    'saphrsdata-to-odw': 180,\n",
					"    'mipins-database': 365,\n",
					"    \n",
					"    # Other containers - default\n",
					"    'synapse': 120,\n",
					"    'dart': 120,\n",
					"    'ims-poc': 120,\n",
					"    'odw-standardised-delta': 120\n",
					"}\n",
					"\n",
					"def get_storage_account_info():\n",
					"    \"\"\"Extract storage account information\"\"\"\n",
					"    storage_account_url = mssparkutils.notebook.run('/utils/py_utils_get_storage_account').strip()\n",
					"    print(f\"Raw Storage Account URL: {storage_account_url}\")\n",
					"    \n",
					"    if '://' in storage_account_url:\n",
					"        domain_part = storage_account_url.split('://')[1]\n",
					"    else:\n",
					"        domain_part = storage_account_url\n",
					"    \n",
					"    domain_part = domain_part.rstrip('/')\n",
					"    storage_account_name = domain_part.split('.')[0]\n",
					"    \n",
					"    print(f\"Extracted Storage Account Name: {storage_account_name}\")\n",
					"    return storage_account_name, storage_account_url\n",
					"\n",
					"def parse_timestamp_safely(timestamp_value):\n",
					"    \"\"\"Parse various timestamp formats commonly found in Azure storage\"\"\"\n",
					"    try:\n",
					"        # If it's already a datetime, use it\n",
					"        if hasattr(timestamp_value, 'year'):\n",
					"            return timestamp_value\n",
					"        \n",
					"        # Convert to string for processing\n",
					"        ts_str = str(timestamp_value)\n",
					"        \n",
					"        # Check if it looks like epoch milliseconds (13 digits)\n",
					"        if ts_str.isdigit() and len(ts_str) == 13:\n",
					"            epoch_seconds = int(ts_str) / 1000\n",
					"            dt = datetime.fromtimestamp(epoch_seconds)\n",
					"            # Sanity check - should be between 2020 and 2030\n",
					"            if 2020 <= dt.year <= 2030:\n",
					"                return dt\n",
					"        \n",
					"        # Check if it looks like epoch seconds (10 digits)\n",
					"        elif ts_str.isdigit() and len(ts_str) == 10:\n",
					"            dt = datetime.fromtimestamp(int(ts_str))\n",
					"            if 2020 <= dt.year <= 2030:\n",
					"                return dt\n",
					"        \n",
					"        # Try pandas parsing as fallback\n",
					"        parsed_dt = pd.to_datetime(timestamp_value, errors='coerce')\n",
					"        if not pd.isna(parsed_dt):\n",
					"            dt = parsed_dt.to_pydatetime().replace(tzinfo=None)\n",
					"            if 2020 <= dt.year <= 2030:\n",
					"                return dt\n",
					"        \n",
					"        return None\n",
					"        \n",
					"    except Exception as e:\n",
					"        return None\n",
					"\n",
					"def detect_comprehensive_dependencies(storage_account_name):\n",
					"    \"\"\"Comprehensive dependency detection including Synapse pipelines\"\"\"\n",
					"    print(\"🔍 Detecting comprehensive pipeline dependencies...\")\n",
					"    \n",
					"    referenced_paths = set()\n",
					"    pipeline_patterns = set()\n",
					"    synapse_dependencies = set()\n",
					"    notebook_dependencies = set()\n",
					"    \n",
					"    try:\n",
					"        spark = SparkSession.builder.getOrCreate()\n",
					"        \n",
					"        # 1. SPARK CATALOG ANALYSIS\n",
					"        print(\"  📊 Analyzing Spark catalog...\")\n",
					"        try:\n",
					"            databases = spark.sql(\"SHOW DATABASES\").collect()\n",
					"            catalog_count = 0\n",
					"            for db in databases:\n",
					"                db_name = db[0]\n",
					"                try:\n",
					"                    tables = spark.sql(f\"SHOW TABLES IN {db_name}\").collect()\n",
					"                    for table in tables:\n",
					"                        table_name = table[1]\n",
					"                        try:\n",
					"                            table_desc = spark.sql(f\"DESCRIBE EXTENDED {db_name}.{table_name}\").collect()\n",
					"                            for row in table_desc:\n",
					"                                if row[0] and 'Location' in str(row[0]):\n",
					"                                    location = str(row[1])\n",
					"                                    if storage_account_name in location:\n",
					"                                        referenced_paths.add(location)\n",
					"                                        catalog_count += 1\n",
					"                        except:\n",
					"                            continue\n",
					"                except:\n",
					"                    continue\n",
					"            print(f\"    ✅ Found {catalog_count} table references in Spark catalog\")\n",
					"        except Exception as catalog_error:\n",
					"            print(f\"    ⚠️ Could not check catalog: {str(catalog_error)}\")\n",
					"        \n",
					"        # 2. SYNAPSE PIPELINE ANALYSIS\n",
					"        print(\"  🔧 Analyzing Synapse pipeline configurations...\")\n",
					"        synapse_refs = analyze_synapse_pipelines(storage_account_name)\n",
					"        referenced_paths.update(synapse_refs)\n",
					"        print(f\"    ✅ Found {len(synapse_refs)} Synapse pipeline references\")\n",
					"        \n",
					"        # 3. NOTEBOOK ANALYSIS\n",
					"        print(\"  📓 Analyzing notebook dependencies...\")\n",
					"        notebook_refs = analyze_notebook_dependencies(storage_account_name)\n",
					"        referenced_paths.update(notebook_refs)\n",
					"        print(f\"    ✅ Found {len(notebook_refs)} notebook references\")\n",
					"        \n",
					"        # 4. CONFIG FILE ANALYSIS\n",
					"        print(\"  ⚙️ Analyzing configuration files...\")\n",
					"        config_refs = analyze_config_files(storage_account_name)\n",
					"        referenced_paths.update(config_refs)\n",
					"        print(f\"    ✅ Found {len(config_refs)} configuration references\")\n",
					"        \n",
					"        # 5. STANDARD PIPELINE PATTERNS\n",
					"        standard_patterns = [\n",
					"            'pipeline', 'etl', 'config', 'schema', 'metadata', 'checkpoint', \n",
					"            'watermark', 'state', 'offset', 'manifest', 'success', '_started', \n",
					"            '_committed', '_delta_log', '_spark_metadata', '.checkpoints',\n",
					"            'orchestration', 'workflow', 'jobconfig', 'dataflow'\n",
					"        ]\n",
					"        pipeline_patterns.update(standard_patterns)\n",
					"        \n",
					"        print(f\"  🎯 Total dependencies detected: {len(referenced_paths)}\")\n",
					"        print(f\"  📋 Pipeline patterns: {len(pipeline_patterns)}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"  ❌ Error in dependency detection: {str(e)}\")\n",
					"    \n",
					"    return referenced_paths, list(pipeline_patterns)\n",
					"\n",
					"def analyze_synapse_pipelines(storage_account_name):\n",
					"    \"\"\"Analyze Synapse pipelines for storage dependencies\"\"\"\n",
					"    pipeline_refs = set()\n",
					"    \n",
					"    try:\n",
					"        # Try to access pipeline metadata through Synapse APIs\n",
					"        # Note: This requires appropriate permissions\n",
					"        \n",
					"        # Method 1: Check for pipeline configuration files\n",
					"        config_containers = ['odw-config', 'data-lake-config', 'synapse']\n",
					"        \n",
					"        for container in config_containers:\n",
					"            try:\n",
					"                container_path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\"\n",
					"                files = mssparkutils.fs.ls(container_path)\n",
					"                \n",
					"                for file in files:\n",
					"                    if not file.isDir and file.name.lower().endswith(('.json', '.yml', '.yaml')):\n",
					"                        try:\n",
					"                            # Read configuration file\n",
					"                            file_content = mssparkutils.fs.head(f\"{container_path}{file.name}\", 10000)\n",
					"                            \n",
					"                            # Look for ABFSS paths in config\n",
					"                            abfss_pattern = r'abfss://[^@]+@' + storage_account_name + r'\\.dfs\\.core\\.windows\\.net/[^\\s\"\\']*'\n",
					"                            matches = re.findall(abfss_pattern, file_content)\n",
					"                            pipeline_refs.update(matches)\n",
					"                            \n",
					"                            # Look for container references\n",
					"                            container_pattern = r'\"container\":\\s*\"([^\"]+)\"'\n",
					"                            container_matches = re.findall(container_pattern, file_content)\n",
					"                            for container_ref in container_matches:\n",
					"                                pipeline_refs.add(f\"abfss://{container_ref}@{storage_account_name}.dfs.core.windows.net/\")\n",
					"                                \n",
					"                        except Exception as file_error:\n",
					"                            continue\n",
					"                            \n",
					"            except Exception as container_error:\n",
					"                continue\n",
					"        \n",
					"        # Method 2: Check for common pipeline dataset patterns\n",
					"        dataset_patterns = [\n",
					"            f\"abfss://odw-raw@{storage_account_name}.dfs.core.windows.net/\",\n",
					"            f\"abfss://odw-standardised@{storage_account_name}.dfs.core.windows.net/\",\n",
					"            f\"abfss://odw-harmonised@{storage_account_name}.dfs.core.windows.net/\",\n",
					"            f\"abfss://odw-curated@{storage_account_name}.dfs.core.windows.net/\"\n",
					"        ]\n",
					"        pipeline_refs.update(dataset_patterns)\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"    ⚠️ Synapse pipeline analysis limited: {str(e)}\")\n",
					"    \n",
					"    return pipeline_refs\n",
					"\n",
					"def analyze_notebook_dependencies(storage_account_name):\n",
					"    \"\"\"Analyze notebooks for storage dependencies\"\"\"\n",
					"    notebook_refs = set()\n",
					"    \n",
					"    try:\n",
					"        # Check for notebook outputs and temporary files\n",
					"        notebook_containers = ['synapse', 'temp-sap-hr-data', 'odw-temp']\n",
					"        \n",
					"        for container in notebook_containers:\n",
					"            try:\n",
					"                container_path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\"\n",
					"                files = mssparkutils.fs.ls(container_path)\n",
					"                \n",
					"                for file in files:\n",
					"                    # Notebooks often create checkpoint and output files\n",
					"                    if any(pattern in file.name.lower() for pattern in \n",
					"                           ['checkpoint', 'output', 'result', 'temp', 'cache', '.ipynb']):\n",
					"                        notebook_refs.add(file.path)\n",
					"                        \n",
					"            except:\n",
					"                continue\n",
					"                \n",
					"    except Exception as e:\n",
					"        print(f\"    ⚠️ Notebook analysis limited: {str(e)}\")\n",
					"    \n",
					"    return notebook_refs\n",
					"\n",
					"def analyze_config_files(storage_account_name):\n",
					"    \"\"\"Analyze configuration files for storage references\"\"\"\n",
					"    config_refs = set()\n",
					"    \n",
					"    try:\n",
					"        # Look in configuration containers\n",
					"        config_containers = ['odw-config', 'data-lake-config', 'odw-config-db']\n",
					"        \n",
					"        for container in config_containers:\n",
					"            try:\n",
					"                container_path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\"\n",
					"                files = mssparkutils.fs.ls(container_path)\n",
					"                \n",
					"                for file in files:\n",
					"                    if not file.isDir and file.name.lower().endswith('.json'):\n",
					"                        try:\n",
					"                            # Read and parse JSON config\n",
					"                            file_content = mssparkutils.fs.head(f\"{container_path}{file.name}\", 5000)\n",
					"                            \n",
					"                            # Extract any storage paths\n",
					"                            path_pattern = f'{storage_account_name}\\\\.dfs\\\\.core\\\\.windows\\\\.net/([^\"\\\\s]*)'\n",
					"                            matches = re.findall(path_pattern, file_content)\n",
					"                            \n",
					"                            for match in matches:\n",
					"                                config_refs.add(f\"abfss://unknown@{storage_account_name}.dfs.core.windows.net/{match}\")\n",
					"                                \n",
					"                        except:\n",
					"                            continue\n",
					"            except:\n",
					"                continue\n",
					"                \n",
					"    except Exception as e:\n",
					"        print(f\"    ⚠️ Config file analysis limited: {str(e)}\")\n",
					"    \n",
					"    return config_refs\n",
					"\n",
					"def is_pipeline_dependency(file_path, file_name, referenced_paths, pipeline_patterns):\n",
					"    \"\"\"Enhanced dependency checking\"\"\"\n",
					"    file_lower = file_name.lower()\n",
					"    path_lower = file_path.lower()\n",
					"    \n",
					"    # 1. Check against discovered pipeline references\n",
					"    for ref_path in referenced_paths:\n",
					"        if ref_path.lower() in path_lower:\n",
					"            return True, f\"Referenced in pipeline/table: {ref_path[:50]}...\"\n",
					"    \n",
					"    # 2. Check for pipeline patterns in filename\n",
					"    for pattern in pipeline_patterns:\n",
					"        if pattern in file_lower:\n",
					"            return True, f\"Contains pipeline pattern: {pattern}\"\n",
					"    \n",
					"    # 3. Check for system directories\n",
					"    system_dirs = ['_delta_log', '_spark_metadata', '.checkpoints', '_checkpoints', \n",
					"                   '_SUCCESS', '_committed', '_started']\n",
					"    for sys_dir in system_dirs:\n",
					"        if sys_dir in path_lower:\n",
					"            return True, f\"System directory: {sys_dir}\"\n",
					"    \n",
					"    # 4. Check for configuration files\n",
					"    config_extensions = ['json', 'xml', 'yaml', 'yml', 'conf', 'properties', 'cfg']\n",
					"    if any(file_lower.endswith(f'.{ext}') for ext in config_extensions):\n",
					"        # Additional check for actual config content\n",
					"        config_keywords = ['pipeline', 'config', 'schema', 'metadata', 'orchestration']\n",
					"        if any(keyword in file_lower for keyword in config_keywords):\n",
					"            return True, \"Configuration file\"\n",
					"    \n",
					"    # 5. Check for recently accessed files (likely active)\n",
					"    # Files modified within last 7 days are probably active\n",
					"    try:\n",
					"        # This would need actual last access time, which isn't always available\n",
					"        pass\n",
					"    except:\n",
					"        pass\n",
					"    \n",
					"    return False, None\n",
					"\n",
					"def analyze_containers_comprehensive(storage_account_name):\n",
					"    \"\"\"Comprehensive analysis with all dependency detection methods\"\"\"\n",
					"    containers = [\n",
					"        'odw-temp', 'test', 'backup-logs', 'logging', \n",
					"        'odw-raw', 'odw-curated', 'temp-sap-hr-data',\n",
					"        'odw-standardised', 'odw-harmonised', 'synapse',\n",
					"        'dart', 'ims-poc', 's51-advice-backup', 'odw-config',\n",
					"        'data-lake-config', 'mipins-database', 'odw-config-db',\n",
					"        'odw-standardised-delta', 'hbttestdbcontainer', 'hbttestfilesystem',\n",
					"        'insights-logs-builtinsqlreqsended'\n",
					"    ]\n",
					"    \n",
					"    # Comprehensive dependency detection\n",
					"    referenced_paths, pipeline_patterns = detect_comprehensive_dependencies(storage_account_name)\n",
					"    \n",
					"    all_files = []\n",
					"    container_stats = {}\n",
					"    \n",
					"    print(f\"\\n📊 COMPREHENSIVE FILE ANALYSIS\")\n",
					"    print(\"=\" * 80)\n",
					"    \n",
					"    for container in containers:\n",
					"        container_threshold = CONTAINER_THRESHOLDS.get(container, 120)\n",
					"        cutoff_date = datetime.now() - timedelta(days=container_threshold)\n",
					"        \n",
					"        try:\n",
					"            path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\"\n",
					"            files = mssparkutils.fs.ls(path)\n",
					"            \n",
					"            container_files = 0\n",
					"            container_unused = 0\n",
					"            container_dependencies = 0\n",
					"            container_invalid_timestamps = 0\n",
					"            container_size = 0\n",
					"            \n",
					"            print(f\"\\n📦 {container} (>{container_threshold} days, before {cutoff_date.strftime('%Y-%m-%d')})\")\n",
					"            print(f\"    Total items: {len(files)}\")\n",
					"            \n",
					"            for file in files:\n",
					"                if not file.isDir and hasattr(file, 'modifyTime') and file.modifyTime:\n",
					"                    container_files += 1\n",
					"                    \n",
					"                    # Enhanced timestamp parsing\n",
					"                    mod_time = parse_timestamp_safely(file.modifyTime)\n",
					"                    \n",
					"                    if mod_time is None:\n",
					"                        container_invalid_timestamps += 1\n",
					"                        print(f\"    ⚠️ Invalid timestamp: {file.name} ({file.modifyTime})\")\n",
					"                        continue\n",
					"                    \n",
					"                    file_size_mb = round(file.size / (1024*1024), 2) if hasattr(file, 'size') else 0\n",
					"                    days_old = (datetime.now() - mod_time).days\n",
					"                    \n",
					"                    # Check if file is a pipeline dependency\n",
					"                    is_dependency, dependency_reason = is_pipeline_dependency(\n",
					"                        file.path, file.name, referenced_paths, pipeline_patterns\n",
					"                    )\n",
					"                    \n",
					"                    if is_dependency:\n",
					"                        container_dependencies += 1\n",
					"                        if days_old > container_threshold:\n",
					"                            print(f\"    🔗 Protected dependency: {file.name} ({days_old} days) - {dependency_reason}\")\n",
					"                    \n",
					"                    elif mod_time < cutoff_date:\n",
					"                        container_unused += 1\n",
					"                        container_size += file_size_mb\n",
					"                        \n",
					"                        file_ext = file.name.split('.')[-1].lower() if '.' in file.name else 'no_extension'\n",
					"                        \n",
					"                        all_files.append({\n",
					"                            'container': container,\n",
					"                            'file_name': file.name,\n",
					"                            'file_path': file.path,\n",
					"                            'size_mb': file_size_mb,\n",
					"                            'size_bytes': file.size if hasattr(file, 'size') else 0,\n",
					"                            'days_old': days_old,\n",
					"                            'threshold_used': container_threshold,\n",
					"                            'last_modified': mod_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
					"                            'file_extension': file_ext,\n",
					"                            'is_dependency': is_dependency,\n",
					"                            'dependency_reason': dependency_reason or 'None'\n",
					"                        })\n",
					"            \n",
					"            container_stats[container] = {\n",
					"                'total_files': container_files,\n",
					"                'unused_files': container_unused,\n",
					"                'unused_size_mb': container_size,\n",
					"                'dependencies': container_dependencies,\n",
					"                'invalid_timestamps': container_invalid_timestamps,\n",
					"                'threshold_days': container_threshold\n",
					"            }\n",
					"            \n",
					"            # Summary for container\n",
					"            status_parts = []\n",
					"            if container_unused > 0:\n",
					"                status_parts.append(f\"💥 {container_unused} unused files ({container_size:.1f} MB)\")\n",
					"            if container_dependencies > 0:\n",
					"                status_parts.append(f\"🔗 {container_dependencies} protected\")\n",
					"            if container_invalid_timestamps > 0:\n",
					"                status_parts.append(f\"⚠️ {container_invalid_timestamps} invalid timestamps\")\n",
					"            \n",
					"            if status_parts:\n",
					"                print(f\"    \" + \" | \".join(status_parts))\n",
					"            else:\n",
					"                print(f\"    ✅ Clean - no unused files found\")\n",
					"                \n",
					"        except Exception as e:\n",
					"            print(f\"❌ Error with {container}: {str(e)}\")\n",
					"            container_stats[container] = {\n",
					"                'total_files': 0, 'unused_files': 0, 'unused_size_mb': 0, \n",
					"                'dependencies': 0, 'invalid_timestamps': 0, 'threshold_days': container_threshold\n",
					"            }\n",
					"    \n",
					"    return all_files, container_stats, referenced_paths\n",
					"\n",
					"def generate_comprehensive_report(unused_files, container_stats, referenced_paths):\n",
					"    \"\"\"Generate detailed analysis report\"\"\"\n",
					"    \n",
					"    print(f\"\\n\" + \"=\"*80)\n",
					"    print(f\"📋 COMPREHENSIVE STORAGE ANALYSIS REPORT\")\n",
					"    print(f\"=\"*80)\n",
					"    print(f\"📅 Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
					"    print(f\"🔗 Dependencies detected: {len(referenced_paths)}\")\n",
					"    \n",
					"    # Overall statistics\n",
					"    total_containers = len(container_stats)\n",
					"    containers_with_unused = len([c for c in container_stats.values() if c['unused_files'] > 0])\n",
					"    total_dependencies = sum(c['dependencies'] for c in container_stats.values())\n",
					"    total_invalid_timestamps = sum(c['invalid_timestamps'] for c in container_stats.values())\n",
					"    \n",
					"    print(f\"\\n📊 OVERVIEW:\")\n",
					"    print(f\"  📦 Containers analyzed: {total_containers}\")\n",
					"    print(f\"  🔗 Total dependencies protected: {total_dependencies}\")\n",
					"    print(f\"  ⚠️ Files with timestamp issues: {total_invalid_timestamps}\")\n",
					"    print(f\"  💥 Containers with unused files: {containers_with_unused}\")\n",
					"    \n",
					"    if not unused_files:\n",
					"        print(f\"\\n🎉 EXCELLENT! No unused files found!\")\n",
					"        print(f\"\\n✨ Your storage is exceptionally well-maintained:\")\n",
					"        print(f\"  ✅ All files are actively used or protected by dependencies\")\n",
					"        print(f\"  ✅ Automated cleanup processes are working effectively\")\n",
					"        print(f\"  ✅ Data governance policies are properly implemented\")\n",
					"        \n",
					"        # Show threshold effectiveness\n",
					"        print(f\"\\n📋 THRESHOLD SUMMARY:\")\n",
					"        for container, stats in container_stats.items():\n",
					"            threshold = stats['threshold_days']\n",
					"            deps = stats['dependencies']\n",
					"            invalid = stats['invalid_timestamps']\n",
					"            \n",
					"            status_icon = \"🟢\"\n",
					"            if deps > 5:\n",
					"                status_icon = \"🔗\"\n",
					"            elif invalid > 0:\n",
					"                status_icon = \"⚠️\"\n",
					"            \n",
					"            print(f\"  {status_icon} {container}: {threshold} days | {deps} protected | {invalid} timestamp issues\")\n",
					"        \n",
					"        return\n",
					"    \n",
					"    # Detailed analysis for unused files\n",
					"    df = pd.DataFrame(unused_files)\n",
					"    total_files = len(df)\n",
					"    total_size_mb = df['size_mb'].sum()\n",
					"    total_size_gb = total_size_mb / 1024\n",
					"    \n",
					"    print(f\"\\n💥 UNUSED FILES FOUND:\")\n",
					"    print(f\"  🗂️ Total unused files: {total_files:,}\")\n",
					"    print(f\"  💾 Total size: {total_size_mb:.1f} MB ({total_size_gb:.2f} GB)\")\n",
					"    \n",
					"    # Container breakdown\n",
					"    print(f\"\\n📦 CONTAINER BREAKDOWN:\")\n",
					"    container_summary = df.groupby('container').agg({\n",
					"        'file_name': 'count',\n",
					"        'size_mb': 'sum',\n",
					"        'threshold_used': 'first'\n",
					"    }).rename(columns={'file_name': 'file_count'})\n",
					"    \n",
					"    for container, row in container_summary.iterrows():\n",
					"        deps = container_stats.get(container, {}).get('dependencies', 0)\n",
					"        threshold = int(row['threshold_used'])\n",
					"        \n",
					"        safety_level = \"🟢 SAFE\" if container in ['test', 'temp-sap-hr-data', 'odw-temp'] else \"🟡 REVIEW\"\n",
					"        \n",
					"        print(f\"  📦 {container}: {safety_level}\")\n",
					"        print(f\"     📊 {row['file_count']} unused files, {row['size_mb']:.1f} MB\")\n",
					"        print(f\"     ⏰ Threshold: {threshold} days\")\n",
					"        print(f\"     🔗 Protected dependencies: {deps}\")\n",
					"    \n",
					"    # Recommendations\n",
					"    print(f\"\\n🎯 RECOMMENDATIONS:\")\n",
					"    \n",
					"    safe_containers = ['test', 'temp-sap-hr-data', 'odw-temp']\n",
					"    safe_files = df[df['container'].isin(safe_containers)]\n",
					"    \n",
					"    if not safe_files.empty:\n",
					"        print(f\"  🟢 SAFE TO DELETE: {len(safe_files)} files ({safe_files['size_mb'].sum():.1f} MB)\")\n",
					"        for container in safe_containers:\n",
					"            container_files = safe_files[safe_files['container'] == container]\n",
					"            if not container_files.empty:\n",
					"                print(f\"     ✅ {container}: {len(container_files)} files\")\n",
					"    \n",
					"    review_files = df[~df['container'].isin(safe_containers)]\n",
					"    if not review_files.empty:\n",
					"        print(f\"  🟡 REVIEW FIRST: {len(review_files)} files ({review_files['size_mb'].sum():.1f} MB)\")\n",
					"    \n",
					"    # Next steps\n",
					"    print(f\"\\n🚀 NEXT STEPS:\")\n",
					"    print(f\"  1. 🔍 Review the {total_dependencies} protected dependencies\")\n",
					"    print(f\"  2. 🧹 Start cleanup with safe containers (test, temp)\")\n",
					"    print(f\"  3. 📋 Document any files you delete\")\n",
					"    print(f\"  4. 🔄 Set up automated cleanup policies\")\n",
					"    print(f\"  5. 📊 Run this analysis monthly\")\n",
					"\n",
					"def main():\n",
					"    \"\"\"Main comprehensive analysis function\"\"\"\n",
					"    print(\"🔍 COMPREHENSIVE AZURE STORAGE ANALYSIS\")\n",
					"    print(\"🔧 Enhanced Pipeline Dependency Detection\")\n",
					"    print(\"⚡ Smart Timestamp Parsing\")\n",
					"    print(\"📊 Container-Specific Thresholds\")\n",
					"    print(\"=\"*80)\n",
					"    \n",
					"    try:\n",
					"        # Get storage account info\n",
					"        storage_account_name, _ = get_storage_account_info()\n",
					"        \n",
					"        # Run comprehensive analysis\n",
					"        unused_files, container_stats, referenced_paths = analyze_containers_comprehensive(storage_account_name)\n",
					"        \n",
					"        # Generate detailed report\n",
					"        generate_comprehensive_report(unused_files, container_stats, referenced_paths)\n",
					"        \n",
					"        print(f\"\\n✅ Comprehensive analysis complete!\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"\\n❌ Analysis failed: {str(e)}\")\n",
					"        import traceback\n",
					"        traceback.print_exc()\n",
					"\n",
					"# Run the comprehensive analysis\n",
					"if __name__ == \"__main__\":\n",
					"    main()"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql import SparkSession\n",
					"import json\n",
					"import re\n",
					"\n",
					"# Configuration - Different thresholds for different container types\n",
					"CONTAINER_THRESHOLDS = {\n",
					"    # Temporary data - aggressive cleanup\n",
					"    'temp-sap-hr-data': 30,\n",
					"    'odw-temp': 30,\n",
					"    \n",
					"    # Test environments - moderate cleanup\n",
					"    'test': 60,\n",
					"    'hbttestdbcontainer': 60,\n",
					"    'hbttestfilesystem': 60,\n",
					"    \n",
					"    # Log files - longer retention\n",
					"    'backup-logs': 90,\n",
					"    'logging': 90,\n",
					"    'insights-logs-builtinsqlreqsended': 90,\n",
					"    \n",
					"    # Production data - conservative cleanup\n",
					"    'odw-raw': 180,\n",
					"    'odw-curated': 180,\n",
					"    'odw-standardised': 180,\n",
					"    'odw-harmonised': 180,\n",
					"    'odw-config': 180,\n",
					"    'odw-config-db': 180,\n",
					"    'data-lake-config': 180,\n",
					"    \n",
					"    # Archive and backup - very conservative\n",
					"    's51-advice-backup': 365,\n",
					"    'saphrsdata-to-odw': 180,\n",
					"    'mipins-database': 365,\n",
					"    \n",
					"    # Other containers - default\n",
					"    'synapse': 120,\n",
					"    'dart': 120,\n",
					"    'ims-poc': 120,\n",
					"    'odw-standardised-delta': 120\n",
					"}\n",
					"\n",
					"def get_storage_account_info():\n",
					"    \"\"\"Extract storage account information\"\"\"\n",
					"    storage_account_url = mssparkutils.notebook.run('/utils/py_utils_get_storage_account').strip()\n",
					"    print(f\"Raw Storage Account URL: {storage_account_url}\")\n",
					"    \n",
					"    if '://' in storage_account_url:\n",
					"        domain_part = storage_account_url.split('://')[1]\n",
					"    else:\n",
					"        domain_part = storage_account_url\n",
					"    \n",
					"    domain_part = domain_part.rstrip('/')\n",
					"    storage_account_name = domain_part.split('.')[0]\n",
					"    \n",
					"    print(f\"Extracted Storage Account Name: {storage_account_name}\")\n",
					"    return storage_account_name, storage_account_url\n",
					"\n",
					"def parse_timestamp_safely(timestamp_value):\n",
					"    \"\"\"Parse various timestamp formats commonly found in Azure storage\"\"\"\n",
					"    try:\n",
					"        # If it's already a datetime, use it\n",
					"        if hasattr(timestamp_value, 'year'):\n",
					"            return timestamp_value\n",
					"        \n",
					"        # Convert to string for processing\n",
					"        ts_str = str(timestamp_value)\n",
					"        \n",
					"        # Check if it looks like epoch milliseconds (13 digits)\n",
					"        if ts_str.isdigit() and len(ts_str) == 13:\n",
					"            epoch_seconds = int(ts_str) / 1000\n",
					"            dt = datetime.fromtimestamp(epoch_seconds)\n",
					"            # Sanity check - should be between 2020 and 2030\n",
					"            if 2020 <= dt.year <= 2030:\n",
					"                return dt\n",
					"        \n",
					"        # Check if it looks like epoch seconds (10 digits)\n",
					"        elif ts_str.isdigit() and len(ts_str) == 10:\n",
					"            dt = datetime.fromtimestamp(int(ts_str))\n",
					"            if 2020 <= dt.year <= 2030:\n",
					"                return dt\n",
					"        \n",
					"        # Try pandas parsing as fallback\n",
					"        parsed_dt = pd.to_datetime(timestamp_value, errors='coerce')\n",
					"        if not pd.isna(parsed_dt):\n",
					"            dt = parsed_dt.to_pydatetime().replace(tzinfo=None)\n",
					"            if 2020 <= dt.year <= 2030:\n",
					"                return dt\n",
					"        \n",
					"        return None\n",
					"        \n",
					"    except Exception as e:\n",
					"        return None\n",
					"\n",
					"def detect_comprehensive_dependencies(storage_account_name):\n",
					"    \"\"\"Comprehensive dependency detection including Synapse pipelines\"\"\"\n",
					"    print(\"🔍 Detecting comprehensive pipeline dependencies...\")\n",
					"    \n",
					"    referenced_paths = set()\n",
					"    pipeline_patterns = set()\n",
					"    synapse_dependencies = set()\n",
					"    notebook_dependencies = set()\n",
					"    \n",
					"    try:\n",
					"        spark = SparkSession.builder.getOrCreate()\n",
					"        \n",
					"        # 1. SPARK CATALOG ANALYSIS\n",
					"        print(\"   Analyzing Spark catalog...\")\n",
					"        try:\n",
					"            databases = spark.sql(\"SHOW DATABASES\").collect()\n",
					"            catalog_count = 0\n",
					"            for db in databases:\n",
					"                db_name = db[0]\n",
					"                try:\n",
					"                    tables = spark.sql(f\"SHOW TABLES IN {db_name}\").collect()\n",
					"                    for table in tables:\n",
					"                        table_name = table[1]\n",
					"                        try:\n",
					"                            table_desc = spark.sql(f\"DESCRIBE EXTENDED {db_name}.{table_name}\").collect()\n",
					"                            for row in table_desc:\n",
					"                                if row[0] and 'Location' in str(row[0]):\n",
					"                                    location = str(row[1])\n",
					"                                    if storage_account_name in location:\n",
					"                                        referenced_paths.add(location)\n",
					"                                        catalog_count += 1\n",
					"                        except:\n",
					"                            continue\n",
					"                except:\n",
					"                    continue\n",
					"            print(f\"     Found {catalog_count} table references in Spark catalog\")\n",
					"        except Exception as catalog_error:\n",
					"            print(f\"     Could not check catalog: {str(catalog_error)}\")\n",
					"        \n",
					"        # 2. SYNAPSE PIPELINE ANALYSIS\n",
					"        print(\"   Analyzing Synapse pipeline configurations...\")\n",
					"        synapse_refs = analyze_synapse_pipelines(storage_account_name)\n",
					"        referenced_paths.update(synapse_refs)\n",
					"        print(f\"     Found {len(synapse_refs)} Synapse pipeline references\")\n",
					"        \n",
					"        # 3. NOTEBOOK ANALYSIS\n",
					"        print(\"   Analyzing notebook dependencies...\")\n",
					"        notebook_refs = analyze_notebook_dependencies(storage_account_name)\n",
					"        referenced_paths.update(notebook_refs)\n",
					"        print(f\"     Found {len(notebook_refs)} notebook references\")\n",
					"        \n",
					"        # 4. CONFIG FILE ANALYSIS\n",
					"        print(\"  ⚙️ Analyzing configuration files...\")\n",
					"        config_refs = analyze_config_files(storage_account_name)\n",
					"        referenced_paths.update(config_refs)\n",
					"        print(f\"     Found {len(config_refs)} configuration references\")\n",
					"        \n",
					"        # 5. STANDARD PIPELINE PATTERNS\n",
					"        standard_patterns = [\n",
					"            'pipeline', 'etl', 'config', 'schema', 'metadata', 'checkpoint', \n",
					"            'watermark', 'state', 'offset', 'manifest', 'success', '_started', \n",
					"            '_committed', '_delta_log', '_spark_metadata', '.checkpoints',\n",
					"            'orchestration', 'workflow', 'jobconfig', 'dataflow'\n",
					"        ]\n",
					"        pipeline_patterns.update(standard_patterns)\n",
					"        \n",
					"        print(f\"   Total dependencies detected: {len(referenced_paths)}\")\n",
					"        print(f\"   Pipeline patterns: {len(pipeline_patterns)}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"   Error in dependency detection: {str(e)}\")\n",
					"    \n",
					"    return referenced_paths, list(pipeline_patterns)\n",
					"\n",
					"def analyze_synapse_pipelines(storage_account_name):\n",
					"    \"\"\"Analyze Synapse pipelines for storage dependencies\"\"\"\n",
					"    pipeline_refs = set()\n",
					"    \n",
					"    try:\n",
					"        # Try to access pipeline metadata through Synapse APIs\n",
					"        # Note: This requires appropriate permissions\n",
					"        \n",
					"        # Method 1: Check for pipeline configuration files\n",
					"        config_containers = ['odw-config', 'data-lake-config', 'synapse']\n",
					"        \n",
					"        for container in config_containers:\n",
					"            try:\n",
					"                container_path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\"\n",
					"                files = mssparkutils.fs.ls(container_path)\n",
					"                \n",
					"                for file in files:\n",
					"                    if not file.isDir and file.name.lower().endswith(('.json', '.yml', '.yaml')):\n",
					"                        try:\n",
					"                            # Read configuration file\n",
					"                            file_content = mssparkutils.fs.head(f\"{container_path}{file.name}\", 10000)\n",
					"                            \n",
					"                            # Look for ABFSS paths in config\n",
					"                            abfss_pattern = r'abfss://[^@]+@' + storage_account_name + r'\\.dfs\\.core\\.windows\\.net/[^\\s\"\\']*'\n",
					"                            matches = re.findall(abfss_pattern, file_content)\n",
					"                            pipeline_refs.update(matches)\n",
					"                            \n",
					"                            # Look for container references\n",
					"                            container_pattern = r'\"container\":\\s*\"([^\"]+)\"'\n",
					"                            container_matches = re.findall(container_pattern, file_content)\n",
					"                            for container_ref in container_matches:\n",
					"                                pipeline_refs.add(f\"abfss://{container_ref}@{storage_account_name}.dfs.core.windows.net/\")\n",
					"                                \n",
					"                        except Exception as file_error:\n",
					"                            continue\n",
					"                            \n",
					"            except Exception as container_error:\n",
					"                continue\n",
					"        \n",
					"        # Method 2: Check for common pipeline dataset patterns\n",
					"        dataset_patterns = [\n",
					"            f\"abfss://odw-raw@{storage_account_name}.dfs.core.windows.net/\",\n",
					"            f\"abfss://odw-standardised@{storage_account_name}.dfs.core.windows.net/\",\n",
					"            f\"abfss://odw-harmonised@{storage_account_name}.dfs.core.windows.net/\",\n",
					"            f\"abfss://odw-curated@{storage_account_name}.dfs.core.windows.net/\"\n",
					"        ]\n",
					"        pipeline_refs.update(dataset_patterns)\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"     Synapse pipeline analysis limited: {str(e)}\")\n",
					"    \n",
					"    return pipeline_refs\n",
					"\n",
					"def analyze_notebook_dependencies(storage_account_name):\n",
					"    \"\"\"Analyze notebooks for storage dependencies\"\"\"\n",
					"    notebook_refs = set()\n",
					"    \n",
					"    try:\n",
					"        # Check for notebook outputs and temporary files\n",
					"        notebook_containers = ['synapse', 'temp-sap-hr-data', 'odw-temp']\n",
					"        \n",
					"        for container in notebook_containers:\n",
					"            try:\n",
					"                container_path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\"\n",
					"                files = mssparkutils.fs.ls(container_path)\n",
					"                \n",
					"                for file in files:\n",
					"                    # Notebooks often create checkpoint and output files\n",
					"                    if any(pattern in file.name.lower() for pattern in \n",
					"                           ['checkpoint', 'output', 'result', 'temp', 'cache', '.ipynb']):\n",
					"                        notebook_refs.add(file.path)\n",
					"                        \n",
					"            except:\n",
					"                continue\n",
					"                \n",
					"    except Exception as e:\n",
					"        print(f\"    ⚠️ Notebook analysis limited: {str(e)}\")\n",
					"    \n",
					"    return notebook_refs\n",
					"\n",
					"def analyze_config_files(storage_account_name):\n",
					"    \"\"\"Analyze configuration files for storage references\"\"\"\n",
					"    config_refs = set()\n",
					"    \n",
					"    try:\n",
					"        # Look in configuration containers\n",
					"        config_containers = ['odw-config', 'data-lake-config', 'odw-config-db']\n",
					"        \n",
					"        for container in config_containers:\n",
					"            try:\n",
					"                container_path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\"\n",
					"                files = mssparkutils.fs.ls(container_path)\n",
					"                \n",
					"                for file in files:\n",
					"                    if not file.isDir and file.name.lower().endswith('.json'):\n",
					"                        try:\n",
					"                            # Read and parse JSON config\n",
					"                            file_content = mssparkutils.fs.head(f\"{container_path}{file.name}\", 5000)\n",
					"                            \n",
					"                            # Extract any storage paths\n",
					"                            path_pattern = f'{storage_account_name}\\\\.dfs\\\\.core\\\\.windows\\\\.net/([^\"\\\\s]*)'\n",
					"                            matches = re.findall(path_pattern, file_content)\n",
					"                            \n",
					"                            for match in matches:\n",
					"                                config_refs.add(f\"abfss://unknown@{storage_account_name}.dfs.core.windows.net/{match}\")\n",
					"                                \n",
					"                        except:\n",
					"                            continue\n",
					"            except:\n",
					"                continue\n",
					"                \n",
					"    except Exception as e:\n",
					"        print(f\"     Config file analysis limited: {str(e)}\")\n",
					"    \n",
					"    return config_refs\n",
					"\n",
					"def is_pipeline_dependency(file_path, file_name, referenced_paths, pipeline_patterns):\n",
					"    \"\"\"Enhanced dependency checking\"\"\"\n",
					"    file_lower = file_name.lower()\n",
					"    path_lower = file_path.lower()\n",
					"    \n",
					"    # 1. Check against discovered pipeline references\n",
					"    for ref_path in referenced_paths:\n",
					"        if ref_path.lower() in path_lower:\n",
					"            return True, f\"Referenced in pipeline/table: {ref_path[:50]}...\"\n",
					"    \n",
					"    # 2. Check for pipeline patterns in filename\n",
					"    for pattern in pipeline_patterns:\n",
					"        if pattern in file_lower:\n",
					"            return True, f\"Contains pipeline pattern: {pattern}\"\n",
					"    \n",
					"    # 3. Check for system directories\n",
					"    system_dirs = ['_delta_log', '_spark_metadata', '.checkpoints', '_checkpoints', \n",
					"                   '_SUCCESS', '_committed', '_started']\n",
					"    for sys_dir in system_dirs:\n",
					"        if sys_dir in path_lower:\n",
					"            return True, f\"System directory: {sys_dir}\"\n",
					"    \n",
					"    # 4. Check for configuration files\n",
					"    config_extensions = ['json', 'xml', 'yaml', 'yml', 'conf', 'properties', 'cfg']\n",
					"    if any(file_lower.endswith(f'.{ext}') for ext in config_extensions):\n",
					"        # Additional check for actual config content\n",
					"        config_keywords = ['pipeline', 'config', 'schema', 'metadata', 'orchestration']\n",
					"        if any(keyword in file_lower for keyword in config_keywords):\n",
					"            return True, \"Configuration file\"\n",
					"    \n",
					"    # 5. Check for recently accessed files (likely active)\n",
					"    # Files modified within last 7 days are probably active\n",
					"    try:\n",
					"        # This would need actual last access time, which isn't always available\n",
					"        pass\n",
					"    except:\n",
					"        pass\n",
					"    \n",
					"    return False, None\n",
					"\n",
					"def analyze_containers_comprehensive(storage_account_name):\n",
					"    \"\"\"Comprehensive analysis with all dependency detection methods\"\"\"\n",
					"    containers = [\n",
					"        'odw-temp', 'test', 'backup-logs', 'logging', \n",
					"        'odw-raw', 'odw-curated', 'temp-sap-hr-data',\n",
					"        'odw-standardised', 'odw-harmonised', 'synapse',\n",
					"        'dart', 'ims-poc', 's51-advice-backup', 'odw-config',\n",
					"        'data-lake-config', 'mipins-database', 'odw-config-db',\n",
					"        'odw-standardised-delta', 'hbttestdbcontainer', 'hbttestfilesystem',\n",
					"        'insights-logs-builtinsqlreqsended'\n",
					"    ]\n",
					"    \n",
					"    # Comprehensive dependency detection\n",
					"    referenced_paths, pipeline_patterns = detect_comprehensive_dependencies(storage_account_name)\n",
					"    \n",
					"    all_files = []\n",
					"    container_stats = {}\n",
					"    \n",
					"    print(f\"\\n📊 COMPREHENSIVE FILE ANALYSIS\")\n",
					"    print(\"=\" * 80)\n",
					"    \n",
					"    for container in containers:\n",
					"        container_threshold = CONTAINER_THRESHOLDS.get(container, 120)\n",
					"        cutoff_date = datetime.now() - timedelta(days=container_threshold)\n",
					"        \n",
					"        try:\n",
					"            path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\"\n",
					"            files = mssparkutils.fs.ls(path)\n",
					"            \n",
					"            container_files = 0\n",
					"            container_unused = 0\n",
					"            container_dependencies = 0\n",
					"            container_invalid_timestamps = 0\n",
					"            container_size = 0\n",
					"            \n",
					"            print(f\"\\n📦 {container} (>{container_threshold} days, before {cutoff_date.strftime('%Y-%m-%d')})\")\n",
					"            print(f\"    Total items: {len(files)}\")\n",
					"            \n",
					"            for file in files:\n",
					"                if not file.isDir and hasattr(file, 'modifyTime') and file.modifyTime:\n",
					"                    container_files += 1\n",
					"                    \n",
					"                    # Enhanced timestamp parsing\n",
					"                    mod_time = parse_timestamp_safely(file.modifyTime)\n",
					"                    \n",
					"                    if mod_time is None:\n",
					"                        container_invalid_timestamps += 1\n",
					"                        print(f\"     Invalid timestamp: {file.name} ({file.modifyTime})\")\n",
					"                        continue\n",
					"                    \n",
					"                    file_size_mb = round(file.size / (1024*1024), 2) if hasattr(file, 'size') else 0\n",
					"                    days_old = (datetime.now() - mod_time).days\n",
					"                    \n",
					"                    # Check if file is a pipeline dependency\n",
					"                    is_dependency, dependency_reason = is_pipeline_dependency(\n",
					"                        file.path, file.name, referenced_paths, pipeline_patterns\n",
					"                    )\n",
					"                    \n",
					"                    if is_dependency:\n",
					"                        container_dependencies += 1\n",
					"                        if days_old > container_threshold:\n",
					"                            print(f\"     Protected dependency: {file.name} ({days_old} days) - {dependency_reason}\")\n",
					"                    \n",
					"                    elif mod_time < cutoff_date:\n",
					"                        container_unused += 1\n",
					"                        container_size += file_size_mb\n",
					"                        \n",
					"                        file_ext = file.name.split('.')[-1].lower() if '.' in file.name else 'no_extension'\n",
					"                        \n",
					"                        all_files.append({\n",
					"                            'container': container,\n",
					"                            'file_name': file.name,\n",
					"                            'file_path': file.path,\n",
					"                            'size_mb': file_size_mb,\n",
					"                            'size_bytes': file.size if hasattr(file, 'size') else 0,\n",
					"                            'days_old': days_old,\n",
					"                            'threshold_used': container_threshold,\n",
					"                            'last_modified': mod_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
					"                            'file_extension': file_ext,\n",
					"                            'is_dependency': is_dependency,\n",
					"                            'dependency_reason': dependency_reason or 'None'\n",
					"                        })\n",
					"            \n",
					"            container_stats[container] = {\n",
					"                'total_files': container_files,\n",
					"                'unused_files': container_unused,\n",
					"                'unused_size_mb': container_size,\n",
					"                'dependencies': container_dependencies,\n",
					"                'invalid_timestamps': container_invalid_timestamps,\n",
					"                'threshold_days': container_threshold\n",
					"            }\n",
					"            \n",
					"            # Summary for container\n",
					"            status_parts = []\n",
					"            if container_unused > 0:\n",
					"                status_parts.append(f\" {container_unused} unused files ({container_size:.1f} MB)\")\n",
					"            if container_dependencies > 0:\n",
					"                status_parts.append(f\" {container_dependencies} protected\")\n",
					"            if container_invalid_timestamps > 0:\n",
					"                status_parts.append(f\" {container_invalid_timestamps} invalid timestamps\")\n",
					"            \n",
					"            if status_parts:\n",
					"                print(f\"    \" + \" | \".join(status_parts))\n",
					"            else:\n",
					"                print(f\"     Clean - no unused files found\")\n",
					"                \n",
					"        except Exception as e:\n",
					"            print(f\" Error with {container}: {str(e)}\")\n",
					"            container_stats[container] = {\n",
					"                'total_files': 0, 'unused_files': 0, 'unused_size_mb': 0, \n",
					"                'dependencies': 0, 'invalid_timestamps': 0, 'threshold_days': container_threshold\n",
					"            }\n",
					"    \n",
					"    return all_files, container_stats, referenced_paths\n",
					"\n",
					"def generate_comprehensive_report(unused_files, container_stats, referenced_paths):\n",
					"    \"\"\"Generate detailed analysis report with file names\"\"\"\n",
					"    \n",
					"    print(f\"\\n\" + \"=\"*80)\n",
					"    print(f\" COMPREHENSIVE STORAGE ANALYSIS REPORT\")\n",
					"    print(f\"=\"*80)\n",
					"    print(f\" Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
					"    print(f\" Dependencies detected: {len(referenced_paths)}\")\n",
					"    \n",
					"    # Overall statistics\n",
					"    total_containers = len(container_stats)\n",
					"    containers_with_unused = len([c for c in container_stats.values() if c['unused_files'] > 0])\n",
					"    total_dependencies = sum(c['dependencies'] for c in container_stats.values())\n",
					"    total_invalid_timestamps = sum(c['invalid_timestamps'] for c in container_stats.values())\n",
					"    \n",
					"    print(f\"\\n OVERVIEW:\")\n",
					"    print(f\"   Containers analyzed: {total_containers}\")\n",
					"    print(f\"   Total dependencies protected: {total_dependencies}\")\n",
					"    print(f\"   Files with timestamp issues: {total_invalid_timestamps}\")\n",
					"    print(f\"   Containers with unused files: {containers_with_unused}\")\n",
					"    \n",
					"    if not unused_files:\n",
					"        print(f\"\\n EXCELLENT! No unused files found!\")\n",
					"        print(f\"\\n Your storage is exceptionally well-maintained:\")\n",
					"        print(f\"   All files are actively used or protected by dependencies\")\n",
					"        print(f\"   Automated cleanup processes are working effectively\")\n",
					"        print(f\"   Data governance policies are properly implemented\")\n",
					"        \n",
					"        # Show threshold effectiveness\n",
					"        print(f\"\\n THRESHOLD SUMMARY:\")\n",
					"        for container, stats in container_stats.items():\n",
					"            threshold = stats['threshold_days']\n",
					"            deps = stats['dependencies']\n",
					"            invalid = stats['invalid_timestamps']\n",
					"            \n",
					"            status_icon = \"🟢\"\n",
					"            if deps > 5:\n",
					"                status_icon = \"🔗\"\n",
					"            elif invalid > 0:\n",
					"                status_icon = \"⚠️\"\n",
					"            \n",
					"            print(f\"  {status_icon} {container}: {threshold} days | {deps} protected | {invalid} timestamp issues\")\n",
					"        \n",
					"        return\n",
					"    \n",
					"    # Detailed analysis for unused files\n",
					"    df = pd.DataFrame(unused_files)\n",
					"    total_files = len(df)\n",
					"    total_size_mb = df['size_mb'].sum()\n",
					"    total_size_gb = total_size_mb / 1024\n",
					"    \n",
					"    print(f\"\\n UNUSED FILES FOUND:\")\n",
					"    print(f\"   Total unused files: {total_files:,}\")\n",
					"    print(f\"   Total size: {total_size_mb:.1f} MB ({total_size_gb:.2f} GB)\")\n",
					"    \n",
					"    # Container breakdown with file names\n",
					"    print(f\"\\n DETAILED CONTAINER BREAKDOWN:\")\n",
					"    print(\"-\" * 80)\n",
					"    \n",
					"    container_summary = df.groupby('container').agg({\n",
					"        'file_name': 'count',\n",
					"        'size_mb': 'sum',\n",
					"        'threshold_used': 'first'\n",
					"    }).rename(columns={'file_name': 'file_count'})\n",
					"    \n",
					"    for container, row in container_summary.iterrows():\n",
					"        deps = container_stats.get(container, {}).get('dependencies', 0)\n",
					"        threshold = int(row['threshold_used'])\n",
					"        \n",
					"        safety_level = \"🟢 SAFE TO DELETE\" if container in ['test', 'temp-sap-hr-data', 'odw-temp'] else \"🟡 REVIEW CAREFULLY\"\n",
					"        \n",
					"        print(f\"\\n📦 {container}: {safety_level}\")\n",
					"        print(f\"    {row['file_count']} unused files, {row['size_mb']:.1f} MB\")\n",
					"        print(f\"    Threshold: {threshold} days\")\n",
					"        print(f\"    Protected dependencies: {deps}\")\n",
					"        \n",
					"        # Show individual files for this container\n",
					"        container_files = df[df['container'] == container].sort_values('size_mb', ascending=False)\n",
					"        print(f\"    Files to review:\")\n",
					"        \n",
					"        for i, (_, file) in enumerate(container_files.iterrows(), 1):\n",
					"            size_indicator = \"\" if file['size_mb'] > 10 else \"📄\"\n",
					"            age_indicator = \"\" if file['days_old'] < 90 else \"❄️\" if file['days_old'] > 365 else \"\"\n",
					"            \n",
					"            print(f\"      {i:2d}. {size_indicator} {file['file_name']}\")\n",
					"            print(f\"           Size: {file['size_mb']:.1f} MB\")\n",
					"            print(f\"          {age_indicator} Age: {file['days_old']} days (modified: {file['last_modified']})\")\n",
					"            print(f\"           Extension: .{file['file_extension']}\")\n",
					"    \n",
					"    # Summary by safety level\n",
					"    print(f\"\\n CLEANUP RECOMMENDATIONS:\")\n",
					"    print(\"-\" * 60)\n",
					"    \n",
					"    safe_containers = ['test', 'temp-sap-hr-data', 'odw-temp']\n",
					"    safe_files = df[df['container'].isin(safe_containers)]\n",
					"    \n",
					"    if not safe_files.empty:\n",
					"        safe_size = safe_files['size_mb'].sum()\n",
					"        print(f\"\\n SAFE TO DELETE IMMEDIATELY ({len(safe_files)} files, {safe_size:.1f} MB):\")\n",
					"        \n",
					"        for container in safe_containers:\n",
					"            container_files = safe_files[safe_files['container'] == container]\n",
					"            if not container_files.empty:\n",
					"                print(f\"\\n    {container} ({len(container_files)} files):\")\n",
					"                for _, file in container_files.iterrows():\n",
					"                    print(f\"      • {file['file_name']} ({file['size_mb']:.1f} MB, {file['days_old']} days old)\")\n",
					"    \n",
					"    review_files = df[~df['container'].isin(safe_containers)]\n",
					"    if not review_files.empty:\n",
					"        review_size = review_files['size_mb'].sum()\n",
					"        print(f\"\\n REVIEW BEFORE DELETING ({len(review_files)} files, {review_size:.1f} MB):\")\n",
					"        \n",
					"        # Group by container for review files\n",
					"        review_containers = review_files['container'].unique()\n",
					"        for container in review_containers:\n",
					"            container_files = review_files[review_files['container'] == container]\n",
					"            print(f\"\\n    {container} ({len(container_files)} files):\")\n",
					"            \n",
					"            # Show largest files first for review priority\n",
					"            top_files = container_files.nlargest(5, 'size_mb') if len(container_files) > 5 else container_files\n",
					"            for _, file in top_files.iterrows():\n",
					"                print(f\"      • {file['file_name']} ({file['size_mb']:.1f} MB, {file['days_old']} days old)\")\n",
					"            \n",
					"            if len(container_files) > 5:\n",
					"                print(f\"      ... and {len(container_files) - 5} more files\")\n",
					"    \n",
					"    # Top 10 largest files across all containers\n",
					"    print(f\"\\n🔍 TOP 10 LARGEST UNUSED FILES:\")\n",
					"    print(\"-\" * 60)\n",
					"    top_files = df.nlargest(10, 'size_mb')\n",
					"    \n",
					"    for i, (_, file) in enumerate(top_files.iterrows(), 1):\n",
					"        safety = \"🟢 SAFE\" if file['container'] in safe_containers else \" REVIEW\"\n",
					"        size_category = \" HUGE\" if file['size_mb'] > 100 else \" LARGE\" if file['size_mb'] > 10 else \"📄 SMALL\"\n",
					"        \n",
					"        print(f\"{i:2d}. {safety} {size_category} {file['file_name']}\")\n",
					"        print(f\"     Container: {file['container']}\")\n",
					"        print(f\"     Size: {file['size_mb']:.1f} MB\")\n",
					"        print(f\"     Age: {file['days_old']} days (last modified: {file['last_modified']})\")\n",
					"        print()\n",
					"    \n",
					"    # Action plan\n",
					"    print(f\" RECOMMENDED ACTION PLAN:\")\n",
					"    print(\"-\" * 60)\n",
					"    print(f\"1. 🟢 IMMEDIATE ACTION - Delete {len(safe_files)} safe files ({safe_files['size_mb'].sum():.1f} MB):\")\n",
					"    print(f\"   • Start with 'test' container (largest files)\")\n",
					"    print(f\"   • Clean 'temp-sap-hr-data' (temporary files)\")\n",
					"    print(f\"   • These are safe test/temp environments\")\n",
					"    \n",
					"    if not review_files.empty:\n",
					"        # Find the largest file that needs review\n",
					"        largest_review = review_files.nlargest(1, 'size_mb').iloc[0]\n",
					"        print(f\"\\n2. 🟡 PRIORITY REVIEW - Check largest file first:\")\n",
					"        print(f\"   • {largest_review['file_name']} ({largest_review['size_mb']:.1f} MB)\")\n",
					"        print(f\"   • Container: {largest_review['container']}\")\n",
					"        print(f\"   • Could save significant space if safe to delete\")\n",
					"    \n",
					"\n",
					"\n",
					"def main():\n",
					"    \"\"\"Main comprehensive analysis function\"\"\"\n",
					"    print(\" COMPREHENSIVE AZURE STORAGE ANALYSIS\")\n",
					"    print(\" Enhanced Pipeline Dependency Detection\")\n",
					"    print(\" Smart Timestamp Parsing\")\n",
					"    print(\" Container-Specific Thresholds\")\n",
					"    print(\"=\"*80)\n",
					"    \n",
					"    try:\n",
					"        # Get storage account info\n",
					"        storage_account_name, _ = get_storage_account_info()\n",
					"        \n",
					"        # Run comprehensive analysis\n",
					"        unused_files, container_stats, referenced_paths = analyze_containers_comprehensive(storage_account_name)\n",
					"        \n",
					"        # Generate detailed report\n",
					"        generate_comprehensive_report(unused_files, container_stats, referenced_paths)\n",
					"        \n",
					"        print(f\"\\n✅ Comprehensive analysis complete!\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"\\n Analysis failed: {str(e)}\")\n",
					"        import traceback\n",
					"        traceback.print_exc()\n",
					"\n",
					"# Run the comprehensive analysis\n",
					"if __name__ == \"__main__\":\n",
					"    main()"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}