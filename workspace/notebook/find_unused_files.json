{
	"name": "find_unused_files",
	"properties": {
		"folder": {
			"name": "odw-harmonised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b7472c0b-3029-48e1-9da3-f8dbef890644"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql import SparkSession\n",
					"import json\n",
					"import re\n",
					"import logging\n",
					"import time\n",
					"from typing import List, Dict, Tuple, Optional, Set, Any\n",
					"import matplotlib.pyplot as plt\n",
					"import os\n",
					"import functools\n",
					"\n",
					"# Constants\n",
					"CONFIG_FILE = 'storage_cleanup_config.json'\n",
					"REPORT_DIR = 'reports'\n",
					"MAX_RETRIES = 3\n",
					"RETRY_DELAY = 2\n",
					"\n",
					"# Configure logging\n",
					"logging.basicConfig(\n",
					"    level=logging.INFO,\n",
					"    format='%(asctime)s - %(levelname)s - %(message)s',\n",
					"    handlers=[\n",
					"        logging.StreamHandler(),\n",
					"        logging.FileHandler('storage_analysis.log')\n",
					"    ]\n",
					")\n",
					"logger = logging.getLogger(__name__)\n",
					"\n",
					"def retry(exceptions=Exception, tries=MAX_RETRIES, delay=RETRY_DELAY, backoff=2):\n",
					"    \"\"\"Retry decorator with exponential backoff.\"\"\"\n",
					"    def decorator(f):\n",
					"        @functools.wraps(f)\n",
					"        def wrapper(*args, **kwargs):\n",
					"            mtries, mdelay = tries, delay\n",
					"            while mtries > 1:\n",
					"                try:\n",
					"                    return f(*args, **kwargs)\n",
					"                except exceptions as e:\n",
					"                    if \"FilesystemNotFound\" not in str(e):  # Don't retry for missing containers\n",
					"                        logger.warning(f\"{str(e)} - Retrying in {mdelay} seconds...\")\n",
					"                        time.sleep(mdelay)\n",
					"                        mtries -= 1\n",
					"                        mdelay *= backoff\n",
					"                    else:\n",
					"                        raise\n",
					"            return f(*args, **kwargs)\n",
					"        return wrapper\n",
					"    return decorator\n",
					"\n",
					"class StorageAnalyzer:\n",
					"    def __init__(self):\n",
					"        \"\"\"Initialize the analyzer with configuration\"\"\"\n",
					"        self.config = self._load_config()\n",
					"        self.spark = SparkSession.builder.getOrCreate()\n",
					"        self.storage_account_name = None\n",
					"        self.storage_account_url = None\n",
					"        self.start_time = datetime.now()\n",
					"        \n",
					"        # Ensure report directory exists\n",
					"        os.makedirs(REPORT_DIR, exist_ok=True)\n",
					"\n",
					"    def _load_config(self) -> Dict[str, Any]:\n",
					"        \"\"\"Load configuration from file with fallback to defaults\"\"\"\n",
					"        try:\n",
					"            with open(CONFIG_FILE) as f:\n",
					"                config = json.load(f)\n",
					"                logger.info(\"Loaded configuration from file\")\n",
					"                return config\n",
					"        except FileNotFoundError:\n",
					"            logger.warning(\"Config file not found, using defaults\")\n",
					"            return {\n",
					"                \"container_thresholds\": {\n",
					"                    'temp-sap-hr-data': 30,\n",
					"                    'odw-temp': 30,\n",
					"                    'test': 60,\n",
					"                    'hbttestdbcontainer': 60,\n",
					"                    'hbttestfilesystem': 60,\n",
					"                    'backup-logs': 90,\n",
					"                    'logging': 90,\n",
					"                    'insights-logs-builtinsqlreqsended': 90,\n",
					"                    'odw-raw': 180,\n",
					"                    'odw-curated': 180,\n",
					"                    'odw-standardised': 180,\n",
					"                    'odw-harmonised': 180,\n",
					"                    'odw-config': 180,\n",
					"                    'odw-config-db': 180,\n",
					"                    'data-lake-config': 180,\n",
					"                    's51-advice-backup': 365,\n",
					"                    'saphrsdata-to-odw': 180,\n",
					"                    'mipins-database': 365,\n",
					"                    'synapse': 120,\n",
					"                    'dart': 120,\n",
					"                    'ims-poc': 120,\n",
					"                    'odw-standardised-delta': 120\n",
					"                },\n",
					"                \"dependency_patterns\": [\n",
					"                    'pipeline', 'etl', 'config', 'schema', 'metadata', \n",
					"                    'checkpoint', 'watermark', 'state', 'offset', \n",
					"                    'manifest', 'success', '_started', '_committed', \n",
					"                    '_delta_log', '_spark_metadata', '.checkpoints',\n",
					"                    'orchestration', 'workflow', 'jobconfig', 'dataflow',\n",
					"                    'control', 'audit', 'monitoring', 'validation',\n",
					"                    'reference', 'master', 'lookup', 'dimension', 'fact'\n",
					"                ],\n",
					"                \"skip_missing_containers\": True  # New config to skip missing containers\n",
					"            }\n",
					"\n",
					"    @retry()\n",
					"    def get_storage_account_info(self) -> Tuple[str, str]:\n",
					"        \"\"\"Extract storage account information with retry logic\"\"\"\n",
					"        logger.info(\"Fetching storage account information\")\n",
					"        try:\n",
					"            storage_account_url = mssparkutils.notebook.run(\n",
					"                '/utils/py_utils_get_storage_account'\n",
					"            ).strip()\n",
					"            \n",
					"            domain_part = storage_account_url.split('://')[1] if '://' in storage_account_url else storage_account_url\n",
					"            domain_part = domain_part.rstrip('/')\n",
					"            storage_account_name = domain_part.split('.')[0]\n",
					"            \n",
					"            self.storage_account_name = storage_account_name\n",
					"            self.storage_account_url = storage_account_url\n",
					"            \n",
					"            logger.info(f\"Storage Account: {storage_account_name}\")\n",
					"            return storage_account_name, storage_account_url\n",
					"        except Exception as e:\n",
					"            logger.error(f\"Failed to get storage account info: {str(e)}\")\n",
					"            raise\n",
					"\n",
					"    def parse_timestamp_safely(self, timestamp_value) -> Optional[datetime]:\n",
					"        \"\"\"Parse various timestamp formats with enhanced validation\"\"\"\n",
					"        try:\n",
					"            if isinstance(timestamp_value, datetime):\n",
					"                return timestamp_value.replace(tzinfo=None)\n",
					"            \n",
					"            ts_str = str(timestamp_value)\n",
					"            \n",
					"            if ts_str.isdigit() and len(ts_str) == 13:\n",
					"                dt = datetime.fromtimestamp(int(ts_str) / 1000)\n",
					"                if 2010 <= dt.year <= 2030:\n",
					"                    return dt.replace(tzinfo=None)\n",
					"            \n",
					"            elif ts_str.isdigit() and len(ts_str) == 10:\n",
					"                dt = datetime.fromtimestamp(int(ts_str))\n",
					"                if 2010 <= dt.year <= 2030:\n",
					"                    return dt.replace(tzinfo=None)\n",
					"            \n",
					"            parsed_dt = pd.to_datetime(ts_str, errors='coerce')\n",
					"            if not pd.isna(parsed_dt):\n",
					"                dt = parsed_dt.to_pydatetime().replace(tzinfo=None)\n",
					"                if 2010 <= dt.year <= 2030:\n",
					"                    return dt\n",
					"            \n",
					"            logger.warning(f\"Unparseable timestamp: {timestamp_value}\")\n",
					"            return None\n",
					"            \n",
					"        except Exception as e:\n",
					"            logger.error(f\"Timestamp parsing error: {str(e)}\")\n",
					"            return None\n",
					"\n",
					"    def detect_dependencies(self) -> Tuple[Set[str], List[str]]:\n",
					"        \"\"\"Comprehensive dependency detection with caching\"\"\"\n",
					"        logger.info(\"Starting dependency detection\")\n",
					"        \n",
					"        if hasattr(self, '_cached_dependencies'):\n",
					"            logger.info(\"Using cached dependencies\")\n",
					"            return self._cached_dependencies\n",
					"        \n",
					"        referenced_paths = set()\n",
					"        pipeline_patterns = set(self.config.get(\"dependency_patterns\", []))\n",
					"        \n",
					"        try:\n",
					"            # 1. SPARK CATALOG ANALYSIS\n",
					"            logger.info(\"Analyzing Spark catalog...\")\n",
					"            catalog_count = self._analyze_spark_catalog(referenced_paths)\n",
					"            logger.info(f\"Found {catalog_count} table references in Spark catalog\")\n",
					"            \n",
					"            # 2. SYNAPSE PIPELINE ANALYSIS\n",
					"            logger.info(\"Analyzing Synapse pipelines...\")\n",
					"            synapse_refs = self._analyze_synapse_pipelines()\n",
					"            referenced_paths.update(synapse_refs)\n",
					"            logger.info(f\"Found {len(synapse_refs)} Synapse pipeline references\")\n",
					"            \n",
					"            # 3. NOTEBOOK ANALYSIS\n",
					"            logger.info(\"Analyzing notebooks...\")\n",
					"            notebook_refs = self._analyze_notebook_dependencies()\n",
					"            referenced_paths.update(notebook_refs)\n",
					"            logger.info(f\"Found {len(notebook_refs)} notebook references\")\n",
					"            \n",
					"            # 4. CONFIG FILE ANALYSIS\n",
					"            logger.info(\"Analyzing config files...\")\n",
					"            config_refs = self._analyze_config_files()\n",
					"            referenced_paths.update(config_refs)\n",
					"            logger.info(f\"Found {len(config_refs)} config references\")\n",
					"            \n",
					"            self._cached_dependencies = (referenced_paths, list(pipeline_patterns))\n",
					"            return referenced_paths, list(pipeline_patterns)\n",
					"            \n",
					"        except Exception as e:\n",
					"            logger.error(f\"Dependency detection failed: {str(e)}\")\n",
					"            raise\n",
					"\n",
					"    def _analyze_spark_catalog(self, referenced_paths: Set[str]) -> int:\n",
					"        \"\"\"Analyze Spark catalog for storage references\"\"\"\n",
					"        catalog_count = 0\n",
					"        try:\n",
					"            databases = self.spark.sql(\"SHOW DATABASES\").collect()\n",
					"            for db in databases:\n",
					"                db_name = db[0]\n",
					"                try:\n",
					"                    tables = self.spark.sql(f\"SHOW TABLES IN {db_name}\").collect()\n",
					"                    for table in tables:\n",
					"                        table_name = table[1]\n",
					"                        try:\n",
					"                            table_desc = self.spark.sql(\n",
					"                                f\"DESCRIBE EXTENDED {db_name}.{table_name}\"\n",
					"                            ).collect()\n",
					"                            for row in table_desc:\n",
					"                                if row[0] and 'Location' in str(row[0]):\n",
					"                                    location = str(row[1])\n",
					"                                    if self.storage_account_name in location:\n",
					"                                        referenced_paths.add(location)\n",
					"                                        catalog_count += 1\n",
					"                        except Exception as e:\n",
					"                            logger.debug(f\"Could not describe table {db_name}.{table_name}: {str(e)}\")\n",
					"                            continue\n",
					"                except Exception as e:\n",
					"                    logger.debug(f\"Could not list tables in {db_name}: {str(e)}\")\n",
					"                    continue\n",
					"        except Exception as e:\n",
					"            logger.error(f\"Spark catalog analysis failed: {str(e)}\")\n",
					"        \n",
					"        return catalog_count\n",
					"\n",
					"    def _analyze_synapse_pipelines(self) -> Set[str]:\n",
					"        \"\"\"Analyze Synapse pipelines for storage references\"\"\"\n",
					"        pipeline_refs = set()\n",
					"        \n",
					"        try:\n",
					"            config_containers = ['odw-config', 'data-lake-config', 'synapse']\n",
					"            \n",
					"            for container in config_containers:\n",
					"                try:\n",
					"                    container_path = f\"abfss://{container}@{self.storage_account_name}.dfs.core.windows.net/\"\n",
					"                    files = mssparkutils.fs.ls(container_path)\n",
					"                    \n",
					"                    for file in files:\n",
					"                        if not file.isDir and file.name.lower().endswith(('.json', '.yml', '.yaml')):\n",
					"                            try:\n",
					"                                file_content = mssparkutils.fs.head(\n",
					"                                    f\"{container_path}{file.name}\", \n",
					"                                    10000\n",
					"                                )\n",
					"                                \n",
					"                                abfss_pattern = r'abfss://[^@]+@' + \\\n",
					"                                    self.storage_account_name + \\\n",
					"                                    r'\\.dfs\\.core\\.windows\\.net/[^\\s\"\\']*'\n",
					"                                matches = re.findall(abfss_pattern, file_content)\n",
					"                                pipeline_refs.update(matches)\n",
					"                                \n",
					"                                container_pattern = r'\"container\":\\s*\"([^\"]+)\"'\n",
					"                                container_matches = re.findall(container_pattern, file_content)\n",
					"                                for container_ref in container_matches:\n",
					"                                    pipeline_refs.add(\n",
					"                                        f\"abfss://{container_ref}@{self.storage_account_name}.dfs.core.windows.net/\"\n",
					"                                    )\n",
					"                                    \n",
					"                            except Exception as e:\n",
					"                                logger.debug(f\"Could not process {file.name}: {str(e)}\")\n",
					"                                continue\n",
					"                                \n",
					"                except Exception as e:\n",
					"                    if \"FilesystemNotFound\" in str(e):\n",
					"                        logger.info(f\"Container {container} not found - skipping\")\n",
					"                    else:\n",
					"                        logger.debug(f\"Could not access {container}: {str(e)}\")\n",
					"                    continue\n",
					"                \n",
					"            dataset_patterns = [\n",
					"                f\"abfss://odw-raw@{self.storage_account_name}.dfs.core.windows.net/\",\n",
					"                f\"abfss://odw-standardised@{self.storage_account_name}.dfs.core.windows.net/\",\n",
					"                f\"abfss://odw-harmonised@{self.storage_account_name}.dfs.core.windows.net/\",\n",
					"                f\"abfss://odw-curated@{self.storage_account_name}.dfs.core.windows.net/\"\n",
					"            ]\n",
					"            pipeline_refs.update(dataset_patterns)\n",
					"            \n",
					"        except Exception as e:\n",
					"            logger.error(f\"Synapse pipeline analysis failed: {str(e)}\")\n",
					"        \n",
					"        return pipeline_refs\n",
					"\n",
					"    def _analyze_notebook_dependencies(self) -> Set[str]:\n",
					"        \"\"\"Analyze notebooks for storage references\"\"\"\n",
					"        notebook_refs = set()\n",
					"        \n",
					"        try:\n",
					"            notebook_containers = ['synapse', 'temp-sap-hr-data', 'odw-temp']\n",
					"            \n",
					"            for container in notebook_containers:\n",
					"                try:\n",
					"                    container_path = f\"abfss://{container}@{self.storage_account_name}.dfs.core.windows.net/\"\n",
					"                    files = mssparkutils.fs.ls(container_path)\n",
					"                    \n",
					"                    for file in files:\n",
					"                        if any(pattern in file.name.lower() for pattern in \n",
					"                               ['checkpoint', 'output', 'result', 'temp', 'cache', '.ipynb']):\n",
					"                            notebook_refs.add(file.path)\n",
					"                            \n",
					"                except Exception as e:\n",
					"                    if \"FilesystemNotFound\" in str(e):\n",
					"                        logger.info(f\"Container {container} not found - skipping\")\n",
					"                    else:\n",
					"                        logger.debug(f\"Could not access {container}: {str(e)}\")\n",
					"                    continue\n",
					"                \n",
					"        except Exception as e:\n",
					"            logger.error(f\"Notebook analysis failed: {str(e)}\")\n",
					"        \n",
					"        return notebook_refs\n",
					"\n",
					"    def _analyze_config_files(self) -> Set[str]:\n",
					"        \"\"\"Analyze configuration files for storage references\"\"\"\n",
					"        config_refs = set()\n",
					"        \n",
					"        try:\n",
					"            config_containers = ['odw-config', 'data-lake-config', 'odw-config-db']\n",
					"            \n",
					"            for container in config_containers:\n",
					"                try:\n",
					"                    container_path = f\"abfss://{container}@{self.storage_account_name}.dfs.core.windows.net/\"\n",
					"                    files = mssparkutils.fs.ls(container_path)\n",
					"                    \n",
					"                    for file in files:\n",
					"                        if not file.isDir and file.name.lower().endswith('.json'):\n",
					"                            try:\n",
					"                                file_content = mssparkutils.fs.head(\n",
					"                                    f\"{container_path}{file.name}\", \n",
					"                                    5000\n",
					"                                )\n",
					"                                \n",
					"                                path_pattern = f'{self.storage_account_name}\\\\.dfs\\\\.core\\\\.windows\\\\.net/([^\"\\\\s]*)'\n",
					"                                matches = re.findall(path_pattern, file_content)\n",
					"                                \n",
					"                                for match in matches:\n",
					"                                    config_refs.add(\n",
					"                                        f\"abfss://unknown@{self.storage_account_name}.dfs.core.windows.net/{match}\"\n",
					"                                    )\n",
					"                                    \n",
					"                            except Exception as e:\n",
					"                                logger.debug(f\"Could not process {file.name}: {str(e)}\")\n",
					"                                continue\n",
					"                except Exception as e:\n",
					"                    if \"FilesystemNotFound\" in str(e):\n",
					"                        logger.info(f\"Container {container} not found - skipping\")\n",
					"                    else:\n",
					"                        logger.debug(f\"Could not access {container}: {str(e)}\")\n",
					"                    continue\n",
					"                \n",
					"        except Exception as e:\n",
					"            logger.error(f\"Config file analysis failed: {str(e)}\")\n",
					"        \n",
					"        return config_refs\n",
					"\n",
					"    def is_dependency(\n",
					"        self,\n",
					"        file_path: str,\n",
					"        file_name: str,\n",
					"        referenced_paths: Set[str],\n",
					"        pipeline_patterns: List[str]\n",
					"    ) -> Tuple[bool, Optional[str]]:\n",
					"        \"\"\"Enhanced dependency checking with additional patterns\"\"\"\n",
					"        file_lower = file_name.lower()\n",
					"        path_lower = file_path.lower()\n",
					"        \n",
					"        for ref_path in referenced_paths:\n",
					"            if ref_path.lower() in path_lower:\n",
					"                return True, f\"Referenced in pipeline/table: {ref_path[:50]}...\"\n",
					"        \n",
					"        for pattern in pipeline_patterns:\n",
					"            if pattern in file_lower:\n",
					"                return True, f\"Contains pipeline pattern: {pattern}\"\n",
					"        \n",
					"        system_dirs = [\n",
					"            '_delta_log', '_spark_metadata', '.checkpoints', \n",
					"            '_checkpoints', '_SUCCESS', '_committed', '_started'\n",
					"        ]\n",
					"        for sys_dir in system_dirs:\n",
					"            if sys_dir in path_lower:\n",
					"                return True, f\"System directory: {sys_dir}\"\n",
					"        \n",
					"        config_extensions = ['json', 'xml', 'yaml', 'yml', 'conf', 'properties', 'cfg']\n",
					"        if any(file_lower.endswith(f'.{ext}') for ext in config_extensions):\n",
					"            config_keywords = ['pipeline', 'config', 'schema', 'metadata', 'orchestration']\n",
					"            if any(keyword in file_lower for keyword in config_keywords):\n",
					"                return True, \"Configuration file\"\n",
					"        \n",
					"        if any(x in path_lower for x in ['/archive/', '/historical/']):\n",
					"            return False, \"Archive/historical data\"\n",
					"        \n",
					"        if any(x in file_lower for x in ['backup', 'snapshot']):\n",
					"            return True, \"Backup/snapshot file\"\n",
					"        \n",
					"        return False, None\n",
					"\n",
					"    def analyze_container(\n",
					"        self,\n",
					"        container: str\n",
					"    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
					"        \"\"\"Analyze files in a single container with error handling\"\"\"\n",
					"        logger.info(f\"Analyzing container: {container}\")\n",
					"        \n",
					"        container_threshold = self.config[\"container_thresholds\"].get(container, 120)\n",
					"        cutoff_date = datetime.now() - timedelta(days=container_threshold)\n",
					"        \n",
					"        container_files = []\n",
					"        container_stats = {\n",
					"            'total_files': 0,\n",
					"            'unused_files': 0,\n",
					"            'unused_size_mb': 0,\n",
					"            'dependencies': 0,\n",
					"            'invalid_timestamps': 0,\n",
					"            'threshold_days': container_threshold,\n",
					"            'status': 'analyzed'\n",
					"        }\n",
					"        \n",
					"        try:\n",
					"            path = f\"abfss://{container}@{self.storage_account_name}.dfs.core.windows.net/\"\n",
					"            files = mssparkutils.fs.ls(path)\n",
					"            \n",
					"            container_stats['total_files'] = len(files)\n",
					"            referenced_paths, pipeline_patterns = self.detect_dependencies()\n",
					"            \n",
					"            for file in files:\n",
					"                if not file.isDir and hasattr(file, 'modifyTime') and file.modifyTime:\n",
					"                    mod_time = self.parse_timestamp_safely(file.modifyTime)\n",
					"                    \n",
					"                    if mod_time is None:\n",
					"                        container_stats['invalid_timestamps'] += 1\n",
					"                        logger.warning(f\"Invalid timestamp: {file.name} ({file.modifyTime})\")\n",
					"                        continue\n",
					"                    \n",
					"                    file_size_mb = round(file.size / (1024*1024), 2) if hasattr(file, 'size') else 0\n",
					"                    days_old = (datetime.now() - mod_time).days\n",
					"                    \n",
					"                    is_dependency, dependency_reason = self.is_dependency(\n",
					"                        file.path, file.name, referenced_paths, pipeline_patterns\n",
					"                    )\n",
					"                    \n",
					"                    if is_dependency:\n",
					"                        container_stats['dependencies'] += 1\n",
					"                        if days_old > container_threshold:\n",
					"                            logger.info(f\"Protected dependency: {file.name} ({days_old} days) - {dependency_reason}\")\n",
					"                    \n",
					"                    elif mod_time < cutoff_date:\n",
					"                        container_stats['unused_files'] += 1\n",
					"                        container_stats['unused_size_mb'] += file_size_mb\n",
					"                        \n",
					"                        file_ext = file.name.split('.')[-1].lower() if '.' in file.name else 'no_extension'\n",
					"                        \n",
					"                        container_files.append({\n",
					"                            'container': container,\n",
					"                            'file_name': file.name,\n",
					"                            'file_path': file.path,\n",
					"                            'size_mb': file_size_mb,\n",
					"                            'size_bytes': file.size if hasattr(file, 'size') else 0,\n",
					"                            'days_old': days_old,\n",
					"                            'threshold_used': container_threshold,\n",
					"                            'last_modified': mod_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
					"                            'file_extension': file_ext,\n",
					"                            'is_dependency': is_dependency,\n",
					"                            'dependency_reason': dependency_reason or 'None'\n",
					"                        })\n",
					"            \n",
					"            logger.info(f\"Container {container} analysis complete: \"\n",
					"                       f\"{container_stats['unused_files']} unused files found\")\n",
					"            \n",
					"            return container_files, container_stats\n",
					"            \n",
					"        except Exception as e:\n",
					"            if \"FilesystemNotFound\" in str(e):\n",
					"                logger.info(f\"Container {container} not found - skipping\")\n",
					"                container_stats['status'] = 'not_found'\n",
					"                return [], container_stats\n",
					"            else:\n",
					"                logger.error(f\"Failed to analyze container {container}: {str(e)}\")\n",
					"                container_stats['status'] = 'error'\n",
					"                return [], container_stats\n",
					"\n",
					"    def analyze_all_containers(self) -> Tuple[List[Dict[str, Any]], Dict[str, Any], Set[str]]:\n",
					"        \"\"\"Analyze all containers with error handling\"\"\"\n",
					"        logger.info(\"Starting analysis of all containers\")\n",
					"        \n",
					"        containers = list(self.config[\"container_thresholds\"].keys())\n",
					"        all_files = []\n",
					"        container_stats = {}\n",
					"        \n",
					"        referenced_paths, pipeline_patterns = self.detect_dependencies()\n",
					"        \n",
					"        for container in containers:\n",
					"            files, stats = self.analyze_container(container)\n",
					"            all_files.extend(files)\n",
					"            container_stats[container] = stats\n",
					"            \n",
					"        logger.info(f\"Completed analysis of {len(containers)} containers\")\n",
					"        return all_files, container_stats, referenced_paths\n",
					"\n",
					"    def generate_report(\n",
					"        self,\n",
					"        unused_files: List[Dict[str, Any]],\n",
					"        container_stats: Dict[str, Any],\n",
					"        referenced_paths: Set[str]\n",
					"    ) -> str:\n",
					"        \"\"\"Generate comprehensive report with visualizations\"\"\"\n",
					"        logger.info(\"Generating analysis report\")\n",
					"        \n",
					"        report_path = os.path.join(\n",
					"            REPORT_DIR,\n",
					"            f\"storage_analysis_report_{self.start_time.strftime('%Y%m%d_%H%M%S')}.html\"\n",
					"        )\n",
					"        \n",
					"        df = pd.DataFrame(unused_files)\n",
					"        self._generate_visualizations(df, container_stats)\n",
					"        \n",
					"        html_content = self._generate_html_report(df, container_stats, referenced_paths)\n",
					"        \n",
					"        with open(report_path, 'w') as f:\n",
					"            f.write(html_content)\n",
					"        \n",
					"        logger.info(f\"Report generated at: {report_path}\")\n",
					"        return report_path\n",
					"\n",
					"    def _generate_visualizations(self, df: pd.DataFrame, container_stats: Dict[str, Any]):\n",
					"        \"\"\"Generate visualizations for the report\"\"\"\n",
					"        try:\n",
					"            # Size distribution by container\n",
					"            plt.figure(figsize=(12, 6))\n",
					"            df.groupby('container')['size_mb'].sum().sort_values().plot.barh()\n",
					"            plt.title('Unused Storage by Container (MB)')\n",
					"            plt.tight_layout()\n",
					"            plt.savefig(os.path.join(REPORT_DIR, 'size_by_container.png'))\n",
					"            plt.close()\n",
					"            \n",
					"            # File age distribution\n",
					"            plt.figure(figsize=(10, 6))\n",
					"            df['days_old'].plot.hist(bins=20)\n",
					"            plt.title('Distribution of File Ages (Days)')\n",
					"            plt.xlabel('Days Since Last Modification')\n",
					"            plt.savefig(os.path.join(REPORT_DIR, 'file_age_distribution.png'))\n",
					"            plt.close()\n",
					"            \n",
					"            # File type distribution\n",
					"            plt.figure(figsize=(10, 6))\n",
					"            df['file_extension'].value_counts().head(10).plot.pie(autopct='%1.1f%%')\n",
					"            plt.title('Top 10 File Extensions')\n",
					"            plt.savefig(os.path.join(REPORT_DIR, 'file_extensions.png'))\n",
					"            plt.close()\n",
					"            \n",
					"        except Exception as e:\n",
					"            logger.error(f\"Failed to generate visualizations: {str(e)}\")\n",
					"\n",
					"    def _generate_html_report(\n",
					"        self,\n",
					"        df: pd.DataFrame,\n",
					"        container_stats: Dict[str, Any],\n",
					"        referenced_paths: Set[str]\n",
					"    ) -> str:\n",
					"        \"\"\"Generate HTML content for the report\"\"\"\n",
					"        total_files = len(df)\n",
					"        total_size_mb = df['size_mb'].sum()\n",
					"        total_size_gb = total_size_mb / 1024\n",
					"        \n",
					"        safe_containers = ['test', 'temp-sap-hr-data', 'odw-temp']\n",
					"        safe_files = df[df['container'].isin(safe_containers)]\n",
					"        safe_size_mb = safe_files['size_mb'].sum()\n",
					"        \n",
					"        review_files = df[~df['container'].isin(safe_containers)]\n",
					"        review_size_mb = review_files['size_mb'].sum()\n",
					"        \n",
					"        # Count container statuses\n",
					"        analyzed = sum(1 for stats in container_stats.values() if stats['status'] == 'analyzed')\n",
					"        not_found = sum(1 for stats in container_stats.values() if stats['status'] == 'not_found')\n",
					"        errors = sum(1 for stats in container_stats.values() if stats['status'] == 'error')\n",
					"        \n",
					"        html = f\"\"\"\n",
					"        <html>\n",
					"        <head>\n",
					"            <title>Azure Storage Analysis Report</title>\n",
					"            <style>\n",
					"                body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
					"                h1, h2, h3 {{ color: #2e6c80; }}\n",
					"                .summary {{ background-color: #f5f5f5; padding: 15px; border-radius: 5px; }}\n",
					"                .container {{ margin-bottom: 30px; }}\n",
					"                table {{ border-collapse: collapse; width: 100%; }}\n",
					"                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
					"                th {{ background-color: #f2f2f2; }}\n",
					"                .safe {{ color: green; }}\n",
					"                .review {{ color: orange; }}\n",
					"                .error {{ color: red; }}\n",
					"                .warning {{ color: #FFA500; }}\n",
					"                img {{ max-width: 100%; height: auto; }}\n",
					"                .status-ok {{ background-color: #DFF0D8; }}\n",
					"                .status-warning {{ background-color: #FCF8E3; }}\n",
					"                .status-error {{ background-color: #F2DEDE; }}\n",
					"            </style>\n",
					"        </head>\n",
					"        <body>\n",
					"            <h1>Azure Storage Analysis Report</h1>\n",
					"            <p>Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
					"            <p>Storage Account: {self.storage_account_name}</p>\n",
					"            \n",
					"            <div class=\"summary\">\n",
					"                <h2>Executive Summary</h2>\n",
					"                <ul>\n",
					"                    <li>Total unused files found: <strong>{total_files:,}</strong></li>\n",
					"                    <li>Total unused storage: <strong>{total_size_mb:.1f} MB ({total_size_gb:.2f} GB)</strong></li>\n",
					"                    <li>Files in test/temp containers: <span class=\"safe\">{len(safe_files)} ({safe_size_mb:.1f} MB)</span></li>\n",
					"                    <li>Files needing review: <span class=\"review\">{len(review_files)} ({review_size_mb:.1f} MB)</span></li>\n",
					"                    <li>Pipeline dependencies protected: {sum(c['dependencies'] for c in container_stats.values() if c['status'] == 'analyzed')}</li>\n",
					"                    <li>Containers analyzed: {analyzed}</li>\n",
					"                    <li>Containers not found: <span class=\"warning\">{not_found}</span></li>\n",
					"                    <li>Containers with errors: <span class=\"error\">{errors}</span></li>\n",
					"                </ul>\n",
					"            </div>\n",
					"            \n",
					"            <h2>Visualizations</h2>\n",
					"            <div style=\"display: flex; flex-wrap: wrap;\">\n",
					"                <div style=\"flex: 50%; padding: 10px;\">\n",
					"                    <img src=\"size_by_container.png\" alt=\"Size by Container\">\n",
					"                </div>\n",
					"                <div style=\"flex: 50%; padding: 10px;\">\n",
					"                    <img src=\"file_age_distribution.png\" alt=\"File Age Distribution\">\n",
					"                </div>\n",
					"                <div style=\"flex: 50%; padding: 10px;\">\n",
					"                    <img src=\"file_extensions.png\" alt=\"File Extensions\">\n",
					"                </div>\n",
					"            </div>\n",
					"            \n",
					"            <h2>Detailed Findings</h2>\n",
					"            <div class=\"container\">\n",
					"                <h3>Containers Summary</h3>\n",
					"                <table>\n",
					"                    <tr>\n",
					"                        <th>Container</th>\n",
					"                        <th>Status</th>\n",
					"                        <th>Total Files</th>\n",
					"                        <th>Unused Files</th>\n",
					"                        <th>Unused Size (MB)</th>\n",
					"                        <th>Dependencies</th>\n",
					"                        <th>Threshold (Days)</th>\n",
					"                    </tr>\n",
					"                    {''.join(\n",
					"                        f'<tr class=\"status-{\"ok\" if stats[\"status\"] == \"analyzed\" else \"warning\" if stats[\"status\"] == \"not_found\" else \"error\"}\">'\n",
					"                        f'<td>{container}</td>'\n",
					"                        f'<td>{stats[\"status\"].capitalize()}</td>'\n",
					"                        f'<td>{stats[\"total_files\"]}</td>'\n",
					"                        f'<td>{stats[\"unused_files\"]}</td>'\n",
					"                        f'<td>{stats[\"unused_size_mb\"]:.1f}</td>'\n",
					"                        f'<td>{stats[\"dependencies\"]}</td>'\n",
					"                        f'<td>{stats[\"threshold_days\"]}</td></tr>'\n",
					"                        for container, stats in container_stats.items()\n",
					"                    )}\n",
					"                </table>\n",
					"            </div>\n",
					"            \n",
					"            <div class=\"container\">\n",
					"                <h3>Top 20 Largest Unused Files</h3>\n",
					"                <table>\n",
					"                    <tr>\n",
					"                        <th>File Name</th>\n",
					"                        <th>Container</th>\n",
					"                        <th>Size (MB)</th>\n",
					"                        <th>Age (Days)</th>\n",
					"                        <th>Last Modified</th>\n",
					"                        <th>Dependency Reason</th>\n",
					"                    </tr>\n",
					"                    {''.join(\n",
					"                        f'<tr><td>{row[\"file_name\"]}</td>'\n",
					"                        f'<td>{row[\"container\"]}</td>'\n",
					"                        f'<td>{row[\"size_mb\"]:.1f}</td>'\n",
					"                        f'<td>{row[\"days_old\"]}</td>'\n",
					"                        f'<td>{row[\"last_modified\"]}</td>'\n",
					"                        f'<td>{row[\"dependency_reason\"]}</td></tr>'\n",
					"                        for _, row in df.nlargest(20, 'size_mb').iterrows()\n",
					"                    )}\n",
					"                </table>\n",
					"            </div>\n",
					"            \n",
					"            <h2>Recommendations</h2>\n",
					"            <div class=\"container\">\n",
					"                <h3>Potential Cleanup Candidates</h3>\n",
					"                <ul>\n",
					"                    <li>{len(safe_files)} files in test/temp containers ({safe_size_mb:.1f} MB) could potentially be cleaned up</li>\n",
					"                    <li>Review the top 20 largest files identified in the report</li>\n",
					"                </ul>\n",
					"                \n",
					"                <h3>Optimization Suggestions</h3>\n",
					"                <ul>\n",
					"                    <li>Consider implementing lifecycle policies for different data categories</li>\n",
					"                    <li>Monitor storage usage regularly to identify new unused files</li>\n",
					"                    <li>Verify containers that were not found or had errors during analysis</li>\n",
					"                </ul>\n",
					"            </div>\n",
					"        </body>\n",
					"        </html>\n",
					"        \"\"\"\n",
					"        \n",
					"        return html\n",
					"\n",
					"    def run_analysis(self):\n",
					"        \"\"\"Main execution method for analysis only\"\"\"\n",
					"        try:\n",
					"            logger.info(\"Starting Azure Storage Analysis\")\n",
					"            \n",
					"            # Get storage account info\n",
					"            self.get_storage_account_info()\n",
					"            \n",
					"            # Analyze all containers\n",
					"            unused_files, container_stats, referenced_paths = self.analyze_all_containers()\n",
					"            \n",
					"            # Generate report\n",
					"            report_path = self.generate_report(unused_files, container_stats, referenced_paths)\n",
					"            \n",
					"            logger.info(\"Analysis completed successfully\")\n",
					"            return unused_files, container_stats, report_path\n",
					"            \n",
					"        except Exception as e:\n",
					"            logger.error(f\"Analysis failed: {str(e)}\")\n",
					"            raise\n",
					"\n",
					"# For notebook execution\n",
					"analyzer = StorageAnalyzer()\n",
					"analysis_results = analyzer.run_analysis()\n",
					"\n",
					"# Optionally display the report in notebook\n",
					"from IPython.display import HTML\n",
					"HTML(filename=analysis_results[2])"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql import SparkSession\n",
					"import json\n",
					"import re\n",
					"import logging\n",
					"import time\n",
					"from typing import List, Dict, Tuple, Optional, Set, Any\n",
					"import matplotlib.pyplot as plt\n",
					"import os\n",
					"import functools\n",
					"import base64\n",
					"from io import BytesIO\n",
					"from IPython.display import HTML, display\n",
					"\n",
					"# Constants\n",
					"CONFIG_FILE = 'storage_cleanup_config.json'\n",
					"REPORT_DIR = 'reports'\n",
					"MAX_RETRIES = 3\n",
					"RETRY_DELAY = 2\n",
					"\n",
					"# Configure logging\n",
					"logging.basicConfig(\n",
					"    level=logging.INFO,\n",
					"    format='%(asctime)s - %(levelname)s - %(message)s',\n",
					"    handlers=[\n",
					"        logging.StreamHandler(),\n",
					"        logging.FileHandler('storage_analysis.log')\n",
					"    ]\n",
					")\n",
					"logger = logging.getLogger(__name__)\n",
					"\n",
					"def retry(exceptions=Exception, tries=MAX_RETRIES, delay=RETRY_DELAY, backoff=2):\n",
					"    \"\"\"Retry decorator with exponential backoff.\"\"\"\n",
					"    def decorator(f):\n",
					"        @functools.wraps(f)\n",
					"        def wrapper(*args, **kwargs):\n",
					"            mtries, mdelay = tries, delay\n",
					"            while mtries > 1:\n",
					"                try:\n",
					"                    return f(*args, **kwargs)\n",
					"                except exceptions as e:\n",
					"                    if \"FilesystemNotFound\" not in str(e):  # Don't retry for missing containers\n",
					"                        logger.warning(f\"{str(e)} - Retrying in {mdelay} seconds...\")\n",
					"                        time.sleep(mdelay)\n",
					"                        mtries -= 1\n",
					"                        mdelay *= backoff\n",
					"                    else:\n",
					"                        raise\n",
					"            return f(*args, **kwargs)\n",
					"        return wrapper\n",
					"    return decorator\n",
					"\n",
					"class StorageAnalyzer:\n",
					"    def __init__(self):\n",
					"        \"\"\"Initialize the analyzer with configuration\"\"\"\n",
					"        self.config = self._load_config()\n",
					"        self.spark = SparkSession.builder.getOrCreate()\n",
					"        self.storage_account_name = None\n",
					"        self.storage_account_url = None\n",
					"        self.start_time = datetime.now()\n",
					"        self.visualizations = {}  # Store visualization HTML for notebook display\n",
					"        \n",
					"        # Ensure report directory exists\n",
					"        os.makedirs(REPORT_DIR, exist_ok=True)\n",
					"\n",
					"    def _load_config(self) -> Dict[str, Any]:\n",
					"        \"\"\"Load configuration from file with fallback to defaults\"\"\"\n",
					"        try:\n",
					"            with open(CONFIG_FILE) as f:\n",
					"                config = json.load(f)\n",
					"                logger.info(\"Loaded configuration from file\")\n",
					"                return config\n",
					"        except FileNotFoundError:\n",
					"            logger.warning(\"Config file not found, using defaults\")\n",
					"            return {\n",
					"                \"container_thresholds\": {\n",
					"                    'temp-sap-hr-data': 30,\n",
					"                    'odw-temp': 30,\n",
					"                    'test': 60,\n",
					"                    'hbttestdbcontainer': 60,\n",
					"                    'hbttestfilesystem': 60,\n",
					"                    'backup-logs': 90,\n",
					"                    'logging': 90,\n",
					"                    'insights-logs-builtinsqlreqsended': 90,\n",
					"                    'odw-raw': 180,\n",
					"                    'odw-curated': 180,\n",
					"                    'odw-standardised': 180,\n",
					"                    'odw-harmonised': 180,\n",
					"                    'odw-config': 180,\n",
					"                    'odw-config-db': 180,\n",
					"                    'data-lake-config': 180,\n",
					"                    's51-advice-backup': 365,\n",
					"                    'saphrsdata-to-odw': 180,\n",
					"                    'mipins-database': 365,\n",
					"                    'synapse': 120,\n",
					"                    'dart': 120,\n",
					"                    'ims-poc': 120,\n",
					"                    'odw-standardised-delta': 120\n",
					"                },\n",
					"                \"dependency_patterns\": [\n",
					"                    'pipeline', 'etl', 'config', 'schema', 'metadata', \n",
					"                    'checkpoint', 'watermark', 'state', 'offset', \n",
					"                    'manifest', 'success', '_started', '_committed', \n",
					"                    '_delta_log', '_spark_metadata', '.checkpoints',\n",
					"                    'orchestration', 'workflow', 'jobconfig', 'dataflow',\n",
					"                    'control', 'audit', 'monitoring', 'validation',\n",
					"                    'reference', 'master', 'lookup', 'dimension', 'fact'\n",
					"                ],\n",
					"                \"skip_missing_containers\": True\n",
					"            }\n",
					"\n",
					"    @retry()\n",
					"    def get_storage_account_info(self) -> Tuple[str, str]:\n",
					"        \"\"\"Extract storage account information with retry logic\"\"\"\n",
					"        logger.info(\"Fetching storage account information\")\n",
					"        try:\n",
					"            storage_account_url = mssparkutils.notebook.run(\n",
					"                '/utils/py_utils_get_storage_account'\n",
					"            ).strip()\n",
					"            \n",
					"            domain_part = storage_account_url.split('://')[1] if '://' in storage_account_url else storage_account_url\n",
					"            domain_part = domain_part.rstrip('/')\n",
					"            storage_account_name = domain_part.split('.')[0]\n",
					"            \n",
					"            self.storage_account_name = storage_account_name\n",
					"            self.storage_account_url = storage_account_url\n",
					"            \n",
					"            logger.info(f\"Storage Account: {storage_account_name}\")\n",
					"            return storage_account_name, storage_account_url\n",
					"        except Exception as e:\n",
					"            logger.error(f\"Failed to get storage account info: {str(e)}\")\n",
					"            raise\n",
					"\n",
					"    def parse_timestamp_safely(self, timestamp_value) -> Optional[datetime]:\n",
					"        \"\"\"Parse various timestamp formats with enhanced validation\"\"\"\n",
					"        try:\n",
					"            if isinstance(timestamp_value, datetime):\n",
					"                return timestamp_value.replace(tzinfo=None)\n",
					"            \n",
					"            ts_str = str(timestamp_value)\n",
					"            \n",
					"            if ts_str.isdigit() and len(ts_str) == 13:\n",
					"                dt = datetime.fromtimestamp(int(ts_str) / 1000)\n",
					"                if 2010 <= dt.year <= 2030:\n",
					"                    return dt.replace(tzinfo=None)\n",
					"            \n",
					"            elif ts_str.isdigit() and len(ts_str) == 10:\n",
					"                dt = datetime.fromtimestamp(int(ts_str))\n",
					"                if 2010 <= dt.year <= 2030:\n",
					"                    return dt.replace(tzinfo=None)\n",
					"            \n",
					"            parsed_dt = pd.to_datetime(ts_str, errors='coerce')\n",
					"            if not pd.isna(parsed_dt):\n",
					"                dt = parsed_dt.to_pydatetime().replace(tzinfo=None)\n",
					"                if 2010 <= dt.year <= 2030:\n",
					"                    return dt\n",
					"            \n",
					"            logger.warning(f\"Unparseable timestamp: {timestamp_value}\")\n",
					"            return None\n",
					"            \n",
					"        except Exception as e:\n",
					"            logger.error(f\"Timestamp parsing error: {str(e)}\")\n",
					"            return None\n",
					"\n",
					"    def detect_dependencies(self) -> Tuple[Set[str], List[str]]:\n",
					"        \"\"\"Comprehensive dependency detection with caching\"\"\"\n",
					"        logger.info(\"Starting dependency detection\")\n",
					"        \n",
					"        if hasattr(self, '_cached_dependencies'):\n",
					"            logger.info(\"Using cached dependencies\")\n",
					"            return self._cached_dependencies\n",
					"        \n",
					"        referenced_paths = set()\n",
					"        pipeline_patterns = set(self.config.get(\"dependency_patterns\", []))\n",
					"        \n",
					"        try:\n",
					"            # 1. SPARK CATALOG ANALYSIS\n",
					"            logger.info(\"Analyzing Spark catalog...\")\n",
					"            catalog_count = self._analyze_spark_catalog(referenced_paths)\n",
					"            logger.info(f\"Found {catalog_count} table references in Spark catalog\")\n",
					"            \n",
					"            # 2. SYNAPSE PIPELINE ANALYSIS\n",
					"            logger.info(\"Analyzing Synapse pipelines...\")\n",
					"            synapse_refs = self._analyze_synapse_pipelines()\n",
					"            referenced_paths.update(synapse_refs)\n",
					"            logger.info(f\"Found {len(synapse_refs)} Synapse pipeline references\")\n",
					"            \n",
					"            # 3. NOTEBOOK ANALYSIS\n",
					"            logger.info(\"Analyzing notebooks...\")\n",
					"            notebook_refs = self._analyze_notebook_dependencies()\n",
					"            referenced_paths.update(notebook_refs)\n",
					"            logger.info(f\"Found {len(notebook_refs)} notebook references\")\n",
					"            \n",
					"            # 4. CONFIG FILE ANALYSIS\n",
					"            logger.info(\"Analyzing config files...\")\n",
					"            config_refs = self._analyze_config_files()\n",
					"            referenced_paths.update(config_refs)\n",
					"            logger.info(f\"Found {len(config_refs)} config references\")\n",
					"            \n",
					"            self._cached_dependencies = (referenced_paths, list(pipeline_patterns))\n",
					"            return referenced_paths, list(pipeline_patterns)\n",
					"            \n",
					"        except Exception as e:\n",
					"            logger.error(f\"Dependency detection failed: {str(e)}\")\n",
					"            raise\n",
					"\n",
					"    def _analyze_spark_catalog(self, referenced_paths: Set[str]) -> int:\n",
					"        \"\"\"Analyze Spark catalog for storage references\"\"\"\n",
					"        catalog_count = 0\n",
					"        try:\n",
					"            databases = self.spark.sql(\"SHOW DATABASES\").collect()\n",
					"            for db in databases:\n",
					"                db_name = db[0]\n",
					"                try:\n",
					"                    tables = self.spark.sql(f\"SHOW TABLES IN {db_name}\").collect()\n",
					"                    for table in tables:\n",
					"                        table_name = table[1]\n",
					"                        try:\n",
					"                            table_desc = self.spark.sql(\n",
					"                                f\"DESCRIBE EXTENDED {db_name}.{table_name}\"\n",
					"                            ).collect()\n",
					"                            for row in table_desc:\n",
					"                                if row[0] and 'Location' in str(row[0]):\n",
					"                                    location = str(row[1])\n",
					"                                    if self.storage_account_name in location:\n",
					"                                        referenced_paths.add(location)\n",
					"                                        catalog_count += 1\n",
					"                        except Exception as e:\n",
					"                            logger.debug(f\"Could not describe table {db_name}.{table_name}: {str(e)}\")\n",
					"                            continue\n",
					"                except Exception as e:\n",
					"                    logger.debug(f\"Could not list tables in {db_name}: {str(e)}\")\n",
					"                    continue\n",
					"        except Exception as e:\n",
					"            logger.error(f\"Spark catalog analysis failed: {str(e)}\")\n",
					"        \n",
					"        return catalog_count\n",
					"\n",
					"    def _analyze_synapse_pipelines(self) -> Set[str]:\n",
					"        \"\"\"Analyze Synapse pipelines for storage references\"\"\"\n",
					"        pipeline_refs = set()\n",
					"        \n",
					"        try:\n",
					"            config_containers = ['odw-config', 'data-lake-config', 'synapse']\n",
					"            \n",
					"            for container in config_containers:\n",
					"                try:\n",
					"                    container_path = f\"abfss://{container}@{self.storage_account_name}.dfs.core.windows.net/\"\n",
					"                    files = mssparkutils.fs.ls(container_path)\n",
					"                    \n",
					"                    for file in files:\n",
					"                        if not file.isDir and file.name.lower().endswith(('.json', '.yml', '.yaml')):\n",
					"                            try:\n",
					"                                file_content = mssparkutils.fs.head(\n",
					"                                    f\"{container_path}{file.name}\", \n",
					"                                    10000\n",
					"                                )\n",
					"                                \n",
					"                                abfss_pattern = r'abfss://[^@]+@' + \\\n",
					"                                    self.storage_account_name + \\\n",
					"                                    r'\\.dfs\\.core\\.windows\\.net/[^\\s\"\\']*'\n",
					"                                matches = re.findall(abfss_pattern, file_content)\n",
					"                                pipeline_refs.update(matches)\n",
					"                                \n",
					"                                container_pattern = r'\"container\":\\s*\"([^\"]+)\"'\n",
					"                                container_matches = re.findall(container_pattern, file_content)\n",
					"                                for container_ref in container_matches:\n",
					"                                    pipeline_refs.add(\n",
					"                                        f\"abfss://{container_ref}@{self.storage_account_name}.dfs.core.windows.net/\"\n",
					"                                    )\n",
					"                                    \n",
					"                            except Exception as e:\n",
					"                                logger.debug(f\"Could not process {file.name}: {str(e)}\")\n",
					"                                continue\n",
					"                                \n",
					"                except Exception as e:\n",
					"                    if \"FilesystemNotFound\" in str(e):\n",
					"                        logger.info(f\"Container {container} not found - skipping\")\n",
					"                    else:\n",
					"                        logger.debug(f\"Could not access {container}: {str(e)}\")\n",
					"                    continue\n",
					"                \n",
					"            dataset_patterns = [\n",
					"                f\"abfss://odw-raw@{self.storage_account_name}.dfs.core.windows.net/\",\n",
					"                f\"abfss://odw-standardised@{self.storage_account_name}.dfs.core.windows.net/\",\n",
					"                f\"abfss://odw-harmonised@{self.storage_account_name}.dfs.core.windows.net/\",\n",
					"                f\"abfss://odw-curated@{self.storage_account_name}.dfs.core.windows.net/\"\n",
					"            ]\n",
					"            pipeline_refs.update(dataset_patterns)\n",
					"            \n",
					"        except Exception as e:\n",
					"            logger.error(f\"Synapse pipeline analysis failed: {str(e)}\")\n",
					"        \n",
					"        return pipeline_refs\n",
					"\n",
					"    def _analyze_notebook_dependencies(self) -> Set[str]:\n",
					"        \"\"\"Analyze notebooks for storage references\"\"\"\n",
					"        notebook_refs = set()\n",
					"        \n",
					"        try:\n",
					"            notebook_containers = ['synapse', 'temp-sap-hr-data', 'odw-temp']\n",
					"            \n",
					"            for container in notebook_containers:\n",
					"                try:\n",
					"                    container_path = f\"abfss://{container}@{self.storage_account_name}.dfs.core.windows.net/\"\n",
					"                    files = mssparkutils.fs.ls(container_path)\n",
					"                    \n",
					"                    for file in files:\n",
					"                        if any(pattern in file.name.lower() for pattern in \n",
					"                               ['checkpoint', 'output', 'result', 'temp', 'cache', '.ipynb']):\n",
					"                            notebook_refs.add(file.path)\n",
					"                            \n",
					"                except Exception as e:\n",
					"                    if \"FilesystemNotFound\" in str(e):\n",
					"                        logger.info(f\"Container {container} not found - skipping\")\n",
					"                    else:\n",
					"                        logger.debug(f\"Could not access {container}: {str(e)}\")\n",
					"                    continue\n",
					"                \n",
					"        except Exception as e:\n",
					"            logger.error(f\"Notebook analysis failed: {str(e)}\")\n",
					"        \n",
					"        return notebook_refs\n",
					"\n",
					"    def _analyze_config_files(self) -> Set[str]:\n",
					"        \"\"\"Analyze configuration files for storage references\"\"\"\n",
					"        config_refs = set()\n",
					"        \n",
					"        try:\n",
					"            config_containers = ['odw-config', 'data-lake-config', 'odw-config-db']\n",
					"            \n",
					"            for container in config_containers:\n",
					"                try:\n",
					"                    container_path = f\"abfss://{container}@{self.storage_account_name}.dfs.core.windows.net/\"\n",
					"                    files = mssparkutils.fs.ls(container_path)\n",
					"                    \n",
					"                    for file in files:\n",
					"                        if not file.isDir and file.name.lower().endswith('.json'):\n",
					"                            try:\n",
					"                                file_content = mssparkutils.fs.head(\n",
					"                                    f\"{container_path}{file.name}\", \n",
					"                                    5000\n",
					"                                )\n",
					"                                \n",
					"                                path_pattern = f'{self.storage_account_name}\\\\.dfs\\\\.core\\\\.windows\\\\.net/([^\"\\\\s]*)'\n",
					"                                matches = re.findall(path_pattern, file_content)\n",
					"                                \n",
					"                                for match in matches:\n",
					"                                    config_refs.add(\n",
					"                                        f\"abfss://unknown@{self.storage_account_name}.dfs.core.windows.net/{match}\"\n",
					"                                    )\n",
					"                                    \n",
					"                            except Exception as e:\n",
					"                                logger.debug(f\"Could not process {file.name}: {str(e)}\")\n",
					"                                continue\n",
					"                except Exception as e:\n",
					"                    if \"FilesystemNotFound\" in str(e):\n",
					"                        logger.info(f\"Container {container} not found - skipping\")\n",
					"                    else:\n",
					"                        logger.debug(f\"Could not access {container}: {str(e)}\")\n",
					"                    continue\n",
					"                \n",
					"        except Exception as e:\n",
					"            logger.error(f\"Config file analysis failed: {str(e)}\")\n",
					"        \n",
					"        return config_refs\n",
					"\n",
					"    def is_dependency(\n",
					"        self,\n",
					"        file_path: str,\n",
					"        file_name: str,\n",
					"        referenced_paths: Set[str],\n",
					"        pipeline_patterns: List[str]\n",
					"    ) -> Tuple[bool, Optional[str]]:\n",
					"        \"\"\"Enhanced dependency checking with additional patterns\"\"\"\n",
					"        file_lower = file_name.lower()\n",
					"        path_lower = file_path.lower()\n",
					"        \n",
					"        for ref_path in referenced_paths:\n",
					"            if ref_path.lower() in path_lower:\n",
					"                return True, f\"Referenced in pipeline/table: {ref_path[:50]}...\"\n",
					"        \n",
					"        for pattern in pipeline_patterns:\n",
					"            if pattern in file_lower:\n",
					"                return True, f\"Contains pipeline pattern: {pattern}\"\n",
					"        \n",
					"        system_dirs = [\n",
					"            '_delta_log', '_spark_metadata', '.checkpoints', \n",
					"            '_checkpoints', '_SUCCESS', '_committed', '_started'\n",
					"        ]\n",
					"        for sys_dir in system_dirs:\n",
					"            if sys_dir in path_lower:\n",
					"                return True, f\"System directory: {sys_dir}\"\n",
					"        \n",
					"        config_extensions = ['json', 'xml', 'yaml', 'yml', 'conf', 'properties', 'cfg']\n",
					"        if any(file_lower.endswith(f'.{ext}') for ext in config_extensions):\n",
					"            config_keywords = ['pipeline', 'config', 'schema', 'metadata', 'orchestration']\n",
					"            if any(keyword in file_lower for keyword in config_keywords):\n",
					"                return True, \"Configuration file\"\n",
					"        \n",
					"        if any(x in path_lower for x in ['/archive/', '/historical/']):\n",
					"            return False, \"Archive/historical data\"\n",
					"        \n",
					"        if any(x in file_lower for x in ['backup', 'snapshot']):\n",
					"            return True, \"Backup/snapshot file\"\n",
					"        \n",
					"        return False, None\n",
					"\n",
					"    def analyze_container(\n",
					"        self,\n",
					"        container: str\n",
					"    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
					"        \"\"\"Analyze files in a single container with error handling\"\"\"\n",
					"        logger.info(f\"Analyzing container: {container}\")\n",
					"        \n",
					"        container_threshold = self.config[\"container_thresholds\"].get(container, 120)\n",
					"        cutoff_date = datetime.now() - timedelta(days=container_threshold)\n",
					"        \n",
					"        container_files = []\n",
					"        container_stats = {\n",
					"            'total_files': 0,\n",
					"            'unused_files': 0,\n",
					"            'unused_size_mb': 0,\n",
					"            'dependencies': 0,\n",
					"            'invalid_timestamps': 0,\n",
					"            'threshold_days': container_threshold,\n",
					"            'status': 'analyzed'\n",
					"        }\n",
					"        \n",
					"        try:\n",
					"            path = f\"abfss://{container}@{self.storage_account_name}.dfs.core.windows.net/\"\n",
					"            files = mssparkutils.fs.ls(path)\n",
					"            \n",
					"            container_stats['total_files'] = len(files)\n",
					"            referenced_paths, pipeline_patterns = self.detect_dependencies()\n",
					"            \n",
					"            for file in files:\n",
					"                if not file.isDir and hasattr(file, 'modifyTime') and file.modifyTime:\n",
					"                    mod_time = self.parse_timestamp_safely(file.modifyTime)\n",
					"                    \n",
					"                    if mod_time is None:\n",
					"                        container_stats['invalid_timestamps'] += 1\n",
					"                        logger.warning(f\"Invalid timestamp: {file.name} ({file.modifyTime})\")\n",
					"                        continue\n",
					"                    \n",
					"                    file_size_mb = round(file.size / (1024*1024), 2) if hasattr(file, 'size') else 0\n",
					"                    days_old = (datetime.now() - mod_time).days\n",
					"                    \n",
					"                    is_dependency, dependency_reason = self.is_dependency(\n",
					"                        file.path, file.name, referenced_paths, pipeline_patterns\n",
					"                    )\n",
					"                    \n",
					"                    if is_dependency:\n",
					"                        container_stats['dependencies'] += 1\n",
					"                        if days_old > container_threshold:\n",
					"                            logger.info(f\"Protected dependency: {file.name} ({days_old} days) - {dependency_reason}\")\n",
					"                    \n",
					"                    elif mod_time < cutoff_date:\n",
					"                        container_stats['unused_files'] += 1\n",
					"                        container_stats['unused_size_mb'] += file_size_mb\n",
					"                        \n",
					"                        file_ext = file.name.split('.')[-1].lower() if '.' in file.name else 'no_extension'\n",
					"                        \n",
					"                        container_files.append({\n",
					"                            'container': container,\n",
					"                            'file_name': file.name,\n",
					"                            'file_path': file.path,\n",
					"                            'size_mb': file_size_mb,\n",
					"                            'size_bytes': file.size if hasattr(file, 'size') else 0,\n",
					"                            'days_old': days_old,\n",
					"                            'threshold_used': container_threshold,\n",
					"                            'last_modified': mod_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
					"                            'file_extension': file_ext,\n",
					"                            'is_dependency': is_dependency,\n",
					"                            'dependency_reason': dependency_reason or 'None'\n",
					"                        })\n",
					"            \n",
					"            logger.info(f\"Container {container} analysis complete: \"\n",
					"                       f\"{container_stats['unused_files']} unused files found\")\n",
					"            \n",
					"            return container_files, container_stats\n",
					"            \n",
					"        except Exception as e:\n",
					"            if \"FilesystemNotFound\" in str(e):\n",
					"                logger.info(f\"Container {container} not found - skipping\")\n",
					"                container_stats['status'] = 'not_found'\n",
					"                return [], container_stats\n",
					"            else:\n",
					"                logger.error(f\"Failed to analyze container {container}: {str(e)}\")\n",
					"                container_stats['status'] = 'error'\n",
					"                return [], container_stats\n",
					"\n",
					"    def analyze_all_containers(self) -> Tuple[List[Dict[str, Any]], Dict[str, Any], Set[str]]:\n",
					"        \"\"\"Analyze all containers with error handling\"\"\"\n",
					"        logger.info(\"Starting analysis of all containers\")\n",
					"        \n",
					"        containers = list(self.config[\"container_thresholds\"].keys())\n",
					"        all_files = []\n",
					"        container_stats = {}\n",
					"        \n",
					"        referenced_paths, pipeline_patterns = self.detect_dependencies()\n",
					"        \n",
					"        for container in containers:\n",
					"            files, stats = self.analyze_container(container)\n",
					"            all_files.extend(files)\n",
					"            container_stats[container] = stats\n",
					"            \n",
					"        logger.info(f\"Completed analysis of {len(containers)} containers\")\n",
					"        return all_files, container_stats, referenced_paths\n",
					"\n",
					"    def _generate_visualizations(self, df: pd.DataFrame, container_stats: Dict[str, Any]):\n",
					"        \"\"\"Generate visualizations and store as HTML embeddable images\"\"\"\n",
					"        try:\n",
					"            # Size distribution by container\n",
					"            plt.figure(figsize=(12, 6))\n",
					"            df.groupby('container')['size_mb'].sum().sort_values().plot.barh()\n",
					"            plt.title('Unused Storage by Container (MB)')\n",
					"            plt.tight_layout()\n",
					"            buf = BytesIO()\n",
					"            plt.savefig(buf, format='png')\n",
					"            plt.close()\n",
					"            self.visualizations['size_by_container'] = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
					"            \n",
					"            # File age distribution\n",
					"            plt.figure(figsize=(10, 6))\n",
					"            df['days_old'].plot.hist(bins=20)\n",
					"            plt.title('Distribution of File Ages (Days)')\n",
					"            plt.xlabel('Days Since Last Modification')\n",
					"            buf = BytesIO()\n",
					"            plt.savefig(buf, format='png')\n",
					"            plt.close()\n",
					"            self.visualizations['file_age_distribution'] = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
					"            \n",
					"            # File type distribution\n",
					"            plt.figure(figsize=(10, 6))\n",
					"            df['file_extension'].value_counts().head(10).plot.pie(autopct='%1.1f%%')\n",
					"            plt.title('Top 10 File Extensions')\n",
					"            buf = BytesIO()\n",
					"            plt.savefig(buf, format='png')\n",
					"            plt.close()\n",
					"            self.visualizations['file_extensions'] = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
					"            \n",
					"            # Save to files as well\n",
					"            os.makedirs(REPORT_DIR, exist_ok=True)\n",
					"            with open(os.path.join(REPORT_DIR, 'size_by_container.png'), 'wb') as f:\n",
					"                f.write(base64.b64decode(self.visualizations['size_by_container']))\n",
					"            with open(os.path.join(REPORT_DIR, 'file_age_distribution.png'), 'wb') as f:\n",
					"                f.write(base64.b64decode(self.visualizations['file_age_distribution']))\n",
					"            with open(os.path.join(REPORT_DIR, 'file_extensions.png'), 'wb') as f:\n",
					"                f.write(base64.b64decode(self.visualizations['file_extensions']))\n",
					"                \n",
					"        except Exception as e:\n",
					"            logger.error(f\"Failed to generate visualizations: {str(e)}\")\n",
					"            # Create blank images if visualization fails\n",
					"            blank_img = base64.b64encode(b'blank').decode('utf-8')\n",
					"            self.visualizations = {\n",
					"                'size_by_container': blank_img,\n",
					"                'file_age_distribution': blank_img,\n",
					"                'file_extensions': blank_img\n",
					"            }\n",
					"\n",
					"    def generate_report(\n",
					"        self,\n",
					"        unused_files: List[Dict[str, Any]],\n",
					"        container_stats: Dict[str, Any],\n",
					"        referenced_paths: Set[str]\n",
					"    ) -> str:\n",
					"        \"\"\"Generate comprehensive report with visualizations\"\"\"\n",
					"        logger.info(\"Generating analysis report\")\n",
					"        \n",
					"        df = pd.DataFrame(unused_files)\n",
					"        self._generate_visualizations(df, container_stats)\n",
					"        \n",
					"        total_files = len(df)\n",
					"        total_size_mb = df['size_mb'].sum()\n",
					"        total_size_gb = total_size_mb / 1024\n",
					"        \n",
					"        safe_containers = ['test', 'temp-sap-hr-data', 'odw-temp']\n",
					"        safe_files = df[df['container'].isin(safe_containers)]\n",
					"        safe_size_mb = safe_files['size_mb'].sum()\n",
					"        \n",
					"        review_files = df[~df['container'].isin(safe_containers)]\n",
					"        review_size_mb = review_files['size_mb'].sum()\n",
					"        \n",
					"        # Count container statuses\n",
					"        analyzed = sum(1 for stats in container_stats.values() if stats['status'] == 'analyzed')\n",
					"        not_found = sum(1 for stats in container_stats.values() if stats['status'] == 'not_found')\n",
					"        errors = sum(1 for stats in container_stats.values() if stats['status'] == 'error')\n",
					"        \n",
					"        # Generate HTML content\n",
					"        html_content = f\"\"\"\n",
					"        <html>\n",
					"        <head>\n",
					"            <title>Azure Storage Analysis Report</title>\n",
					"            <style>\n",
					"                body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
					"                h1, h2, h3 {{ color: #2e6c80; }}\n",
					"                .summary {{ background-color: #f5f5f5; padding: 15px; border-radius: 5px; }}\n",
					"                .container {{ margin-bottom: 30px; }}\n",
					"                table {{ border-collapse: collapse; width: 100%; }}\n",
					"                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
					"                th {{ background-color: #f2f2f2; }}\n",
					"                .safe {{ color: green; }}\n",
					"                .review {{ color: orange; }}\n",
					"                .error {{ color: red; }}\n",
					"                .warning {{ color: #FFA500; }}\n",
					"                img {{ max-width: 100%; height: auto; }}\n",
					"                .status-ok {{ background-color: #DFF0D8; }}\n",
					"                .status-warning {{ background-color: #FCF8E3; }}\n",
					"                .status-error {{ background-color: #F2DEDE; }}\n",
					"            </style>\n",
					"        </head>\n",
					"        <body>\n",
					"            <h1>Azure Storage Analysis Report</h1>\n",
					"            <p>Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
					"            <p>Storage Account: {self.storage_account_name}</p>\n",
					"            \n",
					"            <div class=\"summary\">\n",
					"                <h2>Executive Summary</h2>\n",
					"                <ul>\n",
					"                    <li>Total unused files found: <strong>{total_files:,}</strong></li>\n",
					"                    <li>Total unused storage: <strong>{total_size_mb:.1f} MB ({total_size_gb:.2f} GB)</strong></li>\n",
					"                    <li>Files in test/temp containers: <span class=\"safe\">{len(safe_files)} ({safe_size_mb:.1f} MB)</span></li>\n",
					"                    <li>Files needing review: <span class=\"review\">{len(review_files)} ({review_size_mb:.1f} MB)</span></li>\n",
					"                    <li>Pipeline dependencies protected: {sum(c['dependencies'] for c in container_stats.values() if c['status'] == 'analyzed')}</li>\n",
					"                    <li>Containers analyzed: {analyzed}</li>\n",
					"                    <li>Containers not found: <span class=\"warning\">{not_found}</span></li>\n",
					"                    <li>Containers with errors: <span class=\"error\">{errors}</span></li>\n",
					"                </ul>\n",
					"            </div>\n",
					"            \n",
					"            <h2>Visualizations</h2>\n",
					"            <div style=\"display: flex; flex-wrap: wrap;\">\n",
					"                <div style=\"flex: 50%; padding: 10px;\">\n",
					"                    <h3>Size by Container</h3>\n",
					"                    <img src=\"data:image/png;base64,{self.visualizations['size_by_container']}\" alt=\"Size by Container\">\n",
					"                </div>\n",
					"                <div style=\"flex: 50%; padding: 10px;\">\n",
					"                    <h3>File Age Distribution</h3>\n",
					"                    <img src=\"data:image/png;base64,{self.visualizations['file_age_distribution']}\" alt=\"File Age Distribution\">\n",
					"                </div>\n",
					"                <div style=\"flex: 50%; padding: 10px;\">\n",
					"                    <h3>File Extensions</h3>\n",
					"                    <img src=\"data:image/png;base64,{self.visualizations['file_extensions']}\" alt=\"File Extensions\">\n",
					"                </div>\n",
					"            </div>\n",
					"            \n",
					"            <h2>Detailed Findings</h2>\n",
					"            <div class=\"container\">\n",
					"                <h3>Containers Summary</h3>\n",
					"                <table>\n",
					"                    <tr>\n",
					"                        <th>Container</th>\n",
					"                        <th>Status</th>\n",
					"                        <th>Total Files</th>\n",
					"                        <th>Unused Files</th>\n",
					"                        <th>Unused Size (MB)</th>\n",
					"                        <th>Dependencies</th>\n",
					"                        <th>Threshold (Days)</th>\n",
					"                    </tr>\n",
					"                    {''.join(\n",
					"                        f'<tr class=\"status-{\"ok\" if stats[\"status\"] == \"analyzed\" else \"warning\" if stats[\"status\"] == \"not_found\" else \"error\"}\">'\n",
					"                        f'<td>{container}</td>'\n",
					"                        f'<td>{stats[\"status\"].capitalize()}</td>'\n",
					"                        f'<td>{stats[\"total_files\"]}</td>'\n",
					"                        f'<td>{stats[\"unused_files\"]}</td>'\n",
					"                        f'<td>{stats[\"unused_size_mb\"]:.1f}</td>'\n",
					"                        f'<td>{stats[\"dependencies\"]}</td>'\n",
					"                        f'<td>{stats[\"threshold_days\"]}</td></tr>'\n",
					"                        for container, stats in container_stats.items()\n",
					"                    )}\n",
					"                </table>\n",
					"            </div>\n",
					"            \n",
					"            <div class=\"container\">\n",
					"                <h3>Top 20 Largest Unused Files</h3>\n",
					"                <table>\n",
					"                    <tr>\n",
					"                        <th>File Name</th>\n",
					"                        <th>Container</th>\n",
					"                        <th>Size (MB)</th>\n",
					"                        <th>Age (Days)</th>\n",
					"                        <th>Last Modified</th>\n",
					"                        <th>Dependency Reason</th>\n",
					"                    </tr>\n",
					"                    {''.join(\n",
					"                        f'<tr><td>{row[\"file_name\"]}</td>'\n",
					"                        f'<td>{row[\"container\"]}</td>'\n",
					"                        f'<td>{row[\"size_mb\"]:.1f}</td>'\n",
					"                        f'<td>{row[\"days_old\"]}</td>'\n",
					"                        f'<td>{row[\"last_modified\"]}</td>'\n",
					"                        f'<td>{row[\"dependency_reason\"]}</td></tr>'\n",
					"                        for _, row in df.nlargest(20, 'size_mb').iterrows()\n",
					"                    )}\n",
					"                </table>\n",
					"            </div>\n",
					"            \n",
					"            <h2>Recommendations</h2>\n",
					"            <div class=\"container\">\n",
					"                <h3>Potential Cleanup Candidates</h3>\n",
					"                <ul>\n",
					"                    <li>{len(safe_files)} files in test/temp containers ({safe_size_mb:.1f} MB) could potentially be cleaned up</li>\n",
					"                    <li>Review the top 20 largest files identified in the report</li>\n",
					"                </ul>\n",
					"                \n",
					"                <h3>Optimization Suggestions</h3>\n",
					"                <ul>\n",
					"                    <li>Consider implementing lifecycle policies for different data categories</li>\n",
					"                    <li>Monitor storage usage regularly to identify new unused files</li>\n",
					"                    <li>Verify containers that were not found or had errors during analysis</li>\n",
					"                </ul>\n",
					"            </div>\n",
					"        </body>\n",
					"        </html>\n",
					"        \"\"\"\n",
					"        \n",
					"        # Save report to file\n",
					"        report_path = os.path.join(\n",
					"            REPORT_DIR,\n",
					"            f\"storage_analysis_report_{self.start_time.strftime('%Y%m%d_%H%M%S')}.html\"\n",
					"        )\n",
					"        \n",
					"        with open(report_path, 'w') as f:\n",
					"            f.write(html_content)\n",
					"        \n",
					"        logger.info(f\"Report generated at: {report_path}\")\n",
					"        return report_path\n",
					"\n",
					"    def display_notebook_summary(self, unused_files: List[Dict[str, Any]], container_stats: Dict[str, Any]):\n",
					"        \"\"\"Display a condensed version of the report in the notebook\"\"\"\n",
					"        df = pd.DataFrame(unused_files)\n",
					"        \n",
					"        # Display summary stats\n",
					"        display(HTML(f\"\"\"\n",
					"        <h2>Azure Storage Analysis Summary</h2>\n",
					"        <p><strong>Storage Account:</strong> {self.storage_account_name}</p>\n",
					"        <p><strong>Total Unused Files:</strong> {len(df):,}</p>\n",
					"        <p><strong>Total Unused Storage:</strong> {df['size_mb'].sum():.1f} MB</p>\n",
					"        \"\"\"))\n",
					"        \n",
					"        # Display visualizations\n",
					"        display(HTML(\"\"\"\n",
					"        <h3>Visualizations</h3>\n",
					"        <div style=\"display: flex; flex-wrap: wrap;\">\n",
					"            <div style=\"flex: 50%; padding: 10px;\">\n",
					"                <h4>Size by Container</h4>\n",
					"                <img src=\"data:image/png;base64,{}\" alt=\"Size by Container\">\n",
					"            </div>\n",
					"            <div style=\"flex: 50%; padding: 10px;\">\n",
					"                <h4>File Age Distribution</h4>\n",
					"                <img src=\"data:image/png;base64,{}\" alt=\"File Age Distribution\">\n",
					"            </div>\n",
					"        </div>\n",
					"        \"\"\".format(\n",
					"            self.visualizations['size_by_container'],\n",
					"            self.visualizations['file_age_distribution']\n",
					"        )))\n",
					"        \n",
					"        # Display top files table\n",
					"        display(HTML(\"<h3>Top 10 Largest Unused Files</h3>\"))\n",
					"        display(df.nlargest(10, 'size_mb')[['container', 'file_name', 'size_mb', 'days_old']])\n",
					"\n",
					"    def run_analysis(self):\n",
					"        \"\"\"Main execution method for analysis only\"\"\"\n",
					"        try:\n",
					"            logger.info(\"Starting Azure Storage Analysis\")\n",
					"            \n",
					"            # Get storage account info\n",
					"            self.get_storage_account_info()\n",
					"            \n",
					"            # Analyze all containers\n",
					"            unused_files, container_stats, referenced_paths = self.analyze_all_containers()\n",
					"            \n",
					"            # Generate report\n",
					"            report_path = self.generate_report(unused_files, container_stats, referenced_paths)\n",
					"            \n",
					"            # Display in notebook\n",
					"            self.display_notebook_summary(unused_files, container_stats)\n",
					"            \n",
					"            logger.info(\"Analysis completed successfully\")\n",
					"            return unused_files, container_stats, report_path\n",
					"            \n",
					"        except Exception as e:\n",
					"            logger.error(f\"Analysis failed: {str(e)}\")\n",
					"            raise\n",
					"\n",
					"# Execute the analysis\n",
					"analyzer = StorageAnalyzer()\n",
					"analysis_results = analyzer.run_analysis()\n",
					"\n",
					"# Provide direct access to results\n",
					"unused_files_df = pd.DataFrame(analysis_results[0])\n",
					"container_stats_df = pd.DataFrame(analysis_results[1]).T\n",
					"\n",
					"# Display the full report in notebook if needed\n",
					"display(HTML(filename=analysis_results[2]))"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql import SparkSession\n",
					"import json\n",
					"import re\n",
					"\n",
					"# Configuration - Different thresholds for different container types\n",
					"CONTAINER_THRESHOLDS = {\n",
					"    # Temporary data - aggressive cleanup\n",
					"    'temp-sap-hr-data': 30,\n",
					"    'odw-temp': 30,\n",
					"    \n",
					"    # Test environments - moderate cleanup\n",
					"    'test': 60,\n",
					"    'hbttestdbcontainer': 60,\n",
					"    'hbttestfilesystem': 60,\n",
					"    \n",
					"    # Log files - longer retention\n",
					"    'backup-logs': 90,\n",
					"    'logging': 90,\n",
					"    'insights-logs-builtinsqlreqsended': 90,\n",
					"    \n",
					"    # Production data - conservative cleanup\n",
					"    'odw-raw': 180,\n",
					"    'odw-curated': 180,\n",
					"    'odw-standardised': 180,\n",
					"    'odw-harmonised': 180,\n",
					"    'odw-config': 180,\n",
					"    'odw-config-db': 180,\n",
					"    'data-lake-config': 180,\n",
					"    \n",
					"    # Archive and backup - very conservative\n",
					"    's51-advice-backup': 365,\n",
					"    'saphrsdata-to-odw': 180,\n",
					"    'mipins-database': 365,\n",
					"    \n",
					"    # Other containers - default\n",
					"    'synapse': 120,\n",
					"    'dart': 120,\n",
					"    'ims-poc': 120,\n",
					"    'odw-standardised-delta': 120\n",
					"}\n",
					"\n",
					"def get_storage_account_info():\n",
					"    \"\"\"Extract storage account information\"\"\"\n",
					"    storage_account_url = mssparkutils.notebook.run('/utils/py_utils_get_storage_account').strip()\n",
					"    print(f\"Raw Storage Account URL: {storage_account_url}\")\n",
					"    \n",
					"    if '://' in storage_account_url:\n",
					"        domain_part = storage_account_url.split('://')[1]\n",
					"    else:\n",
					"        domain_part = storage_account_url\n",
					"    \n",
					"    domain_part = domain_part.rstrip('/')\n",
					"    storage_account_name = domain_part.split('.')[0]\n",
					"    \n",
					"    print(f\"Extracted Storage Account Name: {storage_account_name}\")\n",
					"    return storage_account_name, storage_account_url\n",
					"\n",
					"def parse_timestamp_safely(timestamp_value):\n",
					"    \"\"\"Parse various timestamp formats commonly found in Azure storage\"\"\"\n",
					"    try:\n",
					"        # If it's already a datetime, use it\n",
					"        if hasattr(timestamp_value, 'year'):\n",
					"            return timestamp_value\n",
					"        \n",
					"        # Convert to string for processing\n",
					"        ts_str = str(timestamp_value)\n",
					"        \n",
					"        # Check if it looks like epoch milliseconds (13 digits)\n",
					"        if ts_str.isdigit() and len(ts_str) == 13:\n",
					"            epoch_seconds = int(ts_str) / 1000\n",
					"            dt = datetime.fromtimestamp(epoch_seconds)\n",
					"            # Sanity check - should be between 2020 and 2030\n",
					"            if 2020 <= dt.year <= 2030:\n",
					"                return dt\n",
					"        \n",
					"        # Check if it looks like epoch seconds (10 digits)\n",
					"        elif ts_str.isdigit() and len(ts_str) == 10:\n",
					"            dt = datetime.fromtimestamp(int(ts_str))\n",
					"            if 2020 <= dt.year <= 2030:\n",
					"                return dt\n",
					"        \n",
					"        # Try pandas parsing as fallback\n",
					"        parsed_dt = pd.to_datetime(timestamp_value, errors='coerce')\n",
					"        if not pd.isna(parsed_dt):\n",
					"            dt = parsed_dt.to_pydatetime().replace(tzinfo=None)\n",
					"            if 2020 <= dt.year <= 2030:\n",
					"                return dt\n",
					"        \n",
					"        return None\n",
					"        \n",
					"    except Exception as e:\n",
					"        return None\n",
					"\n",
					"def detect_comprehensive_dependencies(storage_account_name):\n",
					"    \"\"\"Comprehensive dependency detection including Synapse pipelines\"\"\"\n",
					"    print(\" Detecting comprehensive pipeline dependencies...\")\n",
					"    \n",
					"    referenced_paths = set()\n",
					"    pipeline_patterns = set()\n",
					"    synapse_dependencies = set()\n",
					"    notebook_dependencies = set()\n",
					"    \n",
					"    try:\n",
					"        spark = SparkSession.builder.getOrCreate()\n",
					"        \n",
					"        # 1. SPARK CATALOG ANALYSIS\n",
					"        print(\"   Analyzing Spark catalog...\")\n",
					"        try:\n",
					"            databases = spark.sql(\"SHOW DATABASES\").collect()\n",
					"            catalog_count = 0\n",
					"            for db in databases:\n",
					"                db_name = db[0]\n",
					"                try:\n",
					"                    tables = spark.sql(f\"SHOW TABLES IN {db_name}\").collect()\n",
					"                    for table in tables:\n",
					"                        table_name = table[1]\n",
					"                        try:\n",
					"                            table_desc = spark.sql(f\"DESCRIBE EXTENDED {db_name}.{table_name}\").collect()\n",
					"                            for row in table_desc:\n",
					"                                if row[0] and 'Location' in str(row[0]):\n",
					"                                    location = str(row[1])\n",
					"                                    if storage_account_name in location:\n",
					"                                        referenced_paths.add(location)\n",
					"                                        catalog_count += 1\n",
					"                        except:\n",
					"                            continue\n",
					"                except:\n",
					"                    continue\n",
					"            print(f\"     Found {catalog_count} table references in Spark catalog\")\n",
					"        except Exception as catalog_error:\n",
					"            print(f\"     Could not check catalog: {str(catalog_error)}\")\n",
					"        \n",
					"        # 2. SYNAPSE PIPELINE ANALYSIS\n",
					"        print(\"   Analyzing Synapse pipeline configurations...\")\n",
					"        synapse_refs = analyze_synapse_pipelines(storage_account_name)\n",
					"        referenced_paths.update(synapse_refs)\n",
					"        print(f\"     Found {len(synapse_refs)} Synapse pipeline references\")\n",
					"        \n",
					"        # 3. NOTEBOOK ANALYSIS\n",
					"        print(\"   Analyzing notebook dependencies...\")\n",
					"        notebook_refs = analyze_notebook_dependencies(storage_account_name)\n",
					"        referenced_paths.update(notebook_refs)\n",
					"        print(f\"     Found {len(notebook_refs)} notebook references\")\n",
					"        \n",
					"        # 4. CONFIG FILE ANALYSIS\n",
					"        print(\"   Analyzing configuration files...\")\n",
					"        config_refs = analyze_config_files(storage_account_name)\n",
					"        referenced_paths.update(config_refs)\n",
					"        print(f\"     Found {len(config_refs)} configuration references\")\n",
					"        \n",
					"        # 5. STANDARD PIPELINE PATTERNS\n",
					"        standard_patterns = [\n",
					"            'pipeline', 'etl', 'config', 'schema', 'metadata', 'checkpoint', \n",
					"            'watermark', 'state', 'offset', 'manifest', 'success', '_started', \n",
					"            '_committed', '_delta_log', '_spark_metadata', '.checkpoints',\n",
					"            'orchestration', 'workflow', 'jobconfig', 'dataflow'\n",
					"        ]\n",
					"        pipeline_patterns.update(standard_patterns)\n",
					"        \n",
					"        print(f\"   Total dependencies detected: {len(referenced_paths)}\")\n",
					"        print(f\"   Pipeline patterns: {len(pipeline_patterns)}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"   Error in dependency detection: {str(e)}\")\n",
					"    \n",
					"    return referenced_paths, list(pipeline_patterns)\n",
					"\n",
					"def analyze_synapse_pipelines(storage_account_name):\n",
					"    \"\"\"Analyze Synapse pipelines for storage dependencies\"\"\"\n",
					"    pipeline_refs = set()\n",
					"    \n",
					"    try:\n",
					"        # Try to access pipeline metadata through Synapse APIs\n",
					"        # Note: This requires appropriate permissions\n",
					"        \n",
					"        # Method 1: Check for pipeline configuration files\n",
					"        config_containers = ['odw-config', 'data-lake-config', 'synapse']\n",
					"        \n",
					"        for container in config_containers:\n",
					"            try:\n",
					"                container_path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\"\n",
					"                files = mssparkutils.fs.ls(container_path)\n",
					"                \n",
					"                for file in files:\n",
					"                    if not file.isDir and file.name.lower().endswith(('.json', '.yml', '.yaml')):\n",
					"                        try:\n",
					"                            # Read configuration file\n",
					"                            file_content = mssparkutils.fs.head(f\"{container_path}{file.name}\", 10000)\n",
					"                            \n",
					"                            # Look for ABFSS paths in config\n",
					"                            abfss_pattern = r'abfss://[^@]+@' + storage_account_name + r'\\.dfs\\.core\\.windows\\.net/[^\\s\"\\']*'\n",
					"                            matches = re.findall(abfss_pattern, file_content)\n",
					"                            pipeline_refs.update(matches)\n",
					"                            \n",
					"                            # Look for container references\n",
					"                            container_pattern = r'\"container\":\\s*\"([^\"]+)\"'\n",
					"                            container_matches = re.findall(container_pattern, file_content)\n",
					"                            for container_ref in container_matches:\n",
					"                                pipeline_refs.add(f\"abfss://{container_ref}@{storage_account_name}.dfs.core.windows.net/\")\n",
					"                                \n",
					"                        except Exception as file_error:\n",
					"                            continue\n",
					"                            \n",
					"            except Exception as container_error:\n",
					"                continue\n",
					"        \n",
					"        # Method 2: Check for common pipeline dataset patterns\n",
					"        dataset_patterns = [\n",
					"            f\"abfss://odw-raw@{storage_account_name}.dfs.core.windows.net/\",\n",
					"            f\"abfss://odw-standardised@{storage_account_name}.dfs.core.windows.net/\",\n",
					"            f\"abfss://odw-harmonised@{storage_account_name}.dfs.core.windows.net/\",\n",
					"            f\"abfss://odw-curated@{storage_account_name}.dfs.core.windows.net/\"\n",
					"        ]\n",
					"        pipeline_refs.update(dataset_patterns)\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"     Synapse pipeline analysis limited: {str(e)}\")\n",
					"    \n",
					"    return pipeline_refs\n",
					"\n",
					"def analyze_notebook_dependencies(storage_account_name):\n",
					"    \"\"\"Analyze notebooks for storage dependencies\"\"\"\n",
					"    notebook_refs = set()\n",
					"    \n",
					"    try:\n",
					"        # Check for notebook outputs and temporary files\n",
					"        notebook_containers = ['synapse', 'temp-sap-hr-data', 'odw-temp']\n",
					"        \n",
					"        for container in notebook_containers:\n",
					"            try:\n",
					"                container_path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\"\n",
					"                files = mssparkutils.fs.ls(container_path)\n",
					"                \n",
					"                for file in files:\n",
					"                    # Notebooks often create checkpoint and output files\n",
					"                    if any(pattern in file.name.lower() for pattern in \n",
					"                           ['checkpoint', 'output', 'result', 'temp', 'cache', '.ipynb']):\n",
					"                        notebook_refs.add(file.path)\n",
					"                        \n",
					"            except:\n",
					"                continue\n",
					"                \n",
					"    except Exception as e:\n",
					"        print(f\"     Notebook analysis limited: {str(e)}\")\n",
					"    \n",
					"    return notebook_refs\n",
					"\n",
					"def analyze_config_files(storage_account_name):\n",
					"    \"\"\"Analyze configuration files for storage references\"\"\"\n",
					"    config_refs = set()\n",
					"    \n",
					"    try:\n",
					"        # Look in configuration containers\n",
					"        config_containers = ['odw-config', 'data-lake-config', 'odw-config-db']\n",
					"        \n",
					"        for container in config_containers:\n",
					"            try:\n",
					"                container_path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\"\n",
					"                files = mssparkutils.fs.ls(container_path)\n",
					"                \n",
					"                for file in files:\n",
					"                    if not file.isDir and file.name.lower().endswith('.json'):\n",
					"                        try:\n",
					"                            # Read and parse JSON config\n",
					"                            file_content = mssparkutils.fs.head(f\"{container_path}{file.name}\", 5000)\n",
					"                            \n",
					"                            # Extract any storage paths\n",
					"                            path_pattern = f'{storage_account_name}\\\\.dfs\\\\.core\\\\.windows\\\\.net/([^\"\\\\s]*)'\n",
					"                            matches = re.findall(path_pattern, file_content)\n",
					"                            \n",
					"                            for match in matches:\n",
					"                                config_refs.add(f\"abfss://unknown@{storage_account_name}.dfs.core.windows.net/{match}\")\n",
					"                                \n",
					"                        except:\n",
					"                            continue\n",
					"            except:\n",
					"                continue\n",
					"                \n",
					"    except Exception as e:\n",
					"        print(f\"     Config file analysis limited: {str(e)}\")\n",
					"    \n",
					"    return config_refs\n",
					"\n",
					"def is_pipeline_dependency(file_path, file_name, referenced_paths, pipeline_patterns):\n",
					"    \"\"\"Enhanced dependency checking\"\"\"\n",
					"    file_lower = file_name.lower()\n",
					"    path_lower = file_path.lower()\n",
					"    \n",
					"    # 1. Check against discovered pipeline references\n",
					"    for ref_path in referenced_paths:\n",
					"        if ref_path.lower() in path_lower:\n",
					"            return True, f\"Referenced in pipeline/table: {ref_path[:50]}...\"\n",
					"    \n",
					"    # 2. Check for pipeline patterns in filename\n",
					"    for pattern in pipeline_patterns:\n",
					"        if pattern in file_lower:\n",
					"            return True, f\"Contains pipeline pattern: {pattern}\"\n",
					"    \n",
					"    # 3. Check for system directories\n",
					"    system_dirs = ['_delta_log', '_spark_metadata', '.checkpoints', '_checkpoints', \n",
					"                   '_SUCCESS', '_committed', '_started']\n",
					"    for sys_dir in system_dirs:\n",
					"        if sys_dir in path_lower:\n",
					"            return True, f\"System directory: {sys_dir}\"\n",
					"    \n",
					"    # 4. Check for configuration files\n",
					"    config_extensions = ['json', 'xml', 'yaml', 'yml', 'conf', 'properties', 'cfg']\n",
					"    if any(file_lower.endswith(f'.{ext}') for ext in config_extensions):\n",
					"        # Additional check for actual config content\n",
					"        config_keywords = ['pipeline', 'config', 'schema', 'metadata', 'orchestration']\n",
					"        if any(keyword in file_lower for keyword in config_keywords):\n",
					"            return True, \"Configuration file\"\n",
					"    \n",
					"    # 5. Check for recently accessed files (likely active)\n",
					"    # Files modified within last 7 days are probably active\n",
					"    try:\n",
					"        # This would need actual last access time, which isn't always available\n",
					"        pass\n",
					"    except:\n",
					"        pass\n",
					"    \n",
					"    return False, None\n",
					"\n",
					"def analyze_containers_comprehensive(storage_account_name):\n",
					"    \"\"\"Comprehensive analysis with all dependency detection methods\"\"\"\n",
					"    containers = [\n",
					"        'odw-temp', 'test', 'backup-logs', 'logging', \n",
					"        'odw-raw', 'odw-curated', 'temp-sap-hr-data',\n",
					"        'odw-standardised', 'odw-harmonised', 'synapse',\n",
					"        'dart', 'ims-poc', 's51-advice-backup', 'odw-config',\n",
					"        'data-lake-config', 'mipins-database', 'odw-config-db',\n",
					"        'odw-standardised-delta', 'hbttestdbcontainer', 'hbttestfilesystem',\n",
					"        'insights-logs-builtinsqlreqsended'\n",
					"    ]\n",
					"    \n",
					"    # Comprehensive dependency detection\n",
					"    referenced_paths, pipeline_patterns = detect_comprehensive_dependencies(storage_account_name)\n",
					"    \n",
					"    all_files = []\n",
					"    container_stats = {}\n",
					"    \n",
					"    print(f\"\\n COMPREHENSIVE FILE ANALYSIS\")\n",
					"    print(\"=\" * 80)\n",
					"    \n",
					"    for container in containers:\n",
					"        container_threshold = CONTAINER_THRESHOLDS.get(container, 120)\n",
					"        cutoff_date = datetime.now() - timedelta(days=container_threshold)\n",
					"        \n",
					"        try:\n",
					"            path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\"\n",
					"            files = mssparkutils.fs.ls(path)\n",
					"            \n",
					"            container_files = 0\n",
					"            container_unused = 0\n",
					"            container_dependencies = 0\n",
					"            container_invalid_timestamps = 0\n",
					"            container_size = 0\n",
					"            \n",
					"            print(f\"\\n {container} (>{container_threshold} days, before {cutoff_date.strftime('%Y-%m-%d')})\")\n",
					"            print(f\"    Total items: {len(files)}\")\n",
					"            \n",
					"            for file in files:\n",
					"                if not file.isDir and hasattr(file, 'modifyTime') and file.modifyTime:\n",
					"                    container_files += 1\n",
					"                    \n",
					"                    # Enhanced timestamp parsing\n",
					"                    mod_time = parse_timestamp_safely(file.modifyTime)\n",
					"                    \n",
					"                    if mod_time is None:\n",
					"                        container_invalid_timestamps += 1\n",
					"                        print(f\"     Invalid timestamp: {file.name} ({file.modifyTime})\")\n",
					"                        continue\n",
					"                    \n",
					"                    file_size_mb = round(file.size / (1024*1024), 2) if hasattr(file, 'size') else 0\n",
					"                    days_old = (datetime.now() - mod_time).days\n",
					"                    \n",
					"                    # Check if file is a pipeline dependency\n",
					"                    is_dependency, dependency_reason = is_pipeline_dependency(\n",
					"                        file.path, file.name, referenced_paths, pipeline_patterns\n",
					"                    )\n",
					"                    \n",
					"                    if is_dependency:\n",
					"                        container_dependencies += 1\n",
					"                        if days_old > container_threshold:\n",
					"                            print(f\"     Protected dependency: {file.name} ({days_old} days) - {dependency_reason}\")\n",
					"                    \n",
					"                    elif mod_time < cutoff_date:\n",
					"                        container_unused += 1\n",
					"                        container_size += file_size_mb\n",
					"                        \n",
					"                        file_ext = file.name.split('.')[-1].lower() if '.' in file.name else 'no_extension'\n",
					"                        \n",
					"                        all_files.append({\n",
					"                            'container': container,\n",
					"                            'file_name': file.name,\n",
					"                            'file_path': file.path,\n",
					"                            'size_mb': file_size_mb,\n",
					"                            'size_bytes': file.size if hasattr(file, 'size') else 0,\n",
					"                            'days_old': days_old,\n",
					"                            'threshold_used': container_threshold,\n",
					"                            'last_modified': mod_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
					"                            'file_extension': file_ext,\n",
					"                            'is_dependency': is_dependency,\n",
					"                            'dependency_reason': dependency_reason or 'None'\n",
					"                        })\n",
					"            \n",
					"            container_stats[container] = {\n",
					"                'total_files': container_files,\n",
					"                'unused_files': container_unused,\n",
					"                'unused_size_mb': container_size,\n",
					"                'dependencies': container_dependencies,\n",
					"                'invalid_timestamps': container_invalid_timestamps,\n",
					"                'threshold_days': container_threshold\n",
					"            }\n",
					"            \n",
					"            # Summary for container\n",
					"            status_parts = []\n",
					"            if container_unused > 0:\n",
					"                status_parts.append(f\" {container_unused} unused files ({container_size:.1f} MB)\")\n",
					"            if container_dependencies > 0:\n",
					"                status_parts.append(f\" {container_dependencies} protected\")\n",
					"            if container_invalid_timestamps > 0:\n",
					"                status_parts.append(f\" {container_invalid_timestamps} invalid timestamps\")\n",
					"            \n",
					"            if status_parts:\n",
					"                print(f\"    \" + \" | \".join(status_parts))\n",
					"            else:\n",
					"                print(f\"     Clean - no unused files found\")\n",
					"                \n",
					"        except Exception as e:\n",
					"            print(f\" Error with {container}: {str(e)}\")\n",
					"            container_stats[container] = {\n",
					"                'total_files': 0, 'unused_files': 0, 'unused_size_mb': 0, \n",
					"                'dependencies': 0, 'invalid_timestamps': 0, 'threshold_days': container_threshold\n",
					"            }\n",
					"    \n",
					"    return all_files, container_stats, referenced_paths\n",
					"\n",
					"def generate_comprehensive_report(unused_files, container_stats, referenced_paths):\n",
					"    \"\"\"Generate detailed analysis report\"\"\"\n",
					"    \n",
					"    print(f\"\\n\" + \"=\"*80)\n",
					"    print(f\" COMPREHENSIVE STORAGE ANALYSIS REPORT\")\n",
					"    print(f\"=\"*80)\n",
					"    print(f\" Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
					"    print(f\" Dependencies detected: {len(referenced_paths)}\")\n",
					"    \n",
					"    # Overall statistics\n",
					"    total_containers = len(container_stats)\n",
					"    containers_with_unused = len([c for c in container_stats.values() if c['unused_files'] > 0])\n",
					"    total_dependencies = sum(c['dependencies'] for c in container_stats.values())\n",
					"    total_invalid_timestamps = sum(c['invalid_timestamps'] for c in container_stats.values())\n",
					"    \n",
					"    print(f\"\\n OVERVIEW:\")\n",
					"    print(f\"   Containers analyzed: {total_containers}\")\n",
					"    print(f\"   Total dependencies protected: {total_dependencies}\")\n",
					"    print(f\"   Files with timestamp issues: {total_invalid_timestamps}\")\n",
					"    print(f\"   Containers with unused files: {containers_with_unused}\")\n",
					"    \n",
					"    if not unused_files:\n",
					"        print(f\"\\n EXCELLENT! No unused files found!\")\n",
					"        print(f\"\\n Your storage is exceptionally well-maintained:\")\n",
					"        print(f\"   All files are actively used or protected by dependencies\")\n",
					"        print(f\"   Automated cleanup processes are working effectively\")\n",
					"        print(f\"   Data governance policies are properly implemented\")\n",
					"        \n",
					"        # Show threshold effectiveness\n",
					"        print(f\"\\n THRESHOLD SUMMARY:\")\n",
					"        for container, stats in container_stats.items():\n",
					"            threshold = stats['threshold_days']\n",
					"            deps = stats['dependencies']\n",
					"            invalid = stats['invalid_timestamps']\n",
					"            \n",
					"            status_icon = \"\"\n",
					"            if deps > 5:\n",
					"                status_icon = \"\"\n",
					"            elif invalid > 0:\n",
					"                status_icon = \"\"\n",
					"            \n",
					"            print(f\"  {status_icon} {container}: {threshold} days | {deps} protected | {invalid} timestamp issues\")\n",
					"        \n",
					"        return\n",
					"    \n",
					"    # Detailed analysis for unused files\n",
					"    df = pd.DataFrame(unused_files)\n",
					"    total_files = len(df)\n",
					"    total_size_mb = df['size_mb'].sum()\n",
					"    total_size_gb = total_size_mb / 1024\n",
					"    \n",
					"    print(f\"\\n UNUSED FILES FOUND:\")\n",
					"    print(f\"   Total unused files: {total_files:,}\")\n",
					"    print(f\"   Total size: {total_size_mb:.1f} MB ({total_size_gb:.2f} GB)\")\n",
					"    \n",
					"    # Container breakdown\n",
					"    print(f\"\\n CONTAINER BREAKDOWN:\")\n",
					"    container_summary = df.groupby('container').agg({\n",
					"        'file_name': 'count',\n",
					"        'size_mb': 'sum',\n",
					"        'threshold_used': 'first'\n",
					"    }).rename(columns={'file_name': 'file_count'})\n",
					"    \n",
					"    for container, row in container_summary.iterrows():\n",
					"        deps = container_stats.get(container, {}).get('dependencies', 0)\n",
					"        threshold = int(row['threshold_used'])\n",
					"        \n",
					"        safety_level = \" SAFE\" if container in ['test', 'temp-sap-hr-data', 'odw-temp'] else \" REVIEW\"\n",
					"        \n",
					"        print(f\"   {container}: {safety_level}\")\n",
					"        print(f\"      {row['file_count']} unused files, {row['size_mb']:.1f} MB\")\n",
					"        print(f\"      Threshold: {threshold} days\")\n",
					"        print(f\"      Protected dependencies: {deps}\")\n",
					"    \n",
					"    # Recommendations\n",
					"    print(f\"\\n RECOMMENDATIONS:\")\n",
					"    \n",
					"    safe_containers = ['test', 'temp-sap-hr-data', 'odw-temp']\n",
					"    safe_files = df[df['container'].isin(safe_containers)]\n",
					"    \n",
					"    if not safe_files.empty:\n",
					"        print(f\"   SAFE TO DELETE: {len(safe_files)} files ({safe_files['size_mb'].sum():.1f} MB)\")\n",
					"        for container in safe_containers:\n",
					"            container_files = safe_files[safe_files['container'] == container]\n",
					"            if not container_files.empty:\n",
					"                print(f\"      {container}: {len(container_files)} files\")\n",
					"    \n",
					"    review_files = df[~df['container'].isin(safe_containers)]\n",
					"    if not review_files.empty:\n",
					"        print(f\"   REVIEW FIRST: {len(review_files)} files ({review_files['size_mb'].sum():.1f} MB)\")\n",
					"    \n",
					"    # Next steps\n",
					"    print(f\"\\n NEXT STEPS:\")\n",
					"    print(f\"  1.  Review the {total_dependencies} protected dependencies\")\n",
					"    print(f\"  2.  Start cleanup with safe containers (test, temp)\")\n",
					"    print(f\"  3.  Document any files you delete\")\n",
					"    print(f\"  4.  Set up automated cleanup policies\")\n",
					"    print(f\"  5.  Run this analysis monthly\")\n",
					"\n",
					"def main():\n",
					"    \"\"\"Main comprehensive analysis function\"\"\"\n",
					"    print(\" COMPREHENSIVE AZURE STORAGE ANALYSIS\")\n",
					"    print(\" Enhanced Pipeline Dependency Detection\")\n",
					"    print(\" Smart Timestamp Parsing\")\n",
					"    print(\" Container-Specific Thresholds\")\n",
					"    print(\"=\"*80)\n",
					"    \n",
					"    try:\n",
					"        # Get storage account info\n",
					"        storage_account_name, _ = get_storage_account_info()\n",
					"        \n",
					"        # Run comprehensive analysis\n",
					"        unused_files, container_stats, referenced_paths = analyze_containers_comprehensive(storage_account_name)\n",
					"        \n",
					"        # Generate detailed report\n",
					"        generate_comprehensive_report(unused_files, container_stats, referenced_paths)\n",
					"        \n",
					"        print(f\"\\n Comprehensive analysis complete!\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"\\n Analysis failed: {str(e)}\")\n",
					"        import traceback\n",
					"        traceback.print_exc()\n",
					"\n",
					"# Run the comprehensive analysis\n",
					"if __name__ == \"__main__\":\n",
					"    main()"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"source": [
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql import SparkSession\n",
					"import json\n",
					"import re\n",
					"\n",
					"# Configuration - Different thresholds for different container types\n",
					"CONTAINER_THRESHOLDS = {\n",
					"    # Temporary data - aggressive cleanup\n",
					"    'temp-sap-hr-data': 30,\n",
					"    'odw-temp': 30,\n",
					"    \n",
					"    # Test environments - moderate cleanup\n",
					"    'test': 60,\n",
					"    'hbttestdbcontainer': 60,\n",
					"    'hbttestfilesystem': 60,\n",
					"    \n",
					"    # Log files - longer retention\n",
					"    'backup-logs': 90,\n",
					"    'logging': 90,\n",
					"    'insights-logs-builtinsqlreqsended': 90,\n",
					"    \n",
					"    # Production data - conservative cleanup\n",
					"    'odw-raw': 180,\n",
					"    'odw-curated': 180,\n",
					"    'odw-standardised': 180,\n",
					"    'odw-harmonised': 180,\n",
					"    'odw-config': 180,\n",
					"    'odw-config-db': 180,\n",
					"    'data-lake-config': 180,\n",
					"    \n",
					"    # Archive and backup - very conservative\n",
					"    's51-advice-backup': 365,\n",
					"    'saphrsdata-to-odw': 180,\n",
					"    'mipins-database': 365,\n",
					"    \n",
					"    # Other containers - default\n",
					"    'synapse': 120,\n",
					"    'dart': 120,\n",
					"    'ims-poc': 120,\n",
					"    'odw-standardised-delta': 120\n",
					"}\n",
					"\n",
					"def get_storage_account_info():\n",
					"    \"\"\"Extract storage account information\"\"\"\n",
					"    storage_account_url = mssparkutils.notebook.run('/utils/py_utils_get_storage_account').strip()\n",
					"    print(f\"Raw Storage Account URL: {storage_account_url}\")\n",
					"    \n",
					"    if '://' in storage_account_url:\n",
					"        domain_part = storage_account_url.split('://')[1]\n",
					"    else:\n",
					"        domain_part = storage_account_url\n",
					"    \n",
					"    domain_part = domain_part.rstrip('/')\n",
					"    storage_account_name = domain_part.split('.')[0]\n",
					"    \n",
					"    print(f\"Extracted Storage Account Name: {storage_account_name}\")\n",
					"    return storage_account_name, storage_account_url\n",
					"\n",
					"def parse_timestamp_safely(timestamp_value):\n",
					"    \"\"\"Parse various timestamp formats commonly found in Azure storage\"\"\"\n",
					"    try:\n",
					"        # If it's already a datetime, use it\n",
					"        if hasattr(timestamp_value, 'year'):\n",
					"            return timestamp_value\n",
					"        \n",
					"        # Convert to string for processing\n",
					"        ts_str = str(timestamp_value)\n",
					"        \n",
					"        # Check if it looks like epoch milliseconds (13 digits)\n",
					"        if ts_str.isdigit() and len(ts_str) == 13:\n",
					"            epoch_seconds = int(ts_str) / 1000\n",
					"            dt = datetime.fromtimestamp(epoch_seconds)\n",
					"            # Sanity check - should be between 2020 and 2030\n",
					"            if 2020 <= dt.year <= 2030:\n",
					"                return dt\n",
					"        \n",
					"        # Check if it looks like epoch seconds (10 digits)\n",
					"        elif ts_str.isdigit() and len(ts_str) == 10:\n",
					"            dt = datetime.fromtimestamp(int(ts_str))\n",
					"            if 2020 <= dt.year <= 2030:\n",
					"                return dt\n",
					"        \n",
					"        # Try pandas parsing as fallback\n",
					"        parsed_dt = pd.to_datetime(timestamp_value, errors='coerce')\n",
					"        if not pd.isna(parsed_dt):\n",
					"            dt = parsed_dt.to_pydatetime().replace(tzinfo=None)\n",
					"            if 2020 <= dt.year <= 2030:\n",
					"                return dt\n",
					"        \n",
					"        return None\n",
					"        \n",
					"    except Exception as e:\n",
					"        return None\n",
					"\n",
					"def detect_comprehensive_dependencies(storage_account_name):\n",
					"    \"\"\"Comprehensive dependency detection including Synapse pipelines\"\"\"\n",
					"    print(\" Detecting comprehensive pipeline dependencies...\")\n",
					"    \n",
					"    referenced_paths = set()\n",
					"    pipeline_patterns = set()\n",
					"    synapse_dependencies = set()\n",
					"    notebook_dependencies = set()\n",
					"    \n",
					"    try:\n",
					"        spark = SparkSession.builder.getOrCreate()\n",
					"        \n",
					"        # 1. SPARK CATALOG ANALYSIS\n",
					"        print(\"   Analyzing Spark catalog...\")\n",
					"        try:\n",
					"            databases = spark.sql(\"SHOW DATABASES\").collect()\n",
					"            catalog_count = 0\n",
					"            for db in databases:\n",
					"                db_name = db[0]\n",
					"                try:\n",
					"                    tables = spark.sql(f\"SHOW TABLES IN {db_name}\").collect()\n",
					"                    for table in tables:\n",
					"                        table_name = table[1]\n",
					"                        try:\n",
					"                            table_desc = spark.sql(f\"DESCRIBE EXTENDED {db_name}.{table_name}\").collect()\n",
					"                            for row in table_desc:\n",
					"                                if row[0] and 'Location' in str(row[0]):\n",
					"                                    location = str(row[1])\n",
					"                                    if storage_account_name in location:\n",
					"                                        referenced_paths.add(location)\n",
					"                                        catalog_count += 1\n",
					"                        except:\n",
					"                            continue\n",
					"                except:\n",
					"                    continue\n",
					"            print(f\"     Found {catalog_count} table references in Spark catalog\")\n",
					"        except Exception as catalog_error:\n",
					"            print(f\"     Could not check catalog: {str(catalog_error)}\")\n",
					"        \n",
					"        # 2. SYNAPSE PIPELINE ANALYSIS\n",
					"        print(\"   Analyzing Synapse pipeline configurations...\")\n",
					"        synapse_refs = analyze_synapse_pipelines(storage_account_name)\n",
					"        referenced_paths.update(synapse_refs)\n",
					"        print(f\"     Found {len(synapse_refs)} Synapse pipeline references\")\n",
					"        \n",
					"        # 3. NOTEBOOK ANALYSIS\n",
					"        print(\"   Analyzing notebook dependencies...\")\n",
					"        notebook_refs = analyze_notebook_dependencies(storage_account_name)\n",
					"        referenced_paths.update(notebook_refs)\n",
					"        print(f\"     Found {len(notebook_refs)} notebook references\")\n",
					"        \n",
					"        # 4. CONFIG FILE ANALYSIS\n",
					"        print(\"   Analyzing configuration files...\")\n",
					"        config_refs = analyze_config_files(storage_account_name)\n",
					"        referenced_paths.update(config_refs)\n",
					"        print(f\"     Found {len(config_refs)} configuration references\")\n",
					"        \n",
					"        # 5. STANDARD PIPELINE PATTERNS\n",
					"        standard_patterns = [\n",
					"            'pipeline', 'etl', 'config', 'schema', 'metadata', 'checkpoint', \n",
					"            'watermark', 'state', 'offset', 'manifest', 'success', '_started', \n",
					"            '_committed', '_delta_log', '_spark_metadata', '.checkpoints',\n",
					"            'orchestration', 'workflow', 'jobconfig', 'dataflow'\n",
					"        ]\n",
					"        pipeline_patterns.update(standard_patterns)\n",
					"        \n",
					"        print(f\"   Total dependencies detected: {len(referenced_paths)}\")\n",
					"        print(f\"   Pipeline patterns: {len(pipeline_patterns)}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"   Error in dependency detection: {str(e)}\")\n",
					"    \n",
					"    return referenced_paths, list(pipeline_patterns)\n",
					"\n",
					"def analyze_synapse_pipelines(storage_account_name):\n",
					"    \"\"\"Analyze Synapse pipelines for storage dependencies\"\"\"\n",
					"    pipeline_refs = set()\n",
					"    \n",
					"    try:\n",
					"        # Try to access pipeline metadata through Synapse APIs\n",
					"        # Note: This requires appropriate permissions\n",
					"        \n",
					"        # Method 1: Check for pipeline configuration files\n",
					"        config_containers = ['odw-config', 'data-lake-config', 'synapse']\n",
					"        \n",
					"        for container in config_containers:\n",
					"            try:\n",
					"                container_path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\"\n",
					"                files = mssparkutils.fs.ls(container_path)\n",
					"                \n",
					"                for file in files:\n",
					"                    if not file.isDir and file.name.lower().endswith(('.json', '.yml', '.yaml')):\n",
					"                        try:\n",
					"                            # Read configuration file\n",
					"                            file_content = mssparkutils.fs.head(f\"{container_path}{file.name}\", 10000)\n",
					"                            \n",
					"                            # Look for ABFSS paths in config\n",
					"                            abfss_pattern = r'abfss://[^@]+@' + storage_account_name + r'\\.dfs\\.core\\.windows\\.net/[^\\s\"\\']*'\n",
					"                            matches = re.findall(abfss_pattern, file_content)\n",
					"                            pipeline_refs.update(matches)\n",
					"                            \n",
					"                            # Look for container references\n",
					"                            container_pattern = r'\"container\":\\s*\"([^\"]+)\"'\n",
					"                            container_matches = re.findall(container_pattern, file_content)\n",
					"                            for container_ref in container_matches:\n",
					"                                pipeline_refs.add(f\"abfss://{container_ref}@{storage_account_name}.dfs.core.windows.net/\")\n",
					"                                \n",
					"                        except Exception as file_error:\n",
					"                            continue\n",
					"                            \n",
					"            except Exception as container_error:\n",
					"                continue\n",
					"        \n",
					"        # Method 2: Check for common pipeline dataset patterns\n",
					"        dataset_patterns = [\n",
					"            f\"abfss://odw-raw@{storage_account_name}.dfs.core.windows.net/\",\n",
					"            f\"abfss://odw-standardised@{storage_account_name}.dfs.core.windows.net/\",\n",
					"            f\"abfss://odw-harmonised@{storage_account_name}.dfs.core.windows.net/\",\n",
					"            f\"abfss://odw-curated@{storage_account_name}.dfs.core.windows.net/\"\n",
					"        ]\n",
					"        pipeline_refs.update(dataset_patterns)\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"     Synapse pipeline analysis limited: {str(e)}\")\n",
					"    \n",
					"    return pipeline_refs\n",
					"\n",
					"def analyze_notebook_dependencies(storage_account_name):\n",
					"    \"\"\"Analyze notebooks for storage dependencies\"\"\"\n",
					"    notebook_refs = set()\n",
					"    \n",
					"    try:\n",
					"        # Check for notebook outputs and temporary files\n",
					"        notebook_containers = ['synapse', 'temp-sap-hr-data', 'odw-temp']\n",
					"        \n",
					"        for container in notebook_containers:\n",
					"            try:\n",
					"                container_path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\"\n",
					"                files = mssparkutils.fs.ls(container_path)\n",
					"                \n",
					"                for file in files:\n",
					"                    # Notebooks often create checkpoint and output files\n",
					"                    if any(pattern in file.name.lower() for pattern in \n",
					"                           ['checkpoint', 'output', 'result', 'temp', 'cache', '.ipynb']):\n",
					"                        notebook_refs.add(file.path)\n",
					"                        \n",
					"            except:\n",
					"                continue\n",
					"                \n",
					"    except Exception as e:\n",
					"        print(f\"     Notebook analysis limited: {str(e)}\")\n",
					"    \n",
					"    return notebook_refs\n",
					"\n",
					"def analyze_config_files(storage_account_name):\n",
					"    \"\"\"Analyze configuration files for storage references\"\"\"\n",
					"    config_refs = set()\n",
					"    \n",
					"    try:\n",
					"        # Look in configuration containers\n",
					"        config_containers = ['odw-config', 'data-lake-config', 'odw-config-db']\n",
					"        \n",
					"        for container in config_containers:\n",
					"            try:\n",
					"                container_path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\"\n",
					"                files = mssparkutils.fs.ls(container_path)\n",
					"                \n",
					"                for file in files:\n",
					"                    if not file.isDir and file.name.lower().endswith('.json'):\n",
					"                        try:\n",
					"                            # Read and parse JSON config\n",
					"                            file_content = mssparkutils.fs.head(f\"{container_path}{file.name}\", 5000)\n",
					"                            \n",
					"                            # Extract any storage paths\n",
					"                            path_pattern = f'{storage_account_name}\\\\.dfs\\\\.core\\\\.windows\\\\.net/([^\"\\\\s]*)'\n",
					"                            matches = re.findall(path_pattern, file_content)\n",
					"                            \n",
					"                            for match in matches:\n",
					"                                config_refs.add(f\"abfss://unknown@{storage_account_name}.dfs.core.windows.net/{match}\")\n",
					"                                \n",
					"                        except:\n",
					"                            continue\n",
					"            except:\n",
					"                continue\n",
					"                \n",
					"    except Exception as e:\n",
					"        print(f\"     Config file analysis limited: {str(e)}\")\n",
					"    \n",
					"    return config_refs\n",
					"\n",
					"def is_pipeline_dependency(file_path, file_name, referenced_paths, pipeline_patterns):\n",
					"    \"\"\"Enhanced dependency checking\"\"\"\n",
					"    file_lower = file_name.lower()\n",
					"    path_lower = file_path.lower()\n",
					"    \n",
					"    # 1. Check against discovered pipeline references\n",
					"    for ref_path in referenced_paths:\n",
					"        if ref_path.lower() in path_lower:\n",
					"            return True, f\"Referenced in pipeline/table: {ref_path[:50]}...\"\n",
					"    \n",
					"    # 2. Check for pipeline patterns in filename\n",
					"    for pattern in pipeline_patterns:\n",
					"        if pattern in file_lower:\n",
					"            return True, f\"Contains pipeline pattern: {pattern}\"\n",
					"    \n",
					"    # 3. Check for system directories\n",
					"    system_dirs = ['_delta_log', '_spark_metadata', '.checkpoints', '_checkpoints', \n",
					"                   '_SUCCESS', '_committed', '_started']\n",
					"    for sys_dir in system_dirs:\n",
					"        if sys_dir in path_lower:\n",
					"            return True, f\"System directory: {sys_dir}\"\n",
					"    \n",
					"    # 4. Check for configuration files\n",
					"    config_extensions = ['json', 'xml', 'yaml', 'yml', 'conf', 'properties', 'cfg']\n",
					"    if any(file_lower.endswith(f'.{ext}') for ext in config_extensions):\n",
					"        # Additional check for actual config content\n",
					"        config_keywords = ['pipeline', 'config', 'schema', 'metadata', 'orchestration']\n",
					"        if any(keyword in file_lower for keyword in config_keywords):\n",
					"            return True, \"Configuration file\"\n",
					"    \n",
					"    # 5. Check for recently accessed files (likely active)\n",
					"    # Files modified within last 7 days are probably active\n",
					"    try:\n",
					"        # This would need actual last access time, which isn't always available\n",
					"        pass\n",
					"    except:\n",
					"        pass\n",
					"    \n",
					"    return False, None\n",
					"\n",
					"def analyze_containers_comprehensive(storage_account_name):\n",
					"    \"\"\"Comprehensive analysis with all dependency detection methods\"\"\"\n",
					"    containers = [\n",
					"        'odw-temp', 'test', 'backup-logs', 'logging', \n",
					"        'odw-raw', 'odw-curated', 'temp-sap-hr-data',\n",
					"        'odw-standardised', 'odw-harmonised', 'synapse',\n",
					"        'dart', 'ims-poc', 's51-advice-backup', 'odw-config',\n",
					"        'data-lake-config', 'mipins-database', 'odw-config-db',\n",
					"        'odw-standardised-delta', 'hbttestdbcontainer', 'hbttestfilesystem',\n",
					"        'insights-logs-builtinsqlreqsended'\n",
					"    ]\n",
					"    \n",
					"    # Comprehensive dependency detection\n",
					"    referenced_paths, pipeline_patterns = detect_comprehensive_dependencies(storage_account_name)\n",
					"    \n",
					"    all_files = []\n",
					"    container_stats = {}\n",
					"    \n",
					"    print(f\"\\n COMPREHENSIVE FILE ANALYSIS\")\n",
					"    print(\"=\" * 80)\n",
					"    \n",
					"    for container in containers:\n",
					"        container_threshold = CONTAINER_THRESHOLDS.get(container, 120)\n",
					"        cutoff_date = datetime.now() - timedelta(days=container_threshold)\n",
					"        \n",
					"        try:\n",
					"            path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\"\n",
					"            files = mssparkutils.fs.ls(path)\n",
					"            \n",
					"            container_files = 0\n",
					"            container_unused = 0\n",
					"            container_dependencies = 0\n",
					"            container_invalid_timestamps = 0\n",
					"            container_size = 0\n",
					"            \n",
					"            print(f\"\\n {container} (>{container_threshold} days, before {cutoff_date.strftime('%Y-%m-%d')})\")\n",
					"            print(f\"    Total items: {len(files)}\")\n",
					"            \n",
					"            for file in files:\n",
					"                if not file.isDir and hasattr(file, 'modifyTime') and file.modifyTime:\n",
					"                    container_files += 1\n",
					"                    \n",
					"                    # Enhanced timestamp parsing\n",
					"                    mod_time = parse_timestamp_safely(file.modifyTime)\n",
					"                    \n",
					"                    if mod_time is None:\n",
					"                        container_invalid_timestamps += 1\n",
					"                        print(f\"     Invalid timestamp: {file.name} ({file.modifyTime})\")\n",
					"                        continue\n",
					"                    \n",
					"                    file_size_mb = round(file.size / (1024*1024), 2) if hasattr(file, 'size') else 0\n",
					"                    days_old = (datetime.now() - mod_time).days\n",
					"                    \n",
					"                    # Check if file is a pipeline dependency\n",
					"                    is_dependency, dependency_reason = is_pipeline_dependency(\n",
					"                        file.path, file.name, referenced_paths, pipeline_patterns\n",
					"                    )\n",
					"                    \n",
					"                    if is_dependency:\n",
					"                        container_dependencies += 1\n",
					"                        if days_old > container_threshold:\n",
					"                            print(f\"     Protected dependency: {file.name} ({days_old} days) - {dependency_reason}\")\n",
					"                    \n",
					"                    elif mod_time < cutoff_date:\n",
					"                        container_unused += 1\n",
					"                        container_size += file_size_mb\n",
					"                        \n",
					"                        file_ext = file.name.split('.')[-1].lower() if '.' in file.name else 'no_extension'\n",
					"                        \n",
					"                        all_files.append({\n",
					"                            'container': container,\n",
					"                            'file_name': file.name,\n",
					"                            'file_path': file.path,\n",
					"                            'size_mb': file_size_mb,\n",
					"                            'size_bytes': file.size if hasattr(file, 'size') else 0,\n",
					"                            'days_old': days_old,\n",
					"                            'threshold_used': container_threshold,\n",
					"                            'last_modified': mod_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
					"                            'file_extension': file_ext,\n",
					"                            'is_dependency': is_dependency,\n",
					"                            'dependency_reason': dependency_reason or 'None'\n",
					"                        })\n",
					"            \n",
					"            container_stats[container] = {\n",
					"                'total_files': container_files,\n",
					"                'unused_files': container_unused,\n",
					"                'unused_size_mb': container_size,\n",
					"                'dependencies': container_dependencies,\n",
					"                'invalid_timestamps': container_invalid_timestamps,\n",
					"                'threshold_days': container_threshold\n",
					"            }\n",
					"            \n",
					"            # Summary for container\n",
					"            status_parts = []\n",
					"            if container_unused > 0:\n",
					"                status_parts.append(f\" {container_unused} unused files ({container_size:.1f} MB)\")\n",
					"            if container_dependencies > 0:\n",
					"                status_parts.append(f\" {container_dependencies} protected\")\n",
					"            if container_invalid_timestamps > 0:\n",
					"                status_parts.append(f\" {container_invalid_timestamps} invalid timestamps\")\n",
					"            \n",
					"            if status_parts:\n",
					"                print(f\"    \" + \" | \".join(status_parts))\n",
					"            else:\n",
					"                print(f\"     Clean - no unused files found\")\n",
					"                \n",
					"        except Exception as e:\n",
					"            print(f\" Error with {container}: {str(e)}\")\n",
					"            container_stats[container] = {\n",
					"                'total_files': 0, 'unused_files': 0, 'unused_size_mb': 0, \n",
					"                'dependencies': 0, 'invalid_timestamps': 0, 'threshold_days': container_threshold\n",
					"            }\n",
					"    \n",
					"    return all_files, container_stats, referenced_paths\n",
					"\n",
					"def generate_comprehensive_report(unused_files, container_stats, referenced_paths):\n",
					"    \"\"\"Generate detailed analysis report with file names\"\"\"\n",
					"    \n",
					"    print(f\"\\n\" + \"=\"*80)\n",
					"    print(f\" COMPREHENSIVE STORAGE ANALYSIS REPORT\")\n",
					"    print(f\"=\"*80)\n",
					"    print(f\" Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
					"    print(f\" Dependencies detected: {len(referenced_paths)}\")\n",
					"    \n",
					"    # Overall statistics\n",
					"    total_containers = len(container_stats)\n",
					"    containers_with_unused = len([c for c in container_stats.values() if c['unused_files'] > 0])\n",
					"    total_dependencies = sum(c['dependencies'] for c in container_stats.values())\n",
					"    total_invalid_timestamps = sum(c['invalid_timestamps'] for c in container_stats.values())\n",
					"    \n",
					"    print(f\"\\n OVERVIEW:\")\n",
					"    print(f\"   Containers analyzed: {total_containers}\")\n",
					"    print(f\"   Total dependencies protected: {total_dependencies}\")\n",
					"    print(f\"   Files with timestamp issues: {total_invalid_timestamps}\")\n",
					"    print(f\"   Containers with unused files: {containers_with_unused}\")\n",
					"    \n",
					"    if not unused_files:\n",
					"        print(f\"\\n EXCELLENT! No unused files found!\")\n",
					"        print(f\"\\n Your storage is exceptionally well-maintained:\")\n",
					"        print(f\"   All files are actively used or protected by dependencies\")\n",
					"        print(f\"   Automated cleanup processes are working effectively\")\n",
					"        print(f\"   Data governance policies are properly implemented\")\n",
					"        \n",
					"        # Show threshold effectiveness\n",
					"        print(f\"\\n THRESHOLD SUMMARY:\")\n",
					"        for container, stats in container_stats.items():\n",
					"            threshold = stats['threshold_days']\n",
					"            deps = stats['dependencies']\n",
					"            invalid = stats['invalid_timestamps']\n",
					"            \n",
					"            status_icon = \"\"\n",
					"            if deps > 5:\n",
					"                status_icon = \"\"\n",
					"            elif invalid > 0:\n",
					"                status_icon = \"\"\n",
					"            \n",
					"            print(f\"  {status_icon} {container}: {threshold} days | {deps} protected | {invalid} timestamp issues\")\n",
					"        \n",
					"        return\n",
					"    \n",
					"    # Detailed analysis for unused files\n",
					"    df = pd.DataFrame(unused_files)\n",
					"    total_files = len(df)\n",
					"    total_size_mb = df['size_mb'].sum()\n",
					"    total_size_gb = total_size_mb / 1024\n",
					"    \n",
					"    print(f\"\\n UNUSED FILES FOUND:\")\n",
					"    print(f\"   Total unused files: {total_files:,}\")\n",
					"    print(f\"   Total size: {total_size_mb:.1f} MB ({total_size_gb:.2f} GB)\")\n",
					"    \n",
					"    # Container breakdown with file names\n",
					"    print(f\"\\n DETAILED CONTAINER BREAKDOWN:\")\n",
					"    print(\"-\" * 80)\n",
					"    \n",
					"    container_summary = df.groupby('container').agg({\n",
					"        'file_name': 'count',\n",
					"        'size_mb': 'sum',\n",
					"        'threshold_used': 'first'\n",
					"    }).rename(columns={'file_name': 'file_count'})\n",
					"    \n",
					"    for container, row in container_summary.iterrows():\n",
					"        deps = container_stats.get(container, {}).get('dependencies', 0)\n",
					"        threshold = int(row['threshold_used'])\n",
					"        \n",
					"        safety_level = \" SAFE TO DELETE\" if container in ['test', 'temp-sap-hr-data', 'odw-temp'] else \" REVIEW CAREFULLY\"\n",
					"        \n",
					"        print(f\"\\n {container}: {safety_level}\")\n",
					"        print(f\"    {row['file_count']} unused files, {row['size_mb']:.1f} MB\")\n",
					"        print(f\"    Threshold: {threshold} days\")\n",
					"        print(f\"    Protected dependencies: {deps}\")\n",
					"        \n",
					"        # Show individual files for this container\n",
					"        container_files = df[df['container'] == container].sort_values('size_mb', ascending=False)\n",
					"        print(f\"    Files to review:\")\n",
					"        \n",
					"        for i, (_, file) in enumerate(container_files.iterrows(), 1):\n",
					"            size_indicator = \"\" if file['size_mb'] > 10 else \"\"\n",
					"            age_indicator = \"\" if file['days_old'] < 90 else \"\" if file['days_old'] > 365 else \"\"\n",
					"            \n",
					"            print(f\"      {i:2d}. {size_indicator} {file['file_name']}\")\n",
					"            print(f\"           Size: {file['size_mb']:.1f} MB\")\n",
					"            print(f\"          {age_indicator} Age: {file['days_old']} days (modified: {file['last_modified']})\")\n",
					"            print(f\"           Extension: .{file['file_extension']}\")\n",
					"    \n",
					"    # Summary by safety level\n",
					"    print(f\"\\n CLEANUP RECOMMENDATIONS:\")\n",
					"    print(\"-\" * 60)\n",
					"    \n",
					"    safe_containers = ['test', 'temp-sap-hr-data', 'odw-temp']\n",
					"    safe_files = df[df['container'].isin(safe_containers)]\n",
					"    \n",
					"    if not safe_files.empty:\n",
					"        safe_size = safe_files['size_mb'].sum()\n",
					"        print(f\"\\n SAFE TO DELETE IMMEDIATELY ({len(safe_files)} files, {safe_size:.1f} MB):\")\n",
					"        \n",
					"        for container in safe_containers:\n",
					"            container_files = safe_files[safe_files['container'] == container]\n",
					"            if not container_files.empty:\n",
					"                print(f\"\\n    {container} ({len(container_files)} files):\")\n",
					"                for _, file in container_files.iterrows():\n",
					"                    print(f\"       {file['file_name']} ({file['size_mb']:.1f} MB, {file['days_old']} days old)\")\n",
					"    \n",
					"    review_files = df[~df['container'].isin(safe_containers)]\n",
					"    if not review_files.empty:\n",
					"        review_size = review_files['size_mb'].sum()\n",
					"        print(f\"\\n REVIEW BEFORE DELETING ({len(review_files)} files, {review_size:.1f} MB):\")\n",
					"        \n",
					"        # Group by container for review files\n",
					"        review_containers = review_files['container'].unique()\n",
					"        for container in review_containers:\n",
					"            container_files = review_files[review_files['container'] == container]\n",
					"            print(f\"\\n    {container} ({len(container_files)} files):\")\n",
					"            \n",
					"            # Show largest files first for review priority\n",
					"            top_files = container_files.nlargest(5, 'size_mb') if len(container_files) > 5 else container_files\n",
					"            for _, file in top_files.iterrows():\n",
					"                print(f\"       {file['file_name']} ({file['size_mb']:.1f} MB, {file['days_old']} days old)\")\n",
					"            \n",
					"            if len(container_files) > 5:\n",
					"                print(f\"      ... and {len(container_files) - 5} more files\")\n",
					"    \n",
					"    # Top 10 largest files across all containers\n",
					"    print(f\"\\n TOP 10 LARGEST UNUSED FILES:\")\n",
					"    print(\"-\" * 60)\n",
					"    top_files = df.nlargest(10, 'size_mb')\n",
					"    \n",
					"    for i, (_, file) in enumerate(top_files.iterrows(), 1):\n",
					"        safety = \" SAFE\" if file['container'] in safe_containers else \" REVIEW\"\n",
					"        size_category = \" HUGE\" if file['size_mb'] > 100 else \" LARGE\" if file['size_mb'] > 10 else \" SMALL\"\n",
					"        \n",
					"        print(f\"{i:2d}. {safety} {size_category} {file['file_name']}\")\n",
					"        print(f\"     Container: {file['container']}\")\n",
					"        print(f\"     Size: {file['size_mb']:.1f} MB\")\n",
					"        print(f\"     Age: {file['days_old']} days (last modified: {file['last_modified']})\")\n",
					"        print()\n",
					"    \n",
					"    # Action plan\n",
					"    print(f\" RECOMMENDED ACTION PLAN:\")\n",
					"    print(\"-\" * 60)\n",
					"    print(f\"1.  IMMEDIATE ACTION - Delete {len(safe_files)} safe files ({safe_files['size_mb'].sum():.1f} MB):\")\n",
					"    print(f\"    Start with 'test' container (largest files)\")\n",
					"    print(f\"    Clean 'temp-sap-hr-data' (temporary files)\")\n",
					"    print(f\"    These are safe test/temp environments\")\n",
					"    \n",
					"    if not review_files.empty:\n",
					"        # Find the largest file that needs review\n",
					"        largest_review = review_files.nlargest(1, 'size_mb').iloc[0]\n",
					"        print(f\"\\n2.  PRIORITY REVIEW - Check largest file first:\")\n",
					"        print(f\"    {largest_review['file_name']} ({largest_review['size_mb']:.1f} MB)\")\n",
					"        print(f\"    Container: {largest_review['container']}\")\n",
					"        print(f\"    Could save significant space if safe to delete\")\n",
					"    \n",
					"\n",
					"\n",
					"def main():\n",
					"    \"\"\"Main comprehensive analysis function\"\"\"\n",
					"    print(\" COMPREHENSIVE AZURE STORAGE ANALYSIS\")\n",
					"    print(\" Enhanced Pipeline Dependency Detection\")\n",
					"    print(\" Smart Timestamp Parsing\")\n",
					"    print(\" Container-Specific Thresholds\")\n",
					"    print(\"=\"*80)\n",
					"    \n",
					"    try:\n",
					"        # Get storage account info\n",
					"        storage_account_name, _ = get_storage_account_info()\n",
					"        \n",
					"        # Run comprehensive analysis\n",
					"        unused_files, container_stats, referenced_paths = analyze_containers_comprehensive(storage_account_name)\n",
					"        \n",
					"        # Generate detailed report\n",
					"        generate_comprehensive_report(unused_files, container_stats, referenced_paths)\n",
					"        \n",
					"        print(f\"\\n Comprehensive analysis complete!\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"\\n Analysis failed: {str(e)}\")\n",
					"        import traceback\n",
					"        traceback.print_exc()\n",
					"\n",
					"# Run the comprehensive analysis\n",
					"if __name__ == \"__main__\":\n",
					"    main()"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}