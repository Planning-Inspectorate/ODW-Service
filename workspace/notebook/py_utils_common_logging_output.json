{
	"name": "py_utils_common_logging_output",
	"properties": {
		"folder": {
			"name": "utils/main"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "c4fab7d6-c1df-43c7-80b6-e5c4a0fa9b86"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this pyspark notebook is to reads all recent files from the given odw-raw folder path and load the data into standardised_db lakehouse database's Delta tables\r\n",
					"\r\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \r\n",
					"Rohit Shukla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;19-Jan-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The functionality of this notebook is generic to cater to `.xlsx` and `.csv` files for creating Delta Tables.\r\n",
					"\r\n",
					"**Spark Cluster Configuration** -> Apache Spark Version- 3.4, Python Version \t\t- 3.10, Delta Lake Version \t- 2.4\r\n",
					"\r\n",
					"\r\n",
					"##### The input parameters are:\r\n",
					"###### Param_FileFolder_Path => This is a mandatory parameter which refers to a folder path of the entities like 'Timesheets', 'SapHrData'\r\n",
					"###### Param_File_Load_Type  => This is an optional parameter refers to a subfolders if there is any like Monthly,Daily, Quarterly etc.\r\n",
					"###### Param_Json_SchemaFolder_Name => This is a mandatory parameter which refers to a schema file in json format required to create delta tables.\r\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Import required Python libraries"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": []
				},
				"source": [
					"import json\r\n",
					"import traceback\r\n",
					"from datetime import datetime\r\n",
					"from typing import Dict, Any, Union\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Enable message logging"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define all files and dataframe processing related functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Initialize processing results structure\r\n",
					"def initialize_processing_results() -> Dict[str, Any]:\r\n",
					"    \"\"\"\r\n",
					"    Initialize the processing results structure  \r\n",
					"    Returns:\r\n",
					"        Dict containing the processing results structure\r\n",
					"    \"\"\"\r\n",
					"    return {\r\n",
					"        \"processing_summary\": {\r\n",
					"            \"total_tables_processed\": 0,\r\n",
					"            \"successful_tables\": 0,\r\n",
					"            \"failed_tables\": 0\r\n",
					"        },\r\n",
					"        \"table_details\": []\r\n",
					"    }\r\n",
					"\r\n",
					"# Time difference calculation function\r\n",
					"def time_diff_seconds(start: str | datetime, end: str | datetime) -> int:\r\n",
					"    \"\"\"\r\n",
					"    Calculate time difference in seconds between start and end times\r\n",
					"    Args:\r\n",
					"        start: Start time (string or datetime)\r\n",
					"        end: End time (string or datetime)    \r\n",
					"    Returns:\r\n",
					"        Time difference in seconds\r\n",
					"    \"\"\"\r\n",
					"    try:\r\n",
					"        if not start or not end:\r\n",
					"            return 0\r\n",
					"\r\n",
					"        # Parse strings into datetime objects if needed\r\n",
					"        if isinstance(start, str):\r\n",
					"            start = datetime.strptime(start, \"%Y-%m-%d %H:%M:%S.%f\")\r\n",
					"        if isinstance(end, str):\r\n",
					"            end = datetime.strptime(end, \"%Y-%m-%d %H:%M:%S.%f\")\r\n",
					"\r\n",
					"        diff_seconds = int((end - start).total_seconds())\r\n",
					"        return diff_seconds if diff_seconds > 0 else 0\r\n",
					"\r\n",
					"    except Exception as e:\r\n",
					"        return 0\r\n",
					"\r\n",
					"# DateTime handler for JSON serialization\r\n",
					"def datetime_handler(obj) -> str:\r\n",
					"    \"\"\"\r\n",
					"    Handle datetime objects for JSON serialization\r\n",
					"    \r\n",
					"    Args:\r\n",
					"        obj: Object to serialize\r\n",
					"        \r\n",
					"    Returns:\r\n",
					"        ISO formatted string for datetime objects\r\n",
					"    \"\"\"\r\n",
					"    if isinstance(obj, datetime):\r\n",
					"        return obj.isoformat()\r\n",
					"    raise TypeError(f\"Object of type {obj.__class__.__name__} is not JSON serializable\")\r\n",
					"\r\n",
					"# Add table result function\r\n",
					"def add_table_result(processing_results: Dict[str, Any],\r\n",
					"                    delta_table_name: str, \r\n",
					"                    insert_count: int = 0, \r\n",
					"                    table_result: str = \"success\",\r\n",
					"                    start_exec_time: str = \"\",\r\n",
					"                    end_exec_time: str = \"\",\r\n",
					"                    total_exec_time: Union[str, int] = \"\", \r\n",
					"                    error_message: str = \"\") -> Dict[str, Any]:\r\n",
					"    \"\"\"\r\n",
					"    Add processing result for a table to the tracking structure\r\n",
					"    \r\n",
					"    Args:\r\n",
					"        processing_results: The processing results dictionary\r\n",
					"        delta_table_name: Name of the delta table\r\n",
					"        insert_count: Number of records inserted\r\n",
					"        table_result: Result status (\"success\" or \"failed\")\r\n",
					"        start_exec_time: Start execution time\r\n",
					"        end_exec_time: End execution time\r\n",
					"        total_exec_time: Total execution time (calculated if not provided)\r\n",
					"        error_message: Error message if failed\r\n",
					"        \r\n",
					"    Returns:\r\n",
					"        Updated processing_results dictionary\r\n",
					"    \"\"\"\r\n",
					"    # Calculate total execution time if not provided\r\n",
					"    if not total_exec_time and start_exec_time and end_exec_time:\r\n",
					"        total_exec_time = time_diff_seconds(start_exec_time, end_exec_time)\r\n",
					"    \r\n",
					"    table_detail = {\r\n",
					"        \"delta_table_name\": delta_table_name,\r\n",
					"        \"insert_count\": insert_count,\r\n",
					"        \"table_result\": table_result,\r\n",
					"        \"start_exec_time\": start_exec_time,\r\n",
					"        \"end_exec_time\": end_exec_time,\r\n",
					"        \"total_exec_time\": total_exec_time,\r\n",
					"        \"error_message\": error_message\r\n",
					"    }\r\n",
					"    \r\n",
					"    processing_results[\"table_details\"].append(table_detail)\r\n",
					"    processing_results[\"processing_summary\"][\"total_tables_processed\"] += 1\r\n",
					"    \r\n",
					"    if table_result == \"success\":\r\n",
					"        processing_results[\"processing_summary\"][\"successful_tables\"] += 1\r\n",
					"    else:\r\n",
					"        processing_results[\"processing_summary\"][\"failed_tables\"] += 1\r\n",
					"    \r\n",
					"    return processing_results\r\n",
					"\r\n",
					"# Generate processing results function\r\n",
					"def generate_processing_results(processing_results: Dict[str, Any]) -> Dict[str, Any]:\r\n",
					"    \"\"\"\r\n",
					"    Generate and display the final processing results in JSON format\r\n",
					"    \r\n",
					"    Args:\r\n",
					"        processing_results: The processing results dictionary\r\n",
					"        \r\n",
					"    Returns:\r\n",
					"        Final processing results dictionary\r\n",
					"    \"\"\"\r\n",
					"    # Create the final result structure\r\n",
					"    exit_value = {\r\n",
					"        \"processing_summary\": processing_results[\"processing_summary\"],\r\n",
					"        \"table_details\": processing_results[\"table_details\"]\r\n",
					"    }\r\n",
					"    \r\n",
					"    # Convert to JSON string for display\r\n",
					"    json_output = json.dumps(exit_value, indent=2, default=datetime_handler)\r\n",
					"    \r\n",
					"    # Display summary information\r\n",
					"    logInfo(f\"Total tables processed: {exit_value['processing_summary']['total_tables_processed']}\")\r\n",
					"    logInfo(f\"Successful tables: {exit_value['processing_summary']['successful_tables']}\")\r\n",
					"    logInfo(f\"Failed tables: {exit_value['processing_summary']['failed_tables']}\")\r\n",
					"    \r\n",
					"    # Display detailed results\r\n",
					"    logInfo(f\"ExitValue: {json_output}\")\r\n",
					"    \r\n",
					"    # Log any failures for quick reference\r\n",
					"    failed_tables = [table for table in exit_value['table_details'] if table['table_result'] == 'failed']\r\n",
					"    if failed_tables:\r\n",
					"        logInfo(\"\\nFailed Tables:\")\r\n",
					"        for table in failed_tables:\r\n",
					"            logInfo(f\"Table: {table['delta_table_name']} - Error: {table['error_message']}\")\r\n",
					"    \r\n",
					"    return exit_value\r\n",
					"\r\n",
					"# Format error message function\r\n",
					"def format_error_message(error: Exception, max_length: int = 300) -> str:\r\n",
					"    \"\"\"\r\n",
					"    Format error message with traceback, truncated to specified length\r\n",
					"    \r\n",
					"    Args:\r\n",
					"        error: Exception object\r\n",
					"        max_length: Maximum length of error message\r\n",
					"        \r\n",
					"    Returns:\r\n",
					"        Formatted error message string\r\n",
					"    \"\"\"\r\n",
					"    # Get full traceback\r\n",
					"    full_trace = traceback.format_exc()\r\n",
					"    \r\n",
					"    # Combine error message and trace\r\n",
					"    table_error_msg = str(error)\r\n",
					"    complete_msg = table_error_msg + \"\\n\" + full_trace\r\n",
					"    error_text = complete_msg[:max_length]           \r\n",
					"    \r\n",
					"    # Find the position of the last full stop before max_length characters\r\n",
					"    last_period_index = error_text.rfind('.')\r\n",
					"\r\n",
					"    # Use up to the last full stop, if found; else fall back to max_length chars\r\n",
					"    if last_period_index != -1:\r\n",
					"        error_message = error_text[:last_period_index + 1] \r\n",
					"    else:\r\n",
					"        error_message = error_text\r\n",
					"    \r\n",
					"    return error_message"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Process main logging output produced in Json format"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"@logging_to_appins\r\n",
					"def process_definitions(Param_File_Load_Type):\r\n",
					"    matched_definitions = []\r\n",
					"    unmatched_definitions = []\r\n",
					"    all_latest_files = []\r\n",
					"\r\n",
					"    # Filter only matched definitions\r\n",
					"    for definition in definitions:\r\n",
					"    \r\n",
					"        freq_folder = definition.get('Source_Frequency_Folder', '').lower()\r\n",
					"        source_folder = definition.get('Source_Folder', '').lower()\r\n",
					"        param_freq = (Param_File_Load_Type or '').lower()\r\n",
					"        param_path = (Param_FileFolder_Path or '').lower()\r\n",
					"\r\n",
					"        if Param_File_Load_Type and not (\r\n",
					"            freq_folder == param_freq and source_folder == param_path\r\n",
					"        ):\r\n",
					"            continue\r\n",
					"\r\n",
					"        source_path = f\"abfss://odw-raw@{storage_account}{Param_FileFolder_Path}/\"\r\n",
					"        \r\n",
					"        if Param_File_Load_Type:            \r\n",
					"            source_path += f\"{Param_File_Load_Type}/\"\r\n",
					"\r\n",
					"        try:\r\n",
					"            latest_folder = get_latest_folder(source_path)\r\n",
					"            if not latest_folder:\r\n",
					"                logInfo(f\"No folders in path {source_path}\")\r\n",
					"                continue\r\n",
					"\r\n",
					"            latest_path = f\"{source_path}{latest_folder}/\"\r\n",
					"            files = [f.name for f in mssparkutils.fs.ls(latest_path) if not f.isDir]\r\n",
					"            all_latest_files.extend(files)\r\n",
					"\r\n",
					"            # Automatically detect multi-file processing based on filename patterns\r\n",
					"            filename_start = definition['Source_Filename_Start']\r\n",
					"            \r\n",
					"            # Find files with the same base name but different dates\r\n",
					"            matching_files, processing_mode = find_multi_date_files(files, filename_start)\r\n",
					"            \r\n",
					"            logInfo(f\"File analysis for '{filename_start}': found {len(matching_files)} files, mode: {processing_mode}\")\r\n",
					"            logInfo(f\"Matching files: {matching_files}\")\r\n",
					"            \r\n",
					"            if processing_mode == 'multi_file':\r\n",
					"                # Multi-file processing mode - sort files by date\r\n",
					"                logInfo(f\"Enabling multi-file processing for {definition['Standardised_Table_Name']} - found {len(matching_files)} files with date patterns\")\r\n",
					"                \r\n",
					"                # Sort files by date extracted from filename\r\n",
					"                matching_files_with_dates = []\r\n",
					"                for file in matching_files:\r\n",
					"                    file_date = extract_date_from_filename(file)\r\n",
					"                    matching_files_with_dates.append((file, file_date))\r\n",
					"                \r\n",
					"                # Sort by date (oldest first for sequential processing)\r\n",
					"                matching_files_with_dates.sort(key=lambda x: x[1])\r\n",
					"                sorted_files = [file for file, _ in matching_files_with_dates]\r\n",
					"                \r\n",
					"                logInfo(f\"Files sorted by date: {sorted_files}\")\r\n",
					"                \r\n",
					"                definition['matched_files'] = [f\"{latest_path}{f}\" for f in sorted_files]\r\n",
					"                definition['latest_path'] = latest_path\r\n",
					"                definition['processing_mode'] = 'multi_file'\r\n",
					"                matched_definitions.append(definition)\r\n",
					"                logInfo(f\"Setup multi-file processing for {definition['Standardised_Table_Name']} with {len(sorted_files)} files\")\r\n",
					"                \r\n",
					"            elif processing_mode == 'single_file':\r\n",
					"                # Single file processing mode\r\n",
					"                matching_file = matching_files[0]\r\n",
					"                definition['matched_file'] = matching_file\r\n",
					"                definition['latest_path'] = latest_path\r\n",
					"                definition['processing_mode'] = 'single_file'\r\n",
					"                matched_definitions.append(definition)\r\n",
					"                logInfo(f\"Setup single-file processing for {definition['Standardised_Table_Name']} with file: {matching_file}\")\r\n",
					"                \r\n",
					"            else:\r\n",
					"                # No matching files found\r\n",
					"                unmatched_definitions.append(definition['Standardised_Table_Name'])\r\n",
					"                logInfo(f\"No files found matching pattern '{filename_start}' for {definition['Standardised_Table_Name']}\")\r\n",
					"\r\n",
					"        except Exception as e:\r\n",
					"            logError(f\"Could not read from {source_path}: {e}\")\r\n",
					"\r\n",
					"    #List filenames if nothing matched in Orchestration.json\r\n",
					"    processed_files = set()\r\n",
					"    for d in matched_definitions:\r\n",
					"        if d['processing_mode'] == 'multi_file':\r\n",
					"            processed_files.update([os.path.basename(f) for f in d['matched_files']])\r\n",
					"        else:\r\n",
					"            processed_files.add(d['matched_file'])\r\n",
					"    \r\n",
					"    unmatched_files = set(all_latest_files) - processed_files\r\n",
					"    if unmatched_files:\r\n",
					"        logError(f\"Files found in source but not defined in orchestration.json: {', '.join(unmatched_files)}\")\r\n",
					"\r\n",
					"    # Step 3: Process each matched definition\r\n",
					"    for definition in matched_definitions:\r\n",
					"        \r\n",
					"        if definition['processing_mode'] == 'multi_file':\r\n",
					"            # Process each file individually and create separate entries\r\n",
					"            file_index = 0\r\n",
					"            table_created = False  # Track if table has been successfully created\r\n",
					"            \r\n",
					"            for file_path in definition['matched_files']:\r\n",
					"                file_name = os.path.basename(file_path)\r\n",
					"                \r\n",
					"                # Create individual result entry for each file\r\n",
					"                result_entry = {\r\n",
					"                    \"delta_table_name\": definition['Standardised_Table_Name'],\r\n",
					"                    \"csv_file_name\": file_name,\r\n",
					"                    \"record_count\": 0,\r\n",
					"                    \"table_result\": \"failed\",\r\n",
					"                    \"start_exec_time\": \"\",\r\n",
					"                    \"end_exec_time\": \"\",\r\n",
					"                    \"total_exec_time\": \"\",\r\n",
					"                    \"error_message\": \"\"\r\n",
					"                }\r\n",
					"\r\n",
					"                try:\r\n",
					"                    start_exec_time = str(datetime.now())\r\n",
					"                    result_entry[\"start_exec_time\"] = start_exec_time\r\n",
					"                    \r\n",
					"                    # Process individual file\r\n",
					"                    sparkDF = read_file(file_path)\r\n",
					"                    if sparkDF is None:\r\n",
					"                        logError(f\"No data loaded for file: {file_name}\")\r\n",
					"                        result_entry[\"error_message\"] = f\"No data loaded for file: {file_name}\"\r\n",
					"                        result_entry[\"end_exec_time\"] = str(datetime.now())\r\n",
					"                        result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, datetime.now()))\r\n",
					"                        processing_results.append(result_entry)\r\n",
					"                        file_index += 1\r\n",
					"                        continue\r\n",
					"\r\n",
					"                    expected_from = datetime.now() - timedelta(days=definition['Expected_Within_Weekdays'])\r\n",
					"                    expected_to = datetime.now()\r\n",
					"\r\n",
					"                    sparkDF = clean_column_names(sparkDF)\r\n",
					"                    \r\n",
					"                    # Add metadata columns to standardised table\r\n",
					"                    sparkTableDF = sparkDF.withColumn(\"ingested_datetime\", current_timestamp()) \\\r\n",
					"                                          .withColumn(\"expected_from\", lit(expected_from)) \\\r\n",
					"                                          .withColumn(\"expected_to\", lit(expected_to))\r\n",
					"                    \r\n",
					"                    # Reorder metadata columns for the standardised delta table\r\n",
					"                    sparkTableDF = reorder_columns(sparkTableDF)\r\n",
					"\r\n",
					"                    delta_table_path = f\"abfss://odw-standardised@{storage_account}{Param_Json_SchemaFolder_Name}/{definition['Standardised_Table_Name']}\"\r\n",
					"                    full_table_name = f\"{database_name}.{definition['Standardised_Table_Name']}\"\r\n",
					"                    schema_path = f\"abfss://odw-config@{storage_account}{definition['Standardised_Table_Definition']}\"\r\n",
					"\r\n",
					"                    # Strict schema validation - JSON schema is the ultimate truth\r\n",
					"                    expected_schema_fields = [f['name'] for f in json.loads(\"\".join([r.value for r in spark.read.text(schema_path).collect()]))['fields']]\r\n",
					"                    actual_fields = sparkTableDF.columns\r\n",
					"                    \r\n",
					"                    # Remove metadata columns from comparison as they're always added by the system\r\n",
					"                    excluded_columns = {\"ingested_datetime\", \"expected_from\", \"expected_to\", \"source_file\", \"file_processed_datetime\"}\r\n",
					"                    actual_business_fields = set(actual_fields) - excluded_columns\r\n",
					"                    expected_business_fields = set(expected_schema_fields) - excluded_columns\r\n",
					"                    \r\n",
					"                    missing_columns = expected_business_fields - actual_business_fields\r\n",
					"                    extra_columns = actual_business_fields - expected_business_fields\r\n",
					"                    \r\n",
					"                    # STRICT VALIDATION: CSV must match JSON schema exactly\r\n",
					"                    if missing_columns:\r\n",
					"                        logError(f\"CSV file missing required columns from JSON schema for table {definition['Standardised_Table_Name']}: {', '.join(missing_columns)}\")\r\n",
					"                        logInfo(f\"Skipping ingestion for {definition['Standardised_Table_Name']} due to missing required columns.\")\r\n",
					"                        result_entry[\"error_message\"] = f\"Schema validation failed - CSV missing required columns: {', '.join(missing_columns)}\"\r\n",
					"                        result_entry[\"end_exec_time\"] = str(datetime.now())\r\n",
					"                        result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, datetime.now()))\r\n",
					"                        processing_results.append(result_entry)\r\n",
					"                        file_index += 1\r\n",
					"                        continue\r\n",
					"                    \r\n",
					"                    if extra_columns:\r\n",
					"                        logError(f\"CSV file has additional columns not defined in JSON schema for table {definition['Standardised_Table_Name']}: {', '.join(extra_columns)}\")\r\n",
					"                        logError(f\"JSON schema is the ultimate truth. CSV file must match JSON schema exactly.\")\r\n",
					"                        logInfo(f\"Skipping ingestion for {definition['Standardised_Table_Name']} due to extra columns in CSV.\")\r\n",
					"                        result_entry[\"error_message\"] = f\"Schema validation failed - CSV has additional columns not in JSON schema: {', '.join(extra_columns)}\"\r\n",
					"                        result_entry[\"end_exec_time\"] = str(datetime.now())\r\n",
					"                        result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, datetime.now()))\r\n",
					"                        processing_results.append(result_entry)\r\n",
					"                        file_index += 1\r\n",
					"                        continue\r\n",
					"                    \r\n",
					"                    # If we reach here, CSV exactly matches JSON schema\r\n",
					"                    logInfo(f\"CSV schema validation passed for {definition['Standardised_Table_Name']} - CSV matches JSON schema exactly.\")\r\n",
					"                    \r\n",
					"                    # Use JSON schema compliance for all files\r\n",
					"                    write_success = write_dataframe_with_json_schema_compliance(\r\n",
					"                        sparkTableDF, delta_table_path, full_table_name, schema_path, \r\n",
					"                        is_first_file=(not table_created)\r\n",
					"                    )\r\n",
					"                    \r\n",
					"                    if not write_success:\r\n",
					"                        raise Exception(f\"Failed to write data using comprehensive schema handling\")\r\n",
					"                    \r\n",
					"                    # Mark table as created after first successful write\r\n",
					"                    if not table_created:\r\n",
					"                        table_created = True\r\n",
					"\r\n",
					"                    # Count rows for validation\r\n",
					"                    rows_raw = sparkDF.count()\r\n",
					"                    \r\n",
					"                    end_exec_time = str(datetime.now())\r\n",
					"\r\n",
					"                    # Update json result entry with success data\r\n",
					"                    result_entry[\"record_count\"] = rows_raw  # Individual file record count\r\n",
					"                    result_entry[\"table_result\"] = \"success\"\r\n",
					"                    result_entry[\"end_exec_time\"] = end_exec_time\r\n",
					"                    result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, end_exec_time))\r\n",
					"\r\n",
					"                    logInfo(f\"Successfully processed file {file_name} with {rows_raw} rows for {definition['Standardised_Table_Name']}\")\r\n",
					"                \r\n",
					"                except Exception as e:\r\n",
					"                    \r\n",
					"                    #Code added to capture meaningful error message\r\n",
					"                    full_trace = traceback.format_exc()\r\n",
					"                    \r\n",
					"                    table_error_msg = str(e)\r\n",
					"\r\n",
					"                    complete_msg = table_error_msg + \"\\n\" + full_trace\r\n",
					"                    error_text = complete_msg[:300]           \r\n",
					"                    \r\n",
					"                    # Find the position of the last full stop before 300 characters\r\n",
					"                    last_period_index = error_text.rfind('.')\r\n",
					"\r\n",
					"                    # Use up to the last full stop, if found; else fall back to 300 chars\r\n",
					"                    if last_period_index != -1:\r\n",
					"                        error_message = error_text[:last_period_index + 1] \r\n",
					"                    else:\r\n",
					"                        error_message = error_text\r\n",
					"\r\n",
					"                    logError(f\"Failed processing file {file_name} for {definition['Standardised_Table_Name']} - {e}\")\r\n",
					"\r\n",
					"                    result_entry[\"error_message\"] = f\"Failed processing file {file_name} - {error_message}\"\r\n",
					"                    result_entry[\"end_exec_time\"] = str(datetime.now())\r\n",
					"                    result_entry[\"total_exec_time\"] = str(time_diff_seconds(result_entry[\"start_exec_time\"], datetime.now()))\r\n",
					"                \r\n",
					"                # Add result to the json dump\r\n",
					"                processing_results.append(result_entry)\r\n",
					"                file_index += 1\r\n",
					"        \r\n",
					"        else:\r\n",
					"            # Original single file processing\r\n",
					"            result_entry = {\r\n",
					"                \"delta_table_name\": definition['Standardised_Table_Name'],\r\n",
					"                \"csv_file_name\": definition.get('matched_file', 'unknown_file'),\r\n",
					"                \"record_count\": 0,\r\n",
					"                \"table_result\": \"failed\",\r\n",
					"                \"start_exec_time\": \"\",\r\n",
					"                \"end_exec_time\": \"\",\r\n",
					"                \"total_exec_time\": \"\",\r\n",
					"                \"error_message\": \"\"\r\n",
					"            }\r\n",
					"\r\n",
					"            try:\r\n",
					"                start_exec_time = str(datetime.now())\r\n",
					"                result_entry[\"start_exec_time\"] = start_exec_time\r\n",
					"                \r\n",
					"                # Original single file processing\r\n",
					"                sparkDF = read_file(f\"{definition['latest_path']}{definition['matched_file']}\")\r\n",
					"                \r\n",
					"                if sparkDF is None:\r\n",
					"                    logError(f\"No data loaded for: {definition['Standardised_Table_Name']}\")\r\n",
					"                    continue\r\n",
					"\r\n",
					"                expected_from = datetime.now() - timedelta(days=definition['Expected_Within_Weekdays'])\r\n",
					"                expected_to = datetime.now()\r\n",
					"\r\n",
					"                # Clean column names if not already done in multi-file processing\r\n",
					"                sparkDF = clean_column_names(sparkDF)\r\n",
					"                \r\n",
					"                # Add metadata columns to standardised table\r\n",
					"                sparkTableDF = sparkDF.withColumn(\"ingested_datetime\", current_timestamp()) \\\r\n",
					"                                      .withColumn(\"expected_from\", lit(expected_from)) \\\r\n",
					"                                      .withColumn(\"expected_to\", lit(expected_to))\r\n",
					"                \r\n",
					"                # Reorder metadata columns for the standardised delta table\r\n",
					"                sparkTableDF = reorder_columns(sparkTableDF)\r\n",
					"\r\n",
					"                delta_table_path = f\"abfss://odw-standardised@{storage_account}{Param_Json_SchemaFolder_Name}/{definition['Standardised_Table_Name']}\"\r\n",
					"                full_table_name = f\"{database_name}.{definition['Standardised_Table_Name']}\"\r\n",
					"                schema_path = f\"abfss://odw-config@{storage_account}{definition['Standardised_Table_Definition']}\"\r\n",
					"                \r\n",
					"                # Strict schema validation - JSON schema is the ultimate truth\r\n",
					"                expected_schema_fields = [f['name'] for f in json.loads(\"\".join([r.value for r in spark.read.text(schema_path).collect()]))['fields']]\r\n",
					"                actual_fields = sparkTableDF.columns\r\n",
					"                \r\n",
					"                # Remove metadata columns from comparison as they're always added by the system\r\n",
					"                excluded_columns = {\"ingested_datetime\", \"expected_from\", \"expected_to\", \"source_file\", \"file_processed_datetime\"}\r\n",
					"                actual_business_fields = set(actual_fields) - excluded_columns\r\n",
					"                expected_business_fields = set(expected_schema_fields) - excluded_columns\r\n",
					"                \r\n",
					"                missing_columns = expected_business_fields - actual_business_fields\r\n",
					"                extra_columns = actual_business_fields - expected_business_fields\r\n",
					"                \r\n",
					"                # STRICT VALIDATION: CSV must match JSON schema exactly\r\n",
					"                if missing_columns:\r\n",
					"                    logError(f\"CSV file missing required columns from JSON schema for table {definition['Standardised_Table_Name']}: {', '.join(missing_columns)}\")\r\n",
					"                    logInfo(f\"Skipping ingestion for {definition['Standardised_Table_Name']} due to missing required columns.\")\r\n",
					"                    result_entry[\"error_message\"] = f\"Schema validation failed - CSV missing required columns: {', '.join(missing_columns)}\"\r\n",
					"                    result_entry[\"end_exec_time\"] = str(datetime.now())\r\n",
					"                    result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, datetime.now()))\r\n",
					"                    processing_results.append(result_entry)\r\n",
					"                    continue\r\n",
					"                \r\n",
					"                if extra_columns:\r\n",
					"                    logError(f\"CSV file has additional columns not defined in JSON schema for table {definition['Standardised_Table_Name']}: {', '.join(extra_columns)}\")\r\n",
					"                    logError(f\"JSON schema is the ultimate truth. CSV file must match JSON schema exactly.\")\r\n",
					"                    logInfo(f\"Skipping ingestion for {definition['Standardised_Table_Name']} due to extra columns in CSV.\")\r\n",
					"                    result_entry[\"error_message\"] = f\"Schema validation failed - CSV has additional columns not in JSON schema: {', '.join(extra_columns)}\"\r\n",
					"                    result_entry[\"end_exec_time\"] = str(datetime.now())\r\n",
					"                    result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, datetime.now()))\r\n",
					"                    processing_results.append(result_entry)\r\n",
					"                    continue\r\n",
					"                \r\n",
					"                # If we reach here, CSV exactly matches JSON schema\r\n",
					"                logInfo(f\"CSV schema validation passed for {definition['Standardised_Table_Name']} - CSV matches JSON schema exactly.\")\r\n",
					"                \r\n",
					"                # Write using JSON schema compliance\r\n",
					"                write_success = write_dataframe_with_json_schema_compliance(\r\n",
					"                    sparkTableDF, delta_table_path, full_table_name, schema_path, is_first_file=True\r\n",
					"                )\r\n",
					"                \r\n",
					"                if not write_success:\r\n",
					"                    raise Exception(f\"Failed to write data using strategy: {write_strategy}\")\r\n",
					"\r\n",
					"                # Count rows for validation\r\n",
					"                rows_raw = sparkDF.count()\r\n",
					"                standardised_table_df = spark.read.format(\"delta\").load(delta_table_path)\r\n",
					"                rows_new = standardised_table_df.filter(\r\n",
					"                    (col(\"expected_from\") == expected_from) & \r\n",
					"                    (col(\"expected_to\") == expected_to)\r\n",
					"                ).count()\r\n",
					"                \r\n",
					"                end_exec_time = str(datetime.now())\r\n",
					"\r\n",
					"                # Update json result entry with success data\r\n",
					"                result_entry[\"record_count\"] = standardised_table_df.count()  # Total records in delta table\r\n",
					"                result_entry[\"table_result\"] = \"success\"\r\n",
					"                result_entry[\"end_exec_time\"] = end_exec_time\r\n",
					"                result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, end_exec_time))\r\n",
					"\r\n",
					"                if rows_raw <= rows_new:\r\n",
					"                    logInfo(f\"All rows successfully written to {definition['Standardised_Table_Name']} â€” Raw: {rows_raw}, Written: {rows_new}\")\r\n",
					"                else:\r\n",
					"                    logError(f\"Mismatch in row count for {definition['Standardised_Table_Name']}: expected {rows_raw}, got {rows_new}\")\r\n",
					"            \r\n",
					"            except Exception as e:\r\n",
					"                \r\n",
					"                #Code added to capture meaningful error message\r\n",
					"                full_trace = traceback.format_exc()\r\n",
					"                \r\n",
					"                table_error_msg = str(e)\r\n",
					"\r\n",
					"                complete_msg = table_error_msg + \"\\n\" + full_trace\r\n",
					"                error_text = complete_msg[:300]           \r\n",
					"                \r\n",
					"                # Find the position of the last full stop before 300 characters\r\n",
					"                last_period_index = error_text.rfind('.')\r\n",
					"\r\n",
					"                # Use up to the last full stop, if found; else fall back to 300 chars\r\n",
					"                if last_period_index != -1:\r\n",
					"                    error_message = error_text[:last_period_index + 1] \r\n",
					"                else:\r\n",
					"                    error_message = error_text\r\n",
					"\r\n",
					"                logError(f\"Failed processing for {definition['Standardised_Table_Name']} - {e}\")\r\n",
					"\r\n",
					"                result_entry[\"error_message\"] = f\"Failed processing for {definition['Standardised_Table_Name']} - {error_message} \"\r\n",
					"                result_entry[\"end_exec_time\"] = str(datetime.now())\r\n",
					"                result_entry[\"total_exec_time\"] = str(time_diff_seconds(result_entry[\"start_exec_time\"], datetime.now()))\r\n",
					"            \r\n",
					"            # Add result to the json dump\r\n",
					"            processing_results.append(result_entry)\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Execute main process"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# --- Run the main process ---\r\n",
					"process_definitions(Param_File_Load_Type)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Prepare and return JSON result\r\n",
					"json_result = {\r\n",
					"    \"processing_summary\": {\r\n",
					"        \"total_tables_processed\": len(processing_results),\r\n",
					"        \"successful_overwrites\": len([r for r in processing_results if r[\"table_result\"] == \"success\"]),\r\n",
					"        \"failed_overwrites\": len([r for r in processing_results if r[\"table_result\"] == \"failed\"])\r\n",
					"    },\r\n",
					"    \"table_details\": processing_results\r\n",
					"}\r\n",
					"\r\n",
					"# Convert to JSON string\r\n",
					"result_json_str = json.dumps(json_result, indent=2, default=datetime_handler)\r\n",
					"\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Exit with the JSON result for pipeline consumption\r\n",
					"mssparkutils.notebook.exit(result_json_str)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Dropping Delta tables if needed, Code commented"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"editable": true,
					"run_control": {
						"frozen": false
					},
					"collapsed": false
				},
				"source": [
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.hr_absence_monthly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.inspector_addresses_monthly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.inspector_specialisms_monthly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.sap_email_monthly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.sap_hr_history_monthly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.sap_hr_leavers_monthly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.sap_protected_monthly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.inspector_addresses_weekly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.inspector_specialisms_weekly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.sap_hr_weekly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.sap_email_monthly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.sap_email_weekly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_harmonised_db.load_vw_sap_hr_email_weekly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_harmonised_db.load_sap_pins_email_weekly\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.work_schedules\")\r\n",
					"#spark.sql(f\"DROP TABLE IF EXISTS odw_standardised_db.inspector_addresses_monthly\")\r\n",
					""
				],
				"execution_count": null
			}
		]
	}
}