{
	"name": "document_metadata_appeals",
	"properties": {
		"folder": {
			"name": "odw-curated"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "28b59285-5c35-4d1d-b108-b8338de45e0f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"CREATE OR REPLACE VIEW odw_curated_db.vw_appeals_document_metadata\n",
					"\n",
					"AS\n",
					"\n",
					"SELECT DISTINCT \n",
					"ADM.documentId\t                        AS documentId,\n",
					"CAST(ADM.caseId AS INT)\t            AS caseId,\n",
					"ADM.caseReference\t                    AS caseRef,\n",
					"CAST(ADM.Version AS INT)                 AS version,\n",
					"AADM.examinationRefNo\t                AS examinationRefNo,\n",
					"ADM.Name\t                                AS filename,\n",
					"ADM.Name\t                                AS originalFilename,\n",
					"CAST(ADM.DataSize AS INT)\t            AS size,\n",
					"AADM.mime\t                            AS mime,\n",
					"AADM.DocumentURI\t                        AS documentURI,\n",
					"AADM.DocumentURI\t\t                    AS publishedDocumentURI,\n",
					"AADM.Path                                AS path,\n",
					"ADM.virusCheckStatus\t                    AS virusCheckStatus,\n",
					"AADM.fileMd5\t                            AS fileMD5,\n",
					"CAST(ADM.CreateDate AS TIMESTAMP)\t    AS dateCreated,\n",
					"CAST(ADM.ModifyDate AS TIMESTAMP)\t    AS lastModified,\n",
					"LOWER(ADM.CaseworkType)\t                AS caseType,\n",
					"CASE\n",
					"    WHEN AADM.redactedStatus = ' '   \n",
					"    THEN '-1'\n",
					"    ELSE AADM.redactedStatus                \n",
					"END                                     AS redactedStatus,\n",
					"CASE\n",
					"    WHEN ADM.PublishedStatus = 'Depublished'\n",
					"    THEN 'unpublished'\n",
					"    ELSE REPLACE(\n",
					"        LOWER(ADM.PublishedStatus),\n",
					"        ' ',\n",
					"        '_')\n",
					"END                  \t                AS publishedStatus,\n",
					"CAST(ADM.DatePublished AS TIMESTAMP)\t    AS datePublished,\n",
					"ADM.DocumentType\t                        AS documentType,\n",
					"CASE\n",
					"    WHEN ADM.securityClassification = ' '\n",
					"    THEN '-1'\n",
					"    ELSE ADM.securityClassification\n",
					"END\t                                    AS securityClassification,\n",
					"ADM.SourceSystem\t                        AS sourceSystem,\n",
					"CASE\n",
					"    WHEN ADM.origin  = ' '\n",
					"    THEN '-1'\n",
					"    ELSE ADM.origin\n",
					"END\t                                    AS origin,\n",
					"ADM.owner                               AS owner,\n",
					"ADM.Author\t                            AS author,\n",
					"ADM.Representative\t                    AS representative,\n",
					"ADM.DocumentDescription\t                AS description,\n",
					"CASE\n",
					"    WHEN ADM.DocumentCaseStage = \"Developer's Application\"\n",
					"    THEN 'developers_application'\n",
					"    WHEN ADM.DocumentCaseStage = 'Post decision'\n",
					"    THEN 'post_decision'\n",
					"    ELSE LOWER(ADM.DocumentCaseStage)\t            \n",
					"END                                     AS documentCaseStage,\n",
					"ADM.Filter1\t                            AS filter1,\n",
					"ADM.Filter2\t                            AS filter2,\n",
					"ADM.ParentID\t                            AS horizonFolderId,\n",
					"'-1'\t                                AS transcriptId\t\n",
					"\n",
					"FROM odw_harmonised_db.appeals_document_metadata AS ADM\n",
					"WHERE ADM.IsActive = 'Y'"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.sql(f\"drop table if exists odw_curated_db.appeals_document_metadata;\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"This table has a column \"transcriptId\" which is set to NULL. The column is in the json schema but is not in any of the source data sets (yet). The spark data type is set to \"void\" as there is no data in the column and this prevents the table being created using SQL create table as delta command. Setting this column to StringType and creating the table using spark instead avoids this issue."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql.types import StringType\r\n",
					"spark = SparkSession.builder.getOrCreate()\r\n",
					"view_df = spark.sql(\"SELECT * FROM odw_curated_db.vw_appeals_document_metadata\")\r\n",
					"view_df2 = view_df.withColumn(\"transcriptId\", view_df[\"transcriptId\"].cast(StringType()))\r\n",
					"view_df2.write.mode(\"overwrite\").saveAsTable(\"odw_curated_db.appeals_document_metadata\")\r\n",
					"print(\"Table created\")"
				],
				"execution_count": null
			}
		]
	}
}