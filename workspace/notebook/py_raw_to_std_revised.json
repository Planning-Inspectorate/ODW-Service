{
	"name": "py_raw_to_std_revised",
	"properties": {
		"folder": {
			"name": "1-odw-raw-to-standardised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodwpr",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "5462895f-eab9-48f9-93e1-d72bc0532ca4"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodwpr",
				"name": "pinssynspodwpr",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodwpr",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Prerequisites\n",
					"1. Make sure the new raw file's entry has been added to the Orchestration i.e `/infrastructure/configuration/data-lake/orchestration/orchestration.json`\n",
					"2. Make sure the standardised table's schema is present on the path specified in the entry added in step 1.\n",
					"3. Only if the raw file is huge (several GBs), the spark pool might need some upscaling. Hence the following cell"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%%configure -f\n",
					"{\n",
					"    \"conf\": {\n",
					"        \"spark.kryoserializer.buffer.max\": \"2047m\",\n",
					"        \"spark.driver.maxResultSize\": \"10g\",\n",
					"        \"spark.rpc.message.maxSize\": \"1280\"\n",
					"    }\n",
					"}"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Get the Storage Account"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from notebookutils import mssparkutils\n",
					"import json\n",
					"import calendar\n",
					"from datetime import datetime, timedelta, date\n",
					"import pandas as pd\n",
					"import os\n",
					"\n",
					"spark = SparkSession.builder.getOrCreate()\n",
					"\n",
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"raw_container = \"abfss://odw-raw@\" + storage_account"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run \"1-odw-raw-to-standardised/Fileshare/SAP_HR/py_1_raw_to_standardised_hr_functions\""
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Rewriting ingest_adhoc function\n",
					"\n",
					"@logging_to_appins\n",
					"def ingest_adhoc2(storage_account, \n",
					"                    definition,\n",
					"                    folder_path, \n",
					"                    filename, \n",
					"                    expected_from, \n",
					"                    expected_to, \n",
					"                    isMultilineJSON=False, \n",
					"                    dataAttribute=None):\n",
					"\n",
					"    from pyspark.sql import SparkSession\n",
					"    from notebookutils import mssparkutils\n",
					"    import json\n",
					"    from datetime import datetime\n",
					"    import pandas as pd\n",
					"    from pyspark.sql.functions import col, lit\n",
					"    from pyspark.sql.types import StructType\n",
					"    import re\n",
					"    \n",
					"    ingestion_failure = False\n",
					"    spark = SparkSession.builder.getOrCreate()\n",
					"    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
					"\n",
					"    standardised_container = f\"abfss://odw-standardised@{storage_account}\"\n",
					"    standardised_path = definition['Standardised_Path'] + \"/\"\n",
					"    standardised_table_name = definition['Standardised_Table_Name']\n",
					"    \n",
					"    # Load standardised table schema\n",
					"    if 'Standardised_Table_Definition' in definition:\n",
					"        standardised_table_loc = f\"abfss://odw-config@{storage_account}{definition['Standardised_Table_Definition']}\"\n",
					"        standardised_table_def_json = spark.read.text(standardised_table_loc, wholetext=True).first().value\n",
					"    else:\n",
					"        standardised_table_def_json = mssparkutils.notebook.run('/py_get_schema_from_url', 30, \n",
					"                                                                {'db_name': 'odw_standardised_db', \n",
					"                                                                 'entity_name': definition['Source_Frequency_Folder']})\n",
					"\n",
					"    # Create table if not exists\n",
					"    if not any([table.name == standardised_table_name for table in spark.catalog.listTables('odw_standardised_db')]):\n",
					"        create_table_from_schema(standardised_table_def_json, \"odw_standardised_db\", standardised_table_name, \n",
					"                                 standardised_container, standardised_path + standardised_table_name)   \n",
					"\n",
					"    # Ensure table is in Delta format\n",
					"    table_metadata = spark.sql(f\"DESCRIBE EXTENDED odw_standardised_db.{standardised_table_name}\")\n",
					"    data_format = table_metadata.filter(table_metadata.col_name == \"Provider\").collect()[0].data_type\n",
					"\n",
					"    if data_format == \"parquet\":\n",
					"        spark.sql(f\"CONVERT TO DELTA odw_standardised_db.{standardised_table_name}\")\n",
					"\n",
					"    # Get table location\n",
					"    try:\n",
					"        standardised_table_location = spark.sql(f\"DESCRIBE FORMATTED odw_standardised_db.{standardised_table_name}\") \\\n",
					"                    .filter(\"col_name = 'Location'\") \\\n",
					"                    .select(\"data_type\") \\\n",
					"                    .collect()[0][0]\n",
					"    except:\n",
					"        standardised_table_location = standardised_container + standardised_path + standardised_table_name\n",
					"\n",
					"    # Load existing standardised table\n",
					"    standardised_table_df = spark.read.format(\"delta\").load(standardised_table_location)\n",
					"\n",
					"    # Check if the same expected_from and expected_to already exist\n",
					"    rows = standardised_table_df.filter((col(\"expected_from\") == expected_from) & (col(\"expected_to\") == expected_to)).count()\n",
					"\n",
					"    logInfo(f\"Reading {filename}\")\n",
					"\n",
					"    # Read file\n",
					"    file_path = f\"{folder_path}/{filename}\"\n",
					"    if filename.lower().endswith(\".xlsx\"):\n",
					"        sheet_name = definition.get('Source_Sheet_Name', 0)\n",
					"        df = pd.read_excel(file_path, dtype=str, sheet_name=sheet_name, na_filter=False)\n",
					"        sparkDF = spark.createDataFrame(df)\n",
					"    elif filename.lower().endswith('.csv'):\n",
					"        sparkDF = spark.read.options(quote='\"', escape='\\\\', encoding='utf8', header=True, multiLine=True, \n",
					"                                     columnNameOfCorruptRecord='corrupted_records', mode=\"PERMISSIVE\").csv(file_path)\n",
					"                                     ## read the full filepath name using from pyspark.sql.functions import current_timestamp, expr, to_timestamp, lit, input_file_name\n",
					"        if \"corrupted_records\" in sparkDF.columns:\n",
					"            logError(f\"Corrupted Records detected from CSV ingestion in {filename}\")\n",
					"            ingestion_failure = True\n",
					"    elif filename.lower().endswith('.json'):\n",
					"        if isMultilineJSON:\n",
					"            logInfo(\"Reading multiline JSON\")\n",
					"            sparkDF = spark.read.option(\"multiline\", \"true\").json(file_path)\n",
					"            if dataAttribute:\n",
					"                sparkDF = sparkDF.select(dataAttribute).rdd.flatMap(lambda row: row[dataAttribute]).toDF()\n",
					"        else:\n",
					"            sparkDF = spark.read.json(file_path)\n",
					"    else:\n",
					"        raise RuntimeError(f\"Unsupported file type for {filename}\")\n",
					"\n",
					"    # Drop unnamed columns (headerless)\n",
					"    sparkDF = sparkDF.select([col for col in sparkDF.columns if not col.startswith('Unnamed')])\n",
					"\n",
					"    # Add metadata columns\n",
					"    sparkDF = sparkDF.withColumn(\"ingested_datetime\", lit(datetime.now()))\n",
					"    sparkDF = sparkDF.withColumn(\"expected_from\", lit(expected_from))\n",
					"    sparkDF = sparkDF.withColumn(\"expected_to\", lit(expected_to))\n",
					"    sparkDF = sparkDF.withColumn(\"file_name\", lit(filename)) # change according to line 77\n",
					"\n",
					"    # Ensure schema consistency\n",
					"    schema = StructType.fromJson(json.loads(standardised_table_def_json))\n",
					"    for field in schema.fields:\n",
					"        if field.dataType.simpleString() == 'array':\n",
					"            sparkDF = sparkDF.withColumn(field.name, col(field.name).cast(\"string\"))\n",
					"\n",
					"    # Rename columns for Delta compatibility\n",
					"    new_columns = {c: re.sub(r'[^0-9a-zA-Z]+', '_', c).lower().rstrip('_') for c in sparkDF.columns}\n",
					"    sparkDF = sparkDF.toDF(*[new_columns[c] for c in sparkDF.columns])\n",
					"\n",
					"    # Cast fields to match schema\n",
					"    for field in schema.fields:\n",
					"        if field.name in sparkDF.columns:\n",
					"            sparkDF = sparkDF.withColumn(field.name, col(field.name).cast(field.dataType))\n",
					"\n",
					"    # Write to Delta table\n",
					"    logInfo(f\"Writing data to odw_standardised_db.{standardised_table_name}\")\n",
					"    sparkDF.write.option(\"mergeSchema\", \"true\").format(\"delta\").mode(\"append\").saveAsTable(f\"odw_standardised_db.{standardised_table_name}_test\")\n",
					"\n",
					"    # Validate rows written\n",
					"    rows_new = spark.read.format(\"delta\").load(standardised_table_location) \\\n",
					"        .filter((col(\"expected_from\") == expected_from) & (col(\"expected_to\") == expected_to)).count()\n",
					"\n",
					"    if rows_new >= sparkDF.count():\n",
					"        logInfo(\"All rows successfully written\")\n",
					"    else:\n",
					"        logError(f\"Row count mismatch: Expected {sparkDF.count()}, Written {rows_new}\")\n",
					"        ingestion_failure = True\n",
					"\n",
					"    return ingestion_failure, sparkDF.count()\n",
					""
				],
				"execution_count": 43
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Initialise the parameters"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"date_folder='2025-02-04'\n",
					"source_folder='stef_test'\n",
					"source_frequency_folder=''\n",
					"specific_file='' # if not provided, it will ingest all files in the date_folder\n",
					"isMultiLine = True\n",
					"delete_existing_table=False\n",
					"dataAttribute = \"\""
				],
				"execution_count": 45
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Ingest the data from the raw/source into the standardised table. \n",
					"If the table doesn't already exist, this will create the table first and ingest the data."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if date_folder == '':\n",
					"    date_folder = datetime.now().date()\n",
					"else:\n",
					"    date_folder = datetime.strptime(date_folder, \"%Y-%m-%d\")\n",
					"\n",
					"date_folder_str = date_folder.strftime('%Y-%m-%d')\n",
					"source_folder_path = source_folder if not source_frequency_folder else f\"{source_folder}/{source_frequency_folder}\"\n",
					"\n",
					"# READ ORCHESTRATION DATA\n",
					"path_to_orchestration_file = \"abfss://odw-config@\"+storage_account+\"orchestration/orchestration.json\"\n",
					"df = spark.read.option(\"multiline\",\"true\").json(path_to_orchestration_file)\n",
					"definitions = json.loads(df.toJSON().first())['definitions']\n",
					"\n",
					"source_path = f\"{raw_container}{source_folder_path}/{date_folder_str}\"\n",
					"\n",
					"try:\n",
					"    logInfo(f\"Reading from {source_path}\")\n",
					"    files = mssparkutils.fs.ls(source_path)\n",
					"except Exception as e:\n",
					"    logError(f\"Raw file not found at {source_path}\")\n",
					"    logException(e)\n",
					"    mssparkutils.notebook.exit(f\"Raw file not found at {source_path}\")\n",
					"\n",
					"\n",
					"for file in files:\n",
					"\n",
					"    # ignore json raw files if source is service bus\n",
					"    if source_folder == 'ServiceBus' and file.name.endswith('.json'):\n",
					"        continue\n",
					"\n",
					"    # ignore files other than specified file \n",
					"    if specific_file != '' and not file.name.startswith(specific_file + '.'):\n",
					"        continue\n",
					"        \n",
					"    definition = next((d for d in definitions if (specific_file == '' or d['Source_Filename_Start'] == specific_file) \n",
					"                        and (not source_frequency_folder or d['Source_Frequency_Folder'] == source_frequency_folder) \n",
					"                        and file.name.startswith(d['Source_Filename_Start'])), None)\n",
					"    \n",
					"    if definition:\n",
					"        expected_from = date_folder - timedelta(days=1)\n",
					"        expected_from = datetime.combine(expected_from, datetime.min.time())\n",
					"        expected_to = expected_from + timedelta(days=definition['Expected_Within_Weekdays']) \n",
					"\n",
					"        if delete_existing_table:\n",
					"            logInfo(f\"Deleting existing table if exists odw_standardised_db.{definition['Standardised_Table_Name']}\")\n",
					"            mssparkutils.notebook.run('/utils/py_delete_table', 300, arguments={'db_name': 'odw_standardised_db', 'table_name': definition['Standardised_Table_Name']})\n",
					"\n",
					"        logInfo(f\"Ingesting {file.name}\")\n",
					"        (ingestion_failure, row_count) = ingest_adhoc2(storage_account, definition, source_path, file.name, expected_from, expected_to, isMultiLine, dataAttribute)\n",
					"        logInfo(f\"Ingested {row_count} rows\")\n",
					"\n",
					"        if ingestion_failure:\n",
					"            print(f\"Errors reported during Ingestion!!\")\n",
					"            raise RuntimeError(\"Ingestion Failure\")\n",
					"        else:\n",
					"            print(f\"No Errors reported during Ingestion\")\n",
					"\n",
					"    else:\n",
					"        if specific_file != '':\n",
					"            raise ValueError(f\"No definition found for {specific_file}\")\n",
					"        else:\n",
					"            logError(\"No definition found\")\n",
					""
				],
				"execution_count": 46
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Extra functions"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\r\n",
					"def get_max_value_from_column(table_name: str, column_name: str) -> datetime:\r\n",
					"    \"\"\"\r\n",
					"    Gets the maximum value from a given table column\r\n",
					"\r\n",
					"    Args:\r\n",
					"        table_name: the name of the table, e.g. appeal_has\r\n",
					"        column_name: the name of the table column, e.g. ingested_datetime\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        max_value: the maximum value from the table column\r\n",
					"    \"\"\"\r\n",
					"    df = spark.table(table_name)\r\n",
					"    max_value = df.agg(max(column_name)).collect()[0][0]\r\n",
					"\r\n",
					"    return max_value"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"@logging_to_appins\r\n",
					"def get_max_file_date(df: DataFrame) -> datetime:\r\n",
					"    \"\"\"\r\n",
					"    Gets the maximum date from a file path field in a DataFrame.\r\n",
					"    E.g. if the input_file field contained paths such as this:\r\n",
					"    abfss://odw-raw@pinsstodwdevuks9h80mb.dfs.core.windows.net/ServiceBus/appeal-has/2024-12-02/appeal-has_2024-12-02T16:54:35.214679+0000.json\r\n",
					"    It extracts the date from the string for each row and gets the maximum date.\r\n",
					"\r\n",
					"    Args:\r\n",
					"        df: a spark DataFrame\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        formatted_timestamp: a string of the maximum file date\r\n",
					"    \"\"\"\r\n",
					"    date_pattern: str = r'(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{6}\\+\\d{4})'\r\n",
					"    df: DataFrame = df.withColumn(\"file_date\", regexp_extract(df[\"input_file\"], date_pattern, 1))\r\n",
					"    df: DataFrame = df.withColumn(\"file_date\", df[\"file_date\"].cast(TimestampType()))\r\n",
					"    max_timestamp: list = df.agg(max(\"file_date\")).collect()[0][0]\r\n",
					"    formatted_timestamp: str = max_timestamp.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")+\"+0000\"\r\n",
					"    return formatted_timestamp"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\r\n",
					"def get_all_files_recursive(source_path: str) -> list:\r\n",
					"    \"\"\"\r\n",
					"    Lists all files in a given source path.\r\n",
					"    Recursively loops through all directories.\r\n",
					"\r\n",
					"    Args:\r\n",
					"        source_path: the folder path to start from, \r\n",
					"        e.g. abfss://odw-raw@pinsstodwdevuks9h80mb.dfs.core.windows.net/ServiceBus/appeal-has/\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        files: a list of files in the source path.\r\n",
					"    \"\"\"\r\n",
					"    files = []\r\n",
					"    entries = mssparkutils.fs.ls(source_path)\r\n",
					"    \r\n",
					"    for entry in entries:\r\n",
					"        # Check if the entry is a directory\r\n",
					"        if entry.isDir:\r\n",
					"            # Recursively process the directory\r\n",
					"            files.extend(get_all_files_recursive(entry.path))\r\n",
					"        else:\r\n",
					"            # If it's a file, add to the list\r\n",
					"            files.append(entry.path)\r\n",
					"    \r\n",
					"    return files"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"@logging_to_appins\r\n",
					"def get_missing_files(table_name: str, source_path: str) -> list:\r\n",
					"    \"\"\"\r\n",
					"    Gets the difference between the files in the source path and the files in the table.\r\n",
					"    Converts the table column \"filename\" into a set.\r\n",
					"    Creates a set containing all the files int he source path.\r\n",
					"    Compares the two sets to give the missing files not yet loaded to the table.\r\n",
					"\r\n",
					"    Args:\r\n",
					"        table_name: the name of the table, e.g. appeal_has\r\n",
					"        source_path: the folder path to start from, \r\n",
					"        e.g. abfss://odw-raw@pinsstodwdevuks9h80mb.dfs.core.windows.net/ServiceBus/appeal-has/\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        missing_files: a list of missing files not yet loaded to the table.\r\n",
					"    \"\"\"\r\n",
					"    df: DataFrame = spark.table(table_name)\r\n",
					"    files_in_path: set = set(get_all_files_recursive(source_path))\r\n",
					"    files_in_table: set = set(df.select(\"input_file\").rdd.flatMap(lambda x: x).collect())\r\n",
					"    missing_files = list(files_in_path - files_in_table)\r\n",
					"\r\n",
					"    return missing_files"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\r\n",
					"def extract_and_filter_paths(files: list, filter_date: str):\r\n",
					"    \"\"\"\r\n",
					"    Takes a list of file paths and filters them to return the file paths greater than the filter_date\r\n",
					"    \r\n",
					"    Args:\r\n",
					"        files: a list of file paths\r\n",
					"        filter_date: a date to filter on\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        filtered_paths: a list of file paths greater than a given date\r\n",
					"    \"\"\"\r\n",
					"    timestamp_pattern: str = re.compile(r\"(\\d{4}-\\d{2}-\\d{2}T\\d{2}[:_]\\d{2}[:_]\\d{2}[.\\d]*[+-]\\d{4})\")\r\n",
					"    filter_datetime: datetime = datetime.strptime(filter_date, \"%Y-%m-%dT%H:%M:%S.%f%z\")\r\n",
					"    filtered_paths: list = []\r\n",
					"\r\n",
					"    for file in files:\r\n",
					"        match = timestamp_pattern.search(file)\r\n",
					"        if match:\r\n",
					"            timestamp_str = match.group(1).replace('_', ':')\r\n",
					"            file_datetime = datetime.strptime(timestamp_str, \"%Y-%m-%dT%H:%M:%S.%f%z\")\r\n",
					"            if file_datetime > filter_datetime:\r\n",
					"                filtered_paths.append(file)\r\n",
					"\r\n",
					"    return filtered_paths"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\n",
					"def read_raw_messages(filtered_paths: list[str]) -> DataFrame:\n",
					"    \"\"\"\n",
					"    Ingests data from service bus messages stored as json files in the raw layer\n",
					"\n",
					"    Args:\n",
					"        filtered_paths: a list of file paths to ingest\n",
					"\n",
					"    Returns:\n",
					"        A DataFrame of service bus messages with additional columns needed for the standardised table\n",
					"    \"\"\"\n",
					"    try:\n",
					"        # Read JSON files from filtered paths\n",
					"        df = spark.read.json(filtered_paths, schema=spark_schema)\n",
					"        logInfo(f\"Found {df.count()} new rows.\")\n",
					"        # Adding the standardised columns\n",
					"        df = df.withColumn(\"expected_from\", current_timestamp())\n",
					"        df = df.withColumn(\"expected_to\", expr(\"current_timestamp() + INTERVAL 1 DAY\"))\n",
					"        df = df.withColumn(\"ingested_datetime\", to_timestamp(df.message_enqueued_time_utc))\n",
					"        df = df.withColumn(\"input_file\", input_file_name())\n",
					"\n",
					"    except Exception as e:\n",
					"        logError(f'Raw data not found at {source_path}, Exception is {e}')\n",
					"        mssparkutils.notebook.exit('')\n",
					"\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\r\n",
					"def dedupe_dataframe(df: DataFrame) -> DataFrame:\r\n",
					"    \"\"\"\r\n",
					"    Dedupes a DataFrame based on certain columns\r\n",
					"\r\n",
					"    Args:\r\n",
					"        df: a DataFrame of service bus data\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        a deduped DataFrame\r\n",
					"    \"\"\"\r\n",
					"    # removing duplicates while ignoring the ingestion dates columns\r\n",
					"    columns_to_ignore: list = ['expected_to', 'expected_from', 'ingested_datetime']\r\n",
					"    columns_to_consider: list = [c for c in df.columns if c not in columns_to_ignore]\r\n",
					"    df: DataFrame = df.dropDuplicates(subset=columns_to_consider)\r\n",
					"    \r\n",
					"    return df\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\r\n",
					"def append_df_to_table(df: DataFrame, table_name: str) -> None:\r\n",
					"    \"\"\"\r\n",
					"    Appends the new rows to the target table\r\n",
					"\r\n",
					"    Args:\r\n",
					"        df: DataFrame of new rows\r\n",
					"    \"\"\"\r\n",
					"    df \\\r\n",
					"    .write \\\r\n",
					"    .mode(\"append\") \\\r\n",
					"    .format(\"delta\") \\\r\n",
					"    .option(\"mergeSchema\", \"true\") \\\r\n",
					"    .saveAsTable(table_name)\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\r\n",
					"def test_rows_appended(new_table_row_count: int, expected_row_count: int) -> bool:\r\n",
					"    \"\"\"\r\n",
					"    Test if the new row count matches the expected row count.\r\n",
					"    If True then rows have been appended successfully.\r\n",
					"\r\n",
					"    Args:\r\n",
					"        new_table_row_count: count of rows after the append operation\r\n",
					"        expected_row_count: count of rows we expect after the append operation\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        True if the counts match\r\n",
					"    \"\"\"\r\n",
					"    return new_table_row_count == expected_new_count"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Get the filtered list of files we want to ingest.  \r\n",
					"\r\n",
					"If `USE_MAX_DATE_FILTER` is set to `True` then we get the maximum date of the ingested files from the target table. We use this date to filter the json files in the raw storage layer to only ingest files greater than this date.  \r\n",
					"\r\n",
					"If `USE_MAX_DATE_FILTER` is set to `False` then we compare the files ingested already in the target table with the source files and get the difference, i.e. the missing files."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"USE_MAX_DATE_FILTER = False\r\n",
					"\r\n",
					"if USE_MAX_DATE_FILTER:\r\n",
					"    try:\r\n",
					"        logInfo(\"Getting the maximum file date\")\r\n",
					"        table_df = spark.table(table_name)\r\n",
					"        max_extracted_date = get_max_file_date(df=table_df)\r\n",
					"        print(f\"max_extracted_date: {max_extracted_date}\")\r\n",
					"    except Exception as e:\r\n",
					"        logError(f\"Error getting the maximum file date:\\n{e}\")\r\n",
					"\r\n",
					"    try:\r\n",
					"        logInfo(\"Getting the filtered list of paths\")\r\n",
					"        filtered_paths = extract_and_filter_paths(get_all_files_recursive(source_path=source_path), max_extracted_date)\r\n",
					"        df = read_raw_messages(filtered_paths=filtered_paths)\r\n",
					"    except Exception as e:\r\n",
					"        logError(f\"Error getting the filtered list of paths:\\n{e}\")\r\n",
					"else:\r\n",
					"    try:\r\n",
					"        logInfo(\"Getting list of missing files\")\r\n",
					"        missing_files = get_missing_files(table_name, source_path)\r\n",
					"        df = read_raw_messages(filtered_paths=missing_files)\r\n",
					"    except Exception as e:\r\n",
					"        logError(f\"Error getting list of missing files:\\n{e}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Dedupe the DataFrame and generate counts.\r\n",
					"\r\n",
					"1. Get existing row count of target table  \r\n",
					"2. Dedupe the DataFrame  \r\n",
					"3. Get count of rows to append  \r\n",
					"4. Get the expected new row count which is existing count + new rows"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"try:\r\n",
					"    table_row_count = spark.table(table_name).count()\r\n",
					"    logInfo(f\"Row count before append: {table_row_count}\")\r\n",
					"    df = dedupe_dataframe(df=df)\r\n",
					"    rows_to_append = df.count()\r\n",
					"    logInfo(f\"Rows to append: {rows_to_append}\")\r\n",
					"    expected_new_count = table_row_count + rows_to_append\r\n",
					"    logInfo(f\"Expected new count: {expected_new_count}\")\r\n",
					"except Exception as e:\r\n",
					"    logError(f\"Error deduping and generating counts:\\n{e}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Append rows to the target table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"try:\n",
					"    logInfo(f\"Appending new rows to table {table_name}\")\n",
					"    append_df_to_table(df=df, table_name=table_name)\n",
					"    logInfo(\"Done appending rows to table\")\n",
					"    new_table_row_count = spark.table(table_name).count()\n",
					"except Exception as e:\n",
					"    logError(f\"Error appending rows:\\n{e}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Test rows were appended successfully\r\n",
					"\r\n",
					"Test that the new row count in the target table matches the expected new row count above"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if not test_rows_appended(new_table_row_count=new_table_row_count, expected_row_count=expected_new_count):\r\n",
					"    logError(\"Error appending rows. The new row count isn't as expected.\")\r\n",
					"else:\r\n",
					"    logInfo(\"Rows appended successfully.\")"
				],
				"execution_count": null
			}
		]
	}
}