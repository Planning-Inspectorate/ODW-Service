{
	"name": "horizon_deleted_records",
	"properties": {
		"folder": {
			"name": "archive/utils"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4bcd9799-4645-4666-a43e-77bf84935b09"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 32,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql.functions import when, col, date_add"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"service_bus_table = \"odw_harmonised_db.sb_s51_advice\"\n",
					"horizon_table = \"odw_standardised_db.horizon_nsip_advice\"\n",
					"spark_table_final = \"odw_harmonised_db.nsip_s51_advice\"\n",
					"curated_migration_table = \"odw_curated_migration_db.s51_advice\"\n",
					"horizon_primary_key = \"advicenodeid\"\n",
					"servicebus_primary_key = \"adviceId\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"servicebus_df = spark.table(service_bus_table)\n",
					"horizon_df = spark.table(horizon_table)\n",
					"horizon_latest_df = spark.sql(f\"select * from {horizon_table} where ingested_datetime = (select max(ingested_datetime) from {horizon_table})\")\n",
					"horizon_deleted = horizon_df.join(horizon_latest_df, on=horizon_primary_key, how=\"left_anti\").select(horizon_primary_key).distinct()\n",
					"harmonised_df = spark.table(spark_table_final)\n",
					"curated_migration_df = spark.table(curated_migration_table)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def test_no_deleted_horizon_records_in_harmonised(\n",
					"    horizon_latest_df: DataFrame, \n",
					"    harmonised_df: DataFrame, \n",
					"    horizon_primary_key: str, \n",
					"    servicebus_primary_key: str\n",
					") -> bool:\n",
					"    \"\"\"\n",
					"    Tests if there are any records in harmonised that have been deleted from Horizon.\n",
					"    Returns True if no deleted records exist, otherwise False.\n",
					"    \"\"\"\n",
					"    harmonised_alias = harmonised_df.alias(\"hrm\")\n",
					"    horizon_alias = horizon_latest_df.alias(\"hzn\")\n",
					"\n",
					"    harmonised_filtered = harmonised_alias.filter(\"ODTSourceSystem = 'Horizon' and IsActive = 'Y'\")\n",
					"\n",
					"    result_df = harmonised_filtered.join(\n",
					"        horizon_alias, \n",
					"        on=col(f\"hrm.{servicebus_primary_key}\") == col(f\"hzn.{horizon_primary_key}\"),\n",
					"        how=\"left_anti\"\n",
					"    )\n",
					"\n",
					"    return result_df.count() == 0"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def test_valid_to_set_for_inactive_records(df: DataFrame) -> bool:\n",
					"    \"\"\"\n",
					"    Tests if the ValidTo date is populated for all inactive records\n",
					"    \"\"\"\n",
					"    results = df.filter(\"IsActive = 'N' and ValidTo is null\")\n",
					"    return results.count() == 0"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def test_valid_to_not_set_for_active_records(df: DataFrame) -> bool:\n",
					"    \"\"\"\n",
					"    Tests if the ValidTo date is not populated for active records\n",
					"    \"\"\"\n",
					"    results = df.filter(\"IsActive = 'Y' and ValidTo is not null\")\n",
					"    return results.count() == 0"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"test_results = [\n",
					"    ((\"test_no_deleted_horizon_records_in_harmonised\", test_no_deleted_horizon_records_in_harmonised(horizon_latest_df, harmonised_df, horizon_primary_key, servicebus_primary_key))),\n",
					"    ((\"test_valid_to_set_for_inactive_records\", test_valid_to_set_for_inactive_records(harmonised_df))),\n",
					"    ((\"test_valid_to_not_set_for_active_records\", test_valid_to_not_set_for_active_records(harmonised_df)))\n",
					"    ]\n",
					"\n",
					"results = spark.createDataFrame(test_results, schema=[\"test\", \"result\"])\n",
					"results.show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# Get deleted primary keys into a list\n",
					"deleted_ids = [row[0] for row in horizon_deleted.collect()]\n",
					"\n",
					"# Set IsActive to N if the primary key has been deleted from Horizon\n",
					"hrm_updated = harmonised_df.withColumn(\n",
					"    \"IsActive\", when(col(\"adviceId\").isin(deleted_ids), \"N\").otherwise(col(\"IsActive\"))\n",
					")\n",
					"\n",
					"# Set ValidTo to null if IsActive = Y\n",
					"hrm_updated = hrm_updated.withColumn(\n",
					"    \"ValidTo\", when(col(\"IsActive\") == \"Y\", None).otherwise(col(\"ValidTo\"))\n",
					")\n",
					"\n",
					"# Set ValidTo if IsActive = N to the IngestionDate + 1 day\n",
					"hrm_final = hrm_updated.withColumn(\n",
					"    \"ValidTo\", when((col(\"ValidTo\").isNull()) & (col(\"IsActive\") == \"N\"), date_add(col(\"IngestionDate\"), 1).cast(\"timestamp\")).otherwise(col(\"ValidTo\"))\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"new_test_results = [\n",
					"    ((\"test_no_deleted_horizon_records_in_harmonised\", test_no_deleted_horizon_records_in_harmonised(horizon_latest_df, hrm_final, horizon_primary_key, servicebus_primary_key))),\n",
					"    ((\"test_valid_to_set_for_inactive_records\", test_valid_to_set_for_inactive_records(hrm_final))),\n",
					"    ((\"test_valid_to_not_set_for_active_records\", test_valid_to_not_set_for_active_records(hrm_final)))\n",
					"    ]\n",
					"\n",
					"new_results = spark.createDataFrame(new_test_results, schema=[\"test\", \"result\"])\n",
					"new_results.show()"
				],
				"execution_count": null
			}
		]
	}
}