{
	"name": "py_horizon_Refactored_log",
	"properties": {
		"folder": {
			"name": "utils/unit-tests"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "f28b8bbf-cba4-40b4-904b-b302da50fd8e"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"%run /utils/py_mount_storage"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Get the Storage Account"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import asyncio\n",
					"import nest_asyncio\n",
					"import tracemalloc\n",
					"tracemalloc.start()\n",
					"from pyspark.sql import SparkSession\n",
					"from notebookutils import mssparkutils\n",
					"import json\n",
					"import calendar\n",
					"from datetime import datetime, timedelta, date\n",
					"import pandas as pd\n",
					"import os\n",
					"import re\n",
					"from pyspark.sql.functions import col, lit, input_file_name, current_timestamp, sha2, concat,max as spark_max,date_format,to_date\n",
					"from pyspark.sql.types import StringType, DateType, TimestampType, IntegerType, FloatType, StructType, StructField\n",
					"from delta import DeltaTable"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Initialise the parameters"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"date_folder=''\n",
					"source_folder='Horizon' #need change\n",
					"specific_file='' # if not provided, it will ingest all files in the date_folder\n",
					"Load_data_source=''"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"source": [
					"spark = SparkSession.builder.getOrCreate()\n",
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"source_path: str = f\"abfss://odw-raw@{storage_account}{source_folder}/\"\n",
					"standardised_container = f\"abfss://odw-standardised@{storage_account}\"\n",
					"harmonised_container = f\"abfss://odw-harmonised@{storage_account}\"\n",
					"path_to_orchestration_file = f\"abfss://odw-config@{storage_account}/orchestration/orchestration.json\"\n",
					"ingestion_log_schema_loc = f\"abfss://odw-config@{storage_account}horizon_refactored_log_schema.json\"\n",
					"ingestion_log_table_location = f\"abfss://logging@{storage_account}horizon_refactored_log.json\"\n",
					"# print(source_path)\n",
					"# print(standardised_container)\n",
					"# print(path_to_orchestration_file)\n",
					"# print(ingestion_log_schema_loc)\n",
					"# print(ingestion_log_table_location)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Retrieve and Display Latest Modified Folder in Directory"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"#   df = spark.sql(\"Drop Table logging.horizon_refactored_log\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# from pyspark.sql import SparkSession\n",
					"# from pyspark.sql.types import StructType\n",
					"\n",
					"# # Initialize Spark session (if not already running)\n",
					"# spark = SparkSession.builder \\\n",
					"#     .appName(\"Create Delta Table from JSON Schema\") \\\n",
					"#     .enableHiveSupport() \\\n",
					"#     .getOrCreate()\n",
					"\n",
					"# database_name = \"logging\"\n",
					"# table_name = \"Horizon_Refactored_log\"\n",
					"\n",
					"# # Step 1: Load schema from JSON file\n",
					"# schema_json_df = spark.read.option(\"multiline\", \"true\").json(ingestion_log_schema_loc)\n",
					"# fields = schema_json_df.select(\"fields\").collect()[0][\"fields\"]\n",
					"\n",
					"# # Step 2: Convert Row objects to dictionaries and add 'metadata'\n",
					"# fields = [dict(field.asDict(), metadata={}) for field in fields]\n",
					"\n",
					"# # Step 3: Convert JSON schema to StructType\n",
					"# schema = StructType.fromJson({\"type\": \"struct\", \"fields\": fields})\n",
					"\n",
					"# # Step 4: Create an empty DataFrame with the loaded schema\n",
					"# empty_df = spark.createDataFrame([], schema)\n",
					"\n",
					"# # Step 5: Write the empty DataFrame as a Delta table\n",
					"# empty_df.write.format(\"delta\").mode(\"overwrite\").save(ingestion_log_table_location)\n",
					"\n",
					"# # Step 6: Ensure the database exists\n",
					"# spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
					"\n",
					"# # Step 7: Register the Delta table in the metastore\n",
					"# spark.sql(f\"\"\"\n",
					"#     CREATE TABLE IF NOT EXISTS {database_name}.{table_name}\n",
					"#     USING DELTA\n",
					"#     LOCATION '{ingestion_log_table_location}'\n",
					"# \"\"\")\n",
					"\n",
					"# print(f\"Delta table '{database_name}.{table_name}' created and registered successfully.\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# List all items in the directory\n",
					"items = mssparkutils.fs.ls(source_path)\n",
					"# Filter for directories and get their names and modification times\n",
					"folders = [(item.name, item.modifyTime) for item in items if item.isDir]\n",
					"# Sort folders by modification time in descending order\n",
					"sorted_folders = sorted(folders, key=lambda x: x[1], reverse=True)\n",
					"# Get the name of the latest modified folder\n",
					"if sorted_folders:\n",
					"    latest_folder = sorted_folders[0][0]\n",
					"    source_path=f\"{source_path}{latest_folder}\"\n",
					"    print(f\"Latest modified folder: {latest_folder}\")\n",
					"    print(source_path)\n",
					"else:\n",
					"    print(\"No folders found in the specified directory.\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### List and Retrieve File Names from Directory\""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"files = mssparkutils.fs.ls(source_path)\n",
					"horizon_files = [file.name for file in files]\n",
					"# horizon_files= horizon_files[:3]\n",
					"horizon_files\n",
					" # comment this during the full execution"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"nest_asyncio.apply()\n",
					"\n",
					"async def refactored_log(date_folder: str, file: str):\n",
					"    try:\n",
					"        spark = SparkSession.builder.getOrCreate()\n",
					"\n",
					"        # Handle date formatting\n",
					"        if not date_folder:\n",
					"            date_folder = datetime.now().date()\n",
					"        else:\n",
					"            date_folder = datetime.strptime(date_folder, \"%Y-%m-%d\")\n",
					"\n",
					"        date_folder_str = date_folder.strftime('%Y-%m-%d')\n",
					"\n",
					"        # Load orchestration definitions\n",
					"        df = spark.read.option(\"multiline\", \"true\").json(path_to_orchestration_file)\n",
					"        definitions = json.loads(df.toJSON().first())['definitions']\n",
					"\n",
					"        # Match definition for the specific file\n",
					"        definition = next((d for d in definitions if file.startswith(d.get('Source_Filename_Start', ''))), None)\n",
					"\n",
					"        if not definition:\n",
					"            print(f\"No matching definition found for file: {file}\")\n",
					"            return\n",
					"\n",
					"        # Clean up paths and names\n",
					"        standardised_path = definition.get('Standardised_Path', '').strip('/') + '/'\n",
					"        standardised_table_name = definition.get('Standardised_Table_Name', '').strip()\n",
					"        harmonised_table_name = definition.get(\"Harmonised_Table_Name\", \"\").strip()\n",
					"        \n",
					"        # Construct table locations\n",
					"        standardised_table_location = standardised_container + standardised_path + standardised_table_name\n",
					"\n",
					"        # Build full file path\n",
					"        full_file_path = f\"{source_path}/{file}\"\n",
					"        print(\"Full File Path:\", full_file_path)\n",
					"\n",
					"        # Get file metadata (modification time)\n",
					"        files = mssparkutils.fs.ls(source_path)\n",
					"        file_info = next((f for f in files if f.name == file and not f.isDir), None)\n",
					"        if not file_info:\n",
					"            raise FileNotFoundError(f\"File {file} not found in {source_path}\")\n",
					"\n",
					"        modification_time = datetime.fromtimestamp(file_info.modifyTime / 1000)\n",
					"\n",
					"        # Read input file and count rows\n",
					"        input_df = spark.read.csv(full_file_path, header=True)\n",
					"        input_file_row_count = input_df.count()\n",
					"\n",
					"        # Read standardised table\n",
					"        standardised_table_df = spark.read.parquet(standardised_table_location)\n",
					"\n",
					"        # Determine rows_new_count based on Load_data_source\n",
					"        if Load_data_source == 'pln_master':\n",
					"            max_ingested_date = standardised_table_df.select(\n",
					"                spark_max(to_date(\"ingested_datetime\"))\n",
					"            ).collect()[0][0]\n",
					"            print(max_ingested_date)\n",
					"\n",
					"            standardised_new_count = standardised_table_df.filter(\n",
					"                to_date(col(\"ingested_datetime\")) == lit(max_ingested_date)\n",
					"            ).count()\n",
					"\n",
					"        elif Load_data_source == 'pln_master_Refactored_Horizon':\n",
					"             today_str = datetime.today().strftime('%Y-%m-%d')\n",
					"             datetime_after_6pm = f\"{today_str} 17:00:00\"\n",
					"             standardised_new_count = standardised_table_df.filter(col(\"ingested_datetime\") > lit(datetime_after_6pm)).count()\n",
					"        else:\n",
					"            standardised_new_count = 0\n",
					"\n",
					"        # Read harmonised table and count rows\n",
					"        if \"Harmonised_NoteBook\" in definition and definition[\"Harmonised_NoteBook\"]:\n",
					"            harmonised_table_df = spark.read.table(f\"odw_harmonised_db.{harmonised_table_name}\")\n",
					"            harmonised_table_row_count = harmonised_table_df.count()\n",
					"        else:\n",
					"            harmonised_table_name = \"not available\"\n",
					"            harmonised_table_row_count = 0\n",
					"\n",
					"        # Create log DataFrame\n",
					"        orchestration_df = spark.createDataFrame([definition])\n",
					"        refactored_log_df = orchestration_df.select(\n",
					"            lit(full_file_path).alias(\"raw_input_file_name\"),\n",
					"            lit(modification_time).cast(\"timestamp\").alias(\"input_file_updated_timestamp\"),\n",
					"            lit(input_file_row_count).alias(\"input_file_row_count\"),\n",
					"            col(\"Standardised_Table_Name\").alias(\"standardised_table_name\"),\n",
					"            lit(standardised_new_count).cast(\"int\").alias(\"standardised_table_row_count\"),\n",
					"            lit(harmonised_table_name).alias(\"Harmonised_table_name\"),\n",
					"            lit(harmonised_table_row_count).cast(\"int\").alias(\"Harmonised_table_row_count\"),\n",
					"            lit(Load_data_source).alias(\"load_data_source\"),\n",
					"            current_timestamp().alias(\"ingested_datetime\")\n",
					"        )\n",
					"\n",
					"        # Append to target table\n",
					"        refactored_log_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"logging.Horizon_Refactored_log\")\n",
					"\n",
					"        print(f\"Ingestion completed successfully for {file}\")\n",
					"\n",
					"    except Exception as e:\n",
					"        print(f\"Failed to process {file}: {e}\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"async def load_horizon_async():\n",
					"    tasks: list = [\n",
					"        refactored_log(date_folder=latest_folder, file=file) for file in horizon_files\n",
					"    ]\n",
					"    await asyncio.gather(*tasks)\n",
					"\n",
					"nest_asyncio.apply()\n",
					"loop = asyncio.get_event_loop()\n",
					"loop.run_until_complete(load_horizon_async())"
				],
				"execution_count": null
			}
		]
	}
}