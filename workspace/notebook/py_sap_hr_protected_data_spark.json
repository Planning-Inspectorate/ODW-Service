{
	"name": "py_sap_hr_protected_data_spark",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "5673f13e-5179-475b-80d9-cb649782cb22"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Harmoised layer and build a table for Curated Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;25-02-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This Notebook is designed to facilitate Enriches data with HR attributes."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Entity Name : Protected Data</u>\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Intialisations"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np\n",
					"\n",
					""
				],
				"execution_count": 44
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"%run utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Data Load into SAP_HR_PC\n",
					"\n",
					"###### handling of sensitive information (such as ethnic origin and disability status) is crucial for compliance with regulations like GDPR or equal employment laws. Ensuring that this data is accurately recorded and updated helps mitigate legal risks"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"\n",
					"\n",
					"try:\n",
					"   logInfo(\"Starting SAP HR Protected Characteristics data processing\")\n",
					"   \n",
					"   # Disable ANSI SQL mode for more flexible handling\n",
					"   logInfo(\"Disabling ANSI SQL mode\")\n",
					"   spark.sql(\"\"\"\n",
					"   SET spark.sql.ansi.enabled = false\n",
					"   \"\"\")\n",
					"   logInfo(\"ANSI SQL mode disabled successfully\")\n",
					"   \n",
					"   # Clear the target table\n",
					"   logInfo(\"Clearing target table odw_harmonised_db.SAP_HR_PC\")\n",
					"   spark.sql(\"\"\"\n",
					"   DELETE FROM odw_harmonised_db.SAP_HR_PC\n",
					"   \"\"\")\n",
					"   logInfo(\"Target table cleared successfully\")\n",
					"   \n",
					"   # Insert data with transformations\n",
					"   logInfo(\"Inserting transformed data into odw_harmonised_db.SAP_HR_PC\")\n",
					"   spark.sql(\"\"\"\n",
					"   INSERT INTO odw_harmonised_db.SAP_HR_PC\n",
					"   SELECT \n",
					"       RefNo,\n",
					"       NULLIF(EthnicOrigin, '') AS EthnicOrigin,\n",
					"       NULLIF(ReligiousDenominationKey, '') AS ReligiousDenominationKey,\n",
					"       NULLIF(SxO, '') AS SxO,\n",
					"       Grade,\n",
					"       NULLIF(DisabilityText, '') AS DisabilityText,\n",
					"       CAST(to_timestamp(Report_MonthEnd_Date, \"dd/MM/yyyy\") AS DATE) AS Report_MonthEnd_Date,\n",
					"       'saphr' AS SourceSystemID,\n",
					"       CURRENT_DATE() AS IngestionDate,\n",
					"       CURRENT_TIMESTAMP() AS ValidTo,\n",
					"       md5(concat_ws('|', RefNo, EthnicOrigin, ReligiousDenominationKey, SxO, Grade, DisabilityText)) AS RowID,\n",
					"       'Y' AS IsActive\n",
					"   FROM odw_standardised_db.sap_protected_monthly\n",
					"   \"\"\")\n",
					"   \n",
					"   # Get count of records inserted\n",
					"   record_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.SAP_HR_PC\").collect()[0]['count']\n",
					"   logInfo(f\"Successfully inserted {record_count} records into odw_harmonised_db.SAP_HR_PC\")\n",
					"   \n",
					"   logInfo(\"SAP HR Protected Characteristics data processing completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"   # Log the exception in detail\n",
					"   logError(f\"Error in SAP HR Protected Characteristics data processing: {str(e)}\")\n",
					"   logException(e)\n",
					"   \n",
					"   # Re-raise the exception to ensure the notebook fails properly\n",
					"   raise e\n",
					"finally:\n",
					"   # Always flush logs regardless of success or failure\n",
					"   logInfo(\"Flushing logs\")\n",
					"   flushLogging()"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## PC_INSERT_DELETE"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"try:\n",
					"   logInfo(\"Starting complex MERGE operation for protected data\")\n",
					"   \n",
					"   # Complex MERGE operation with data preparation\n",
					"   logInfo(\"Executing MERGE to update sap_hr_protected_data\")\n",
					"   spark.sql(\"\"\"\n",
					"   -- First, ensure SAP_HR_PC is properly formatted with dates\n",
					"   WITH source_data AS (\n",
					"       SELECT \n",
					"           RefNo,\n",
					"           NULLIF(EthnicOrigin, '') AS EthnicOrigin,\n",
					"           NULLIF(ReligiousDenominationKey, '') AS ReligiousDenominationKey,\n",
					"           NULLIF(SxO, '') AS SxO,\n",
					"           Grade,\n",
					"           NULLIF(DisabilityText, '') AS DisabilityText,\n",
					"           cast(to_timestamp(Report_MonthEnd_Date, \"dd/MM/yyyy\") as date) AS Report_MonthEnd_Date,\n",
					"           'saphr' AS SourceSystemID,\n",
					"           CURRENT_DATE() AS IngestionDate,\n",
					"           CURRENT_TIMESTAMP() AS ValidTo,\n",
					"           NULL AS RowID,\n",
					"           'Y' AS IsActive,\n",
					"           -- Calculate hash in the source directly\n",
					"           md5(concat(\n",
					"               coalesce(RefNo, ''),\n",
					"               coalesce(NULLIF(EthnicOrigin, ''), ''),\n",
					"               coalesce(NULLIF(ReligiousDenominationKey, ''), ''),\n",
					"               coalesce(NULLIF(SxO, ''), ''),\n",
					"               coalesce(Grade, ''),\n",
					"               coalesce(NULLIF(DisabilityText, ''), '')\n",
					"           )) AS record_hash\n",
					"       FROM odw_harmonised_db.SAP_HR_PC\n",
					"   ),\n",
					"   -- Deduplicate source data to ensure only one row per key\n",
					"   deduplicated_source AS (\n",
					"       SELECT *,\n",
					"              ROW_NUMBER() OVER (\n",
					"                  PARTITION BY RefNo, Report_MonthEnd_Date \n",
					"                  ORDER BY IngestionDate DESC\n",
					"              ) AS row_num\n",
					"       FROM source_data\n",
					"   )\n",
					"   \n",
					"   -- Use MERGE with deduplicated source\n",
					"   MERGE INTO odw_harmonised_db.sap_hr_protected_data target\n",
					"   USING (SELECT * FROM deduplicated_source WHERE row_num = 1) source\n",
					"   ON source.RefNo = target.RefNo AND source.Report_MonthEnd_Date = target.Report_MonthEnd_Date\n",
					"   WHEN MATCHED AND \n",
					"        md5(concat(\n",
					"           coalesce(target.RefNo, ''),\n",
					"           coalesce(target.EthnicOrigin, ''),\n",
					"           coalesce(target.ReligiousDenominationKey, ''),\n",
					"           coalesce(target.SxO, ''),\n",
					"           coalesce(target.Grade, ''),\n",
					"           coalesce(target.DisabilityText, '')\n",
					"        )) != source.record_hash \n",
					"   THEN UPDATE SET\n",
					"       EthnicOrigin = source.EthnicOrigin,\n",
					"       ReligiousDenominationKey = source.ReligiousDenominationKey,\n",
					"       SxO = source.SxO,\n",
					"       Grade = source.Grade,\n",
					"       DisabilityText = source.DisabilityText,\n",
					"       SourceSystemID = source.SourceSystemID,\n",
					"       IngestionDate = source.IngestionDate,\n",
					"       ValidTo = source.ValidTo,\n",
					"       IsActive = source.IsActive\n",
					"   WHEN NOT MATCHED THEN\n",
					"   INSERT (\n",
					"       RefNo, EthnicOrigin, ReligiousDenominationKey, SxO, Grade, \n",
					"       DisabilityText, Report_MonthEnd_Date, SourceSystemID, \n",
					"       IngestionDate, ValidTo, RowID, IsActive\n",
					"   )\n",
					"   VALUES (\n",
					"       source.RefNo, source.EthnicOrigin, source.ReligiousDenominationKey, \n",
					"       source.SxO, source.Grade, source.DisabilityText, source.Report_MonthEnd_Date, \n",
					"       source.SourceSystemID, source.IngestionDate, source.ValidTo, \n",
					"       source.RowID, source.IsActive\n",
					"   )\n",
					"   \"\"\")\n",
					"   \n",
					"   # Get counts for audit\n",
					"   source_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.SAP_HR_PC\").collect()[0]['count']\n",
					"   target_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.sap_hr_protected_data\").collect()[0]['count']\n",
					"   \n",
					"   logInfo(f\"MERGE operation completed. Source record count: {source_count}, Target record count: {target_count}\")\n",
					"   logInfo(\"Complex MERGE operation completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"   # Log the exception in detail\n",
					"   logError(f\"Error in complex MERGE operation: {str(e)}\")\n",
					"   logException(e)\n",
					"   \n",
					"   # Re-raise the exception to ensure the notebook fails properly\n",
					"   raise e\n",
					"finally:\n",
					"   # Always flush logs regardless of success or failure\n",
					"   logInfo(\"Flushing logs\")\n",
					"   flushLogging()"
				],
				"execution_count": 2
			}
		]
	}
}