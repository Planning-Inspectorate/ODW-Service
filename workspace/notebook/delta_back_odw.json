{
	"name": "delta_back_odw",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "01ce5c2f-3220-48a2-a7cf-aa694943f1ff"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###### This Notebook was created to perform delta backup using existing-tables-metadata (ETM). \n",
					"###### First lists all the files in the container and filters the delta files based on format from ETM\n",
					"###### All files minus the delta files gives non-delta files which is sent as dict to a copy activity in the pipeline. \n",
					"###### There were inconsistencies in ETM formats hence using another notebook delta_back_odw_validate"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import pprint\n",
					"from pyspark.sql.types import *\n",
					"import json\n",
					"import re\n",
					"from pyspark.sql.functions import col, explode"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"source_storage_account_path = re.search('url=https://(.+?);', mssparkutils.credentials.getFullConnectionString('ls_backup_source')).group(1)\n",
					"container            = 'odw-standardised'\n",
					"full_storage_path    = f'abfss://{container}@{source_storage_account_path}'"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"backup_storage_account_path=re.search('url=https://(.+?);', mssparkutils.credentials.getFullConnectionString('ls_backup_destination')).group(1)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"storage_account: str = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"metadata_path: str = \"abfss://odw-config@\"+storage_account+\"existing-tables-metadata.json\"\n",
					"df_metadata  = spark.read.json(metadata_path, multiLine=True)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"all_file_path_list = []\n",
					"for i in mssparkutils.fs.ls(full_storage_path):\n",
					"    all_file_path_list.append(str(i).split(\"=\")[1].split(\",\")[0])\n",
					"all_file_path_list = [ x for x in all_file_path_list if \"test\" not in x ]"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Explode each metadata category separately (as they are nested lists)\n",
					"standardised_df = df_metadata.select(explode(col(\"standardised_metadata\")).alias(\"metadata\"))\n",
					"harmonised_df   = df_metadata.select(explode(col(\"harmonised_metadata\")).alias(\"metadata\"))\n",
					"curated_df      = df_metadata.select(explode(col(\"curated_metadata\")).alias(\"metadata\"))\n",
					"logging_df      = df_metadata.select(explode(col(\"logging_metadata\")).alias(\"metadata\"))\n",
					"config_df       = df_metadata.select(explode(col(\"config_metadata\")).alias(\"metadata\"))\n",
					"\n",
					"# Select the relevant fields\n",
					"df_exploded = standardised_df.union(harmonised_df).union(curated_df).union(logging_df).union(config_df).selectExpr(\"metadata.*\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"ls_delta = df_exploded.filter(col(\"table_format\") == 'delta').filter(col(\"database_name\")==\"odw_standardised_db\").select(\"table_location\").collect()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"delta_file_list = []\n",
					"for i in ls_delta:\n",
					"    delta_file_list.append(str(i).split(\"=\")[1].split(\")\")[0])"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def list_all_files(base_path, relative_path=\"\"):\n",
					"    files = []\n",
					"    full_path = f\"{base_path}/{relative_path}\" if relative_path else base_path\n",
					"    \n",
					"    try:\n",
					"        items = mssparkutils.fs.ls(full_path)\n",
					"    except Exception as e:\n",
					"        print(f\"Warning: Could not access path '{full_path}': {str(e)}\")\n",
					"        return full_path  # Skip this path and continue\n",
					"\n",
					"    for item in items:\n",
					"        item_path = f\"{relative_path}/{item.name}\".rstrip(\"/\")\n",
					"        if item.isDir:\n",
					"            files.extend(list_all_files(base_path, item_path))\n",
					"        else:\n",
					"            files.append(item_path)\n",
					"\n",
					"    return set(files)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"for file in delta_file_list:\n",
					"    try:\n",
					"        container_name = file.split(\"@\")[0].split(\"//\")[1]\n",
					"        base_path = file.split(\"'\")[1]\n",
					"        backup_path = base_path.replace(source_storage_account_path, backup_storage_account_path).replace(container_name, \"delta-backup-container\")\n",
					"\n",
					"        # Try to list files in base path\n",
					"        try:\n",
					"            base_path_lst = list_all_files(base_path)\n",
					"        except Exception as e:\n",
					"            print(f\"Error listing files at base path '{base_path}': {e}\")\n",
					"            continue\n",
					"\n",
					"        # Create the backup directory\n",
					"        try:\n",
					"            mssparkutils.fs.mkdirs(backup_path)\n",
					"        except Exception as e:\n",
					"            print(f\"Error creating directory at backup path '{backup_path}': {e}\")\n",
					"            continue\n",
					"\n",
					"        # Try to list files in backup path\n",
					"        try:\n",
					"            backup_path_lst = list_all_files(backup_path)\n",
					"        except Exception as e:\n",
					"            print(f\"Error listing files at backup path '{backup_path}': {e}\")\n",
					"            continue\n",
					"\n",
					"        # Identify delta files\n",
					"        delta_files = set(base_path_lst) - set(backup_path_lst)\n",
					"        print(f\"{len(delta_files)} new delta files found for backup.\")\n",
					"\n",
					"        # Copy each delta file\n",
					"        for f in delta_files:\n",
					"            try:\n",
					"                # mssparkutils.fs.cp(base_path + f, backup_path + f)\n",
					"                print(f\"Copied file: {base_path + f} to {backup_path + f}\")\n",
					"            except Exception as e:\n",
					"                print(f\"Error copying file '{f}': {e}\")\n",
					"\n",
					"    except Exception as e:\n",
					"        print(f\"Unexpected error processing file '{file}': {e}\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"delta_file_list = []\n",
					"for i in ls_delta:\n",
					"    delta_file_list.append(str(i).split(\"=\")[1].split(\")\")[0])"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"non_delta_file_list = set(all_file_path_list)-set(delta_file_list)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"non_delta_file_dict = {\n",
					"    \"backup_storage_name\":backup_storage_account_path.split(\".\")[0],\n",
					"    \"non_delta_file_list\":list(non_delta_file_list)\n",
					"}"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"mssparkutils.notebook.exit(non_delta_file_dict)"
				],
				"execution_count": null
			}
		],
		"folder": {
			"name": "archive/"
		}
	}
}