{
	"name": "delta_back_odw",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodwpr",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "16ff634f-26a6-4104-a468-477d1e4bbb9d"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodwpr",
				"name": "pinssynspodwpr",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodwpr",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import pprint\n",
					"from pyspark.sql.types import *\n",
					"import json\n",
					"import re\n",
					"from pyspark.sql.functions import col, explode"
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"source": [
					"source_storage_account_path = re.search('url=https://(.+?);', mssparkutils.credentials.getFullConnectionString('ls_backup_source')).group(1)\n",
					"container            = 'odw-standardised'\n",
					"full_storage_path    = f'abfss://{container}@{source_storage_account_path}'"
				],
				"execution_count": 38
			},
			{
				"cell_type": "code",
				"source": [
					"backup_storage_account_path=re.search('url=https://(.+?);', mssparkutils.credentials.getFullConnectionString('ls_backup_destination')).group(1)"
				],
				"execution_count": 39
			},
			{
				"cell_type": "code",
				"source": [
					"storage_account: str = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"metadata_path: str = \"abfss://odw-config@\"+storage_account+\"existing-tables-metadata.json\"\n",
					"df_metadata  = spark.read.json(metadata_path, multiLine=True)"
				],
				"execution_count": 40
			},
			{
				"cell_type": "code",
				"source": [
					"all_file_path_list = []\n",
					"for i in mssparkutils.fs.ls(full_storage_path):\n",
					"    all_file_path_list.append(str(i).split(\"=\")[1].split(\",\")[0])\n",
					"all_file_path_list = [ x for x in all_file_path_list if \"test\" not in x ]"
				],
				"execution_count": 41
			},
			{
				"cell_type": "code",
				"source": [
					"# Explode each metadata category separately (as they are nested lists)\n",
					"standardised_df = df_metadata.select(explode(col(\"standardised_metadata\")).alias(\"metadata\"))\n",
					"harmonised_df   = df_metadata.select(explode(col(\"harmonised_metadata\")).alias(\"metadata\"))\n",
					"curated_df      = df_metadata.select(explode(col(\"curated_metadata\")).alias(\"metadata\"))\n",
					"logging_df      = df_metadata.select(explode(col(\"logging_metadata\")).alias(\"metadata\"))\n",
					"config_df       = df_metadata.select(explode(col(\"config_metadata\")).alias(\"metadata\"))\n",
					"\n",
					"# Select the relevant fields\n",
					"df_exploded = standardised_df.union(harmonised_df).union(curated_df).union(logging_df).union(config_df).selectExpr(\"metadata.*\")"
				],
				"execution_count": 42
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"ls_delta = df_exploded.filter(col(\"table_format\") == 'delta').filter(col(\"database_name\")==\"odw_standardised_db\").select(\"table_location\").collect()"
				],
				"execution_count": 43
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": true
					}
				},
				"source": [
					"# # identify _delta_log and process the files from source\n",
					"\n",
					"\n",
					"# def list_all_paths(path):\n",
					"#     \"\"\"Recursively list all paths under the given DBFS path.\"\"\"\n",
					"#     try:\n",
					"#         files = dbutils.fs.ls(path)\n",
					"#     except Exception as e:\n",
					"#         print(f\"Error accessing {path}: {e}\")\n",
					"#         return []\n",
					"\n",
					"#     all_paths = []\n",
					"#     for f in files:\n",
					"#         if f.isDir():\n",
					"#             all_paths.append(f.path)\n",
					"#             all_paths.extend(list_all_paths(f.path))\n",
					"#     return all_paths\n",
					"\n",
					"# def is_delta_table(path):\n",
					"#     \"\"\"Check if the given path is a Delta table by looking for the _delta_log directory.\"\"\"\n",
					"#     try:\n",
					"#         delta_log_path = path.rstrip(\"/\") + \"/_delta_log\"\n",
					"#         dbutils.fs.ls(delta_log_path)  # will throw if _delta_log doesn't exist\n",
					"#         return True\n",
					"#     except:\n",
					"#         return False\n",
					"\n",
					"# # Replace with your container path\n",
					"# container_path = \"abfss://dfs.core.windows.net/rnw/\"  # Example: \"dbfs:/mnt/datalake-container/\"\n",
					"\n",
					"# # List all directories\n",
					"# all_dirs = list_all_paths(container_path)\n",
					"\n",
					"# # Filter for Delta tables\n",
					"# delta_tables = [p for p in all_dirs if is_delta_table(p)]\n",
					"\n",
					"# # Display Delta table paths\n",
					"# for table_path in delta_tables:\n",
					"#     print(f\"Delta Table Found: {table_path}\")\n",
					""
				],
				"execution_count": 44
			},
			{
				"cell_type": "code",
				"source": [
					"delta_file_list = []\n",
					"for i in ls_delta:\n",
					"    delta_file_list.append(str(i).split(\"=\")[1].split(\")\")[0])"
				],
				"execution_count": 45
			},
			{
				"cell_type": "code",
				"source": [
					"def list_all_files(base_path, relative_path=\"\"):\n",
					"    files = []\n",
					"    full_path = f\"{base_path}/{relative_path}\" if relative_path else base_path\n",
					"    \n",
					"    try:\n",
					"        items = mssparkutils.fs.ls(full_path)\n",
					"    except Exception as e:\n",
					"        print(f\"Warning: Could not access path '{full_path}': {str(e)}\")\n",
					"        return full_path  # Skip this path and continue\n",
					"\n",
					"    for item in items:\n",
					"        item_path = f\"{relative_path}/{item.name}\".rstrip(\"/\")\n",
					"        if item.isDir:\n",
					"            files.extend(list_all_files(base_path, item_path))\n",
					"        else:\n",
					"            files.append(item_path)\n",
					"\n",
					"    return set(files)"
				],
				"execution_count": 46
			},
			{
				"cell_type": "code",
				"source": [
					"len(delta_file_list)"
				],
				"execution_count": 47
			},
			{
				"cell_type": "code",
				"source": [
					"type(base_path_lst)"
				],
				"execution_count": 48
			},
			{
				"cell_type": "code",
				"source": [
					"for file in delta_file_list:\n",
					"    container_name = file.split(\"@\")[0].split(\"//\")[1]\n",
					"    base_path = file.split(\"'\")[1]\n",
					"    base_path_lst = list_all_files(file.split(\"'\")[1])\n",
					"    backup_path = base_path.replace(source_storage_account_path,backup_storage_account_path).replace(container_name,\"delta-backup-container\")\n",
					"    mssparkutils.fs.mkdirs(file.split(\"'\")[1].replace(source_storage_account_path,backup_storage_account_path).replace(container_name,\"delta-backup-container\"))\n",
					"    backup_path_lst = list_all_files(file.split(\"'\")[1].replace(source_storage_account_path,backup_storage_account_path).replace(container_name,\"delta-backup-container\"))\n",
					"    delta_files = set(base_path_lst)-backup_path_lst\n",
					"    print(len(delta_files))\n",
					"    for file in delta_files:\n",
					"        total_tile = len(file)\n",
					"        # print(base_path+file,backup_path+file)\n",
					"        # mssparkutils.fs.cp(base_path+file,backup_path+file)"
				],
				"execution_count": 49
			},
			{
				"cell_type": "code",
				"source": [
					"delta_file_list = []\n",
					"for i in ls_delta:\n",
					"    delta_file_list.append(str(i).split(\"=\")[1].split(\")\")[0])"
				],
				"execution_count": 50
			},
			{
				"cell_type": "code",
				"source": [
					"non_delta_file_list = set(all_file_path_list)-set(delta_file_list)"
				],
				"execution_count": 51
			},
			{
				"cell_type": "code",
				"source": [
					"non_delta_file_dict = {\n",
					"    \"backup_storage_name\":backup_storage_account_path.split(\".\")[0],\n",
					"    \"non_delta_file_list\":list(non_delta_file_list)\n",
					"}"
				],
				"execution_count": 52
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"std_total_size = 2000\n",
					"std_delta_size = 20\n",
					"std_diff % = 99%"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"mssparkutils.notebook.exit(non_delta_file_dict)"
				],
				"execution_count": 53
			},
			{
				"cell_type": "code",
				"source": [
					"#mssparkutils.notebook.exit(list(non_delta_file_list))"
				],
				"execution_count": null
			}
		]
	}
}