{
	"name": "Curated_table_load_raw_representation",
	"properties": {
		"folder": {
			"name": "service-bus"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "bec54710-7941-4ce6-a60b-7fb6cdb72105"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"entity_name='nsip-representation'"
				],
				"execution_count": 68
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"from notebookutils import mssparkutils\r\n",
					"import json\r\n",
					"from datetime import datetime, date\r\n",
					"from pyspark.sql.functions import current_timestamp, expr, to_timestamp, lit\r\n",
					"from pyspark.sql import DataFrame\r\n",
					"from pyspark.sql.functions import *\r\n",
					"from pyspark.sql.functions import col, lit\r\n",
					"from pyspark.sql.types import LongType"
				],
				"execution_count": 69
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark: SparkSession = SparkSession.builder.getOrCreate()\r\n",
					"storage_account: str = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\r\n",
					"table_name: str = f\"odw_standardised_db.sb_{entity_name.replace('-', '_')}\"\r\n",
					"today: str = datetime.now().date().strftime('%Y-%m-%d')\r\n",
					"source_path: str = f\"abfss://odw-raw@{storage_account}ServiceBus/{entity_name}/{today}/\""
				],
				"execution_count": 70
			},
			{
				"cell_type": "code",
				"source": [
					"def load_data_from_raw():\r\n",
					"    \r\n",
					"    nsip_project_folder: str = \"abfss://odw-raw@\"+storage_account+\"ServiceBus/nsip-representation\"\r\n",
					"    df=(spark.read.format(\"json\") \r\n",
					"        .option(\"recursiveFileLookup\", \"true\")\r\n",
					"        .option(\"pathGlobFilter\",\"*.json\")\r\n",
					"        .load(nsip_project_folder))\r\n",
					"    # drop corrupt records loaded with bad files and any test columns created during testing\r\n",
					"    #valid_df = df.filter(col(\"_corrupt_record\").isNull()).drop(\"_corrupt_record\", \"baddata\")\r\n",
					"    return df"
				],
				"execution_count": 71
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df = load_data_from_raw()\r\n",
					"display(df)\r\n",
					"\r\n",
					"\r\n",
					"existing = spark.sql(\"SELECT message_id, ingested_datetime, expected_from, expected_to FROM odw_standardised_db.nsip_representation\")\r\n",
					"\r\n",
					"df = df.join(existing, ['message_id'], how='inner')\r\n",
					"df = df.withColumn(\"migrated\", lit(\"N\"))\r\n",
					"df = df.withColumn(\"NSIPRepresentaionID\", lit(None).cast(LongType()))\r\n",
					"\r\n",
					"\r\n",
					"df = df.selectExpr(\"message_id\"\r\n",
					"                    ,\"attachmentIds\"\r\n",
					"                    ,\"caseId\"\r\n",
					"                    ,\"caseRef\"\r\n",
					"                    ,\"dateReceived\"\r\n",
					"                    ,\"examinationLibraryRef\"\r\n",
					"                    ,\"message_enqueued_time_utc\"\r\n",
					"                    ,\"message_type\"\r\n",
					"                    ,\"originalRepresentation\"\r\n",
					"                    ,\"redacted\"\r\n",
					"                    ,\"redactedBy\"\r\n",
					"                    ,\"redactedNotes\"\r\n",
					"                    ,\"redactedRepresentation\"\r\n",
					"                    ,\"referenceId\"\r\n",
					"                    ,\"registerFor\"\r\n",
					"                    ,\"representationFrom\"\r\n",
					"                    ,\"representationId\"\r\n",
					"                    ,\"status\"\r\n",
					"                    ,\"ingested_datetime\"\r\n",
					"                    ,\"expected_from\"\r\n",
					"                    ,\"expected_to\"\r\n",
					"                    ,\"migrated\"\r\n",
					"                    ,\"NSIPRepresentaionID\")\r\n",
					""
				],
				"execution_count": 72
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def apply_df_to_table(df: DataFrame, db_name: str, table_name: str) -> None:\r\n",
					"\r\n",
					"    from notebookutils import mssparkutils\r\n",
					"\r\n",
					"    # Write the DataFrame with the new column to a new temporary table\r\n",
					"    temp_table_name: str = 'temporary_table'\r\n",
					"    df.write.mode(\"overwrite\").saveAsTable(f\"{db_name}.{temp_table_name}\")\r\n",
					"\r\n",
					"    # Drop the original table\r\n",
					"    spark.sql(f\"DROP TABLE IF EXISTS {db_name}.{table_name}\")\r\n",
					"\r\n",
					"    # Rename the temporary table to replace the original table\r\n",
					"    spark.sql(f\"ALTER TABLE {db_name}.{temp_table_name} RENAME TO {db_name}.{table_name}\")"
				],
				"execution_count": 73
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"apply_df_to_table(df, 'odw_standardised_db', 'sb_nsip_representation')"
				],
				"execution_count": 74
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"REFRESH TABLE odw_standardised_db.sb_nsip_representation"
				],
				"execution_count": 75
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"check = spark.sql(\"select * from odw_standardised_db.sb_nsip_representation order by message_id\")\r\n",
					"\r\n",
					"display(check)"
				],
				"execution_count": 76
			}
		]
	}
}