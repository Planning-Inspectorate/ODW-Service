{
	"name": "py_fact_absence_sickness",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "d9c79934-75dc-4bf3-93b4-6d7072bf32fb"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Harmoised layer and build a table for Curated Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;25-02-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This Notebook is designed to facilitate the monthly processing and harmonization of Sickness data. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The Notebook ensures that Sickness data is accurately transformed, stored, and made available for reporting and analysis.;"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Import required libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import col, when, sum, max, min, count, row_number, lag\n",
					"from pyspark.sql.types import StringType, IntegerType, DoubleType\n",
					"from datetime import datetime\n",
					"import json"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Function with Variable Initializations"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def create_staff_number_transformation_sql():\n",
					"    \"\"\"\n",
					"    Returns SQL for staff number transformation logic\n",
					"    Extracted from complex CASE statement for reusability and maintainability\n",
					"    \"\"\"\n",
					"    return \"\"\"\n",
					"        CASE\n",
					"            WHEN trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0')) LIKE '4%' THEN \n",
					"                '50' || trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0'))\n",
					"            WHEN trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0')) LIKE '5%' THEN \n",
					"                '00' || trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0'))\n",
					"            WHEN trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0')) LIKE '6%' THEN\n",
					"                CASE \n",
					"                    WHEN length(trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0'))) >= 8 THEN \n",
					"                        trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0'))\n",
					"                    WHEN lpad(CAST(StaffNumber AS STRING), 8, '0') LIKE '006%' THEN\n",
					"                        '6' || substring(lpad(CAST(StaffNumber AS STRING), 8, '0'), 3)\n",
					"                    ELSE '60' || trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0'))\n",
					"                END\n",
					"            ELSE lpad(CAST(StaffNumber AS STRING), 7, '0')\n",
					"        END\n",
					"    \"\"\"\n",
					"\n",
					"# Configuration setup\n",
					"try:\n",
					"    logInfo(\"Starting unified absence sickness fact table processing with corrected holiday logic\")\n",
					"    \n",
					"    # First, set ANSI mode to false to handle potential type conversion issues  \n",
					"    logInfo(\"Setting ANSI mode to false\")\n",
					"    spark.sql(\"SET spark.sql.ansi.enabled=false\")\n",
					"    \n",
					"    # Set timezone to ensure consistent date handling\n",
					"    spark.sql(\"SET spark.sql.session.timeZone=UTC\")\n",
					"    logInfo(\"ANSI mode and timezone set successfully\")\n",
					"    \n",
					"    # Define configuration constants\n",
					"    TARGET_TABLE = \"odw_harmonised_db.sap_hr_fact_absence_sickness\"\n",
					"    SOURCE_TABLE = \"odw_harmonised_db.sap_hr_absence_all\"  \n",
					"    HOLIDAYS_TABLE = \"odw_standardised_db.Live_Holidays\"\n",
					"    DATE_DIM_TABLE = \"odw_harmonised_db.live_dim_date\"\n",
					"    PROCESSING_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
					"    \n",
					"    # Clean the target table\n",
					"    logInfo(f\"Starting deletion of all rows from {TARGET_TABLE}\")\n",
					"    spark.sql(f\"DELETE FROM {TARGET_TABLE}\")\n",
					"    logInfo(f\"Successfully deleted all rows from {TARGET_TABLE}\")\n",
					"    \n",
					"except Exception as e:\n",
					"    logError(f\"Error in initialization: {str(e)}\")\n",
					"    raise e"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Holiday View Creation"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"     \n",
					"    logInfo(\"Creating holidays view for integration\")\n",
					"    spark.sql(f\"\"\"\n",
					"    CREATE OR REPLACE TEMP VIEW holidays_clean AS\n",
					"    SELECT DISTINCT\n",
					"        CASE \n",
					"            WHEN HolidayDate RLIKE '^[0-9]{{1,2}}/[0-9]{{1,2}}/[0-9]{{4}}$' THEN \n",
					"                CAST(TO_TIMESTAMP(HolidayDate, 'dd/MM/yyyy') AS DATE)\n",
					"            WHEN HolidayDate RLIKE '^[0-9]{{4}}-[0-9]{{1,2}}-[0-9]{{1,2}}$' THEN \n",
					"                CAST(HolidayDate AS DATE)\n",
					"            ELSE NULL\n",
					"        END as holiday_date\n",
					"    FROM {HOLIDAYS_TABLE}\n",
					"    WHERE HolidayDate IS NOT NULL\n",
					"    \"\"\")\n",
					"    \n",
					"    # Create the initial sickness view with base data\n",
					"    logInfo(\"Creating initial sickness view with base data\")\n",
					"    spark.sql(f\"\"\"\n",
					"    CREATE OR REPLACE TEMP VIEW sickness AS\n",
					"    SELECT \n",
					"        CAST(a.StaffNumber AS STRING) AS StaffNumber,\n",
					"        CAST(a.StartDate AS DATE) AS sickness_start,\n",
					"        CAST(a.EndDate AS DATE) AS sickness_end,\n",
					"        CAST(a.Days AS DOUBLE) AS original_days,\n",
					"        a.SicknessGroup,\n",
					"        a.WorkScheduleRule,\n",
					"        CAST(ds.FY AS STRING) AS FY_start,\n",
					"        CAST(de.FY AS STRING) AS FY_end\n",
					"    FROM {SOURCE_TABLE} a\n",
					"    LEFT JOIN {DATE_DIM_TABLE} ds ON CAST(a.StartDate AS DATE) = ds.date\n",
					"    LEFT JOIN {DATE_DIM_TABLE} de ON CAST(a.EndDate AS DATE) = de.date\n",
					"    WHERE a.AttendanceorAbsenceType = 'Sickness'\n",
					"    \"\"\")\n",
					"    logInfo(\"Successfully created initial sickness view\")\n",
					"    \n",
					"except Exception as e:\n",
					"    logError(f\"Error creating base views: {str(e)}\")\n",
					"    raise e"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    # Step 3: Create view with holiday analysis  \n",
					"    logInfo(\"Creating view with holiday analysis for filtering\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMP VIEW sickness_with_holidays AS\n",
					"    SELECT \n",
					"        s.*,\n",
					"        -- Count holidays in the sickness period\n",
					"        COUNT(h.holiday_date) as holidays_in_period,\n",
					"        -- Calculate calendar days in the period\n",
					"        DATEDIFF(s.sickness_end, s.sickness_start) + 1 as calendar_days_in_period,\n",
					"        -- Keep original days (do NOT subtract holidays)\n",
					"        s.original_days as adjusted_days,\n",
					"        -- Flag if the entire period is only holidays\n",
					"        CASE \n",
					"            WHEN COUNT(h.holiday_date) = (DATEDIFF(s.sickness_end, s.sickness_start) + 1) THEN 1\n",
					"            ELSE 0\n",
					"        END as all_days_are_holidays\n",
					"    FROM sickness s\n",
					"    LEFT JOIN holidays_clean h \n",
					"        ON h.holiday_date BETWEEN s.sickness_start AND s.sickness_end\n",
					"        AND h.holiday_date IS NOT NULL\n",
					"    GROUP BY \n",
					"        s.StaffNumber, s.sickness_start, s.sickness_end, s.original_days,\n",
					"        s.SicknessGroup, s.WorkScheduleRule, s.FY_start, s.FY_end\n",
					"    \"\"\")\n",
					"    logInfo(\"Successfully created holiday-analyzed sickness view\")\n",
					"    \n",
					"    # Step 4: Create a view with proper ordering and previous period information\n",
					"    logInfo(\"Creating view with proper ordering for contiguity detection\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMP VIEW sickness_ordered AS\n",
					"    SELECT \n",
					"        ROW_NUMBER() OVER (ORDER BY StaffNumber, sickness_start, sickness_end) AS row_num,\n",
					"        StaffNumber,\n",
					"        sickness_start,\n",
					"        sickness_end,\n",
					"        adjusted_days as Days,  -- Use original days (not reduced by holidays)\n",
					"        original_days,\n",
					"        holidays_in_period,\n",
					"        calendar_days_in_period,\n",
					"        all_days_are_holidays,\n",
					"        SicknessGroup,\n",
					"        WorkScheduleRule,\n",
					"        FY_start,\n",
					"        FY_end,\n",
					"        -- Get the previous record's end date for the same staff member\n",
					"        LAG(sickness_end) OVER (PARTITION BY StaffNumber ORDER BY sickness_start, sickness_end) AS prev_sickness_end\n",
					"    FROM sickness_with_holidays\n",
					"    WHERE all_days_are_holidays = 0  -- Only exclude periods where ALL days are holidays\n",
					"    ORDER BY StaffNumber, sickness_start, sickness_end\n",
					"    \"\"\")\n",
					"    logInfo(\"Successfully created ordered view with corrected holiday logic\")\n",
					"    \n",
					"except Exception as e:\n",
					"    logError(f\"Error in holiday analysis and ordering: {str(e)}\")\n",
					"    raise e"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    #  Identify contiguous periods with deterministic logic\n",
					"    logInfo(\"Creating view to identify sickness periods\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMP VIEW sickness_with_groups AS\n",
					"    SELECT \n",
					"        *,\n",
					"        -- Deterministic group identification - new group starts when not contiguous\n",
					"        CASE \n",
					"            WHEN prev_sickness_end IS NULL THEN 1\n",
					"            WHEN sickness_start <= date_add(prev_sickness_end, 1) THEN 0\n",
					"            ELSE 1\n",
					"        END AS new_group_flag\n",
					"    FROM sickness_ordered\n",
					"    \"\"\")\n",
					"    logInfo(\"Successfully created contiguity detection view\")\n",
					"    \n",
					"    # Step 6: Assign group IDs using cumulative sum\n",
					"    logInfo(\"Creating view to assign sickness group IDs\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMP VIEW sickness_with_ids AS\n",
					"    SELECT \n",
					"        *,\n",
					"        -- Create deterministic group ID using cumulative sum\n",
					"        SUM(new_group_flag) OVER (\n",
					"            PARTITION BY StaffNumber \n",
					"            ORDER BY sickness_start, sickness_end \n",
					"            ROWS UNBOUNDED PRECEDING\n",
					"        ) AS sickness_group_id\n",
					"    FROM sickness_with_groups\n",
					"    \"\"\")\n",
					"    logInfo(\"Successfully created view with sickness group IDs\")\n",
					"    \n",
					"    # Step 7: Group the results by sickness group with fixed date logic\n",
					"    logInfo(\"Creating final sickness view with consistent date categorization\")\n",
					"    spark.sql(f\"\"\"\n",
					"    CREATE OR REPLACE TEMP VIEW sickness_final AS\n",
					"    SELECT \n",
					"        StaffNumber,\n",
					"        -- Create simple sequential sickness_id\n",
					"        ROW_NUMBER() OVER (ORDER BY StaffNumber, MIN(sickness_start), MAX(sickness_end)) AS sickness_id,\n",
					"        SUM(Days) AS Total_Days,\n",
					"        MIN(sickness_start) AS sickness_start,\n",
					"        MAX(sickness_end) AS sickness_end,\n",
					"        MAX(FY_start) AS FY,\n",
					"        -- Use fixed date for consistent categorization\n",
					"        CASE \n",
					"            WHEN MAX(sickness_end) < date_sub(DATE('{PROCESSING_DATE}'), 2 * 365) THEN 'Older FY'\n",
					"            WHEN MAX(sickness_end) BETWEEN date_sub(DATE('{PROCESSING_DATE}'), 2 * 365) \n",
					"                 AND date_sub(DATE('{PROCESSING_DATE}'), 1 * 365) THEN 'Previous FY'\n",
					"            WHEN MAX(sickness_end) BETWEEN date_sub(DATE('{PROCESSING_DATE}'), 1 * 365) \n",
					"                 AND DATE('{PROCESSING_DATE}') THEN 'Current FY'\n",
					"            ELSE 'Next FY'\n",
					"        END AS financial_year,\n",
					"        CASE \n",
					"            WHEN MAX(sickness_end) < date_sub(DATE('{PROCESSING_DATE}'), 2 * 365) THEN 'Older CY'\n",
					"            WHEN MAX(sickness_end) BETWEEN date_sub(DATE('{PROCESSING_DATE}'), 2 * 365) \n",
					"                 AND date_sub(DATE('{PROCESSING_DATE}'), 1 * 365) THEN 'Previous CY'\n",
					"            WHEN MAX(sickness_end) BETWEEN date_sub(DATE('{PROCESSING_DATE}'), 1 * 365) \n",
					"                 AND DATE('{PROCESSING_DATE}') THEN 'Current CY'\n",
					"            ELSE 'Future CY'\n",
					"        END AS calendar_year\n",
					"    FROM sickness_with_ids\n",
					"    GROUP BY StaffNumber, sickness_group_id\n",
					"    \"\"\")\n",
					"    logInfo(\"Successfully created final grouped sickness view\")\n",
					"    \n",
					"except Exception as e:\n",
					"    logError(f\"Error in contiguity detection and grouping: {str(e)}\")\n",
					"    raise e"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"    #  Insert with improved RowID generation\n",
					"    logInfo(f\"Starting insertion of final data into {TARGET_TABLE}\")\n",
					"    \n",
					"    # Get staff number transformation SQL from helper function\n",
					"    staff_number_transform_sql = create_staff_number_transformation_sql()\n",
					"    \n",
					"    spark.sql(f\"\"\"\n",
					"    INSERT OVERWRITE TABLE {TARGET_TABLE}\n",
					"    SELECT \n",
					"        CAST(sickness_id AS INT) AS sickness_id,\n",
					"        {staff_number_transform_sql} AS StaffNumber,\n",
					"        CAST(Total_Days AS DOUBLE) AS Days,\n",
					"        CAST(sickness_start AS DATE) AS sickness_start,\n",
					"        CAST(sickness_end AS DATE) AS sickness_end,\n",
					"        CAST(FY AS STRING) AS FY,\n",
					"        CAST(financial_year AS STRING) AS financial_year,\n",
					"        CAST(calendar_year AS STRING) AS calendar_year,\n",
					"        'saphr' AS SourceSystemID,\n",
					"        DATE('{PROCESSING_DATE}') AS IngestionDate,\n",
					"        TIMESTAMP('{PROCESSING_DATE} 00:00:00') AS ValidFrom,\n",
					"        TIMESTAMP('{PROCESSING_DATE} 00:00:00') AS ValidTo,\n",
					"        md5(concat_ws('|', CAST(sickness_id AS STRING), StaffNumber)) AS RowID,\n",
					"        'Y' AS IsActive,\n",
					"        TIMESTAMP('{PROCESSING_DATE} 00:00:00') AS LastUpdated\n",
					"    FROM sickness_final\n",
					"    ORDER BY StaffNumber, sickness_start\n",
					"    \"\"\")\n",
					"    logInfo(f\"Successfully inserted data into {TARGET_TABLE}\")\n",
					"    \n",
					"    # Step 9: Final verification and result preparation\n",
					"    result = {\n",
					"        \"status\": \"success\",\n",
					"        \"record_count\": 0,\n",
					"        \"error_message\": None\n",
					"    }\n",
					"    \n",
					"    final_count = spark.sql(f\"SELECT COUNT(*) as count FROM {TARGET_TABLE}\").collect()[0]['count']\n",
					"    logInfo(f\"Final record count: {final_count}\")\n",
					"    \n",
					"    # Update result with success information\n",
					"    result[\"status\"] = \"success\"\n",
					"    result[\"record_count\"] = final_count\n",
					"    result[\"error_message\"] = None\n",
					"    \n",
					"    logInfo(\"Unified absence sickness fact table processing completed successfully\")\n",
					"    \n",
					"except Exception as e:\n",
					"    logError(f\"Error in unified absence sickness fact table processing: {str(e)}\")\n",
					"    logException(e)\n",
					"    \n",
					"    # Try to get current count even in case of error\n",
					"    try:\n",
					"        error_count = spark.sql(f\"SELECT COUNT(*) as count FROM {TARGET_TABLE}\").collect()[0]['count']\n",
					"    except:\n",
					"        error_count = 0\n",
					"    \n",
					"    # Update result with error information\n",
					"    result = {\n",
					"        \"status\": \"failed\",\n",
					"        \"record_count\": error_count,\n",
					"        \"error_message\": str(e)\n",
					"    }\n",
					"    \n",
					"    logError(f\"Holiday processing failed. Current count: {error_count}, Error: {str(e)}\")\n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    # Always flush logs\n",
					"    logInfo(\"Flushing logs\")  \n",
					"    flushLogging()\n",
					"    \n",
					"    # Return the result as JSON\n",
					"    logInfo(f\"Returning result: {json.dumps(result)}\")\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": null
			}
		]
	}
}