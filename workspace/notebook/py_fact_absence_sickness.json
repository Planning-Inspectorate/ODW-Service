{
	"name": "py_fact_absence_sickness",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "2c1e639c-cb16-4fcf-a403-bca854e0aca6"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Harmoised layer and build a table for Curated Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;25-02-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This Notebook is designed to facilitate the monthly processing and harmonization of Sickness data. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The Notebook ensures that Sickness data is accurately transformed, stored, and made available for reporting and analysis.;"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Sickness Data"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# Import logging utilities\n",
					"\n",
					"try:\n",
					"   logInfo(\"Starting absence sickness fact table processing\")\n",
					"   \n",
					"   # First, set ANSI mode to false to handle potential type conversion issues\n",
					"   logInfo(\"Setting ANSI mode to false\")\n",
					"   spark.sql(\"SET spark.sql.ansi.enabled=false\")\n",
					"   logInfo(\"ANSI mode set successfully\")\n",
					"   \n",
					"   # Then clean the target table\n",
					"   logInfo(\"Starting deletion of all rows from odw_harmonised_db.sap_hr_fact_absence_sickness\")\n",
					"   spark.sql(\"\"\"\n",
					"   DELETE FROM odw_harmonised_db.sap_hr_fact_absence_sickness\n",
					"   \"\"\")\n",
					"   logInfo(\"Successfully deleted all rows from odw_harmonised_db.sap_hr_fact_absence_sickness\")\n",
					"   \n",
					"   # Step 1: Create the initial sickness view with base data\n",
					"   logInfo(\"Creating initial sickness view with base data\")\n",
					"   spark.sql(\"\"\"\n",
					"   CREATE OR REPLACE TEMP VIEW sickness AS\n",
					"   SELECT \n",
					"       CAST(a.StaffNumber AS STRING) AS StaffNumber,\n",
					"       CAST(a.StartDate AS DATE) AS sickness_start,\n",
					"       CAST(a.EndDate AS DATE) AS sickness_end,\n",
					"       CAST(a.Days AS DOUBLE) AS Days,\n",
					"       a.SicknessGroup,\n",
					"       a.WorkScheduleRule,\n",
					"       CAST(ds.FY AS STRING) AS FY_start,\n",
					"       CAST(de.FY AS STRING) AS FY_end\n",
					"   FROM odw_harmonised_db.sap_hr_absence_all a\n",
					"   LEFT JOIN odw_harmonised_db.live_dim_date ds ON CAST(a.StartDate AS DATE) = ds.date\n",
					"   LEFT JOIN odw_harmonised_db.live_dim_date de ON CAST(a.EndDate AS DATE) = de.date\n",
					"   WHERE a.AttendanceorAbsenceType = 'Sickness'\n",
					"   \"\"\")\n",
					"   logInfo(\"Successfully created initial sickness view\")\n",
					"   \n",
					"   # Step 2: Create a view with row numbers to help identify unique records\n",
					"   logInfo(\"Creating view with row numbers for unique record identification\")\n",
					"   spark.sql(\"\"\"\n",
					"   CREATE OR REPLACE TEMP VIEW sickness_with_row_numbers AS\n",
					"   SELECT \n",
					"       ROW_NUMBER() OVER (ORDER BY StaffNumber, sickness_start) AS row_num,\n",
					"       StaffNumber,\n",
					"       sickness_start,\n",
					"       sickness_end,\n",
					"       Days,\n",
					"       SicknessGroup,\n",
					"       WorkScheduleRule,\n",
					"       FY_start,\n",
					"       FY_end,\n",
					"       -- Add lead and lag information for contiguity detection\n",
					"       LEAD(sickness_start) OVER (PARTITION BY StaffNumber ORDER BY sickness_start DESC) AS prev_sickness_start,\n",
					"       LEAD(sickness_end) OVER (PARTITION BY StaffNumber ORDER BY sickness_start DESC) AS prev_sickness_end\n",
					"   FROM sickness\n",
					"   \"\"\")\n",
					"   logInfo(\"Successfully created view with row numbers\")\n",
					"   \n",
					"   # Step 3: Identify contiguous periods using a date function that respects working days\n",
					"   logInfo(\"Creating view to identify contiguous sickness periods\")\n",
					"   spark.sql(\"\"\"\n",
					"   CREATE OR REPLACE TEMP VIEW sickness_with_contiguity AS\n",
					"   SELECT \n",
					"       row_num,\n",
					"       StaffNumber,\n",
					"       sickness_start,\n",
					"       sickness_end,\n",
					"       Days,\n",
					"       SicknessGroup,\n",
					"       WorkScheduleRule,\n",
					"       FY_start, \n",
					"       FY_end,\n",
					"       prev_sickness_start,\n",
					"       prev_sickness_end,\n",
					"       -- This determines if the current sickness period is contiguous with the previous one\n",
					"       CASE \n",
					"           WHEN date_add(prev_sickness_end, 1) = sickness_start THEN uuid()\n",
					"           ELSE NULL \n",
					"       END AS contiguous_sickness,\n",
					"       date_add(prev_sickness_end, 1) AS next_working_day_after_previous_sickness\n",
					"   FROM sickness_with_row_numbers\n",
					"   \"\"\")\n",
					"   logInfo(\"Successfully created contiguity detection view\")\n",
					"   \n",
					"   # Step 4: Create a view that assigns sickness IDs\n",
					"   logInfo(\"Creating view to assign sickness IDs\")\n",
					"   spark.sql(\"\"\"\n",
					"   CREATE OR REPLACE TEMP VIEW sickness_with_ids AS\n",
					"   SELECT \n",
					"       s.*,\n",
					"       -- This is a cumulative sum that increases by 1 whenever we encounter a non-contiguous sickness period\n",
					"       CAST(SUM(CASE \n",
					"               WHEN contiguous_sickness IS NULL OR sickness_start <> next_working_day_after_previous_sickness \n",
					"               THEN 1 \n",
					"               ELSE 0 \n",
					"           END) OVER (PARTITION BY StaffNumber ORDER BY sickness_start) AS BIGINT) AS sickness_id\n",
					"   FROM sickness_with_contiguity s\n",
					"   \"\"\")\n",
					"   logInfo(\"Successfully created view with sickness IDs\")\n",
					"   \n",
					"   # Step 5: Group the results properly by sickness ID\n",
					"   logInfo(\"Creating final sickness view with grouping by sickness ID\")\n",
					"   spark.sql(\"\"\"\n",
					"   CREATE OR REPLACE TEMP VIEW sickness_final AS\n",
					"   SELECT \n",
					"       StaffNumber,\n",
					"       sickness_id,\n",
					"       SUM(Days) AS Total_Days,\n",
					"       MIN(sickness_start) AS sickness_start,\n",
					"       MAX(sickness_end) AS sickness_end,\n",
					"       MAX(FY_start) AS FY,\n",
					"       CASE \n",
					"           WHEN MAX(sickness_end) < date_sub(current_date(), 2 * 365) THEN 'Older FY'\n",
					"           WHEN MAX(sickness_end) BETWEEN date_sub(current_date(), 2 * 365) AND date_sub(current_date(), 1 * 365) THEN 'Previous FY'\n",
					"           WHEN MAX(sickness_end) BETWEEN date_sub(current_date(), 1 * 365) AND current_date() THEN 'Current FY'\n",
					"           ELSE 'Next FY'\n",
					"       END AS financial_year,\n",
					"       CASE \n",
					"           WHEN MAX(sickness_end) < date_sub(current_date(), 2 * 365) THEN 'Older CY'\n",
					"           WHEN MAX(sickness_end) BETWEEN date_sub(current_date(), 2 * 365) AND date_sub(current_date(), 1 * 365) THEN 'Previous CY'\n",
					"           WHEN MAX(sickness_end) BETWEEN date_sub(current_date(), 1 * 365) AND current_date() THEN 'Current CY'\n",
					"           ELSE 'Future CY'\n",
					"       END AS calendar_year\n",
					"   FROM sickness_with_ids\n",
					"   GROUP BY StaffNumber, sickness_id\n",
					"   \"\"\")\n",
					"   logInfo(\"Successfully created final grouped sickness view\")\n",
					"   \n",
					"   # Step 6: Insert the final results into the target table with RowID included\n",
					"   logInfo(\"Starting insertion of final data into odw_harmonised_db.sap_hr_fact_absence_sickness\")\n",
					"   spark.sql(\"\"\"\n",
					"   INSERT OVERWRITE TABLE odw_harmonised_db.sap_hr_fact_absence_sickness\n",
					"   SELECT \n",
					"       CAST(sickness_id AS STRING) AS sickness_id,\n",
					"       CASE\n",
					"           WHEN trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0')) LIKE '4%' THEN \n",
					"               '50' || trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0'))\n",
					"           WHEN trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0')) LIKE '5%' THEN \n",
					"               '00' || trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0'))\n",
					"           WHEN trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0')) LIKE '6%' THEN\n",
					"               -- For staff numbers starting with 6, we have special handling:\n",
					"               -- If it's already 8 digits (or more), keep it as is\n",
					"               -- If it starts with 006, convert to 606 (adding a 6 at the beginning)\n",
					"               CASE \n",
					"                   WHEN length(trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0'))) >= 8 THEN \n",
					"                       trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0'))\n",
					"                   WHEN lpad(CAST(StaffNumber AS STRING), 8, '0') LIKE '006%' THEN\n",
					"                       '6' || substring(lpad(CAST(StaffNumber AS STRING), 8, '0'), 3)\n",
					"                   ELSE '60' || trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0'))\n",
					"               END\n",
					"           ELSE lpad(CAST(StaffNumber AS STRING), 7, '0')\n",
					"       END AS StaffNumber,\n",
					"       CAST(Total_Days AS DOUBLE) AS Days,\n",
					"       CAST(sickness_start AS DATE) AS sickness_start,\n",
					"       CAST(sickness_end AS DATE) AS sickness_end,\n",
					"       CAST(FY AS STRING) AS FY,\n",
					"       CAST(financial_year AS STRING) AS financial_year,\n",
					"       CAST(calendar_year AS STRING) AS calendar_year,\n",
					"       'saphr' AS SourceSystemID,\n",
					"       current_date() AS IngestionDate,\n",
					"       current_timestamp() AS ValidFrom,\n",
					"       current_timestamp() AS ValidTo,\n",
					"       -- Generate RowID directly in the INSERT statement\n",
					"       md5(\n",
					"           concat_ws('|',\n",
					"               CAST(sickness_id AS STRING),\n",
					"               CASE\n",
					"                   WHEN trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0')) LIKE '4%' THEN \n",
					"                       '50' || trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0'))\n",
					"                   WHEN trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0')) LIKE '5%' THEN \n",
					"                       '00' || trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0'))\n",
					"                   WHEN trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0')) LIKE '6%' THEN\n",
					"                       -- For staff numbers starting with 6, we have special handling:\n",
					"                       -- If it's already 8 digits (or more), keep it as is\n",
					"                       -- If it starts with 006, convert to 606 (adding a 6 at the beginning)\n",
					"                       CASE \n",
					"                           WHEN length(trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0'))) >= 8 THEN \n",
					"                               trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0'))\n",
					"                           WHEN lpad(CAST(StaffNumber AS STRING), 8, '0') LIKE '006%' THEN\n",
					"                               '6' || substring(lpad(CAST(StaffNumber AS STRING), 8, '0'), 3)\n",
					"                           ELSE '60' || trim(LEADING '0' FROM lpad(CAST(StaffNumber AS STRING), 8, '0'))\n",
					"                       END\n",
					"                   ELSE lpad(CAST(StaffNumber AS STRING), 7, '0')\n",
					"               END,\n",
					"               CAST(Total_Days AS STRING),\n",
					"               CAST(sickness_start AS STRING),\n",
					"               CAST(sickness_end AS STRING),\n",
					"               CAST(FY AS STRING),\n",
					"               CAST(financial_year AS STRING),\n",
					"               CAST(calendar_year AS STRING)\n",
					"           )\n",
					"       ) AS RowID,\n",
					"       'Y' AS IsActive,\n",
					"       current_timestamp() AS LastUpdated\n",
					"   FROM sickness_final\n",
					"   \"\"\")\n",
					"   logInfo(\"Successfully inserted data into odw_harmonised_db.sap_hr_fact_absence_sickness\")\n",
					"   \n",
					"   logInfo(\"Absence sickness fact table processing completed successfully\")\n",
					"   \n",
					"except Exception as e:\n",
					"   # Log the exception in detail\n",
					"   logError(f\"Error in absence sickness fact table processing: {str(e)}\")\n",
					"   logException(e)\n",
					"   \n",
					"   # Re-raise the exception to ensure the notebook fails properly\n",
					"   raise e\n",
					"finally:\n",
					"   # Always flush logs regardless of success or failure\n",
					"   logInfo(\"Flushing logs\")\n",
					"   flushLogging()"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Holiday's Exclusion"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"\n",
					"# Step 1: Create a view of holidays\n",
					"try:\n",
					"    logInfo(\"Creating temporary view of holidays\")\n",
					"    spark.sql(\"\"\"\n",
					"    CREATE OR REPLACE TEMPORARY VIEW holidays_view AS\n",
					"    SELECT \n",
					"        HolidayDate\n",
					"    FROM \n",
					"        odw_standardised_db.Live_Holidays;\n",
					"    \"\"\")\n",
					"    logInfo(\"Successfully created holidays temporary view\")\n",
					"    \n",
					"    # Step 2: Use MERGE to delete rows where absencedate matches a holiday\n",
					"    logInfo(\"Starting deletion of sickness records that fall on holidays\")\n",
					"    spark.sql(\"\"\"\n",
					"    MERGE INTO odw_harmonised_db.sap_hr_fact_absence_sickness AS target\n",
					"    USING holidays_view AS source\n",
					"    ON CAST(target.sickness_start AS DATE) =  CAST(TO_TIMESTAMP(HolidayDate, 'dd/MM/yyyy') AS DATE)\n",
					"    WHEN MATCHED THEN DELETE;\n",
					"    \"\"\")\n",
					"    logInfo(\"Successfully removed sickness records that fall on holidays\")\n",
					"    \n",
					"except Exception as e:\n",
					"    # Log the exception in detail\n",
					"    logError(f\"Error in holiday processing: {str(e)}\")\n",
					"    logException(e)\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"finally:\n",
					"    # Always flush logs\n",
					"    logInfo(\"Flushing logs\")\n",
					"    flushLogging()"
				],
				"execution_count": 4
			}
		]
	}
}