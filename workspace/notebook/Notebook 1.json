{
	"name": "Notebook 1",
	"properties": {
		"folder": {
			"name": "odw-standardised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "f1282d46-1c52-4a91-a2e9-ba5c57e4d6f6"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"\n",
					"#Define Input Parameters to get values from the pipleline\n",
					"Param_File_Load_Type = ''\n",
					"Param_FileFolder_Path = ''\n",
					"Param_Json_SchemaFolder_Name = ''\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"# Import all required Python Libraries\n",
					"#import all libraries and initialise Spark Session\n",
					"import json\n",
					"import traceback\n",
					"import calendar\n",
					"import time\n",
					"from datetime import datetime, timedelta, date\n",
					"import requests\n",
					"import pyspark.sql.functions as F \n",
					"import os\n",
					"import re\n",
					"from itertools import chain\n",
					"from collections.abc import Mapping\n",
					"from operator import add\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql.functions import regexp_replace,lit, current_timestamp, to_date ,expr, md5, col, date_format, when, length, lpad,input_file_name,sha2,concat\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.sql import SparkSession\n",
					"spark = SparkSession.builder.getOrCreate()\n",
					"from delta.tables import DeltaTable\n",
					"#ignore FutureWarning messages \n",
					"import warnings\n",
					"warnings.filterwarnings(\"ignore\", message=\"iteritems is deprecated\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"#!/usr/bin/env python\n",
					"# coding: utf-8\n",
					"\n",
					"# ## py_utils_common_raw_standardised\n",
					"# \n",
					"# The purpose of this pyspark notebook is to reads all recent files from the given odw-raw folder path and load the data into standardised_db lakehouse database's Delta tables\n",
					"\n",
					"# Author               Created Date              Description\n",
					"# Rohit Shukla         19-Jan-2025             The functionality of this notebook is generic to cater to .xlsx and .csv files for creating Delta Tables.\n",
					"# Enhanced             [Current Date]          Added functionality for multi-file processing with date-based ordering and MERGE schema support\n",
					"\n",
					"# Spark Cluster Configuration -> Apache Spark Version- 3.4, Python Version - 3.10, Delta Lake Version - 2.4\n",
					"\n",
					"# The input parameters are:\n",
					"# Param_FileFolder_Path => This is a mandatory parameter which refers to a folder path of the entities like 'Timesheets', 'SapHrData'\n",
					"# Param_File_Load_Type => This is an optional parameter refers to a subfolders if there is any like Monthly,Daily, Quarterly etc.\n",
					"# Param_Json_SchemaFolder_Name => This is a mandatory parameter which refers to a schema file in json format required to create delta tables.\n",
					"\n",
					"\n",
					"# Get Storage account\n",
					"#Get Storage account name\n",
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"#print(storage_account)\n",
					"\n",
					"# Enable message logging\n",
					"%run utils/py_logging_decorator\n",
					"\n",
					"# Define all required folder paths\n",
					"# Define all Folder paths used in the notebook\n",
					"\n",
					"Param_Json_SchemaFolder_Name = Param_FileFolder_Path.lower()\n",
					"\n",
					"odw_raw_base_folder_path = f\"abfss://odw-raw@{storage_account}{Param_FileFolder_Path}/\"\n",
					"delta_table_base_path = f\"abfss://odw-standardised@{storage_account}{Param_Json_SchemaFolder_Name}\"\n",
					"json_schema_file_path = f\"abfss://odw-config@{storage_account}/orchestration/orchestration_saphr.json\"\n",
					"\n",
					"database_name = \"odw_standardised_db\"\n",
					"process_name = 'py_raw_to_std'\n",
					"\n",
					"logging_container = f\"abfss://logging@{storage_account}\"\n",
					"logging_table_name = 'tables_logs'\n",
					"ingestion_log_table_location = logging_container + logging_table_name\n",
					"\n",
					"# json result result dump list\n",
					"processing_results = []\n",
					"\n",
					"# --- Load Orchestration Config ---\n",
					"df_orch = spark.read.option(\"multiline\", \"true\").json(json_schema_file_path)\n",
					"definitions = json.loads(df_orch.toJSON().first())[\"definitions\"]\n",
					"\n",
					"# Find the recent Date subfolder path and construct the odw-raw path dynamically\n",
					"# Get latest folder\n",
					"def get_latest_folder(path):\n",
					"    folders = [f.name for f in mssparkutils.fs.ls(path) if f.isDir]\n",
					"    folders = sorted([f for f in folders if re.match(r\"\\d{4}-\\d{2}-\\d{2}\", f)], reverse=True)\n",
					"    return folders[0] if folders else None\n",
					"\n",
					"# ENHANCED: Function to extract date from filename with various date formats\n",
					"def extract_date_from_filename(filename):\n",
					"    \"\"\"Extract date from filename supporting multiple date formats\"\"\"\n",
					"    try:\n",
					"        # Pattern for YYYYMMDD format (8 digits)\n",
					"        date_pattern_8 = r'(\\d{8})'\n",
					"        # Pattern for YYYY-MM-DD format\n",
					"        date_pattern_dash = r'(\\d{4}-\\d{2}-\\d{2})'\n",
					"        # Pattern for YYYY_MM_DD format\n",
					"        date_pattern_underscore = r'(\\d{4}_\\d{2}_\\d{2})'\n",
					"        \n",
					"        match_8 = re.search(date_pattern_8, filename)\n",
					"        match_dash = re.search(date_pattern_dash, filename)\n",
					"        match_underscore = re.search(date_pattern_underscore, filename)\n",
					"        \n",
					"        if match_8:\n",
					"            date_str = match_8.group(1)\n",
					"            return datetime.strptime(date_str, '%Y%m%d')\n",
					"        elif match_dash:\n",
					"            date_str = match_dash.group(1)\n",
					"            return datetime.strptime(date_str, '%Y-%m-%d')\n",
					"        elif match_underscore:\n",
					"            date_str = match_underscore.group(1)\n",
					"            return datetime.strptime(date_str, '%Y_%m_%d')\n",
					"        else:\n",
					"            logInfo(f\"No date found in filename: {filename}\")\n",
					"            return datetime.min\n",
					"    except Exception as e:\n",
					"        logError(f\"Error extracting date from filename {filename}: {e}\")\n",
					"        return datetime.min\n",
					"\n",
					"# ENHANCED: Function to find and sort files by date in filename\n",
					"def get_files_sorted_by_date(path, filename_pattern):\n",
					"    \"\"\"Get files matching pattern sorted by date extracted from filename\"\"\"\n",
					"    try:\n",
					"        files = [f.name for f in mssparkutils.fs.ls(path) if not f.isDir]\n",
					"        \n",
					"        # Filter files matching the pattern\n",
					"        matching_files = []\n",
					"        for file in files:\n",
					"            if re.search(filename_pattern, file, re.IGNORECASE):\n",
					"                file_date = extract_date_from_filename(file)\n",
					"                matching_files.append((file, file_date))\n",
					"        \n",
					"        # Sort by date (oldest first for sequential processing)\n",
					"        matching_files.sort(key=lambda x: x[1])\n",
					"        \n",
					"        logInfo(f\"Found {len(matching_files)} matching files in {path}\")\n",
					"        for file, file_date in matching_files:\n",
					"            logInfo(f\"  File: {file}, Date: {file_date.strftime('%Y-%m-%d') if file_date != datetime.min else 'No date'}\")\n",
					"        \n",
					"        return [file for file, _ in matching_files]\n",
					"    except Exception as e:\n",
					"        logError(f\"Error getting sorted files from {path}: {e}\")\n",
					"        return []\n",
					"\n",
					"# ENHANCED: Function to combine multiple files into a single DataFrame\n",
					"@logging_to_appins\n",
					"def read_and_combine_files(file_paths, definition):\n",
					"    \"\"\"Read multiple files and combine them into a single DataFrame\"\"\"\n",
					"    combined_df = None\n",
					"    total_files_processed = 0\n",
					"    \n",
					"    try:\n",
					"        for file_path in file_paths:\n",
					"            logInfo(f\"Processing file: {file_path}\")\n",
					"            \n",
					"            # Read individual file\n",
					"            df = read_file(file_path)\n",
					"            if df is None:\n",
					"                logError(f\"Failed to read file: {file_path}\")\n",
					"                continue\n",
					"            \n",
					"            # Clean column names for consistency\n",
					"            df = clean_column_names(df)\n",
					"            \n",
					"            # Add file source information\n",
					"            df = df.withColumn(\"source_file\", lit(os.path.basename(file_path)))\n",
					"            df = df.withColumn(\"file_processed_datetime\", current_timestamp())\n",
					"            \n",
					"            if combined_df is None:\n",
					"                combined_df = df\n",
					"            else:\n",
					"                # Union with the combined DataFrame\n",
					"                # Handle potential schema differences\n",
					"                try:\n",
					"                    combined_df = combined_df.unionByName(df, allowMissingColumns=True)\n",
					"                except Exception as union_error:\n",
					"                    logInfo(f\"Union by name failed, trying standard union: {union_error}\")\n",
					"                    # If schemas differ, align them\n",
					"                    combined_df = align_and_union_dataframes(combined_df, df)\n",
					"            \n",
					"            total_files_processed += 1\n",
					"            logInfo(f\"Successfully processed file {total_files_processed}: {os.path.basename(file_path)}\")\n",
					"        \n",
					"        if combined_df is not None:\n",
					"            logInfo(f\"Combined DataFrame created with {combined_df.count()} total rows from {total_files_processed} files\")\n",
					"        \n",
					"        return combined_df\n",
					"        \n",
					"    except Exception as e:\n",
					"        logError(f\"Error combining files: {e}\")\n",
					"        return None\n",
					"\n",
					"# ENHANCED: Function to align DataFrames with different schemas\n",
					"def align_and_union_dataframes(df1, df2):\n",
					"    \"\"\"Align two DataFrames with potentially different schemas and union them\"\"\"\n",
					"    try:\n",
					"        # Get all unique columns from both DataFrames\n",
					"        all_columns = list(set(df1.columns + df2.columns))\n",
					"        \n",
					"        # Add missing columns to df1\n",
					"        for col_name in all_columns:\n",
					"            if col_name not in df1.columns:\n",
					"                df1 = df1.withColumn(col_name, lit(None))\n",
					"        \n",
					"        # Add missing columns to df2\n",
					"        for col_name in all_columns:\n",
					"            if col_name not in df2.columns:\n",
					"                df2 = df2.withColumn(col_name, lit(None))\n",
					"        \n",
					"        # Reorder columns to match\n",
					"        df1 = df1.select(all_columns)\n",
					"        df2 = df2.select(all_columns)\n",
					"        \n",
					"        # Union the DataFrames\n",
					"        return df1.union(df2)\n",
					"        \n",
					"    except Exception as e:\n",
					"        logError(f\"Error aligning DataFrames: {e}\")\n",
					"        return df1  # Return original df1 if alignment fails\n",
					"\n",
					"# ENHANCED: Function to check schema compatibility and determine write mode\n",
					"def determine_write_strategy(spark_df, delta_table_path, schema_path, table_name):\n",
					"    \"\"\"Determine the appropriate write strategy based on schema compatibility\"\"\"\n",
					"    try:\n",
					"        # Load expected schema from JSON\n",
					"        schema_json = json.loads(\"\".join([r.value for r in spark.read.text(schema_path).collect()]))\n",
					"        expected_fields = {f['name'].lower(): f['type'] for f in schema_json[\"fields\"]}\n",
					"        \n",
					"        # Get actual DataFrame schema\n",
					"        actual_fields = {field.name.lower(): str(field.dataType) for field in spark_df.schema.fields}\n",
					"        \n",
					"        # Check if Delta table exists\n",
					"        table_exists = DeltaTable.isDeltaTable(spark, delta_table_path)\n",
					"        \n",
					"        if not table_exists:\n",
					"            logInfo(f\"Delta table does not exist for {table_name}. Will create new table.\")\n",
					"            return \"create_new\", None\n",
					"        \n",
					"        # If table exists, get its current schema\n",
					"        try:\n",
					"            current_table_df = spark.read.format(\"delta\").load(delta_table_path)\n",
					"            current_fields = {field.name.lower(): str(field.dataType) for field in current_table_df.schema.fields}\n",
					"        except Exception as e:\n",
					"            logError(f\"Error reading existing table schema: {e}\")\n",
					"            return \"create_new\", None\n",
					"        \n",
					"        # Check schema compatibility\n",
					"        missing_in_current = set(actual_fields.keys()) - set(current_fields.keys())\n",
					"        missing_in_actual = set(current_fields.keys()) - set(actual_fields.keys())\n",
					"        \n",
					"        if missing_in_current and not missing_in_actual:\n",
					"            logInfo(f\"New columns detected in source data: {missing_in_current}\")\n",
					"            logInfo(f\"Will use MERGE schema option for {table_name}\")\n",
					"            return \"merge_schema\", missing_in_current\n",
					"        elif not missing_in_current and not missing_in_actual:\n",
					"            logInfo(f\"Schema matches exactly for {table_name}. Will use standard overwrite.\")\n",
					"            return \"overwrite\", None\n",
					"        else:\n",
					"            logInfo(f\"Schema mismatch detected for {table_name}\")\n",
					"            logInfo(f\"Missing in current table: {missing_in_current}\")\n",
					"            logInfo(f\"Missing in source data: {missing_in_actual}\")\n",
					"            logInfo(f\"Will use MERGE schema option to handle differences\")\n",
					"            return \"merge_schema\", missing_in_current\n",
					"            \n",
					"    except Exception as e:\n",
					"        logError(f\"Error determining write strategy for {table_name}: {e}\")\n",
					"        return \"overwrite\", None\n",
					"\n",
					"# ENHANCED: Function to write DataFrame with appropriate strategy\n",
					"@logging_to_appins\n",
					"def write_dataframe_with_strategy(spark_df, delta_table_path, full_table_name, schema_path, strategy, additional_columns=None):\n",
					"    \"\"\"Write DataFrame using the determined strategy\"\"\"\n",
					"    try:\n",
					"        if strategy == \"create_new\":\n",
					"            # Create table if it doesn't exist\n",
					"            create_table_if_not_exists(delta_table_path, full_table_name, schema_path)\n",
					"            spark_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
					"            logInfo(f\"Created new Delta table: {full_table_name}\")\n",
					"            \n",
					"        elif strategy == \"merge_schema\":\n",
					"            # Use mergeSchema option for schema evolution\n",
					"            spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(delta_table_path)\n",
					"            logInfo(f\"Updated Delta table with schema merge: {full_table_name}\")\n",
					"            if additional_columns:\n",
					"                logInfo(f\"Added new columns: {additional_columns}\")\n",
					"                \n",
					"        else:  # strategy == \"overwrite\"\n",
					"            # Standard overwrite\n",
					"            spark_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
					"            logInfo(f\"Overwrote Delta table: {full_table_name}\")\n",
					"            \n",
					"        return True\n",
					"        \n",
					"    except Exception as e:\n",
					"        logError(f\"Error writing DataFrame with strategy {strategy}: {e}\")\n",
					"        return False\n",
					"\n",
					"# Define all files and dataframe processing related functions\n",
					"#Defining all functions\n",
					"\n",
					"@logging_to_appins\n",
					"def read_file(file_path):\n",
					"    try:\n",
					"        if file_path.endswith(\".csv\"):\n",
					"            return spark.read.option(\"header\", True).csv(file_path)\n",
					"\t\t\t\n",
					"        elif file_path.endswith(\".xlsx\"):\n",
					"            return spark.read.format(\"com.crealytics.spark.excel\") \\\n",
					"                             .option(\"header\", \"true\") \\\n",
					"                             .option(\"inferSchema\", \"true\") \\\n",
					"                             .load(file_path)        \n",
					"        else:\n",
					"            raise Exception(\"Unsupported file format\")\n",
					"    except Exception as e:\n",
					"        logError(f\"Failed to load file {file_path}: {e}\")\n",
					"        return None\n",
					"\n",
					"def clean_column_names(df):\n",
					"    cols = [re.sub(r\"[^a-zA-Z0-9_]+\", \"\", c).strip('_') for c in df.columns]\n",
					"    deduped = []\n",
					"    for i, c in enumerate(cols):\n",
					"        count = cols[:i].count(c)\n",
					"        deduped.append(f\"{c}{count + 1}\" if count else c)\n",
					"    return df.toDF(*deduped)\n",
					"\n",
					"# Reorder dataframe columns to bring additional metadata columns to the front\n",
					"def reorder_columns(df):\n",
					"    \"\"\"Reorders the columns so that metadata columns come first.\"\"\"\n",
					"    try:\n",
					"        metadata_cols = [\"ingested_datetime\", \"expected_from\", \"expected_to\"]\n",
					"        remaining_cols = [col for col in df.columns if col not in metadata_cols]\n",
					"        return df.select(metadata_cols + remaining_cols)\n",
					"    except Exception as e:\n",
					"        logError(f\"Error reordering columns: {e}\")\n",
					"        return df\n",
					"\n",
					"@logging_to_appins\n",
					"def create_table_if_not_exists(path, table_name, schema_path):\n",
					"    try:\n",
					"        if not DeltaTable.isDeltaTable(spark, path):\n",
					"            schema_json = json.loads(\"\".join([r.value for r in spark.read.text(schema_path).collect()]))\n",
					"            schema_str = \", \".join([f\"{f['name']} {f['type']}\" for f in schema_json[\"fields\"]])\n",
					"            spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} ({schema_str}) USING DELTA LOCATION '{path}'\")\n",
					"    except Exception as e:\n",
					"        logError(f\"Error creating table {table_name}: {e}\")\n",
					"\n",
					"@logging_to_appins\n",
					"def time_diff_seconds(start, end):\n",
					"    try:\n",
					"        if not start or not end:\n",
					"            return 0\n",
					"\n",
					"        # Parse strings into datetime objects if needed\n",
					"        if isinstance(start, str):\n",
					"            start = datetime.strptime(start, \"%Y-%m-%d %H:%M:%S.%f\")\n",
					"        if isinstance(end, str):\n",
					"            end = datetime.strptime(end, \"%Y-%m-%d %H:%M:%S.%f\")\n",
					"\n",
					"        diff_seconds = int((end - start).total_seconds())\n",
					"        return diff_seconds if diff_seconds > 0 else 0\n",
					"\n",
					"    except Exception as e:\n",
					"        return 0\n",
					"\n",
					"#This funtion handles datetime object and covert into string\n",
					"def datetime_handler(obj):\n",
					"    if isinstance(obj, datetime):\n",
					"        return obj.isoformat()\n",
					"    raise TypeError(f\"Object of type {obj.__class__.__name__} is not JSON serializable\")\n",
					"\n",
					"# ENHANCED: Main Delta Table Ingestion Logic with multi-file support\n",
					"#-- Main Delta Table Ingestion Logic---\n",
					"\n",
					"@logging_to_appins\n",
					"def process_definitions(Param_File_Load_Type):\n",
					"    matched_definitions = []\n",
					"    unmatched_definitions = []\n",
					"    all_latest_files = []\n",
					"\n",
					"    # Filter only matched definitions\n",
					"    for definition in definitions:\n",
					"    \n",
					"        freq_folder = definition.get('Source_Frequency_Folder', '').lower()\n",
					"        source_folder = definition.get('Source_Folder', '').lower()\n",
					"        param_freq = (Param_File_Load_Type or '').lower()\n",
					"        param_path = (Param_FileFolder_Path or '').lower()\n",
					"\n",
					"        if Param_File_Load_Type and not (\n",
					"            freq_folder == param_freq and source_folder == param_path\n",
					"        ):\n",
					"            continue\n",
					"\n",
					"        source_path = f\"abfss://odw-raw@{storage_account}{Param_FileFolder_Path}/\"\n",
					"        \n",
					"        if Param_File_Load_Type:            \n",
					"            source_path += f\"{Param_File_Load_Type}/\"\n",
					"\n",
					"        try:\n",
					"            latest_folder = get_latest_folder(source_path)\n",
					"            if not latest_folder:\n",
					"                logInfo(f\"No folders in path {source_path}\")\n",
					"                continue\n",
					"\n",
					"            latest_path = f\"{source_path}{latest_folder}/\"\n",
					"            files = [f.name for f in mssparkutils.fs.ls(latest_path) if not f.isDir]\n",
					"            all_latest_files.extend(files)\n",
					"\n",
					"            # ENHANCED: Check if this definition supports multi-file processing\n",
					"            filename_start = definition['Source_Filename_Start']\n",
					"            \n",
					"            # Look for files matching the pattern (including date-based files)\n",
					"            if definition.get('Multi_File_Processing', False):\n",
					"                # Multi-file processing mode\n",
					"                date_pattern = definition.get('Date_Pattern', r'_\\d{8}')  # Default pattern for _YYYYMMDD\n",
					"                filename_pattern = f\"{re.escape(filename_start)}.*{date_pattern}\"\n",
					"                matching_files = get_files_sorted_by_date(latest_path, filename_pattern)\n",
					"                \n",
					"                if matching_files:\n",
					"                    definition['matched_files'] = [f\"{latest_path}{f}\" for f in matching_files]\n",
					"                    definition['latest_path'] = latest_path\n",
					"                    definition['processing_mode'] = 'multi_file'\n",
					"                    matched_definitions.append(definition)\n",
					"                    logInfo(f\"Found {len(matching_files)} files for multi-file processing: {definition['Standardised_Table_Name']}\")\n",
					"                else:\n",
					"                    unmatched_definitions.append(definition['Standardised_Table_Name'])\n",
					"            else:\n",
					"                # Original single file processing mode\n",
					"                matching_file = next((f for f in files if f.startswith(filename_start)), None)\n",
					"                if matching_file:\n",
					"                    definition['matched_file'] = matching_file\n",
					"                    definition['latest_path'] = latest_path\n",
					"                    definition['processing_mode'] = 'single_file'\n",
					"                    matched_definitions.append(definition)\n",
					"                else:\n",
					"                    unmatched_definitions.append(definition['Standardised_Table_Name'])\n",
					"\n",
					"        except Exception as e:\n",
					"            logError(f\"Could not read from {source_path}: {e}\")\n",
					"\n",
					"    #List filenames if nothing matched in Orchestration.json\n",
					"    processed_files = set()\n",
					"    for d in matched_definitions:\n",
					"        if d['processing_mode'] == 'multi_file':\n",
					"            processed_files.update([os.path.basename(f) for f in d['matched_files']])\n",
					"        else:\n",
					"            processed_files.add(d['matched_file'])\n",
					"    \n",
					"    unmatched_files = set(all_latest_files) - processed_files\n",
					"    if unmatched_files:\n",
					"        logError(f\"Files found in source but not defined in orchestration.json: {', '.join(unmatched_files)}\")\n",
					"\n",
					"    # Step 3: Process each matched definition\n",
					"    for definition in matched_definitions:\n",
					"        \n",
					"        # code added for making json dump\n",
					"        result_entry = {\n",
					"            \"delta_table_name\": definition['Standardised_Table_Name'],\n",
					"            \"csv_file_name\": definition.get('matched_file', 'multiple_files'),\n",
					"            \"record_count\": 0,\n",
					"            \"table_result\": \"failed\",\n",
					"            \"start_exec_time\": \"\",\n",
					"            \"end_exec_time\": \"\",\n",
					"            \"total_exec_time\": \"\",\n",
					"            \"error_message\": \"\",\n",
					"            \"processing_mode\": definition['processing_mode'],\n",
					"            \"files_processed\": 0\n",
					"        }\n",
					"\n",
					"        try:\n",
					"            start_exec_time = str(datetime.now())\n",
					"            result_entry[\"start_exec_time\"] = start_exec_time\n",
					"            \n",
					"            # ENHANCED: Handle both single and multi-file processing\n",
					"            if definition['processing_mode'] == 'multi_file':\n",
					"                sparkDF = read_and_combine_files(definition['matched_files'], definition)\n",
					"                result_entry[\"files_processed\"] = len(definition['matched_files'])\n",
					"                result_entry[\"csv_file_name\"] = f\"Combined from {len(definition['matched_files'])} files\"\n",
					"            else:\n",
					"                # Original single file processing\n",
					"                sparkDF = read_file(f\"{definition['latest_path']}{definition['matched_file']}\")\n",
					"                result_entry[\"files_processed\"] = 1\n",
					"            \n",
					"            if sparkDF is None:\n",
					"                logError(f\"No data loaded for: {definition['Standardised_Table_Name']}\")\n",
					"                continue\n",
					"\n",
					"            expected_from = datetime.now() - timedelta(days=definition['Expected_Within_Weekdays'])\n",
					"            expected_to = datetime.now()\n",
					"\n",
					"            # Clean column names if not already done in multi-file processing\n",
					"            if definition['processing_mode'] == 'single_file':\n",
					"                sparkDF = clean_column_names(sparkDF)\n",
					"            \n",
					"            # Add metadata columns to standardised table\n",
					"            sparkTableDF = sparkDF.withColumn(\"ingested_datetime\", current_timestamp()) \\\n",
					"                                  .withColumn(\"expected_from\", lit(expected_from)) \\\n",
					"                                  .withColumn(\"expected_to\", lit(expected_to))\n",
					"            \n",
					"            # Reorder metadata columns for the standardised delta table\n",
					"            sparkTableDF = reorder_columns(sparkTableDF)\n",
					"\n",
					"            delta_table_path = f\"abfss://odw-standardised@{storage_account}{Param_Json_SchemaFolder_Name}/{definition['Standardised_Table_Name']}\"\n",
					"            full_table_name = f\"{database_name}.{definition['Standardised_Table_Name']}\"\n",
					"            schema_path = f\"abfss://odw-config@{storage_account}{definition['Standardised_Table_Definition']}\"\n",
					"\n",
					"            # ENHANCED: Determine write strategy based on schema compatibility\n",
					"            write_strategy, additional_columns = determine_write_strategy(\n",
					"                sparkTableDF, delta_table_path, schema_path, definition['Standardised_Table_Name']\n",
					"            )\n",
					"            \n",
					"            # Schema validation to catch critical mismatches\n",
					"            expected_schema_fields = [f['name'] for f in json.loads(\"\".join([r.value for r in spark.read.text(schema_path).collect()]))['fields']]\n",
					"            actual_fields = sparkTableDF.columns\n",
					"            missing_columns = set(expected_schema_fields) - set(actual_fields)\n",
					"            extra_columns = set(actual_fields) - set(expected_schema_fields)\n",
					"            \n",
					"            if missing_columns:\n",
					"                logError(f\"Missing expected columns for table {definition['Standardised_Table_Name']}: {', '.join(missing_columns)}\")\n",
					"                if extra_columns:\n",
					"                    logInfo(f\"Extra columns in data for table {definition['Standardised_Table_Name']}: {', '.join(extra_columns)}\")\n",
					"                \n",
					"                # Only skip if critical columns are missing and not using merge schema\n",
					"                if write_strategy != \"merge_schema\":\n",
					"                    logInfo(f\"Skipping ingestion for {definition['Standardised_Table_Name']} due to schema mismatch.\")\n",
					"                    continue\n",
					"            \n",
					"            # ENHANCED: Write using determined strategy\n",
					"            write_success = write_dataframe_with_strategy(\n",
					"                sparkTableDF, delta_table_path, full_table_name, schema_path, \n",
					"                write_strategy, additional_columns\n",
					"            )\n",
					"            \n",
					"            if not write_success:\n",
					"                raise Exception(f\"Failed to write data using strategy: {write_strategy}\")\n",
					"\n",
					"            # Count rows for validation\n",
					"            rows_raw = sparkDF.count()\n",
					"            standardised_table_df = spark.read.format(\"delta\").load(delta_table_path)\n",
					"            rows_new = standardised_table_df.filter(\n",
					"                (col(\"expected_from\") == expected_from) & \n",
					"                (col(\"expected_to\") == expected_to)\n",
					"            ).count()\n",
					"            \n",
					"            end_exec_time = str(datetime.now())\n",
					"\n",
					"            # Update json result entry with success data\n",
					"            result_entry[\"record_count\"] = standardised_table_df.count()  # Total records in delta table\n",
					"            result_entry[\"table_result\"] = \"success\"\n",
					"            result_entry[\"end_exec_time\"] = end_exec_time\n",
					"            result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, end_exec_time))\n",
					"            result_entry[\"write_strategy\"] = write_strategy\n",
					"\n",
					"            if rows_raw <= rows_new:\n",
					"                logInfo(f\"All rows successfully written to {definition['Standardised_Table_Name']} â€” Raw: {rows_raw}, Written: {rows_new}\")\n",
					"            else:\n",
					"                logError(f\"Mismatch in row count for {definition['Standardised_Table_Name']}: expected {rows_raw}, got {rows_new}\")\n",
					"        \n",
					"        except Exception as e:\n",
					"            \n",
					"            #Code added to capture meaningful error message\n",
					"            full_trace = traceback.format_exc()\n",
					"            \n",
					"            table_error_msg = str(e)\n",
					"\n",
					"            complete_msg = table_error_msg + \"\\n\" + full_trace\n",
					"            error_text = complete_msg[:300]           \n",
					"            \n",
					"            # Find the position of the last full stop before 300 characters\n",
					"            last_period_index = error_text.rfind('.')\n",
					"\n",
					"            # Use up to the last full stop, if found; else fall back to 300 chars\n",
					"            if last_period_index != -1:\n",
					"                error_message = error_text[:last_period_index + 1] \n",
					"            else:\n",
					"                error_message = error_text\n",
					"\n",
					"            logError(f\"Failed processing for {definition['Standardised_Table_Name']} - {e}\")\n",
					"\n",
					"            result_entry[\"error_message\"] = f\"Failed processing for {definition['Standardised_Table_Name']} - {error_message} \"\n",
					"            result_entry[\"end_exec_time\"] = str(datetime.now())\n",
					"            result_entry[\"total_exec_time\"] = str(time_diff_seconds(result_entry[\"start_exec_time\"], datetime.now()))\n",
					"        \n",
					"        # Add result to the json dump\n",
					"        processing_results.append(result_entry)\n",
					"\n",
					"# Execute main process\n",
					"# --- Run the main process ---\n",
					"process_definitions(Param_File_Load_Type)\n",
					"\n",
					"# Prepare and return JSON result\n",
					"json_result = {\n",
					"    \"processing_summary\": {\n",
					"        \"total_tables_processed\": len(processing_results),\n",
					"        \"successful_overwrites\": len([r for r in processing_results if r[\"table_result\"] == \"success\"]),\n",
					"        \"failed_overwrites\": len([r for r in processing_results if r[\"table_result\"] == \"failed\"]),\n",
					"        \"multi_file_tables\": len([r for r in processing_results if r.get(\"processing_mode\") == \"multi_file\"]),\n",
					"        \"single_file_tables\": len([r for r in processing_results if r.get(\"processing_mode\") == \"single_file\"])\n",
					"    },\n",
					"    \"table_details\": processing_results\n",
					"}\n",
					"\n",
					"# Convert to JSON string\n",
					"result_json_str = json.dumps(json_result, indent=2, default=datetime_handler)\n",
					"\n",
					"# Exit with the JSON result for pipeline consumption\n",
					"mssparkutils.notebook.exit(result_json_str)"
				],
				"execution_count": null
			}
		]
	}
}