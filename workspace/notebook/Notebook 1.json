{
	"name": "Notebook 1",
	"properties": {
		"folder": {
			"name": "odw-standardised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "5d1d328e-4b16-4472-8c9e-32e3a0b3038c"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"tags": [
						"parameters"
					]
				},
				"source": [
					"\n",
					"#Define Input Parameters to get values from the pipleline\n",
					"Param_File_Load_Type = 'WEEKLY'\n",
					"Param_FileFolder_Path = 'SapHr'\n",
					"Param_Json_SchemaFolder_Name = ''\n",
					""
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"# Import all required Python Libraries\n",
					"#import all libraries and initialise Spark Session\n",
					"import json\n",
					"import traceback\n",
					"import calendar\n",
					"import time\n",
					"from datetime import datetime, timedelta, date\n",
					"import requests\n",
					"import pyspark.sql.functions as F \n",
					"import os\n",
					"import re\n",
					"from itertools import chain\n",
					"from collections.abc import Mapping\n",
					"from operator import add\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql.functions import regexp_replace,lit, current_timestamp, to_date ,expr, md5, col, date_format, when, length, lpad,input_file_name,sha2,concat\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.sql import SparkSession\n",
					"spark = SparkSession.builder.getOrCreate()\n",
					"from delta.tables import DeltaTable\n",
					"#ignore FutureWarning messages \n",
					"import warnings\n",
					"warnings.filterwarnings(\"ignore\", message=\"iteritems is deprecated\")"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"source": [
					"#!/usr/bin/env python\n",
					"# coding: utf-8\n",
					"\n",
					"# ## py_utils_common_raw_standardised\n",
					"# \n",
					"# The purpose of this pyspark notebook is to reads all recent files from the given odw-raw folder path and load the data into standardised_db lakehouse database's Delta tables\n",
					"\n",
					"# Author               Created Date              Description\n",
					"# Rohit Shukla         19-Jan-2025             The functionality of this notebook is generic to cater to .xlsx and .csv files for creating Delta Tables.\n",
					"# Enhanced             [Current Date]          Added functionality for multi-file processing with date-based ordering and MERGE schema support\n",
					"\n",
					"# Spark Cluster Configuration -> Apache Spark Version- 3.4, Python Version - 3.10, Delta Lake Version - 2.4\n",
					"\n",
					"# The input parameters are:\n",
					"# Param_FileFolder_Path => This is a mandatory parameter which refers to a folder path of the entities like 'Timesheets', 'SapHrData'\n",
					"# Param_File_Load_Type => This is an optional parameter refers to a subfolders if there is any like Monthly,Daily, Quarterly etc.\n",
					"# Param_Json_SchemaFolder_Name => This is a mandatory parameter which refers to a schema file in json format required to create delta tables.\n",
					"\n",
					"\n",
					"# Get Storage account\n",
					"#Get Storage account name\n",
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"#print(storage_account)\n",
					"\n",
					"\n",
					"\n",
					"# Define all required folder paths\n",
					"# Define all Folder paths used in the notebook\n",
					"\n",
					"Param_Json_SchemaFolder_Name = Param_FileFolder_Path.lower()\n",
					"\n",
					"odw_raw_base_folder_path = f\"abfss://odw-raw@{storage_account}{Param_FileFolder_Path}/\"\n",
					"delta_table_base_path = f\"abfss://odw-standardised@{storage_account}{Param_Json_SchemaFolder_Name}\"\n",
					"json_schema_file_path = f\"abfss://odw-config@{storage_account}/orchestration/orchestration_saphr.json\"\n",
					"\n",
					"database_name = \"odw_standardised_db\"\n",
					"process_name = 'py_raw_to_std'\n",
					"\n",
					"logging_container = f\"abfss://logging@{storage_account}\"\n",
					"logging_table_name = 'tables_logs'\n",
					"ingestion_log_table_location = logging_container + logging_table_name\n",
					"\n",
					"# json result result dump list\n",
					"processing_results = []\n",
					"\n",
					"# --- Load Orchestration Config ---\n",
					"df_orch = spark.read.option(\"multiline\", \"true\").json(json_schema_file_path)\n",
					"definitions = json.loads(df_orch.toJSON().first())[\"definitions\"]\n",
					"\n",
					"# Find the recent Date subfolder path and construct the odw-raw path dynamically\n",
					"# Get latest folder\n",
					"def get_latest_folder(path):\n",
					"    folders = [f.name for f in mssparkutils.fs.ls(path) if f.isDir]\n",
					"    folders = sorted([f for f in folders if re.match(r\"\\d{4}-\\d{2}-\\d{2}\", f)], reverse=True)\n",
					"    return folders[0] if folders else None\n",
					"\n",
					"# ENHANCED: Function to check if filename has date pattern\n",
					"def has_date_pattern_in_filename(filename):\n",
					"    \"\"\"Check if filename contains a date pattern\"\"\"\n",
					"    date_patterns = [\n",
					"        r'_\\d{8}',           # _YYYYMMDD\n",
					"        r'_\\d{4}-\\d{2}-\\d{2}', # _YYYY-MM-DD\n",
					"        r'_\\d{4}_\\d{2}_\\d{2}', # _YYYY_MM_DD\n",
					"        r'\\d{8}',            # YYYYMMDD (anywhere in filename)\n",
					"        r'\\d{4}-\\d{2}-\\d{2}', # YYYY-MM-DD (anywhere in filename)\n",
					"        r'\\d{4}_\\d{2}_\\d{2}'  # YYYY_MM_DD (anywhere in filename)\n",
					"    ]\n",
					"    \n",
					"    for pattern in date_patterns:\n",
					"        if re.search(pattern, filename):\n",
					"            return True\n",
					"    return False\n",
					"\n",
					"# ENHANCED: Function to find files with same base name but different dates\n",
					"def find_multi_date_files(files, filename_start):\n",
					"    \"\"\"Find files that have the same base name but different dates\"\"\"\n",
					"    matching_files = []\n",
					"    base_files = []\n",
					"    \n",
					"    for file in files:\n",
					"        if file.startswith(filename_start):\n",
					"            if has_date_pattern_in_filename(file):\n",
					"                matching_files.append(file)\n",
					"            else:\n",
					"                base_files.append(file)\n",
					"    \n",
					"    # If we found files with date patterns, use those for multi-file processing\n",
					"    if len(matching_files) > 1:\n",
					"        return matching_files, 'multi_file'\n",
					"    elif len(matching_files) == 1:\n",
					"        return matching_files, 'single_file'\n",
					"    elif len(base_files) == 1:\n",
					"        # Fallback to single file without date pattern\n",
					"        return base_files, 'single_file'\n",
					"    elif len(base_files) > 1:\n",
					"        # Multiple files without clear date pattern - take the first one\n",
					"        return [base_files[0]], 'single_file'\n",
					"    else:\n",
					"        return [], 'none'\n",
					"\n",
					"# ENHANCED: Function to extract date from filename with various date formats\n",
					"def extract_date_from_filename(filename):\n",
					"    \"\"\"Extract date from filename supporting multiple date formats\"\"\"\n",
					"    try:\n",
					"        # Pattern for YYYYMMDD format (8 digits)\n",
					"        date_pattern_8 = r'(\\d{8})'\n",
					"        # Pattern for YYYY-MM-DD format\n",
					"        date_pattern_dash = r'(\\d{4}-\\d{2}-\\d{2})'\n",
					"        # Pattern for YYYY_MM_DD format\n",
					"        date_pattern_underscore = r'(\\d{4}_\\d{2}_\\d{2})'\n",
					"        \n",
					"        match_8 = re.search(date_pattern_8, filename)\n",
					"        match_dash = re.search(date_pattern_dash, filename)\n",
					"        match_underscore = re.search(date_pattern_underscore, filename)\n",
					"        \n",
					"        if match_8:\n",
					"            date_str = match_8.group(1)\n",
					"            return datetime.strptime(date_str, '%Y%m%d')\n",
					"        elif match_dash:\n",
					"            date_str = match_dash.group(1)\n",
					"            return datetime.strptime(date_str, '%Y-%m-%d')\n",
					"        elif match_underscore:\n",
					"            date_str = match_underscore.group(1)\n",
					"            return datetime.strptime(date_str, '%Y_%m_%d')\n",
					"        else:\n",
					"            logInfo(f\"No date found in filename: {filename}\")\n",
					"            return datetime.min\n",
					"    except Exception as e:\n",
					"        logError(f\"Error extracting date from filename {filename}: {e}\")\n",
					"        return datetime.min\n",
					"\n",
					"# ENHANCED: Function to check schema compatibility and determine write mode\n",
					"def determine_write_strategy(spark_df, delta_table_path, schema_path, table_name):\n",
					"    \"\"\"Determine the appropriate write strategy based on schema compatibility\"\"\"\n",
					"    try:\n",
					"        # Load expected schema from JSON\n",
					"        schema_json = json.loads(\"\".join([r.value for r in spark.read.text(schema_path).collect()]))\n",
					"        expected_fields = {f['name'].lower(): f['type'] for f in schema_json[\"fields\"]}\n",
					"        \n",
					"        # Get actual DataFrame schema (excluding metadata columns we always add)\n",
					"        metadata_columns = {'ingested_datetime', 'expected_from', 'expected_to'}\n",
					"        actual_fields = {field.name.lower(): str(field.dataType) for field in spark_df.schema.fields if field.name.lower() not in metadata_columns}\n",
					"        \n",
					"        # Check if Delta table exists\n",
					"        table_exists = DeltaTable.isDeltaTable(spark, delta_table_path)\n",
					"        \n",
					"        if not table_exists:\n",
					"            logInfo(f\"Delta table does not exist for {table_name}. Will create new table.\")\n",
					"            return \"create_new\", None\n",
					"        \n",
					"        # If table exists, get its current schema (excluding old tracking columns)\n",
					"        try:\n",
					"            current_table_df = spark.read.format(\"delta\").load(delta_table_path)\n",
					"            # Exclude old tracking columns and metadata columns from comparison\n",
					"            excluded_columns = {'source_file', 'file_processed_datetime', 'ingested_datetime', 'expected_from', 'expected_to'}\n",
					"            current_fields = {field.name.lower(): str(field.dataType) for field in current_table_df.schema.fields if field.name.lower() not in excluded_columns}\n",
					"        except Exception as e:\n",
					"            logError(f\"Error reading existing table schema: {e}\")\n",
					"            return \"create_new\", None\n",
					"        \n",
					"        # Compare only business data columns (not metadata or tracking columns)\n",
					"        missing_in_current = set(actual_fields.keys()) - set(current_fields.keys())\n",
					"        missing_in_actual = set(current_fields.keys()) - set(actual_fields.keys())\n",
					"        \n",
					"        if missing_in_current and not missing_in_actual:\n",
					"            logInfo(f\"New business columns detected in source data: {missing_in_current}\")\n",
					"            logInfo(f\"Will use MERGE schema option for {table_name}\")\n",
					"            return \"merge_schema\", missing_in_current\n",
					"        elif not missing_in_current and not missing_in_actual:\n",
					"            logInfo(f\"Business schema matches exactly for {table_name}. Will use standard overwrite/append.\")\n",
					"            return \"overwrite\", None\n",
					"        elif missing_in_actual and not missing_in_current:\n",
					"            logInfo(f\"Some columns missing in source data for {table_name}: {missing_in_actual}\")\n",
					"            logInfo(f\"Will use standard overwrite/append - missing columns will be null.\")\n",
					"            return \"overwrite\", None\n",
					"        else:\n",
					"            logInfo(f\"Schema differences detected for {table_name}\")\n",
					"            logInfo(f\"Missing in current table: {missing_in_current}\")\n",
					"            logInfo(f\"Missing in source data: {missing_in_actual}\")\n",
					"            logInfo(f\"Will use standard overwrite/append\")\n",
					"            return \"overwrite\", None\n",
					"            \n",
					"    except Exception as e:\n",
					"        logError(f\"Error determining write strategy for {table_name}: {e}\")\n",
					"        return \"overwrite\", None\n",
					"\n",
					"# ENHANCED: Function to check if JSON schema has evolved vs existing Delta table\n",
					"def check_json_schema_evolution(delta_table_path, schema_path, table_name):\n",
					"    \"\"\"Check if JSON schema has evolved compared to existing Delta table schema\"\"\"\n",
					"    try:\n",
					"        # Check if Delta table exists\n",
					"        table_exists = DeltaTable.isDeltaTable(spark, delta_table_path)\n",
					"        \n",
					"        if not table_exists:\n",
					"            logInfo(f\"Delta table does not exist for {table_name}. Will create new table.\")\n",
					"            return False, \"create_new\"\n",
					"        \n",
					"        # Get JSON schema\n",
					"        schema_json = json.loads(\"\".join([r.value for r in spark.read.text(schema_path).collect()]))\n",
					"        expected_fields = set([f['name'] for f in schema_json[\"fields\"]])\n",
					"        \n",
					"        # Get existing table schema\n",
					"        try:\n",
					"            existing_table_df = spark.read.format(\"delta\").load(delta_table_path)\n",
					"            # Remove metadata columns from comparison\n",
					"            metadata_columns = {\"ingested_datetime\", \"expected_from\", \"expected_to\"}\n",
					"            existing_business_fields = set(existing_table_df.columns) - metadata_columns\n",
					"            expected_business_fields = expected_fields - metadata_columns\n",
					"            \n",
					"            # Check for schema differences between JSON and existing table\n",
					"            if existing_business_fields != expected_business_fields:\n",
					"                missing_in_table = expected_business_fields - existing_business_fields\n",
					"                missing_in_json = existing_business_fields - expected_business_fields\n",
					"                \n",
					"                logInfo(f\"JSON schema evolution detected for {table_name}:\")\n",
					"                logInfo(f\"  Columns in JSON schema but not in existing table: {missing_in_table}\")\n",
					"                logInfo(f\"  Columns in existing table but not in JSON schema: {missing_in_json}\")\n",
					"                \n",
					"                if missing_in_json:\n",
					"                    # Columns have been removed from JSON schema - need schema reset\n",
					"                    logInfo(f\"JSON schema has removed columns. Will use schema reset for {table_name}\")\n",
					"                    return True, \"schema_reset\"\n",
					"                elif missing_in_table:\n",
					"                    # New columns added to JSON schema - can use merge schema\n",
					"                    logInfo(f\"JSON schema has new columns. Will use merge schema for {table_name}\")\n",
					"                    return False, \"merge_schema\"\n",
					"            \n",
					"            logInfo(f\"JSON schema matches existing table exactly for {table_name}. Will use standard overwrite.\")\n",
					"            return False, \"standard\"\n",
					"            \n",
					"        except Exception as e:\n",
					"            logError(f\"Error reading existing table schema for {table_name}: {e}\")\n",
					"            logInfo(f\"Will reset schema due to read error\")\n",
					"            return True, \"schema_reset\"\n",
					"            \n",
					"    except Exception as e:\n",
					"        logError(f\"Error checking JSON schema evolution for {table_name}: {e}\")\n",
					"        return False, \"standard\"\n",
					"\n",
					"# ENHANCED: Function to write DataFrame based on JSON schema evolution\n",
					"@logging_to_appins\n",
					"def write_dataframe_with_json_schema_compliance(spark_df, delta_table_path, full_table_name, schema_path, is_first_file=True):\n",
					"    \"\"\"Write DataFrame with JSON schema compliance - JSON schema is the ultimate truth\"\"\"\n",
					"    try:\n",
					"        # Check if JSON schema has evolved compared to existing table\n",
					"        needs_reset, strategy = check_json_schema_evolution(delta_table_path, schema_path, full_table_name)\n",
					"        \n",
					"        if needs_reset or strategy == \"schema_reset\":\n",
					"            # JSON schema evolution requires table reset (handles column drops)\n",
					"            create_table_if_not_exists(delta_table_path, full_table_name, schema_path)\n",
					"            spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(delta_table_path)\n",
					"            logInfo(f\"Reset Delta table schema to match JSON schema: {full_table_name}\")\n",
					"            \n",
					"        elif strategy == \"merge_schema\":\n",
					"            # JSON schema has new columns - use merge schema\n",
					"            create_table_if_not_exists(delta_table_path, full_table_name, schema_path)\n",
					"            if is_first_file:\n",
					"                spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(delta_table_path)\n",
					"                logInfo(f\"Overwrote with schema merge to match JSON schema: {full_table_name}\")\n",
					"            else:\n",
					"                spark_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(delta_table_path)\n",
					"                logInfo(f\"Appended with schema merge to match JSON schema: {full_table_name}\")\n",
					"            \n",
					"        elif strategy == \"create_new\":\n",
					"            # New table - create based on JSON schema\n",
					"            create_table_if_not_exists(delta_table_path, full_table_name, schema_path)\n",
					"            spark_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
					"            logInfo(f\"Created new Delta table based on JSON schema: {full_table_name}\")\n",
					"            \n",
					"        else:\n",
					"            # Standard write (strategy == \"standard\") - JSON schema matches existing table\n",
					"            if is_first_file:\n",
					"                spark_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
					"                logInfo(f\"Overwrote Delta table (JSON schema unchanged): {full_table_name}\")\n",
					"            else:\n",
					"                spark_df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n",
					"                logInfo(f\"Appended to Delta table (JSON schema unchanged): {full_table_name}\")\n",
					"        \n",
					"        return True\n",
					"        \n",
					"    except Exception as e:\n",
					"        logError(f\"Error writing DataFrame to {full_table_name}: {e}\")\n",
					"        return False\n",
					"\n",
					"# Define all files and dataframe processing related functions\n",
					"#Defining all functions\n",
					"\n",
					"@logging_to_appins\n",
					"def read_file(file_path):\n",
					"    try:\n",
					"        if file_path.endswith(\".csv\"):\n",
					"            return spark.read.option(\"header\", True).csv(file_path)\n",
					"\t\t\t\n",
					"        elif file_path.endswith(\".xlsx\"):\n",
					"            return spark.read.format(\"com.crealytics.spark.excel\") \\\n",
					"                             .option(\"header\", \"true\") \\\n",
					"                             .option(\"inferSchema\", \"true\") \\\n",
					"                             .load(file_path)        \n",
					"        else:\n",
					"            raise Exception(\"Unsupported file format\")\n",
					"    except Exception as e:\n",
					"        logError(f\"Failed to load file {file_path}: {e}\")\n",
					"        return None\n",
					"\n",
					"def clean_column_names(df):\n",
					"    cols = [re.sub(r\"[^a-zA-Z0-9_]+\", \"\", c).strip('_') for c in df.columns]\n",
					"    deduped = []\n",
					"    for i, c in enumerate(cols):\n",
					"        count = cols[:i].count(c)\n",
					"        deduped.append(f\"{c}{count + 1}\" if count else c)\n",
					"    return df.toDF(*deduped)\n",
					"\n",
					"# Reorder dataframe columns to bring additional metadata columns to the front\n",
					"def reorder_columns(df):\n",
					"    \"\"\"Reorders the columns so that metadata columns come first.\"\"\"\n",
					"    try:\n",
					"        metadata_cols = [\"ingested_datetime\", \"expected_from\", \"expected_to\"]\n",
					"        remaining_cols = [col for col in df.columns if col not in metadata_cols]\n",
					"        return df.select(metadata_cols + remaining_cols)\n",
					"    except Exception as e:\n",
					"        logError(f\"Error reordering columns: {e}\")\n",
					"        return df\n",
					"\n",
					"@logging_to_appins\n",
					"def create_table_if_not_exists(path, table_name, schema_path):\n",
					"    try:\n",
					"        if not DeltaTable.isDeltaTable(spark, path):\n",
					"            schema_json = json.loads(\"\".join([r.value for r in spark.read.text(schema_path).collect()]))\n",
					"            schema_str = \", \".join([f\"{f['name']} {f['type']}\" for f in schema_json[\"fields\"]])\n",
					"            spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} ({schema_str}) USING DELTA LOCATION '{path}'\")\n",
					"    except Exception as e:\n",
					"        logError(f\"Error creating table {table_name}: {e}\")\n",
					"\n",
					"@logging_to_appins\n",
					"def time_diff_seconds(start, end):\n",
					"    try:\n",
					"        if not start or not end:\n",
					"            return 0\n",
					"\n",
					"        # Parse strings into datetime objects if needed\n",
					"        if isinstance(start, str):\n",
					"            start = datetime.strptime(start, \"%Y-%m-%d %H:%M:%S.%f\")\n",
					"        if isinstance(end, str):\n",
					"            end = datetime.strptime(end, \"%Y-%m-%d %H:%M:%S.%f\")\n",
					"\n",
					"        diff_seconds = int((end - start).total_seconds())\n",
					"        return diff_seconds if diff_seconds > 0 else 0\n",
					"\n",
					"    except Exception as e:\n",
					"        return 0\n",
					"\n",
					"#This funtion handles datetime object and covert into string\n",
					"def datetime_handler(obj):\n",
					"    if isinstance(obj, datetime):\n",
					"        return obj.isoformat()\n",
					"    raise TypeError(f\"Object of type {obj.__class__.__name__} is not JSON serializable\")\n",
					"\n",
					"# ENHANCED: Main Delta Table Ingestion Logic with multi-file support\n",
					"#-- Main Delta Table Ingestion Logic---\n",
					"\n",
					"@logging_to_appins\n",
					"def process_definitions(Param_File_Load_Type):\n",
					"    matched_definitions = []\n",
					"    unmatched_definitions = []\n",
					"    all_latest_files = []\n",
					"\n",
					"    # Filter only matched definitions\n",
					"    for definition in definitions:\n",
					"    \n",
					"        freq_folder = definition.get('Source_Frequency_Folder', '').lower()\n",
					"        source_folder = definition.get('Source_Folder', '').lower()\n",
					"        param_freq = (Param_File_Load_Type or '').lower()\n",
					"        param_path = (Param_FileFolder_Path or '').lower()\n",
					"\n",
					"        if Param_File_Load_Type and not (\n",
					"            freq_folder == param_freq and source_folder == param_path\n",
					"        ):\n",
					"            continue\n",
					"\n",
					"        source_path = f\"abfss://odw-raw@{storage_account}{Param_FileFolder_Path}/\"\n",
					"        \n",
					"        if Param_File_Load_Type:            \n",
					"            source_path += f\"{Param_File_Load_Type}/\"\n",
					"\n",
					"        try:\n",
					"            latest_folder = get_latest_folder(source_path)\n",
					"            if not latest_folder:\n",
					"                logInfo(f\"No folders in path {source_path}\")\n",
					"                continue\n",
					"\n",
					"            latest_path = f\"{source_path}{latest_folder}/\"\n",
					"            files = [f.name for f in mssparkutils.fs.ls(latest_path) if not f.isDir]\n",
					"            all_latest_files.extend(files)\n",
					"\n",
					"            # ENHANCED: Automatically detect multi-file processing based on filename patterns\n",
					"            filename_start = definition['Source_Filename_Start']\n",
					"            \n",
					"            # Find files with the same base name but different dates\n",
					"            matching_files, processing_mode = find_multi_date_files(files, filename_start)\n",
					"            \n",
					"            logInfo(f\"File analysis for '{filename_start}': found {len(matching_files)} files, mode: {processing_mode}\")\n",
					"            logInfo(f\"Matching files: {matching_files}\")\n",
					"            \n",
					"            if processing_mode == 'multi_file':\n",
					"                # Multi-file processing mode - sort files by date\n",
					"                logInfo(f\"Enabling multi-file processing for {definition['Standardised_Table_Name']} - found {len(matching_files)} files with date patterns\")\n",
					"                \n",
					"                # Sort files by date extracted from filename\n",
					"                matching_files_with_dates = []\n",
					"                for file in matching_files:\n",
					"                    file_date = extract_date_from_filename(file)\n",
					"                    matching_files_with_dates.append((file, file_date))\n",
					"                \n",
					"                # Sort by date (oldest first for sequential processing)\n",
					"                matching_files_with_dates.sort(key=lambda x: x[1])\n",
					"                sorted_files = [file for file, _ in matching_files_with_dates]\n",
					"                \n",
					"                logInfo(f\"Files sorted by date: {sorted_files}\")\n",
					"                \n",
					"                definition['matched_files'] = [f\"{latest_path}{f}\" for f in sorted_files]\n",
					"                definition['latest_path'] = latest_path\n",
					"                definition['processing_mode'] = 'multi_file'\n",
					"                matched_definitions.append(definition)\n",
					"                logInfo(f\"Setup multi-file processing for {definition['Standardised_Table_Name']} with {len(sorted_files)} files\")\n",
					"                \n",
					"            elif processing_mode == 'single_file':\n",
					"                # Single file processing mode\n",
					"                matching_file = matching_files[0]\n",
					"                definition['matched_file'] = matching_file\n",
					"                definition['latest_path'] = latest_path\n",
					"                definition['processing_mode'] = 'single_file'\n",
					"                matched_definitions.append(definition)\n",
					"                logInfo(f\"Setup single-file processing for {definition['Standardised_Table_Name']} with file: {matching_file}\")\n",
					"                \n",
					"            else:\n",
					"                # No matching files found\n",
					"                unmatched_definitions.append(definition['Standardised_Table_Name'])\n",
					"                logInfo(f\"No files found matching pattern '{filename_start}' for {definition['Standardised_Table_Name']}\")\n",
					"\n",
					"        except Exception as e:\n",
					"            logError(f\"Could not read from {source_path}: {e}\")\n",
					"\n",
					"    #List filenames if nothing matched in Orchestration.json\n",
					"    processed_files = set()\n",
					"    for d in matched_definitions:\n",
					"        if d['processing_mode'] == 'multi_file':\n",
					"            processed_files.update([os.path.basename(f) for f in d['matched_files']])\n",
					"        else:\n",
					"            processed_files.add(d['matched_file'])\n",
					"    \n",
					"    unmatched_files = set(all_latest_files) - processed_files\n",
					"    if unmatched_files:\n",
					"        logError(f\"Files found in source but not defined in orchestration.json: {', '.join(unmatched_files)}\")\n",
					"\n",
					"    # Step 3: Process each matched definition\n",
					"    for definition in matched_definitions:\n",
					"        \n",
					"        if definition['processing_mode'] == 'multi_file':\n",
					"            # Process each file individually and create separate entries\n",
					"            file_index = 0\n",
					"            table_created = False  # Track if table has been successfully created\n",
					"            \n",
					"            for file_path in definition['matched_files']:\n",
					"                file_name = os.path.basename(file_path)\n",
					"                \n",
					"                # Create individual result entry for each file\n",
					"                result_entry = {\n",
					"                    \"delta_table_name\": definition['Standardised_Table_Name'],\n",
					"                    \"csv_file_name\": file_name,\n",
					"                    \"record_count\": 0,\n",
					"                    \"table_result\": \"failed\",\n",
					"                    \"start_exec_time\": \"\",\n",
					"                    \"end_exec_time\": \"\",\n",
					"                    \"total_exec_time\": \"\",\n",
					"                    \"error_message\": \"\"\n",
					"                }\n",
					"\n",
					"                try:\n",
					"                    start_exec_time = str(datetime.now())\n",
					"                    result_entry[\"start_exec_time\"] = start_exec_time\n",
					"                    \n",
					"                    # Process individual file\n",
					"                    sparkDF = read_file(file_path)\n",
					"                    if sparkDF is None:\n",
					"                        logError(f\"No data loaded for file: {file_name}\")\n",
					"                        result_entry[\"error_message\"] = f\"No data loaded for file: {file_name}\"\n",
					"                        result_entry[\"end_exec_time\"] = str(datetime.now())\n",
					"                        result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, datetime.now()))\n",
					"                        processing_results.append(result_entry)\n",
					"                        file_index += 1\n",
					"                        continue\n",
					"\n",
					"                    expected_from = datetime.now() - timedelta(days=definition['Expected_Within_Weekdays'])\n",
					"                    expected_to = datetime.now()\n",
					"\n",
					"                    sparkDF = clean_column_names(sparkDF)\n",
					"                    \n",
					"                    # Add metadata columns to standardised table\n",
					"                    sparkTableDF = sparkDF.withColumn(\"ingested_datetime\", current_timestamp()) \\\n",
					"                                          .withColumn(\"expected_from\", lit(expected_from)) \\\n",
					"                                          .withColumn(\"expected_to\", lit(expected_to))\n",
					"                    \n",
					"                    # Reorder metadata columns for the standardised delta table\n",
					"                    sparkTableDF = reorder_columns(sparkTableDF)\n",
					"\n",
					"                    delta_table_path = f\"abfss://odw-standardised@{storage_account}{Param_Json_SchemaFolder_Name}/{definition['Standardised_Table_Name']}\"\n",
					"                    full_table_name = f\"{database_name}.{definition['Standardised_Table_Name']}\"\n",
					"                    schema_path = f\"abfss://odw-config@{storage_account}{definition['Standardised_Table_Definition']}\"\n",
					"\n",
					"                    # ENHANCED: Strict schema validation - JSON schema is the ultimate truth\n",
					"                    expected_schema_fields = [f['name'] for f in json.loads(\"\".join([r.value for r in spark.read.text(schema_path).collect()]))['fields']]\n",
					"                    actual_fields = sparkTableDF.columns\n",
					"                    \n",
					"                    # Remove metadata columns from comparison as they're always added by the system\n",
					"                    metadata_columns = {\"ingested_datetime\", \"expected_from\", \"expected_to\"}\n",
					"                    actual_business_fields = set(actual_fields) - metadata_columns\n",
					"                    expected_business_fields = set(expected_schema_fields) - metadata_columns\n",
					"                    \n",
					"                    missing_columns = expected_business_fields - actual_business_fields\n",
					"                    extra_columns = actual_business_fields - expected_business_fields\n",
					"                    \n",
					"                    # STRICT VALIDATION: CSV must match JSON schema exactly\n",
					"                    if missing_columns:\n",
					"                        logError(f\"CSV file missing required columns from JSON schema for table {definition['Standardised_Table_Name']}: {', '.join(missing_columns)}\")\n",
					"                        logInfo(f\"Skipping ingestion for {definition['Standardised_Table_Name']} due to missing required columns.\")\n",
					"                        result_entry[\"error_message\"] = f\"Schema validation failed - CSV missing required columns: {', '.join(missing_columns)}\"\n",
					"                        result_entry[\"end_exec_time\"] = str(datetime.now())\n",
					"                        result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, datetime.now()))\n",
					"                        processing_results.append(result_entry)\n",
					"                        file_index += 1\n",
					"                        continue\n",
					"                    \n",
					"                    if extra_columns:\n",
					"                        logError(f\"CSV file has additional columns not defined in JSON schema for table {definition['Standardised_Table_Name']}: {', '.join(extra_columns)}\")\n",
					"                        logError(f\"JSON schema is the ultimate truth. CSV file must match JSON schema exactly.\")\n",
					"                        logInfo(f\"Skipping ingestion for {definition['Standardised_Table_Name']} due to extra columns in CSV.\")\n",
					"                        result_entry[\"error_message\"] = f\"Schema validation failed - CSV has additional columns not in JSON schema: {', '.join(extra_columns)}\"\n",
					"                        result_entry[\"end_exec_time\"] = str(datetime.now())\n",
					"                        result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, datetime.now()))\n",
					"                        processing_results.append(result_entry)\n",
					"                        file_index += 1\n",
					"                        continue\n",
					"                    \n",
					"                    # If we reach here, CSV exactly matches JSON schema\n",
					"                    logInfo(f\"CSV schema validation passed for {definition['Standardised_Table_Name']} - CSV matches JSON schema exactly.\")\n",
					"                    \n",
					"                    # ENHANCED: Use JSON schema compliance for all files\n",
					"                    write_success = write_dataframe_with_json_schema_compliance(\n",
					"                        sparkTableDF, delta_table_path, full_table_name, schema_path, \n",
					"                        is_first_file=(not table_created)\n",
					"                    )\n",
					"                    \n",
					"                    if not write_success:\n",
					"                        raise Exception(f\"Failed to write data using comprehensive schema handling\")\n",
					"                    \n",
					"                    # Mark table as created after first successful write\n",
					"                    if not table_created:\n",
					"                        table_created = True\n",
					"\n",
					"                    # Count rows for validation\n",
					"                    rows_raw = sparkDF.count()\n",
					"                    \n",
					"                    end_exec_time = str(datetime.now())\n",
					"\n",
					"                    # Update json result entry with success data\n",
					"                    result_entry[\"record_count\"] = rows_raw  # Individual file record count\n",
					"                    result_entry[\"table_result\"] = \"success\"\n",
					"                    result_entry[\"end_exec_time\"] = end_exec_time\n",
					"                    result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, end_exec_time))\n",
					"\n",
					"                    logInfo(f\"Successfully processed file {file_name} with {rows_raw} rows for {definition['Standardised_Table_Name']}\")\n",
					"                \n",
					"                except Exception as e:\n",
					"                    \n",
					"                    #Code added to capture meaningful error message\n",
					"                    full_trace = traceback.format_exc()\n",
					"                    \n",
					"                    table_error_msg = str(e)\n",
					"\n",
					"                    complete_msg = table_error_msg + \"\\n\" + full_trace\n",
					"                    error_text = complete_msg[:300]           \n",
					"                    \n",
					"                    # Find the position of the last full stop before 300 characters\n",
					"                    last_period_index = error_text.rfind('.')\n",
					"\n",
					"                    # Use up to the last full stop, if found; else fall back to 300 chars\n",
					"                    if last_period_index != -1:\n",
					"                        error_message = error_text[:last_period_index + 1] \n",
					"                    else:\n",
					"                        error_message = error_text\n",
					"\n",
					"                    logError(f\"Failed processing file {file_name} for {definition['Standardised_Table_Name']} - {e}\")\n",
					"\n",
					"                    result_entry[\"error_message\"] = f\"Failed processing file {file_name} - {error_message}\"\n",
					"                    result_entry[\"end_exec_time\"] = str(datetime.now())\n",
					"                    result_entry[\"total_exec_time\"] = str(time_diff_seconds(result_entry[\"start_exec_time\"], datetime.now()))\n",
					"                \n",
					"                # Add result to the json dump\n",
					"                processing_results.append(result_entry)\n",
					"                file_index += 1\n",
					"        \n",
					"        else:\n",
					"            # Original single file processing\n",
					"            result_entry = {\n",
					"                \"delta_table_name\": definition['Standardised_Table_Name'],\n",
					"                \"csv_file_name\": definition.get('matched_file', 'unknown_file'),\n",
					"                \"record_count\": 0,\n",
					"                \"table_result\": \"failed\",\n",
					"                \"start_exec_time\": \"\",\n",
					"                \"end_exec_time\": \"\",\n",
					"                \"total_exec_time\": \"\",\n",
					"                \"error_message\": \"\"\n",
					"            }\n",
					"\n",
					"            try:\n",
					"                start_exec_time = str(datetime.now())\n",
					"                result_entry[\"start_exec_time\"] = start_exec_time\n",
					"                \n",
					"                # Original single file processing\n",
					"                sparkDF = read_file(f\"{definition['latest_path']}{definition['matched_file']}\")\n",
					"                \n",
					"                if sparkDF is None:\n",
					"                    logError(f\"No data loaded for: {definition['Standardised_Table_Name']}\")\n",
					"                    continue\n",
					"\n",
					"                expected_from = datetime.now() - timedelta(days=definition['Expected_Within_Weekdays'])\n",
					"                expected_to = datetime.now()\n",
					"\n",
					"                # Clean column names if not already done in multi-file processing\n",
					"                sparkDF = clean_column_names(sparkDF)\n",
					"                \n",
					"                # Add metadata columns to standardised table\n",
					"                sparkTableDF = sparkDF.withColumn(\"ingested_datetime\", current_timestamp()) \\\n",
					"                                      .withColumn(\"expected_from\", lit(expected_from)) \\\n",
					"                                      .withColumn(\"expected_to\", lit(expected_to))\n",
					"                \n",
					"                # Reorder metadata columns for the standardised delta table\n",
					"                sparkTableDF = reorder_columns(sparkTableDF)\n",
					"\n",
					"                delta_table_path = f\"abfss://odw-standardised@{storage_account}{Param_Json_SchemaFolder_Name}/{definition['Standardised_Table_Name']}\"\n",
					"                full_table_name = f\"{database_name}.{definition['Standardised_Table_Name']}\"\n",
					"                schema_path = f\"abfss://odw-config@{storage_account}{definition['Standardised_Table_Definition']}\"\n",
					"                \n",
					"                # ENHANCED: Strict schema validation - JSON schema is the ultimate truth\n",
					"                expected_schema_fields = [f['name'] for f in json.loads(\"\".join([r.value for r in spark.read.text(schema_path).collect()]))['fields']]\n",
					"                actual_fields = sparkTableDF.columns\n",
					"                \n",
					"                # Remove metadata columns from comparison as they're always added by the system\n",
					"                metadata_columns = {\"ingested_datetime\", \"expected_from\", \"expected_to\"}\n",
					"                actual_business_fields = set(actual_fields) - metadata_columns\n",
					"                expected_business_fields = set(expected_schema_fields) - metadata_columns\n",
					"                \n",
					"                missing_columns = expected_business_fields - actual_business_fields\n",
					"                extra_columns = actual_business_fields - expected_business_fields\n",
					"                \n",
					"                # STRICT VALIDATION: CSV must match JSON schema exactly\n",
					"                if missing_columns:\n",
					"                    logError(f\"CSV file missing required columns from JSON schema for table {definition['Standardised_Table_Name']}: {', '.join(missing_columns)}\")\n",
					"                    logInfo(f\"Skipping ingestion for {definition['Standardised_Table_Name']} due to missing required columns.\")\n",
					"                    result_entry[\"error_message\"] = f\"Schema validation failed - CSV missing required columns: {', '.join(missing_columns)}\"\n",
					"                    result_entry[\"end_exec_time\"] = str(datetime.now())\n",
					"                    result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, datetime.now()))\n",
					"                    processing_results.append(result_entry)\n",
					"                    continue\n",
					"                \n",
					"                if extra_columns:\n",
					"                    logError(f\"CSV file has additional columns not defined in JSON schema for table {definition['Standardised_Table_Name']}: {', '.join(extra_columns)}\")\n",
					"                    logError(f\"JSON schema is the ultimate truth. CSV file must match JSON schema exactly.\")\n",
					"                    logInfo(f\"Skipping ingestion for {definition['Standardised_Table_Name']} due to extra columns in CSV.\")\n",
					"                    result_entry[\"error_message\"] = f\"Schema validation failed - CSV has additional columns not in JSON schema: {', '.join(extra_columns)}\"\n",
					"                    result_entry[\"end_exec_time\"] = str(datetime.now())\n",
					"                    result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, datetime.now()))\n",
					"                    processing_results.append(result_entry)\n",
					"                    continue\n",
					"                \n",
					"                # If we reach here, CSV exactly matches JSON schema\n",
					"                logInfo(f\"CSV schema validation passed for {definition['Standardised_Table_Name']} - CSV matches JSON schema exactly.\")\n",
					"                \n",
					"                # ENHANCED: Write using JSON schema compliance\n",
					"                write_success = write_dataframe_with_json_schema_compliance(\n",
					"                    sparkTableDF, delta_table_path, full_table_name, schema_path, is_first_file=True\n",
					"                )\n",
					"                \n",
					"                if not write_success:\n",
					"                    raise Exception(f\"Failed to write data using strategy: {write_strategy}\")\n",
					"\n",
					"                # Count rows for validation\n",
					"                rows_raw = sparkDF.count()\n",
					"                standardised_table_df = spark.read.format(\"delta\").load(delta_table_path)\n",
					"                rows_new = standardised_table_df.filter(\n",
					"                    (col(\"expected_from\") == expected_from) & \n",
					"                    (col(\"expected_to\") == expected_to)\n",
					"                ).count()\n",
					"                \n",
					"                end_exec_time = str(datetime.now())\n",
					"\n",
					"                # Update json result entry with success data\n",
					"                result_entry[\"record_count\"] = standardised_table_df.count()  # Total records in delta table\n",
					"                result_entry[\"table_result\"] = \"success\"\n",
					"                result_entry[\"end_exec_time\"] = end_exec_time\n",
					"                result_entry[\"total_exec_time\"] = str(time_diff_seconds(start_exec_time, end_exec_time))\n",
					"\n",
					"                if rows_raw <= rows_new:\n",
					"                    logInfo(f\"All rows successfully written to {definition['Standardised_Table_Name']} â€” Raw: {rows_raw}, Written: {rows_new}\")\n",
					"                else:\n",
					"                    logError(f\"Mismatch in row count for {definition['Standardised_Table_Name']}: expected {rows_raw}, got {rows_new}\")\n",
					"            \n",
					"            except Exception as e:\n",
					"                \n",
					"                #Code added to capture meaningful error message\n",
					"                full_trace = traceback.format_exc()\n",
					"                \n",
					"                table_error_msg = str(e)\n",
					"\n",
					"                complete_msg = table_error_msg + \"\\n\" + full_trace\n",
					"                error_text = complete_msg[:300]           \n",
					"                \n",
					"                # Find the position of the last full stop before 300 characters\n",
					"                last_period_index = error_text.rfind('.')\n",
					"\n",
					"                # Use up to the last full stop, if found; else fall back to 300 chars\n",
					"                if last_period_index != -1:\n",
					"                    error_message = error_text[:last_period_index + 1] \n",
					"                else:\n",
					"                    error_message = error_text\n",
					"\n",
					"                logError(f\"Failed processing for {definition['Standardised_Table_Name']} - {e}\")\n",
					"\n",
					"                result_entry[\"error_message\"] = f\"Failed processing for {definition['Standardised_Table_Name']} - {error_message} \"\n",
					"                result_entry[\"end_exec_time\"] = str(datetime.now())\n",
					"                result_entry[\"total_exec_time\"] = str(time_diff_seconds(result_entry[\"start_exec_time\"], datetime.now()))\n",
					"            \n",
					"            # Add result to the json dump\n",
					"            processing_results.append(result_entry)\n",
					"\n",
					"# Execute main process\n",
					"# --- Run the main process ---\n",
					"process_definitions(Param_File_Load_Type)\n",
					"\n",
					"# Prepare and return JSON result\n",
					"json_result = {\n",
					"    \"processing_summary\": {\n",
					"        \"total_tables_processed\": len(processing_results),\n",
					"        \"successful_overwrites\": len([r for r in processing_results if r[\"table_result\"] == \"success\"]),\n",
					"        \"failed_overwrites\": len([r for r in processing_results if r[\"table_result\"] == \"failed\"])\n",
					"    },\n",
					"    \"table_details\": processing_results\n",
					"}\n",
					"\n",
					"# Convert to JSON string\n",
					"result_json_str = json.dumps(json_result, indent=2, default=datetime_handler)\n",
					"\n",
					"# Exit with the JSON result for pipeline consumption\n",
					"mssparkutils.notebook.exit(result_json_str)\n",
					""
				],
				"execution_count": 21
			}
		]
	}
}