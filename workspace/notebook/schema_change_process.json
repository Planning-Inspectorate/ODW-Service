{
	"name": "schema_change_process",
	"properties": {
		"description": "Notebook to be used to make schema changes to tables.",
		"folder": {
			"name": "utils"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "18dc14aa-13aa-41f2-af54-e5cb216a560b"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Notebook to be used to make schema changes to tables\n",
					"Define the parameters  \n",
					"Scroll down and make sure you amend the column names accordingly\n",
					"\n",
					"TODO: add column name paremeters so this can be run in a pipeline in all 3 environments"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define parameters, or pass them in a pipeline or other notebook"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"tags": [
						"parameters"
					]
				},
				"source": [
					"db_name = \"odw_standardised_db\"\n",
					"entity_name = \"appeal-s78\"\n",
					"table_name = \"sb_appeal_s78\""
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"source": [
					"full_table_name = f\"{db_name}.{table_name}\"\n",
					"storage_account: str = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"path_to_orchestration_file: str = \"abfss://odw-config@\"+storage_account+\"orchestration/orchestration.json\""
				],
				"execution_count": 28
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define functions to be used to validate schemas\n",
					"These are copies of functions defined elsewhere but I've added them here to avoid having to run the other notebook"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def get_incremental_key(entity_name: str, storage_account: str, path_to_orchestration_file: str) -> str:\n",
					"    # getting the incremental key from the odw-config/orchestration\n",
					"    df: DataFrame = spark.read.option(\"multiline\",\"true\").json(path_to_orchestration_file)\n",
					"    definitions: list = json.loads(df.toJSON().first())['definitions']\n",
					"    definition: dict = next((d for d in definitions if entity_name == d['Source_Filename_Start']), None)\n",
					"    return definition['Harmonised_Incremental_Key'] if definition and 'Harmonised_Incremental_Key' in definition else None"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"source": [
					"def create_spark_schema(db_name: str, entity_name: str) -> StructType:\n",
					"    incremental_key: str = get_incremental_key(entity_name, storage_account, path_to_orchestration_file) if db_name == 'odw_harmonised_db' else None\n",
					"    schema = mssparkutils.notebook.run(\"/py_create_spark_schema\", 30, {\"db_name\": db_name, \"entity_name\": entity_name, \"incremental_key\": incremental_key, \"is_servicebus_schema\": is_servicebus_schema})\n",
					"    spark_schema = StructType.fromJson(json.loads(schema))\n",
					"    return spark_schema"
				],
				"execution_count": 24
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Import unit test functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/unit-tests/py_unit_tests_functions"
				],
				"execution_count": 25
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define the two schemas to compare"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"data_model_schema = create_spark_schema(db_name, entity_name)\n",
					"table_schema = spark.table(full_table_name).schema"
				],
				"execution_count": 41
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Test schemas match"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"test_compare_schemas(data_model_schema, table_schema)"
				],
				"execution_count": 37
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Enable column mapping on the table you want to change"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"  spark.sql(f\"\"\" \n",
					"    ALTER TABLE {full_table_name} SET TBLPROPERTIES (\n",
					"        'delta.minReaderVersion' = '2',\n",
					"        'delta.minWriterVersion' = '5',\n",
					"        'delta.columnMapping.mode' = 'name'\n",
					"    )\n",
					"  \"\"\"\n",
					"  )"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Add a new column\n",
					"\n",
					"https://docs.delta.io/latest/delta-batch.html#explicitly-update-schema  \n",
					"\n",
					"If the column exists it will error."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"spark.sql(f\"\"\"\n",
					"    ALTER TABLE {full_table_name} ADD COLUMNS (appellantProofsSubmittedDate string) \n",
					"\"\"\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Re-order a column\n",
					"https://docs.delta.io/latest/delta-batch.html#change-column-comment-or-ordering  "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"spark.sql(f\"\"\"\n",
					"    ALTER TABLE {full_table_name} ALTER COLUMN appellantProofsSubmittedDate AFTER typeOfPlanningApplication\n",
					"\"\"\")"
				],
				"execution_count": 33
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Drop a column\n",
					"https://docs.delta.io/latest/delta-batch.html#drop-columns  "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"spark.sql(f\"\"\"\n",
					"    ALTER TABLE {full_table_name} DROP COLUMN (appellantProofsSubmittedDate) \n",
					"\"\"\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Rename a column\n",
					"https://docs.delta.io/latest/delta-batch.html#change-a-column-name  "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"spark.read.table(full_table_name) \\\n",
					"  .withColumnRenamed(\"old_name\", \"new_name\") \\\n",
					"  .write \\\n",
					"  .format(\"delta\") \\\n",
					"  .mode(\"overwrite\") \\\n",
					"  .option(\"overwriteSchema\", \"true\") \\\n",
					"#   .option(\"userMetadata\", \"THEODW-1647 schema changes\") \\\n",
					"  .saveAsTable(full_table_name)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Set user-defined commit metadata\n",
					"This is useful for perhaps adding the Jira ticket reference as a reason why the changes were made.  \n",
					"This can be run on its own, creating a new version of the table jsut with this new metadata, or it can be run as part of a rename or other pyspark command, liek above, specifying an extra option.  "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"df = spark.table(full_table_name)\n",
					"\n",
					"df.write.format(\"delta\") \\\n",
					"  .mode(\"overwrite\") \\\n",
					"  .option(\"userMetadata\", \"THEODW-1647 schema changes\") \\\n",
					"  .saveAsTable(full_table_name)"
				],
				"execution_count": 38
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Test schemas match after making the changes"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"table_schema = spark.table(full_table_name).schema\n",
					"test_compare_schemas(data_model_schema, table_schema)"
				],
				"execution_count": null
			}
		]
	}
}