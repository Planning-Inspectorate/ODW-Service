{
	"name": "schema_change_process",
	"properties": {
		"description": "Notebook to be used to make schema changes to tables.",
		"folder": {
			"name": "utils"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "2c7fbc28-35a1-4a72-9194-1f06e49b473e"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 32,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Notebook to be used to make schema changes to tables\n",
					"\n",
					"In order to keep it simple and to run easily in test and prod environment, please hardcode the changes you need to make into this notebook and add it to a release pipeline and then post deployment."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from odw.core.util.util import Util\n",
					"from pyspark.sql.types import *"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"storage_account: str = Util.get_storage_account()\n",
					"path_to_orchestration_file: str = \"abfss://odw-config@\"+storage_account+\"orchestration/orchestration.json\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define functions to be used to validate schemas\n",
					"These are copies of functions defined elsewhere but I've added them here to avoid having to run the other notebook"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def get_incremental_key(entity_name: str, storage_account: str, path_to_orchestration_file: str) -> str:\n",
					"    # getting the incremental key from the odw-config/orchestration\n",
					"    df: DataFrame = spark.read.option(\"multiline\",\"true\").json(path_to_orchestration_file)\n",
					"    definitions: list = json.loads(df.toJSON().first())['definitions']\n",
					"    definition: dict = next((d for d in definitions if entity_name == d['Source_Filename_Start']), None)\n",
					"    return definition['Harmonised_Incremental_Key'] if definition and 'Harmonised_Incremental_Key' in definition else None"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def create_spark_schema(db_name: str, entity_name: str) -> StructType:\n",
					"    incremental_key: str = get_incremental_key(entity_name, storage_account, path_to_orchestration_file) if db_name == 'odw_harmonised_db' else None\n",
					"    schema = mssparkutils.notebook.run(\"/py_create_spark_schema\", 30, {\"db_name\": db_name, \"entity_name\": entity_name, \"incremental_key\": incremental_key, \"is_servicebus_schema\": is_servicebus_schema})\n",
					"    spark_schema = StructType.fromJson(json.loads(schema))\n",
					"    return spark_schema"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Import unit test functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/unit-tests/py_unit_tests_functions"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Amend appeal_has tables"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define table details"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"tags": []
				},
				"source": [
					"std_db = \"odw_standardised_db\"\n",
					"hrm_db = \"odw_harmonised_db\"\n",
					"entity_name = \"appeal-has\"\n",
					"std_table_name = \"sb_appeal_has\"\n",
					"hrm_table_name = \"sb_appeal_has\"\n",
					"std_full_table_name = f\"{std_db}.{std_table_name}\"\n",
					"hrm_full_table_name = f\"{hrm_db}.{hrm_table_name}\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Enable column mapping on the table you want to change\n",
					"\n",
					"NB: only needs running the very first time on a table"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# standardised\n",
					"\n",
					"spark.sql(f\"\"\" \n",
					"ALTER TABLE {std_full_table_name} SET TBLPROPERTIES (\n",
					"'delta.minReaderVersion' = '2',\n",
					"'delta.minWriterVersion' = '5',\n",
					"'delta.columnMapping.mode' = 'name'\n",
					")\n",
					"\"\"\"\n",
					")\n",
					"\n",
					"# harmonised\n",
					"\n",
					"spark.sql(f\"\"\" \n",
					"ALTER TABLE {hrm_full_table_name} SET TBLPROPERTIES (\n",
					"'delta.minReaderVersion' = '2',\n",
					"'delta.minWriterVersion' = '5',\n",
					"'delta.columnMapping.mode' = 'name'\n",
					")\n",
					"\"\"\"\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define the two schemas to compare"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# standardised\n",
					"std_data_model_schema = create_spark_schema(std_db, entity_name)\n",
					"std_table_schema = spark.table(std_full_table_name).schema\n",
					"\n",
					"# harmonised\n",
					"hrm_data_model_schema = create_spark_schema(hrm_db, entity_name)\n",
					"hrm_table_schema = spark.table(hrm_full_table_name).schema"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Test schemas match"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"results = [((test_compare_schemas(std_data_model_schema, std_table_schema), test_compare_schemas(hrm_data_model_schema, hrm_table_schema)))]\n",
					"result = spark.createDataFrame(results, schema=[\"std\", \"hrm\"])\n",
					"result.show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Add a new column\n",
					"\n",
					"https://docs.delta.io/latest/delta-batch.html#explicitly-update-schema  \n",
					"\n",
					"If the column exists it will error."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# standardised\n",
					"spark.sql(f\"\"\"\n",
					"    ALTER TABLE {std_full_table_name} ADD COLUMNS (reasonForNeighbourVisits string AFTER neighbouringSiteAddresses, typeOfPlanningApplication string AFTER lpaCostsAppliedFor) \n",
					"\"\"\")\n",
					"\n",
					"# harmonised\n",
					"spark.sql(f\"\"\"\n",
					"    ALTER TABLE {hrm_full_table_name} ADD COLUMNS (reasonForNeighbourVisits string AFTER neighbouringSiteAddresses, typeOfPlanningApplication string AFTER lpaCostsAppliedFor) \n",
					"\"\"\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Re-run tests"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# standardised\n",
					"std_data_model_schema = create_spark_schema(std_db, entity_name)\n",
					"std_table_schema = spark.table(std_full_table_name).schema\n",
					"\n",
					"# harmonised\n",
					"hrm_data_model_schema = create_spark_schema(hrm_db, entity_name)\n",
					"hrm_table_schema = spark.table(hrm_full_table_name).schema\n",
					"\n",
					"# test again to see if schemas now match\n",
					"results = [((test_compare_schemas(std_data_model_schema, std_table_schema), test_compare_schemas(hrm_data_model_schema, hrm_table_schema)))]\n",
					"result = spark.createDataFrame(results, schema=[\"std\", \"hrm\"])\n",
					"result.show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Set user-defined metadata"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# standardised\n",
					"df = spark.table(std_full_table_name)\n",
					"\n",
					"df.write.format(\"delta\") \\\n",
					"  .mode(\"overwrite\") \\\n",
					"  .option(\"userMetadata\", \"THEODW-1648 schema changes for appeal-has\") \\\n",
					"  .saveAsTable(std_full_table_name)\n",
					"\n",
					"# harmonised\n",
					"df = spark.table(hrm_full_table_name)\n",
					"\n",
					"df.write.format(\"delta\") \\\n",
					"  .mode(\"overwrite\") \\\n",
					"  .option(\"userMetadata\", \"THEODW-1648 schema changes for appeal-has\") \\\n",
					"  .saveAsTable(hrm_full_table_name)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Amend appeal_s78 tables"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define table details"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"std_db = \"odw_standardised_db\"\n",
					"hrm_db = \"odw_harmonised_db\"\n",
					"entity_name = \"appeal-s78\"\n",
					"std_table_name = \"sb_appeal_s78\"\n",
					"hrm_table_name = \"sb_appeal_s78\"\n",
					"std_full_table_name = f\"{std_db}.{std_table_name}\"\n",
					"hrm_full_table_name = f\"{hrm_db}.{hrm_table_name}\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Enable column mapping\n",
					"\n",
					"NB: only needs running the very first time on a table"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# standardised\n",
					"\n",
					"spark.sql(f\"\"\" \n",
					"ALTER TABLE {std_full_table_name} SET TBLPROPERTIES (\n",
					"'delta.minReaderVersion' = '2',\n",
					"'delta.minWriterVersion' = '5',\n",
					"'delta.columnMapping.mode' = 'name'\n",
					")\n",
					"\"\"\"\n",
					")\n",
					"\n",
					"# harmonised\n",
					"\n",
					"spark.sql(f\"\"\" \n",
					"ALTER TABLE {hrm_full_table_name} SET TBLPROPERTIES (\n",
					"'delta.minReaderVersion' = '2',\n",
					"'delta.minWriterVersion' = '5',\n",
					"'delta.columnMapping.mode' = 'name'\n",
					")\n",
					"\"\"\"\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Define the two schemas to compare"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# standardised\n",
					"std_data_model_schema = create_spark_schema(std_db, entity_name)\n",
					"std_table_schema = spark.table(std_full_table_name).schema\n",
					"\n",
					"# harmonised\n",
					"hrm_data_model_schema = create_spark_schema(hrm_db, entity_name)\n",
					"hrm_table_schema = spark.table(hrm_full_table_name).schema"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Test schemas match"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"results = [((test_compare_schemas(std_data_model_schema, std_table_schema), test_compare_schemas(hrm_data_model_schema, hrm_table_schema)))]\n",
					"result = spark.createDataFrame(results, schema=[\"std\", \"hrm\"])\n",
					"result.show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Drop a column\n",
					"https://docs.delta.io/latest/delta-batch.html#drop-columns  "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# standardised\n",
					"spark.sql(f\"\"\"\n",
					"    ALTER TABLE {std_full_table_name} DROP COLUMN (consultedBodiesDetails) \n",
					"\"\"\")\n",
					"\n",
					"# harmonised\n",
					"spark.sql(f\"\"\"\n",
					"    ALTER TABLE {hrm_full_table_name} DROP COLUMN (consultedBodiesDetails) \n",
					"\"\"\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Rename a column\n",
					"https://docs.delta.io/latest/delta-batch.html#change-a-column-name  "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# standardised\n",
					"spark.read.table(std_full_table_name) \\\n",
					"  .withColumnRenamed(\"eiaConsultedBodiesDetails\", \"consultedBodiesDetails\") \\\n",
					"  .write \\\n",
					"  .format(\"delta\") \\\n",
					"  .mode(\"overwrite\") \\\n",
					"  .option(\"overwriteSchema\", \"true\") \\\n",
					"  .option(\"userMetadata\", \"THEODW-1647 schema changes\") \\\n",
					"  .saveAsTable(std_full_table_name)\n",
					"\n",
					"# harmonised\n",
					"spark.read.table(hrm_full_table_name) \\\n",
					"  .withColumnRenamed(\"eiaConsultedBodiesDetails\", \"consultedBodiesDetails\") \\\n",
					"  .write \\\n",
					"  .format(\"delta\") \\\n",
					"  .mode(\"overwrite\") \\\n",
					"  .option(\"overwriteSchema\", \"true\") \\\n",
					"  .option(\"userMetadata\", \"THEODW-1647 schema changes\") \\\n",
					"  .saveAsTable(hrm_full_table_name)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Re-order a column\n",
					"https://docs.delta.io/latest/delta-batch.html#change-column-comment-or-ordering  "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# standardised\n",
					"spark.sql(f\"\"\"\n",
					"    ALTER TABLE {std_full_table_name} ALTER COLUMN appellantProofsSubmittedDate AFTER appellantStatementSubmittedDate\n",
					"\"\"\")\n",
					"\n",
					"# harmonised\n",
					"spark.sql(f\"\"\"\n",
					"    ALTER TABLE {hrm_full_table_name} ALTER COLUMN appellantProofsSubmittedDate AFTER appellantStatementSubmittedDate\n",
					"\"\"\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Add columns"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# standardised\n",
					"spark.sql(f\"\"\"\n",
					"    ALTER TABLE {std_full_table_name} ADD COLUMNS (reasonForNeighbourVisits string AFTER neighbouringSiteAddresses, developmentType string AFTER typeOfPlanningApplication, appellantProofsSubmittedDate string AFTER appellantStatementSubmittedDate) \n",
					"\"\"\")\n",
					"\n",
					"# harmonised\n",
					"spark.sql(f\"\"\"\n",
					"    ALTER TABLE {hrm_full_table_name} ADD COLUMNS (reasonForNeighbourVisits string AFTER neighbouringSiteAddresses, developmentType string AFTER typeOfPlanningApplication, appellantProofsSubmittedDate string AFTER appellantStatementSubmittedDate) \n",
					"\"\"\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Set user-defined commit metadata\n",
					"This is useful for perhaps adding the Jira ticket reference as a reason why the changes were made.  \n",
					"This can be run on its own, creating a new version of the table jsut with this new metadata, or it can be run as part of a rename or other pyspark command, liek above, specifying an extra option.  "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# standardised\n",
					"df = spark.table(std_full_table_name)\n",
					"\n",
					"df.write.format(\"delta\") \\\n",
					"  .mode(\"overwrite\") \\\n",
					"  .option(\"userMetadata\", \"THEODW-1647 schema changes for appeal-s78\") \\\n",
					"  .saveAsTable(std_full_table_name)\n",
					"\n",
					"# harmonised\n",
					"df = spark.table(hrm_full_table_name)\n",
					"\n",
					"df.write.format(\"delta\") \\\n",
					"  .mode(\"overwrite\") \\\n",
					"  .option(\"userMetadata\", \"THEODW-1647 schema changes for appeal-s78\") \\\n",
					"  .saveAsTable(hrm_full_table_name)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Test schemas match after making the changes"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# standardised\n",
					"std_data_model_schema = create_spark_schema(std_db, entity_name)\n",
					"std_table_schema = spark.table(std_full_table_name).schema\n",
					"\n",
					"# harmonised\n",
					"hrm_data_model_schema = create_spark_schema(hrm_db, entity_name)\n",
					"hrm_table_schema = spark.table(hrm_full_table_name).schema\n",
					"\n",
					"# test again to see if schemas now match\n",
					"results = [((test_compare_schemas(std_data_model_schema, std_table_schema), test_compare_schemas(hrm_data_model_schema, hrm_table_schema)))]\n",
					"result = spark.createDataFrame(results, schema=[\"std\", \"hrm\"])\n",
					"result.show()"
				],
				"execution_count": null
			}
		]
	}
}