{
	"name": "py_utils_curated_validate_functions",
	"properties": {
		"description": "Generic validation functions that can be imported and used in other notebooks.",
		"folder": {
			"name": "archive/utils/data-validation"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "70ccd68d-bfa3-4f87-b19f-d1e85f3f7991"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 32,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Functions to be used for validating data in a table against a defined json schema. To be called from other validation notebooks."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pandas as pd"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"account_name = 'pinsstodwdevuks9h80mb'\r\n",
					"container_name = 'odw-config'\r\n",
					"relative_path = \"data_validation/outputs/curated/\"\r\n",
					"jobId = mssparkutils.env.getJobId()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Validation functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def is_iso8601_date_time(instance):\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"    Function to check if a date matches ISO-8601 format\r\n",
					"    \"\"\"\r\n",
					"    if instance is None:\r\n",
					"        return True\r\n",
					"    try:\r\n",
					"        parse_date(instance)\r\n",
					"        return True\r\n",
					"    except ParseError:\r\n",
					"        return False\r\n",
					"\r\n",
					"def validate_data(data: list, schema: dict, primary_key: str) -> str:\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"    Function to validate a list of rows of data.\r\n",
					"    Validation includes a format check against ISO-8601.\r\n",
					"\r\n",
					"    Args:\r\n",
					"        data: list\r\n",
					"        schema: dictionary\r\n",
					"        primary_key: string\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        success: string\r\n",
					"        \"Success\" or \"Failed\" for the result of the validation\r\n",
					"    \"\"\"\r\n",
					"    \r\n",
					"    format_checker = FormatChecker()\r\n",
					"    format_checker.checks(\"date-time\")(is_iso8601_date_time)\r\n",
					"    validator = Draft202012Validator(schema, format_checker=format_checker)\r\n",
					"\r\n",
					"    success = \"Success\"\r\n",
					"\r\n",
					"    for index, row in enumerate(data, start=1):\r\n",
					"        errors = list(validator.iter_errors(row))\r\n",
					"        if errors:\r\n",
					"            success = \"Failed\"\r\n",
					"            print(f\"{len(errors)} error(s) found in row {index}\")\r\n",
					"            key_value = row.get(primary_key)\r\n",
					"            print(f\"{primary_key}: {key_value}\")\r\n",
					"            for error in errors:\r\n",
					"                print(error.message)\r\n",
					"                print(\"-\"*100)\r\n",
					"            print(\"#\"*100)\r\n",
					"            print(\"#\"*100)\r\n",
					"\r\n",
					"    return success"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Function to convert the rows in the table into a list of dictionaries - FLAT SCHEMA ONLY\r\n",
					"\r\n",
					"This maps the columns in the table to the json schema and creates the array(s) if needed to allow accurate validation of the data against the schema"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def create_json_from_table_flat(data: pd.DataFrame) -> list:\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"    Args:\r\n",
					"        data: pandas dataframe\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        list of dictionaries\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    data_dict = defaultdict(dict)\r\n",
					"\r\n",
					"    final_dict = []\r\n",
					"\r\n",
					"    date_format = \"%Y-%m-%d %H:%M:%S\"\r\n",
					"\r\n",
					"    for index, row in data.iterrows():\r\n",
					"\r\n",
					"        for column, value in row.items():\r\n",
					"            data_dict[column] = value.strftime(date_format) if isinstance(value, pd.Timestamp) else value\r\n",
					"        \r\n",
					"        row_dict = {key: value for key, value in data_dict.items()}\r\n",
					"        final_dict.append(row_dict)\r\n",
					"\r\n",
					"    return final_dict"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Function to convert the rows in the table into a list of dictionaries - SCHEMAS WITH ARRAYS\r\n",
					"\r\n",
					"This maps the columns in the table to the json schema and creates the array(s) if needed to allow accurate validation of the data against the schema"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def create_json_from_table_with_arrays(data: pd.DataFrame, primary_key: str, array: str, array_field: str) -> list:\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"    Args:\r\n",
					"        data: pandas dataframe\r\n",
					"        primary_key: string\r\n",
					"        array: string\r\n",
					"        array_field: string\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        list of dictionaries\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    data_dict = defaultdict(dict)\r\n",
					"\r\n",
					"    array_field_to_exclude = array_field\r\n",
					"\r\n",
					"    date_format = \"%Y-%m-%d %H:%M:%S\"\r\n",
					"\r\n",
					"    for index, row in data.iterrows():\r\n",
					"\r\n",
					"        primary_key_field = row[primary_key]\r\n",
					"\r\n",
					"        if primary_key_field not in data_dict:\r\n",
					"\r\n",
					"            data_dict[primary_key_field][array] = []\r\n",
					"\r\n",
					"            for column, value in row.items():\r\n",
					"                if column in array_field_to_exclude:\r\n",
					"                    continue\r\n",
					"                data_dict[primary_key_field][column] = value.strftime(date_format) if isinstance(value, pd.Timestamp) else value\r\n",
					"\r\n",
					"        array_to_populate = data_dict[primary_key_field][array]\r\n",
					"        new_item = row[array_field]\r\n",
					"\r\n",
					"        if new_item is not None and not np.any(new_item == array_to_populate):\r\n",
					"            array_to_populate.append(new_item)\r\n",
					"\r\n",
					"    final_dict = {key: dict(value) for key, value in data_dict.items()}\r\n",
					"\r\n",
					"    data_to_compare = list(final_dict.values())\r\n",
					"\r\n",
					"    return data_to_compare"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Function to convert the rows in the table into a list of dictionaries - examination_timetable only\r\n",
					"\r\n",
					"Bespoke function for examination_timetable as it's the only schema with nested arrays within arrays so need a bit of extra processing."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def create_json_from_table_examination_timetable(data: pd.DataFrame) -> list:\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"    Args:\r\n",
					"        data: pandas dataframe\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        list of dictionaries\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    data_dict = defaultdict(dict)\r\n",
					"\r\n",
					"    event_fields = [\"eventId\", \"type\", \"eventTitle\", \"description\", \"eventDeadlineStartDate\", \"date\"]\r\n",
					"    eventLineItemFields = [\"description\"]\r\n",
					"\r\n",
					"    date_format = \"%Y-%m-%d %H:%M:%S\"\r\n",
					"\r\n",
					"    for index, row in data.iterrows():\r\n",
					"        caseReference = row[\"caseReference\"]\r\n",
					"\r\n",
					"        for column, value in row.items():\r\n",
					"            if caseReference not in data_dict:\r\n",
					"                data_dict[caseReference][column] = value\r\n",
					"\r\n",
					"                data_dict[caseReference][\"events\"] = []\r\n",
					"\r\n",
					"            events = data_dict[caseReference][\"events\"]\r\n",
					"            new_event = {column: value for column, value in row.items() if column in event_fields}\r\n",
					"            new_eventLineItem = {column: value for column, value in row.items() if column in eventLineItemFields}\r\n",
					"            \r\n",
					"            if new_event not in events:\r\n",
					"                events.append(new_event)\r\n",
					"\r\n",
					"        for event in events:\r\n",
					"            if \"eventLineItems\" not in event:\r\n",
					"                event[\"eventLineItems\"] = []\r\n",
					"                eventLineItems = event[\"eventLineItems\"]\r\n",
					"                if new_eventLineItem not in eventLineItems:\r\n",
					"                    eventLineItems.append(new_eventLineItem)\r\n",
					"\r\n",
					"    final_dict = {key: dict(value) for key, value in data_dict.items()}\r\n",
					"\r\n",
					"    data_to_compare = list(final_dict.values())\r\n",
					"\r\n",
					"    return data_to_compare"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Create path to storage account"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def create_adls_path(account_name: str, container_name: str, relative_path: str):\r\n",
					"    '''\r\n",
					"    Function to create the path to Azure Data Lake Storage\r\n",
					"    '''\r\n",
					"    file_path = f'abfss://{container_name}@{account_name}.dfs.core.windows.net/{relative_path}'\r\n",
					"    return file_path"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Function to mount the storage account"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def mount_storage(relative_path: str, jobId):\r\n",
					"    '''\r\n",
					"    Function to mount the storage path\r\n",
					"    '''\r\n",
					"    path = create_adls_path(account_name, container_name, relative_path)\r\n",
					"    mount_storage(path=path, mount_point=relative_path)\r\n",
					"\r\n",
					"    spark_fs_path = f\"synfs:/{jobId}/{relative_path}\"\r\n",
					"    local_fs_path = f\"/synfs/{jobId}/{relative_path}\"\r\n",
					"\r\n",
					"    return spark_fs_path"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Function to write a file to storage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def write_file_to_storage(path: str, filename: str, data):\r\n",
					"    '''\r\n",
					"    Function to write a file to storage using mssparkutils.fs.put.\r\n",
					"    Requires a path, filename and data / content.\r\n",
					"    '''\r\n",
					"    full_path = create_adls_path(account_name, container_name, relative_path)\r\n",
					"    mssparkutils.fs.put(f'{path}/{filename}', data, overwrite=True)\r\n",
					"    return print(f'File: {filename} - written to storage. Path: {full_path}')"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Define the storage path and full path"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"path = mount_storage(relative_path, jobId)\r\n",
					"full_path = create_adls_path(account_name, container_name, relative_path)"
				],
				"execution_count": null
			}
		]
	}
}