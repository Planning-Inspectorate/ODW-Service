{
	"name": "delta_backup_analysis",
	"properties": {
		"folder": {
			"name": "archive/utils"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "3bcf3f38-6d4d-4c92-8be2-61a255b37015"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 32,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Anslysis of file size difference between a full backup of th table and a delta backup\n",
					"\n",
					"I have used `input_files()` to get the files associated with the latest version of the table and the previous version. Of course every time a table is updated the data can be re-organised into different files for optimisation. The size in MB might not match the total folder size you see from Storage Explorer. This may need investigating further to understand how to accurately capture the true size of the backup for each table.  \n",
					"\n",
					"You need to amend this code to take the table path from the config file as right now I have hardcoded a certain table for testing.  \n",
					"\n",
					"Also amend the code to loop through the whole database and then all databases to get a full size of the daily backups and then compare to a delta backup. Of couse, not all tables are deltas so capturing the size of the curated database for example would not include a delta backup as we may need to copy that one whole.  "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from delta.tables import DeltaTable\n",
					"from pyspark.sql import DataFrame"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_mount_storage"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"database: str = \"odw_standardised_db\"\n",
					"table: str = \"horizon_case_involvement\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"standardised_tables = spark.catalog.listTables(database)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"print([table.name for table in standardised_tables])"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"storage_account: str = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"table_path: str = f\"abfss://odw-standardised@{storage_account}/Horizon/{table}\"\n",
					"delta_table = DeltaTable.forPath(spark, table_path)\n",
					"storage_path = f\"abfss://odw-standardised@{storage_account}\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"mount_storage(path=storage_path)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def get_delta_table(table: str) -> DataFrame:\n",
					"    return DeltaTable.forPath(spark, table_path)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def get_table_path(table: str):\n",
					"    return f\"abfss://odw-standardised@{storage_account}/Horizon/{table}\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def get_table_history(table: str) -> DataFrame:\n",
					"    return delta_table.history()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def get_current_version(table: str) -> int: \n",
					"    return get_table_history(table).head(1)[0]['version']"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def get_previous_version(table: str) -> int:\n",
					" return get_current_version(table) - 1"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def get_current_version_files(table: str) -> list:\n",
					"    return get_delta_table(table).toDF().inputFiles()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def get_previous_version_files(table: str) -> list:\n",
					"    previous_version = get_previous_version(table)\n",
					"    table_path = get_table_path(table)\n",
					"    return spark.read.format(\"delta\").option(\"versionAsOf\", previous_version).load(table_path).inputFiles()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def get_file_sizes(file_paths: list):\n",
					"    total_size = 0\n",
					"    for file_path in file_paths:\n",
					"        # Get the file size using mssparkutils.fs\n",
					"        file_info = mssparkutils.fs.ls(file_path)\n",
					"        file_size = file_info[0].size  # Get file size in bytes\n",
					"        total_size += file_size\n",
					"        # print(f\"File: {file_path}, Size: {file_size} bytes\")\n",
					"    \n",
					"    return total_size / (1024*1024)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"current_version_files = get_current_version_files(table)\n",
					"current_version_size = get_file_sizes(current_version_files)\n",
					"current_version_size"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"previous_version_files = get_previous_version_files(table)\n",
					"previous_version_size = get_file_sizes(previous_version_files)\n",
					"previous_version_size"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"size_delta = current_version_size - previous_version_size"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"print(f\"File size delta between version {get_current_version(table)} and version {get_previous_version(table)} is {size_delta} MB.\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"if previous_version_size != 0:\n",
					"    percentage_difference = (size_delta / previous_version_size) * 100\n",
					"else:\n",
					"    percentage_difference = 0\n",
					"\n",
					"print(f\"Percentage difference: {percentage_difference:.2f}%\")"
				],
				"execution_count": null
			}
		]
	}
}