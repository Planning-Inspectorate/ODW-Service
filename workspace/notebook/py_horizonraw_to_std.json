{
	"name": "py_horizonraw_to_std",
	"properties": {
		"folder": {
			"name": "1-odw-raw-to-standardised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "3777aed5-bb53-4f96-b736-92c3f1ed97e7"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Prerequisites\n",
					"1. Make sure the new raw file's entry has been added to the Orchestration i.e `/infrastructure/configuration/data-lake/orchestration/orchestration.json`\n",
					"2. Make sure the standardised table's schema is present on the path specified in the entry added in step 1.\n",
					"3. Only if the raw file is huge (several GBs), the spark pool might need some upscaling. Hence the following cell"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%%configure -f\n",
					"{\n",
					"    \"conf\": {\n",
					"        \"spark.kryoserializer.buffer.max\": \"2047m\",\n",
					"        \"spark.driver.maxResultSize\": \"10g\",\n",
					"        \"spark.rpc.message.maxSize\": \"1280\"\n",
					"    }\n",
					"}"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Get the Storage Account"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import asyncio\n",
					"import nest_asyncio\n",
					"import tracemalloc\n",
					"tracemalloc.start()\n",
					"from pyspark.sql import SparkSession\n",
					"from notebookutils import mssparkutils\n",
					"import json\n",
					"import calendar\n",
					"from datetime import datetime, timedelta, date\n",
					"import pandas as pd\n",
					"import os"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"spark = SparkSession.builder.getOrCreate()\n",
					"source_folder='Horizon'\n",
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"raw_container = \"abfss://odw-raw@\" + storage_account\n",
					"source_path: str = f\"abfss://odw-raw@{storage_account}{source_folder}/\"\n",
					"process_name = mssparkutils.runtime.context['currentNotebookName']"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run \"1-odw-raw-to-standardised/Fileshare/SAP_HR/py_1_raw_to_standardised_hr_functions\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Initialise the parameters"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"date_folder=''\n",
					"source_folder=''\n",
					"source_frequency_folder=''\n",
					"specific_file='' # if not provided, it will ingest all files in the date_folder\n",
					"isMultiLine = True\n",
					"delete_existing_table=False\n",
					"dataAttribute = \"\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# List all items in the directory\n",
					"items = mssparkutils.fs.ls(source_path)\n",
					"# Filter for directories and get their names and modification times\n",
					"folders = [(item.name, item.modifyTime) for item in items if item.isDir]\n",
					"# Sort folders by modification time in descending order\n",
					"sorted_folders = sorted(folders, key=lambda x: x[1], reverse=True)\n",
					"# Get the name of the latest modified folder\n",
					"if sorted_folders:\n",
					"    latest_folder = sorted_folders[0][0]\n",
					"    source_path=f\"{source_path}{latest_folder}\"\n",
					"    print(f\"Latest modified folder: {latest_folder}\")\n",
					"    print(source_path)\n",
					"else:\n",
					"    print(\"No folders found in the specified directory.\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"files = mssparkutils.fs.ls(source_path)\n",
					"horizon_files = [file.name for file in files]\n",
					"horizon_files \n",
					"# = horizon_files[:1] # comment this during the full execution"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Ingest the data from the raw/source into the standardised table. \n",
					"If the table doesn't already exist, this will create the table first and ingest the data."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"async def ingest_horizon(date_folder: str, file: str):\n",
					"\n",
					"    if date_folder == '':\n",
					"        date_folder = datetime.now().date()\n",
					"    else:\n",
					"        date_folder = datetime.strptime(date_folder, \"%Y-%m-%d\")\n",
					"\n",
					"    date_folder_str = date_folder.strftime('%Y-%m-%d')\n",
					"    # source_folder_path = source_folder if not source_frequency_folder else f\"{source_folder}/{source_frequency_folder}\"\n",
					"\n",
					"    # READ ORCHESTRATION DATA\n",
					"    path_to_orchestration_file = \"abfss://odw-config@\"+storage_account+\"orchestration/orchestration.json\"\n",
					"    df = spark.read.option(\"multiline\",\"true\").json(path_to_orchestration_file)\n",
					"    definitions = json.loads(df.toJSON().first())['definitions']\n",
					"\n",
					"    # source_path = f\"{raw_container}{source_folder_path}/{date_folder_str}\"\n",
					"\n",
					"    # try:\n",
					"    #     files = mssparkutils.fs.ls(source_path)\n",
					"    # except Exception as e:\n",
					"    #     mssparkutils.notebook.exit(f\"Raw file not found at {source_path}\")\n",
					"\n",
					"\n",
					"    # for file in files:\n",
					"\n",
					"    # ignore json raw files if source is service bus\n",
					"    # if source_folder == 'ServiceBus' and file.endswith('.json'):\n",
					"    #     continue\n",
					"\n",
					"    # # ignore files other than specified file \n",
					"    # if specific_file != '' and not file.startswith(specific_file + '.'):\n",
					"    #     continue\n",
					"        \n",
					"    definition = next((d for d in definitions if (specific_file == '' or d['Source_Filename_Start'] == specific_file) \n",
					"                        and (not source_frequency_folder or d['Source_Frequency_Folder'] == source_frequency_folder) \n",
					"                        and file.startswith(d['Source_Filename_Start'])), None)\n",
					"    \n",
					"    if definition:\n",
					"        expected_from = date_folder - timedelta(days=1)\n",
					"        expected_from = datetime.combine(expected_from, datetime.min.time())\n",
					"        expected_to = expected_from + timedelta(days=definition['Expected_Within_Weekdays']) \n",
					"\n",
					"        if delete_existing_table:\n",
					"            print(f\"Deleting existing table if exists odw_standardised_db.{definition['Standardised_Table_Name']}\")\n",
					"            mssparkutils.notebook.run('/utils/py_delete_table', 300, arguments={'db_name': 'odw_standardised_db', 'table_name': definition['Standardised_Table_Name']})\n",
					"\n",
					"        print(f\"Ingesting {file}\")\n",
					"        (ingestion_failure, row_count) = ingest_adhoc(storage_account, definition, source_path, file, expected_from, expected_to,process_name,isMultiLine, dataAttribute)\n",
					"        print(f\"Ingested {row_count} rows\")\n",
					"\n",
					"        if ingestion_failure:\n",
					"            print(f\"Errors reported during Ingestion!!\")\n",
					"            raise RuntimeError(\"Ingestion Failure\")\n",
					"        else:\n",
					"            print(f\"No Errors reported during Ingestion\")\n",
					"\n",
					"    else:\n",
					"        if specific_file != '':\n",
					"            raise ValueError(f\"No definition found for {specific_file}\")\n",
					"        else:\n",
					"            print(\"No definition found\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"async def load_horizon_async():\n",
					"    tasks: list = [\n",
					"        ingest_horizon(date_folder=latest_folder, file=file) for file in horizon_files\n",
					"    ]\n",
					"    await asyncio.gather(*tasks)\n",
					"\n",
					"nest_asyncio.apply()\n",
					"loop = asyncio.get_event_loop()\n",
					"loop.run_until_complete(load_horizon_async())"
				],
				"execution_count": null
			}
		]
	}
}