{
	"name": "nsip_document",
	"properties": {
		"folder": {
			"name": "odw-curated"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "390cf2fa-1667-4701-9d4d-5f665603d91a"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.sql import DataFrame"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Create a view for the data, joining harmonised tables where necessary"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"\r\n",
					"CREATE OR REPLACE VIEW odw_curated_db.vw_nsip_document\r\n",
					"\r\n",
					"AS\r\n",
					"\r\n",
					"SELECT DISTINCT \r\n",
					"doc.documentId,\r\n",
					"doc.caseId,\r\n",
					"doc.caseRef,\r\n",
					"doc.documentReference,\r\n",
					"doc.version,\r\n",
					"doc.examinationRefNo,\r\n",
					"doc.filename,\r\n",
					"doc.originalFilename,\r\n",
					"doc.size,\r\n",
					"doc.mime,\r\n",
					"COALESCE(doc.documentURI, '') AS documentURI,\r\n",
					"doc.publishedDocumentURI,\r\n",
					"doc.path,\r\n",
					"doc.virusCheckStatus,\r\n",
					"doc.fileMD5,\r\n",
					"doc.dateCreated,\r\n",
					"doc.lastModified,\r\n",
					"LOWER(doc.caseType) AS caseType,\r\n",
					"doc.redactedStatus,\r\n",
					"CASE\r\n",
					"    WHEN doc.PublishedStatus = 'Depublished'\r\n",
					"    THEN 'unpublished'\r\n",
					"    ELSE REPLACE(\r\n",
					"        LOWER(doc.PublishedStatus),\r\n",
					"        ' ',\r\n",
					"        '_')\r\n",
					"END AS publishedStatus,\r\n",
					"doc.datePublished,\r\n",
					"doc.documentType,\r\n",
					"doc.securityClassification,\r\n",
					"doc.sourceSystem,\r\n",
					"doc.origin,\r\n",
					"doc.owner,\r\n",
					"doc.author,\r\n",
					"doc.authorWelsh,\r\n",
					"doc.representative,\r\n",
					"doc.description,\r\n",
					"doc.descriptionWelsh,\r\n",
					"CASE\r\n",
					"    WHEN doc.documentCaseStage = \"Developer's Application\"\r\n",
					"    THEN 'developers_application'\r\n",
					"    WHEN doc.documentCaseStage = 'Post decision'\r\n",
					"    THEN 'post_decision'\r\n",
					"    ELSE LOWER(doc.documentCaseStage)\t            \r\n",
					"END AS documentCaseStage,\r\n",
					"doc.filter1,\r\n",
					"doc.filter1Welsh,\r\n",
					"doc.filter2,\r\n",
					"doc.horizonFolderId,\r\n",
					"doc.transcriptId\t\r\n",
					"\r\n",
					"FROM odw_harmonised_db.nsip_document AS doc\r\n",
					"LEFT JOIN odw_curated_db.nsip_project proj\r\n",
					"    ON proj.caseReference = doc.caseRef\r\n",
					"\r\n",
					"WHERE doc.IsActive = 'Y';"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## THE BELOW IS OLDER CODE, AND IT CHANGED THE SCHEMA"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Specify the schema for the data, taken from the curated table which has already been created in advance from the data model"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"data: DataFrame = spark.sql(\"SELECT * FROM odw_curated_db.vw_nsip_document\")\r\n",
					"schema: StructType = spark.table(\"odw_curated_db.nsip_document\").schema"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Cast all field data types in the data to the data types from the curated table schema\n",
					"\n",
					"This is necessary because the view generated above is joining harmonised tables, many of which are sourced from Horizon and will have a different schema to the final table and fields will have different data types. Therefore, taking the curated schema as defined in thr data model and casting all fields correctly, ensures accuracy."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df: DataFrame = data.select(\n",
					"    *[\n",
					"        col(field.name).cast(field.dataType).alias(field.name)\n",
					"        for field in schema.fields\n",
					"    ]\n",
					")"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Print the schema as a visual check"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df.printSchema()"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Write the data to the curated table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"logInfo(\"Writing odw_curated_db.nsip_document\")\n",
					"data.write.mode(\"overwrite\").saveAsTable(\"odw_curated_db.nsip_document\")\n",
					"logInfo(\"Written odw_curated_db.nsip_document\")"
				],
				"execution_count": 7
			}
		]
	}
}