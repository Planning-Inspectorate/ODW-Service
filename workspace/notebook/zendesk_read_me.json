{
	"name": "zendesk_read_me",
	"properties": {
		"description": "this is a file is a user manual for zendesk notebooks. ",
		"folder": {
			"name": ""
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "bb054640-5c58-4451-9a75-f9ed0796730e"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 32,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Zendesk steps:\r\n",
					"1. Take a system extract from the SAP storgae and save it in the the blob storage (abfss://odw-raw@pinsstodwdevuks9h80mb.dfs.core.windows.net/ZenDesk/Export/export-2023-03-15-1051-10932060-13758561992081e5c3.json)\r\n",
					"2. Since the export file holds multiple zendesk tickets, run the odw-raw/zendesk_standardised_and_harmonised notebook to load the file and separate it in one json per ticket. \r\n",
					"3. Then run zendesk_raw_to standardised to load the export into the standardised table and also create the custom_fileds and fileds notebooks which hold the ref number in harmonised layer. Then run the harmonised notebook called zendesk harmonised export to add the remaining nested data fields into tables.\r\n",
					"4. the reason behind creating all diffrent tables from harmonised this way rather than the way other tables were created is becasue all the data in the zendesk tickets is nested. Also, in adition all tickets are diffrent from each other containg diffrent column names and information. The information in columns is also writen by users so it can't always be trusted (eg  the custom_fields/fields column has id and value which holds the ticket reference to other table. That info was added by a user manually and the UI does not require the user to put a valid Ref Number)\r\n",
					"5. After the zendesk standardised and harmonised has been build we have created a pipeline called 0_Zendesk_API_to_RAW which pulls every 24 hours the data from the storage using an api and loads it into the standardised table called zendesk_system_extract\r\n",
					"6. In case of any missing data we use the 0_Zendesk_API_to_RAW historical pipeline whcih downloads all tickets existing in zendesk.\r\n",
					"7. In order to update the standardised table with all the data, go to the zendesk_historical notebook in odw-raw. BEFORE RUNNING THE NOTEBOOK YOU MUST CHNAGE THE NAME OF THE FILE WITH THE NEW ONE. eg go to the zendesk_hist function and change :\"with open(f\"/synfs/{jobId}/zendesk_items/historical_2023-09-04.json\", 'r',encoding=\"utf-8-sig\") as json_file: \" to \"with open(f\"/synfs/{jobId}/zendesk_items/NAME OF THE FILE\", 'r',encoding=\"utf-8-sig\") as json_file: \". NAME OF THE FILE can be found abfss://odw-raw@pinsstodwdevuks9h80mb.dfs.core.windows.net/ZenDesk/historical/historical_2023-09-04.json and should have the date on which the pipeline 0_Zendesk_API_to_RAW historical was run on. "
				]
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}