{
	"name": "py_sb_raw_to_std",
	"properties": {
		"folder": {
			"name": "service-bus"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "ecee0fc4-d757-438c-bdfb-7831cbab1e6f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"entity_name='appeal-has'\n",
					"date_folder=''"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from notebookutils import mssparkutils\n",
					"import json\n",
					"from datetime import datetime, date\n",
					"from pyspark.sql.functions import current_timestamp, expr, to_timestamp, lit,input_file_name\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql.types import StructType,TimestampType\n",
					"from pyspark.sql.functions import *\n",
					"import re\n",
					"from functools import reduce"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark: SparkSession = SparkSession.builder.getOrCreate()\n",
					"storage_account: str = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"table_name: str = f\"odw_standardised_db.sb_{entity_name.replace('-', '_')}\"\n",
					"date_folder = datetime.now().date().strftime('%Y-%m-%d') if date_folder == '' else date_folder\n",
					"source_path: str = f\"abfss://odw-raw@{storage_account}ServiceBus/{entity_name}/\"\n",
					"schema = mssparkutils.notebook.run(\"/py_create_spark_schema\", 30, {\"db_name\": 'odw_standardised_db', \"entity_name\": entity_name})\n",
					"spark_schema = StructType.fromJson(json.loads(schema)) if schema != '' else ''"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Function to Extract the Maximum Datetime from filename in odw_standardised_db.sb_appeal_has"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def process_and_get_max_datetime(table_name, filename_column=\"filename\", default_datetime=\"2024-11-26T17:02:12.011836+0000\"):\r\n",
					"    df = spark.sql(f'select * from {table_name}')\r\n",
					"    \r\n",
					"    if filename_column in df.columns:\r\n",
					"        processed_df = df.withColumn(\r\n",
					"            \"extracted_datetime\",\r\n",
					"            when(\r\n",
					"                to_timestamp(\r\n",
					"                    regexp_replace(\r\n",
					"                        element_at(split(df[filename_column], \"/\"), -1),\r\n",
					"                        r\"appeal-has_|\\.json\",\r\n",
					"                        \"\"\r\n",
					"                    ),\r\n",
					"                    \"yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ\"\r\n",
					"                ).isNotNull(),\r\n",
					"                to_timestamp(\r\n",
					"                    regexp_replace(\r\n",
					"                        element_at(split(df[filename_column], \"/\"), -1),\r\n",
					"                        r\"appeal-has_|\\.json\",\r\n",
					"                        \"\"\r\n",
					"                    ),\r\n",
					"                    \"yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ\"\r\n",
					"                )\r\n",
					"            ).otherwise(lit(default_datetime).cast(TimestampType()))\r\n",
					"        )\r\n",
					"    else:\r\n",
					"        processed_df = df.withColumn(\r\n",
					"            \"extracted_datetime\",\r\n",
					"            lit(default_datetime).cast(TimestampType())\r\n",
					"        )\r\n",
					"        \r\n",
					"\r\n",
					"    max_datetime = processed_df.select(max(\"extracted_datetime\")).collect()[0][0]\r\n",
					"    if max_datetime is None:\r\n",
					"        print(\"No valid datetime found. Using default datetime.\")\r\n",
					"        max_datetime = datetime.strptime(default_datetime, \"%Y-%m-%dT%H:%M:%S.%f%z\")\r\n",
					"    \r\n",
					"    return max_datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\")+'+0000'\r\n",
					"\r\n",
					"# Usage\r\n",
					"try:\r\n",
					"    max_extracted_datetime = process_and_get_max_datetime(table_name)\r\n",
					"    print(f\"Max extracted datetime: {max_extracted_datetime}\")\r\n",
					"except Exception as e:\r\n",
					"    print(f\"An error occurred: {str(e)}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run service-bus/py_spark_df_ingestion_functions"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df = spark.table(table_name)\r\n",
					"display(df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"def get_max_file_date_from_folder(folder_name: str) -> str:\r\n",
					"    df = collect_all_raw_sb_data_historic(folder_name).select(\"caseId\", \"caseReference\", \"input_file\")\r\n",
					"    date_pattern = r'(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{6}\\+\\d{4})'\r\n",
					"    df = df.withColumn(\"file_date\", regexp_extract(df[\"input_file\"], date_pattern, 1))\r\n",
					"    df = df.withColumn(\"file_date\", df[\"file_date\"].cast(TimestampType()))\r\n",
					"    max_timestamp = df.agg(max(\"file_date\")).collect()[0][0]\r\n",
					"    formatted_timestamp = max_timestamp.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")+\"+0000\"\r\n",
					"    return f\"Maximum file date: {formatted_timestamp}\"\r\n",
					"\r\n",
					"folder_name = \"appeal-has\"\r\n",
					"print(get_max_file_date_from_folder(folder_name))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_all_files_recursive(source_path):\r\n",
					"    files = []\r\n",
					"    entries = mssparkutils.fs.ls(source_path)\r\n",
					"    \r\n",
					"    for entry in entries:\r\n",
					"        # Check if the entry is a directory\r\n",
					"        if entry.isDir:\r\n",
					"            # Recursively process the directory\r\n",
					"            files.extend(get_all_files_recursive(entry.path))\r\n",
					"        else:\r\n",
					"            # It's a file, add to the list\r\n",
					"            files.append(entry.path)\r\n",
					"    \r\n",
					"    return files\r\n",
					"\r\n",
					"all_files = get_all_files_recursive(source_path)\r\n",
					"\r\n",
					"print(all_files)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"files: set = set(get_all_files_recursive(source_path))\r\n",
					"files_in_table: set = set(df.select(\"filename\").rdd.flatMap(lambda x: x).collect())\r\n",
					"missing_files = list(files - files_in_table)\r\n",
					"print(missing_files)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"This function recursively lists all files and folders in a specified path and its subdirectories, up to find latest file"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def deep_ls(path: str, maxItem_depth=1):\r\n",
					"    \"\"\"\r\n",
					"    List all files and folders in specified path and\r\n",
					"    subfolders within maItemimum recursion depth.\r\n",
					"    \"\"\"\r\n",
					"    # Itemsst all files in path\r\n",
					"    Items = mssparkutils.fs.ls(path)\r\n",
					"\r\n",
					"    # Return all files\r\n",
					"    for Item in Items:\r\n",
					"        if Item.size != 0:\r\n",
					"            yield Item\r\n",
					"\r\n",
					"    # If the maItem_depth has not been reached, start Itemssting files and folders in subdirectories\r\n",
					"    if maxItem_depth > 1:\r\n",
					"        for Item in Items:\r\n",
					"            if Item.size != 0:\r\n",
					"                continue\r\n",
					"            for sub_Item in deep_ls(Item.path, maxItem_depth - 1):\r\n",
					"                yield sub_Item"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def extract_and_filter_paths(file_info_list, filter_date):\r\n",
					"    timestamp_pattern = re.compile(r\"(\\d{4}-\\d{2}-\\d{2}T\\d{2}[:_]\\d{2}[:_]\\d{2}[.\\d]*[+-]\\d{4})\")\r\n",
					"    filter_datetime = datetime.strptime(filter_date, \"%Y-%m-%dT%H:%M:%S.%f%z\")\r\n",
					"\r\n",
					"    filtered_paths = []\r\n",
					"    for file_info in file_info_list:\r\n",
					"        match = timestamp_pattern.search(file_info.name)\r\n",
					"        if match:\r\n",
					"            timestamp_str = match.group(1).replace('_', ':')\r\n",
					"            file_datetime = datetime.strptime(timestamp_str, \"%Y-%m-%dT%H:%M:%S.%f%z\")\r\n",
					"            if file_datetime > filter_datetime:\r\n",
					"                filtered_paths.append(file_info.path)\r\n",
					"\r\n",
					"    return filtered_paths\r\n",
					"\r\n",
					"# Usage\r\n",
					"filtered_paths = extract_and_filter_paths(list(deep_ls(source_path, 2)), max_extracted_datetime)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"filtered_paths"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run \"service-bus/py_spark_df_ingestion_functions\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Reading raw data and adding standardised master columns to the dataframe"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def read_raw_messages(filtered_paths: List[str]) -> DataFrame:\n",
					"    try:\n",
					"        # Read JSON files from filtered paths\n",
					"        df = spark.read.json(filtered_paths, schema=spark_schema)\n",
					"        print(f\"Found {df.count()} new rows.\")\n",
					"\n",
					"\n",
					"        # Adding the standardised columns\n",
					"        df = df.withColumn(\"expected_from\", current_timestamp())\n",
					"        df = df.withColumn(\"expected_to\", expr(\"current_timestamp() + INTERVAL 1 DAY\"))\n",
					"        df = df.withColumn(\"ingested_datetime\", to_timestamp(df.message_enqueued_time_utc))\n",
					"        df = df.withColumn(\"filename\",input_file_name())\n",
					"        ## array(*path_literals))\n",
					"\n",
					"    except Exception as e:\n",
					"        print('Error reading raw messages:', str(e))\n",
					"        mssparkutils.notebook.exit('')\n",
					"\n",
					"    return df\n",
					"\n",
					"# Usage\n",
					"filtered_paths = extract_and_filter_paths(list(deep_ls(source_path, 2)), max_extracted_datetime)\n",
					"print(filtered_paths)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df: DataFrame = read_raw_messages(filtered_paths)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Comparing and altering the table's schema based on the current data's schema"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"logInfo(f\"Comparing and merging schema for {table_name}\")\n",
					"compare_and_merge_schema(df, table_name)\n",
					"logInfo(f\"Done comparing and merging schema for {table_name}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Appending the new dataframe into the existing dataframe and removing duplicates"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\n",
					"def create_dataframe_to_load(df: DataFrame, table_name: str) -> DataFrame:\n",
					"    table_df: DataFrame = spark.table(table_name)\n",
					"    df: DataFrame = df.select(table_df.columns)\n",
					"    table_df: DataFrame = table_df.union(df)\n",
					"\n",
					"    # removing duplicates while ignoring the ingestion dates columns\n",
					"    columns_to_ignore: list = ['expected_to', 'expected_from', 'ingested_datetime']\n",
					"    columns_to_consider: list = [c for c in table_df.columns if c not in columns_to_ignore]\n",
					"    table_df: DataFrame = table_df.dropDuplicates(subset=columns_to_consider)\n",
					"\n",
					"    return table_df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"logInfo(f\"Applying DF to {table_name}\")\n",
					"table_df: DataFrame = create_dataframe_to_load(df, table_name)\n",
					"apply_df_to_table(table_df, table_name.split('.')[0], table_name.split('.')[1])\n",
					"logInfo(f\"Done applying DF to {table_name}\")"
				],
				"execution_count": null
			}
		]
	}
}