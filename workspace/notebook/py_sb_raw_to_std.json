{
	"name": "py_sb_raw_to_std",
	"properties": {
		"folder": {
			"name": "service-bus"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "d0cb2886-338d-41e5-9afe-56a08e8cd1cf"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this pyspark notebook is to read json format files for service bus and appends into single owb_standarsied_db Delta tables.\r\n",
					"\r\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \r\n",
					"Rohit Shukla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;15-Jul-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The functionality of this notebook is to ingest odw-standardised/ServiceBus Delta Tables after &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;successful reading of json formatted data.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\r\n",
					"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\r\n",
					"Rohit Shukla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;15-Jul-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The addtitional functionality has been added to log the audit information to Application Insight by &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;creating a Json dump at notebook exist&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\r\n",
					"\r\n",
					"**Spark Cluster Configuration** -> Apache Spark Version- 3.4, Python Version \t\t- 3.10, Delta Lake Version \t- 2.4\r\n",
					"\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"entity_name='appeal-has'\r\n",
					"date_folder=''"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"from notebookutils import mssparkutils\r\n",
					"import json\r\n",
					"from datetime import datetime, date\r\n",
					"from pyspark.sql.functions import current_timestamp, expr, to_timestamp, lit, input_file_name\r\n",
					"from pyspark.sql import DataFrame\r\n",
					"from pyspark.sql.types import StructType,TimestampType\r\n",
					"from pyspark.sql.functions import *\r\n",
					"import re"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark: SparkSession = SparkSession.builder.getOrCreate()\r\n",
					"storage_account: str = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\r\n",
					"table_name: str = f\"odw_standardised_db.sb_{entity_name.replace('-', '_')}\"\r\n",
					"date_folder = datetime.now().date().strftime('%Y-%m-%d') if date_folder == '' else date_folder\r\n",
					"source_path: str = f\"abfss://odw-raw@{storage_account}ServiceBus/{entity_name}/\"\r\n",
					"schema = mssparkutils.notebook.run(\"/py_create_spark_schema\", 30, {\"db_name\": 'odw_standardised_db', \"entity_name\": entity_name})\r\n",
					"spark_schema = StructType.fromJson(json.loads(schema)) if schema != '' else ''"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run service-bus/py_spark_df_ingestion_functions"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialize and define all functions required for Application Insight logging"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Initialize json processing results\r\n",
					"processing_results = {\r\n",
					"    \"processing_summary\":{\r\n",
					"    \"total_tables_processed\": 0,\r\n",
					"    \"successful_tables\": 0,\r\n",
					"    \"failed_tables\": 0\r\n",
					"    },\r\n",
					"    \"table_details\": []\r\n",
					"}\r\n",
					"\r\n",
					"@logging_to_appins\r\n",
					"def time_diff_seconds(start, end):\r\n",
					"    try:\r\n",
					"        if not start or not end:\r\n",
					"            return 0\r\n",
					"\r\n",
					"        # Parse strings into datetime objects if needed\r\n",
					"        if isinstance(start, str):\r\n",
					"            start = datetime.strptime(start, \"%Y-%m-%d %H:%M:%S.%f\")\r\n",
					"        if isinstance(end, str):\r\n",
					"            end = datetime.strptime(end, \"%Y-%m-%d %H:%M:%S.%f\")\r\n",
					"\r\n",
					"        diff_seconds = int((end - start).total_seconds())\r\n",
					"        return diff_seconds if diff_seconds > 0 else 0\r\n",
					"\r\n",
					"    except Exception as e:\r\n",
					"        return 0\r\n",
					"\r\n",
					"#This funtion handles datetime object and covert into string\r\n",
					"def datetime_handler(obj):\r\n",
					"    if isinstance(obj, datetime):\r\n",
					"        return obj.isoformat()\r\n",
					"    raise TypeError(f\"Object of type {obj.__class__.__name__} is not JSON serializable\")\r\n",
					"\r\n",
					"@logging_to_appins\r\n",
					"def add_table_result(delta_table_name, insert_count=0, table_result=\"success\",\r\n",
					"start_exec_time=\"\",end_exec_time=\"\",total_exec_time=\"\", error_message=\"\"):\r\n",
					"    \"\"\"Add processing result for a table to the tracking structure\"\"\"\r\n",
					"    table_detail = {\r\n",
					"        \"delta_table_name\": delta_table_name,\r\n",
					"        \"insert_count\": insert_count,\r\n",
					"        \"table_result\": table_result,\r\n",
					"        \"start_exec_time\": start_exec_time,\r\n",
					"        \"end_exec_time\": end_exec_time,\r\n",
					"        \"total_exec_time\": total_exec_time,\r\n",
					"        \"error_message\": error_message\r\n",
					"    }\r\n",
					"    processing_results[\"table_details\"].append(table_detail)\r\n",
					"    processing_results[\"processing_summary\"][\"total_tables_processed\"] += 1\r\n",
					"    \r\n",
					"    if table_result == \"success\":\r\n",
					"        processing_results[\"processing_summary\"][\"successful_tables\"] += 1\r\n",
					"    else:\r\n",
					"        processing_results[\"processing_summary\"][\"failed_tables\"] += 1"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\r\n",
					"def get_max_value_from_column(table_name: str, column_name: str) -> datetime:\r\n",
					"    \"\"\"\r\n",
					"    Gets the maximum value from a given table column\r\n",
					"\r\n",
					"    Args:\r\n",
					"        table_name: the name of the table, e.g. sb_appeal_has\r\n",
					"        column_name: the name of the table column, e.g. ingested_datetime\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        max_value: the maximum value from the table column\r\n",
					"    \"\"\"\r\n",
					"    df = spark.table(table_name)\r\n",
					"    max_value = df.agg(max(column_name)).collect()[0][0]\r\n",
					"\r\n",
					"    return max_value"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"@logging_to_appins\r\n",
					"def get_max_file_date(df: DataFrame) -> datetime:\r\n",
					"    \"\"\"\r\n",
					"    Gets the maximum date from a file path field in a DataFrame.\r\n",
					"    E.g. if the input_file field contained paths such as this:\r\n",
					"    abfss://odw-raw@pinsstodwdevuks9h80mb.dfs.core.windows.net/ServiceBus/appeal-has/2024-12-02/appeal-has_2024-12-02T16:54:35.214679+0000.json\r\n",
					"    It extracts the date from the string for each row and gets the maximum date.\r\n",
					"\r\n",
					"    Args:\r\n",
					"        df: a spark DataFrame\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        formatted_timestamp: a string of the maximum file date\r\n",
					"    \"\"\"\r\n",
					"    date_pattern: str = r'(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{6}\\+\\d{4})'\r\n",
					"    df: DataFrame = df.withColumn(\"file_date\", regexp_extract(df[\"input_file\"], date_pattern, 1))\r\n",
					"    df: DataFrame = df.withColumn(\"file_date\", df[\"file_date\"].cast(TimestampType()))\r\n",
					"    max_timestamp: list = df.agg(max(\"file_date\")).collect()[0][0]\r\n",
					"    formatted_timestamp: str = max_timestamp.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")+\"+0000\"\r\n",
					"    return formatted_timestamp"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\r\n",
					"def get_all_files_recursive(source_path: str) -> list:\r\n",
					"    \"\"\"\r\n",
					"    Lists all files in a given source path.\r\n",
					"    Recursively loops through all directories.\r\n",
					"\r\n",
					"    Args:\r\n",
					"        source_path: the folder path to start from, \r\n",
					"        e.g. abfss://odw-raw@pinsstodwdevuks9h80mb.dfs.core.windows.net/ServiceBus/appeal-has/\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        files: a list of files in the source path.\r\n",
					"    \"\"\"\r\n",
					"    files = []\r\n",
					"    entries = mssparkutils.fs.ls(source_path)\r\n",
					"    \r\n",
					"    for entry in entries:\r\n",
					"        # Check if the entry is a directory\r\n",
					"        if entry.isDir:\r\n",
					"            # Recursively process the directory\r\n",
					"            files.extend(get_all_files_recursive(entry.path))\r\n",
					"        else:\r\n",
					"            # If it's a file, add to the list\r\n",
					"            files.append(entry.path)\r\n",
					"    \r\n",
					"    return files"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"@logging_to_appins\r\n",
					"def get_missing_files(table_name: str, source_path: str) -> list:\r\n",
					"    \"\"\"\r\n",
					"    Gets the difference between the files in the source path and the files in the table.\r\n",
					"    Converts the table column \"filename\" into a set.\r\n",
					"    Creates a set containing all the files int he source path.\r\n",
					"    Compares the two sets to give the missing files not yet loaded to the table.\r\n",
					"\r\n",
					"    Args:\r\n",
					"        table_name: the name of the table, e.g. sb_appeal_has\r\n",
					"        source_path: the folder path to start from, \r\n",
					"        e.g. abfss://odw-raw@pinsstodwdevuks9h80mb.dfs.core.windows.net/ServiceBus/appeal-has/\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        missing_files: a list of missing files not yet loaded to the table.\r\n",
					"    \"\"\"\r\n",
					"    df: DataFrame = spark.table(table_name)\r\n",
					"    files_in_path: set = set(get_all_files_recursive(source_path))\r\n",
					"    files_in_table: set = set(df.select(\"input_file\").rdd.flatMap(lambda x: x).collect())\r\n",
					"    missing_files = list(files_in_path - files_in_table)\r\n",
					"\r\n",
					"    return missing_files"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\r\n",
					"def extract_and_filter_paths(files: list, filter_date: str):\r\n",
					"    \"\"\"\r\n",
					"    Takes a list of file paths and filters them to return the file paths greater than the filter_date\r\n",
					"    \r\n",
					"    Args:\r\n",
					"        files: a list of file paths\r\n",
					"        filter_date: a date to filter on\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        filtered_paths: a list of file paths greater than a given date\r\n",
					"    \"\"\"\r\n",
					"    timestamp_pattern: str = re.compile(r\"(\\d{4}-\\d{2}-\\d{2}T\\d{2}[:_]\\d{2}[:_]\\d{2}[.\\d]*[+-]\\d{4})\")\r\n",
					"    filter_datetime: datetime = datetime.strptime(filter_date, \"%Y-%m-%dT%H:%M:%S.%f%z\")\r\n",
					"    filtered_paths: list = []\r\n",
					"\r\n",
					"    for file in files:\r\n",
					"        match = timestamp_pattern.search(file)\r\n",
					"        if match:\r\n",
					"            timestamp_str = match.group(1).replace('_', ':')\r\n",
					"            file_datetime = datetime.strptime(timestamp_str, \"%Y-%m-%dT%H:%M:%S.%f%z\")\r\n",
					"            if file_datetime > filter_datetime:\r\n",
					"                filtered_paths.append(file)\r\n",
					"\r\n",
					"    return filtered_paths"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\r\n",
					"def read_raw_messages(filtered_paths: list[str]) -> DataFrame:\r\n",
					"    \"\"\"\r\n",
					"    Ingests data from service bus messages stored as json files in the raw layer\r\n",
					"\r\n",
					"    Args:\r\n",
					"        filtered_paths: a list of file paths to ingest\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        A DataFrame of service bus messages with additional columns needed for the standardised table\r\n",
					"    \"\"\"\r\n",
					"    try:\r\n",
					"        # Read JSON files from filtered paths\r\n",
					"        df = spark.read.json(filtered_paths, schema=spark_schema)\r\n",
					"        logInfo(f\"Found {df.count()} new rows.\")\r\n",
					"        # Adding the standardised columns\r\n",
					"        df = df.withColumn(\"expected_from\", current_timestamp())\r\n",
					"        df = df.withColumn(\"expected_to\", expr(\"current_timestamp() + INTERVAL 1 DAY\"))\r\n",
					"        df = df.withColumn(\"ingested_datetime\", to_timestamp(df.message_enqueued_time_utc))\r\n",
					"        df = df.withColumn(\"input_file\", input_file_name())\r\n",
					"\r\n",
					"    except Exception as e:\r\n",
					"        logError(f'Raw data not found at {source_path}, Exception is {e}')\r\n",
					"        mssparkutils.notebook.exit('')\r\n",
					"\r\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\r\n",
					"def dedupe_dataframe(df: DataFrame) -> DataFrame:\r\n",
					"    \"\"\"\r\n",
					"    Dedupes a DataFrame based on certain columns\r\n",
					"\r\n",
					"    Args:\r\n",
					"        df: a DataFrame of service bus data\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        a deduped DataFrame\r\n",
					"    \"\"\"\r\n",
					"    # removing duplicates while ignoring the ingestion dates columns\r\n",
					"    columns_to_ignore: list = ['expected_to', 'expected_from', 'ingested_datetime']\r\n",
					"    columns_to_consider: list = [c for c in df.columns if c not in columns_to_ignore]\r\n",
					"    df: DataFrame = df.dropDuplicates(subset=columns_to_consider)\r\n",
					"    \r\n",
					"    return df\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\r\n",
					"def append_df_to_table(df: DataFrame, table_name: str) -> None:\r\n",
					"    \"\"\"\r\n",
					"    Appends the new rows to the target table\r\n",
					"\r\n",
					"    Args:\r\n",
					"        df: DataFrame of new rows\r\n",
					"    \"\"\"\r\n",
					"    df \\\r\n",
					"    .write \\\r\n",
					"    .mode(\"append\") \\\r\n",
					"    .format(\"delta\") \\\r\n",
					"    .option(\"mergeSchema\", \"true\") \\\r\n",
					"    .saveAsTable(table_name)\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\r\n",
					"def test_rows_appended(new_table_row_count: int, expected_row_count: int) -> bool:\r\n",
					"    \"\"\"\r\n",
					"    Test if the new row count matches the expected row count.\r\n",
					"    If True then rows have been appended successfully.\r\n",
					"\r\n",
					"    Args:\r\n",
					"        new_table_row_count: count of rows after the append operation\r\n",
					"        expected_row_count: count of rows we expect after the append operation\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        True if the counts match\r\n",
					"    \"\"\"\r\n",
					"    return new_table_row_count == expected_new_count"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Get the filtered list of files we want to ingest.  \r\n",
					"\r\n",
					"If `USE_MAX_DATE_FILTER` is set to `True` then we get the maximum date of the ingested files from the target table. We use this date to filter the json files in the raw storage layer to only ingest files greater than this date.  \r\n",
					"\r\n",
					"If `USE_MAX_DATE_FILTER` is set to `False` then we compare the files ingested already in the target table with the source files and get the difference, i.e. the missing files."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"start_exec_time = str(datetime.now())\r\n",
					"insert_count = 0\r\n",
					"\r\n",
					"USE_MAX_DATE_FILTER = False\r\n",
					"\r\n",
					"if USE_MAX_DATE_FILTER:\r\n",
					"    try:\r\n",
					"        logInfo(\"Getting the maximum file date\")\r\n",
					"        table_df = spark.table(table_name)\r\n",
					"        max_extracted_date = get_max_file_date(df=table_df)\r\n",
					"        print(f\"max_extracted_date: {max_extracted_date}\")\r\n",
					"    except Exception as e:\r\n",
					"        logError(f\"Error getting the maximum file date:\\n{e}\")\r\n",
					"\r\n",
					"    try:\r\n",
					"        logInfo(\"Getting the filtered list of paths\")\r\n",
					"        filtered_paths = extract_and_filter_paths(get_all_files_recursive(source_path=source_path), max_extracted_date)\r\n",
					"        df = read_raw_messages(filtered_paths=filtered_paths)\r\n",
					"    except Exception as e:\r\n",
					"        logError(f\"Error getting the filtered list of paths:\\n{e}\")\r\n",
					"else:\r\n",
					"    try:\r\n",
					"        logInfo(\"Getting list of missing files\")\r\n",
					"        missing_files = get_missing_files(table_name, source_path)\r\n",
					"        df = read_raw_messages(filtered_paths=missing_files)\r\n",
					"    except Exception as e:\r\n",
					"        logError(f\"Error getting list of missing files:\\n{e}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Dedupe the DataFrame and generate counts.\r\n",
					"\r\n",
					"1. Get existing row count of target table  \r\n",
					"2. Dedupe the DataFrame  \r\n",
					"3. Get count of rows to append  \r\n",
					"4. Get the expected new row count which is existing count + new rows"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"expected_new_count = 0\r\n",
					"\r\n",
					"try:\r\n",
					"    table_row_count = spark.table(table_name).count()\r\n",
					"    logInfo(f\"Row count before append: {table_row_count}\")\r\n",
					"    df = dedupe_dataframe(df=df)\r\n",
					"    rows_to_append = df.count()\r\n",
					"    insert_count = rows_to_append\r\n",
					"    logInfo(f\"Rows to append: {rows_to_append}\")\r\n",
					"    expected_new_count = table_row_count + rows_to_append\r\n",
					"    logInfo(f\"Expected new count: {expected_new_count}\")\r\n",
					"except Exception as e:\r\n",
					"    logError(f\"Error deduping and generating counts:\\n{e}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Append rows to the target table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"new_table_row_count = 0\r\n",
					"\r\n",
					"try:\r\n",
					"    logInfo(f\"Appending new rows to table {table_name}\")\r\n",
					"    append_df_to_table(df=df, table_name=table_name)\r\n",
					"    \r\n",
					"    end_exec_time = str(datetime.now())\r\n",
					"    time_diff = time_diff_seconds(start_exec_time, end_exec_time)\r\n",
					"    add_table_result(table_name, insert_count, \"success\",start_exec_time, end_exec_time,\r\n",
					"                 time_diff,\"\")\r\n",
					"\r\n",
					"    logInfo(\"Done appending rows to table\")\r\n",
					"    new_table_row_count = spark.table(table_name).count()\r\n",
					"except Exception as e:\r\n",
					"    logError(f\"Error appending rows:\\n{e}\")\r\n",
					"\r\n",
					"    error_msg = f\"Error appending rows:\\n{str(e)}\"\r\n",
					"    \r\n",
					"    end_exec_time = str(datetime.now())\r\n",
					"    time_diff = time_diff_seconds(start_exec_time, end_exec_time)\r\n",
					"    add_table_result(table_name, insert_count, \"failed\",start_exec_time, end_exec_time,\r\n",
					"                 time_diff,error_msg)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Test rows were appended successfully\r\n",
					"\r\n",
					"Test that the new row count in the target table matches the expected new row count above"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if not test_rows_appended(new_table_row_count=new_table_row_count, expected_row_count=expected_new_count):\r\n",
					"    logError(\"Error appending rows. The new row count isn't as expected.\")\r\n",
					"\r\n",
					"    error_msg = f\"Error appending rows: {table_name} . The new row count isn't as expected.\"\r\n",
					"    end_exec_time = str(datetime.now())\r\n",
					"    time_diff = time_diff_seconds(start_exec_time, end_exec_time)\r\n",
					"    add_table_result(table_name, insert_count, \"failed\",start_exec_time, end_exec_time,\r\n",
					"                 time_diff,error_msg)\r\n",
					"else:\r\n",
					"    logInfo(\"Rows appended successfully.\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Generate and display the final results\r\n",
					"final_results = generate_processing_results()\r\n",
					"\r\n",
					"# Convert to JSON string\r\n",
					"result_json_str = json.dumps(final_results, indent=2, default=datetime_handler)\r\n",
					"\r\n",
					"# Exit with the JSON result\r\n",
					"mssparkutils.notebook.exit(result_json_str)"
				],
				"execution_count": null
			}
		]
	}
}