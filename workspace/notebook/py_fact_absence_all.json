{
	"name": "py_fact_absence_all",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b6406275-a98b-4260-96a5-2ecbdb7e3be3"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Harmoised layer and build a table for Curated Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;25-02-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This Notebook is designed to facilitate Enriches data with HR attributes (department, location) to facilitate compliance reporting, cost allocation, and workforce planning by business units."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Intialisations"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load Data Into sap_hr_fact_absence_all"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"-- Step 1: Clear temporary and final tables\n",
					"DELETE FROM odw_harmonised_db.sap_hr_fact_Absence_All_TEMP;\n",
					"DELETE FROM odw_harmonised_db.sap_hr_fact_Absence_All;\n",
					"\n",
					"-- Step 2: Create a date dimension table to handle date expansion\n",
					"-- Using Spark SQL compatible syntax with explode()\n",
					"CREATE OR REPLACE TEMPORARY VIEW date_dimension AS\n",
					"WITH date_sequence AS (\n",
					"    SELECT explode(sequence(\n",
					"        (SELECT MIN(StartDate) FROM odw_harmonised_db.sap_hr_absence_All),\n",
					"        (SELECT MAX(EndDate) FROM odw_harmonised_db.sap_hr_absence_All),\n",
					"        INTERVAL 1 DAY\n",
					"    )) AS date_value\n",
					")\n",
					"SELECT \n",
					"    date_value, \n",
					"    dayofweek(date_value) AS day_of_week\n",
					"FROM date_sequence\n",
					"WHERE dayofweek(date_value) BETWEEN 2 AND 6; -- Only include weekdays (Monday to Friday)\n",
					"\n",
					"-- Step 3: Expand absence records into individual days using a join with date_dimension\n",
					"INSERT INTO odw_harmonised_db.sap_hr_fact_Absence_All_TEMP (\n",
					"    absencedate,\n",
					"    absencehours,\n",
					"    staffnumber,\n",
					"    WorkScheduleRule,\n",
					"    AbsType,\n",
					"    SicknessGroup,\n",
					"    AttendanceorAbsenceType,\n",
					"    Leave,\n",
					"    PSGroup,\n",
					"    PersonnelArea,\n",
					"    PersonnelSubarea,    \n",
					"    SourceSystemID,\n",
					"    IngestionDate,\n",
					"    ValidTo,\n",
					"    RowID,\n",
					"    IsActive\n",
					")\n",
					"SELECT DISTINCT\n",
					"    d.date_value AS absencedate,\n",
					"    CASE\n",
					"        WHEN (aa.Days BETWEEN 0.01 AND 0.99) THEN \n",
					"            CASE d.day_of_week\n",
					"                WHEN 2 THEN (ws.Mo) * (aa.Days)\n",
					"                WHEN 3 THEN (ws.Tu) * (aa.Days)\n",
					"                WHEN 4 THEN (ws.We) * (aa.Days)\n",
					"                WHEN 5 THEN (ws.Th) * (aa.Days)\n",
					"                WHEN 6 THEN (ws.Fr) * (aa.Days)\n",
					"                ELSE 0 \n",
					"            END\n",
					"        ELSE \n",
					"            CASE d.day_of_week\n",
					"                WHEN 2 THEN ws.Mo\n",
					"                WHEN 3 THEN ws.Tu\n",
					"                WHEN 4 THEN ws.We\n",
					"                WHEN 5 THEN ws.Th\n",
					"                WHEN 6 THEN ws.Fr\n",
					"                ELSE 0 \n",
					"            END \n",
					"    END AS absencehours,\n",
					"    CASE\n",
					"        WHEN TRIM(LEADING '0' FROM LPAD(StaffNumber, 8, '0')) LIKE '4%' THEN \n",
					"            '50' || TRIM(LEADING '0' FROM LPAD(StaffNumber, 8, '0'))\n",
					"        WHEN TRIM(LEADING '0' FROM LPAD(StaffNumber, 8, '0')) LIKE '5%' THEN \n",
					"            '00' || TRIM(LEADING '0' FROM LPAD(StaffNumber, 8, '0'))\n",
					"        WHEN TRIM(LEADING '0' FROM LPAD(StaffNumber, 8, '0')) LIKE '6%' THEN \n",
					"            '60' || TRIM(LEADING '0' FROM LPAD(StaffNumber, 8, '0'))\n",
					"        ELSE LPAD(StaffNumber, 7, '0')\n",
					"    END  AS staffnumber,\n",
					"    aa.WorkScheduleRule,\n",
					"    aa.AbsType,\n",
					"    aa.SicknessGroup,\n",
					"    aa.AttendanceorAbsenceType,\n",
					"    ROUND(CASE\n",
					"        WHEN (aa.AbsType = 'CB01' AND aa.Days = 0) THEN 1\n",
					"        WHEN (aa.AbsType = 'MT01' AND aa.Days = 0) THEN 1\n",
					"        WHEN (aa.AttendanceorAbsenceType = 'Spec u/p-up to 3mths' AND aa.Days = 0) THEN 1\n",
					"        WHEN aa.Caldays = 0 THEN aa.Hrs / aa.HrsDay\n",
					"        ELSE 1 \n",
					"    END, 2) AS Leave,\n",
					"    NULL AS PSGroup,\n",
					"    NULL AS PersonnelArea,\n",
					"    NULL AS PersonnelSubarea,\n",
					"    aa.SourceSystemID,\n",
					"    aa.IngestionDate,\n",
					"    aa.ValidTo,\n",
					"    aa.RowID,\n",
					"    aa.IsActive\n",
					"FROM \n",
					"    odw_harmonised_db.sap_hr_absence_All aa\n",
					"JOIN \n",
					"    date_dimension d\n",
					"    ON d.date_value BETWEEN aa.StartDate AND aa.EndDate\n",
					"LEFT JOIN \n",
					"    odw_standardised_db.work_schedules ws\n",
					"    ON aa.WorkScheduleRule = ws.WorkScheduleRule\n",
					"WHERE \n",
					"    -- Calculate working hours directly within the query\n",
					"    CASE d.day_of_week\n",
					"        WHEN 2 THEN ws.Mo\n",
					"        WHEN 3 THEN ws.Tu\n",
					"        WHEN 4 THEN ws.We\n",
					"        WHEN 5 THEN ws.Th\n",
					"        WHEN 6 THEN ws.Fr\n",
					"        ELSE 0\n",
					"    END > 0;\n",
					"\n",
					"-- Step 4: Remove duplicates and insert into final table\n",
					"WITH Duplicate_CTE AS (\n",
					"    SELECT \n",
					"        *,\n",
					"        ROW_NUMBER() OVER(\n",
					"            PARTITION BY absencedate, staffnumber \n",
					"            ORDER BY CASE WHEN AttendanceorAbsenceType = 'NA Time (Inspectors)' THEN 0 ELSE 1 END DESC\n",
					"        ) AS Rno\n",
					"    FROM \n",
					"        odw_harmonised_db.sap_hr_fact_Absence_All_TEMP\n",
					")\n",
					"INSERT INTO odw_harmonised_db.sap_hr_fact_Absence_All (\n",
					"    absencedate,\n",
					"    absencehours,\n",
					"    staffnumber,\n",
					"    WorkScheduleRule,\n",
					"    AbsType,\n",
					"    SicknessGroup,\n",
					"    AttendanceorAbsenceType,\n",
					"    Leave,\n",
					"    PSGroup,\n",
					"    PersonnelArea,\n",
					"    PersonnelSubarea,\n",
					"    SourceSystemID,\n",
					"    IngestionDate,\n",
					"    ValidTo,\n",
					"    RowID,\n",
					"    IsActive\n",
					")\n",
					"SELECT \n",
					"    cast(absencedate  as date),\n",
					"    absencehours,\n",
					"    staffnumber,\n",
					"    WorkScheduleRule,\n",
					"    AbsType,\n",
					"    SicknessGroup,\n",
					"    AttendanceorAbsenceType,\n",
					"    Leave,\n",
					"    NULL AS PSGroup,\n",
					"    NULL AS PersonnelArea,\n",
					"    NULL AS PersonnelSubarea,\n",
					"    SourceSystemID,\n",
					"    IngestionDate,\n",
					"    ValidTo,\n",
					"    RowID,\n",
					"    IsActive\n",
					"FROM \n",
					"    Duplicate_CTE\n",
					"WHERE \n",
					"    Rno = 1;\n",
					"\n",
					"-- Step 5: Delete rows for staff numbers that do not exist in SAP_HR data history\n",
					"DELETE FROM odw_harmonised_db.sap_hr_fact_Absence_All\n",
					"WHERE staffnumber IN ('50410587', '50422294');\n",
					"\n",
					"-- Step 6: Create a staging table with best HR data matches\n",
					"CREATE OR REPLACE TEMPORARY VIEW hr_data_staging AS\n",
					"SELECT \n",
					"    a.staffnumber,\n",
					"    a.absencedate,\n",
					"    -- For each absence record, find the most recent HR data\n",
					"    FIRST_VALUE(h.PSGroup) OVER (\n",
					"        PARTITION BY a.staffnumber, a.absencedate\n",
					"        ORDER BY \n",
					"            -- First prioritize records where the absence date falls within the reporting period\n",
					"            CASE WHEN a.absencedate BETWEEN DATE_ADD(DATE_ADD(h.Report_MonthEnd_Date, 1), -30) AND h.Report_MonthEnd_Date THEN 0 ELSE 1 END,\n",
					"            -- Then get the most recent HR record\n",
					"            h.Report_MonthEnd_Date DESC\n",
					"    ) AS PSGroup,\n",
					"    FIRST_VALUE(h.PersonnelArea) OVER (\n",
					"        PARTITION BY a.staffnumber, a.absencedate\n",
					"        ORDER BY \n",
					"            CASE WHEN a.absencedate BETWEEN DATE_ADD(DATE_ADD(h.Report_MonthEnd_Date, 1), -30) AND h.Report_MonthEnd_Date THEN 0 ELSE 1 END,\n",
					"            h.Report_MonthEnd_Date DESC\n",
					"    ) AS PersonnelArea,\n",
					"    FIRST_VALUE(h.PersonnelSubarea) OVER (\n",
					"        PARTITION BY a.staffnumber, a.absencedate\n",
					"        ORDER BY \n",
					"            CASE WHEN a.absencedate BETWEEN DATE_ADD(DATE_ADD(h.Report_MonthEnd_Date, 1), -30) AND h.Report_MonthEnd_Date THEN 0 ELSE 1 END,\n",
					"            h.Report_MonthEnd_Date DESC\n",
					"    ) AS PersonnelSubarea\n",
					"FROM \n",
					"    odw_harmonised_db.sap_hr_fact_Absence_All a\n",
					"JOIN \n",
					"    odw_harmonised_db.Hist_SAP_HR h\n",
					"ON \n",
					"    a.staffnumber = h.PersNo;\n",
					"\n",
					"-- Step 7: Deduplicate the staging data to ensure one row per staffnumber/absencedate\n",
					"CREATE OR REPLACE TEMPORARY VIEW hr_data_final AS\n",
					"SELECT \n",
					"    staffnumber,\n",
					"    absencedate,\n",
					"    PSGroup,\n",
					"    PersonnelArea,\n",
					"    PersonnelSubarea\n",
					"FROM (\n",
					"    SELECT \n",
					"        *,\n",
					"        ROW_NUMBER() OVER (PARTITION BY staffnumber, absencedate ORDER BY absencedate) AS rn\n",
					"    FROM \n",
					"        hr_data_staging\n",
					") t\n",
					"WHERE \n",
					"    rn = 1;\n",
					"\n",
					"-- Step 8: Use MERGE to update the target table\n",
					"MERGE INTO odw_harmonised_db.sap_hr_fact_Absence_All a\n",
					"USING hr_data_final s\n",
					"ON a.staffnumber = s.staffnumber AND a.absencedate = s.absencedate\n",
					"WHEN MATCHED THEN\n",
					"UPDATE SET\n",
					"    PSGroup = s.PSGroup,\n",
					"    PersonnelArea = s.PersonnelArea,\n",
					"    PersonnelSubarea = s.PersonnelSubarea;\n",
					"    \n",
					""
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import col, to_date\n",
					"\n",
					"# verify that the holiday dates are properly formatted\n",
					"# and that May 30, 2022 exists in the holidays table\n",
					"\n",
					"\n",
					"try:\n",
					"    # Create the filtered view\n",
					"    spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW absences_without_holidays AS\n",
					"        SELECT a.*\n",
					"        FROM odw_harmonised_db.sap_hr_fact_absence_all a\n",
					"        LEFT JOIN (\n",
					"            SELECT DISTINCT to_date(HolidayDate, 'dd/MM/yyyy') AS holiday_date\n",
					"            FROM odw_standardised_db.Live_Holidays\n",
					"        ) h ON to_date(a.absencedate) = h.holiday_date\n",
					"        WHERE h.holiday_date IS NULL\n",
					"    \"\"\")\n",
					"    \n",
					"    # Get the counts\n",
					"    original_count = spark.sql(\"SELECT COUNT(*) FROM odw_harmonised_db.sap_hr_fact_absence_all\").collect()[0][0]\n",
					"    filtered_count = spark.sql(\"SELECT COUNT(*) FROM absences_without_holidays\").collect()[0][0]\n",
					"    \n",
					"    print(f\"Original count: {original_count}\")\n",
					"    print(f\"After holiday filtering: {filtered_count}\")\n",
					"    print(f\"Records to be removed: {original_count - filtered_count}\")\n",
					"    \n",
					"    # Method 1: Try using Delta Lake operations\n",
					"    try:\n",
					"        from delta.tables import DeltaTable\n",
					"        \n",
					"        # Get the list of holiday dates\n",
					"        holiday_dates = spark.sql(\"\"\"\n",
					"            SELECT DISTINCT to_date(HolidayDate, 'dd/MM/yyyy') AS holiday_date\n",
					"            FROM odw_standardised_db.Live_Holidays\n",
					"        \"\"\").collect()\n",
					"        \n",
					"        absence_delta = DeltaTable.forName(spark, \"odw_harmonised_db.sap_hr_fact_absence_all\")\n",
					"        \n",
					"        # Process each holiday date individually to avoid syntax errors\n",
					"        for row in holiday_dates:\n",
					"            holiday = row.holiday_date.strftime('%Y-%m-%d')\n",
					"            absence_delta.delete(f\"to_date(absencedate) = '{holiday}'\")\n",
					"            print(f\"Deleted records for holiday: {holiday}\")\n",
					"        \n",
					"        # Verify final count\n",
					"        final_count = spark.sql(\"SELECT COUNT(*) FROM odw_harmonised_db.sap_hr_fact_absence_all\").collect()[0][0]\n",
					"        print(f\"Final count after deletion: {final_count}\")\n",
					"        print(f\"Total records removed: {original_count - final_count}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"Delta operations failed: {e}\")\n",
					"        print(\"Trying alternative method...\")\n",
					"        \n",
					"        # Method 2: Save to temporary table and replace\n",
					"        try:\n",
					"            # First, save the filtered data to a temporary location\n",
					"            filtered_df = spark.sql(\"SELECT * FROM absences_without_holidays\")\n",
					"            temp_path = \"/tmp/filtered_absences\"\n",
					"            \n",
					"            filtered_df.write.format(\"delta\").mode(\"overwrite\").save(temp_path)\n",
					"            \n",
					"            # Then truncate and reload\n",
					"            spark.sql(\"DELETE FROM odw_harmonised_db.sap_hr_fact_absence_all\")\n",
					"            \n",
					"            # Read from temp location and insert\n",
					"            temp_df = spark.read.format(\"delta\").load(temp_path)\n",
					"            temp_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"odw_harmonised_db.sap_hr_fact_absence_all\")\n",
					"            \n",
					"            # Verify final count\n",
					"            final_count = spark.sql(\"SELECT COUNT(*) FROM odw_harmonised_db.sap_hr_fact_absence_all\").collect()[0][0]\n",
					"            print(f\"Final count after replacement: {final_count}\")\n",
					"            print(f\"Total records removed: {original_count - final_count}\")\n",
					"            \n",
					"        except Exception as e2:\n",
					"            print(f\"Alternative method failed: {e2}\")\n",
					"            print(\"Trying simplest approach...\")\n",
					"            \n",
					"            # Method 3: INSERT OVERWRITE approach\n",
					"            try:\n",
					"                # Use INSERT OVERWRITE instead of TRUNCATE + INSERT\n",
					"                spark.sql(\"\"\"\n",
					"                    INSERT OVERWRITE TABLE odw_harmonised_db.sap_hr_fact_absence_all\n",
					"                    SELECT * FROM absences_without_holidays\n",
					"                \"\"\")\n",
					"                \n",
					"                # Verify final count\n",
					"                final_count = spark.sql(\"SELECT COUNT(*) FROM odw_harmonised_db.sap_hr_fact_absence_all\").collect()[0][0]\n",
					"                print(f\"Final count after INSERT OVERWRITE: {final_count}\")\n",
					"                print(f\"Total records removed: {original_count - final_count}\")\n",
					"                \n",
					"            except Exception as e3:\n",
					"                print(f\"All methods failed. Final error: {e3}\")\n",
					"                print(\"Please contact your database administrator for assistance.\")\n",
					"    \n",
					"except Exception as e:\n",
					"    print(f\"Error in initial setup: {e}\")"
				],
				"execution_count": null
			}
		]
	}
}