{
	"name": "py_fact_absence_all",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "3ece2874-4796-41ac-b2c5-ec562aff0f7c"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Harmoised layer and build a table for Curated Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;25-02-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This Notebook is designed to facilitate Enriches data with HR attributes (department, location) to facilitate compliance reporting, cost allocation, and workforce planning by business units."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Intialisations"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from odw.core.util.util import Util\n",
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from odw.core.util.logging_util import LoggingUtil"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"#Get Storage account name\n",
					"storage_account=Util.get_storage_account()\n",
					"print(storage_account)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"source_database_name = \"odw_standardised_db\"\n",
					"target_database_name = \"odw_harmonised_db\"\n",
					"#source_delta_table = f\"{source_database_name}.inspector_addresses\"\n",
					"target_delta_table = f\"{target_database_name}.sap_hr_fact_Absence_All\"\n",
					"delta_table_path = f\"abfss://odw-harmonised@{storage_account}saphr/sap_hr_fact_Absence_All\"\n",
					"delta_table_path = f\"abfss://odw-harmonised@{storage_account}saphr/sap_hr_fact_Absence_All_TEMP\"\n",
					"\n",
					"\n",
					"\n",
					"#print(source_delta_table)\n",
					"print(target_delta_table)\n",
					"print(delta_table_path)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load Data Into sap_hr_fact_absence_all"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"   # Set ANSI mode to false to handle potential type conversion issues\n",
					"   LoggingUtil().log_info(\"Setting ANSI mode to false\")\n",
					"   spark.sql(\"SET spark.sql.ansi.enabled=false\")\n",
					"   \n",
					"   # Step 1: Clear temporary and final tables\n",
					"   LoggingUtil().log_info(\"Clearing temporary table\")\n",
					"   spark.sql(\"\"\"\n",
					"   DELETE FROM odw_harmonised_db.sap_hr_fact_Absence_All_TEMP\n",
					"   \"\"\")\n",
					"   \n",
					"   LoggingUtil().log_info(\"Clearing final table\")\n",
					"   spark.sql(\"\"\"\n",
					"   DELETE FROM odw_harmonised_db.sap_hr_fact_Absence_All\n",
					"   \"\"\")\n",
					"   \n",
					"   # Step 2: Create a date dimension table to handle date expansion with 2-week cycle logic\n",
					"   LoggingUtil().log_info(\"Creating date dimension table with 2-week cycle logic\")\n",
					"   spark.sql(\"\"\"\n",
					"   CREATE OR REPLACE TEMPORARY VIEW date_dimension AS\n",
					"   WITH date_sequence AS (\n",
					"       SELECT explode(sequence(\n",
					"           (SELECT MIN(StartDate) FROM odw_harmonised_db.sap_hr_absence_All),\n",
					"           (SELECT MAX(EndDate) FROM odw_harmonised_db.sap_hr_absence_All),\n",
					"           INTERVAL 1 DAY\n",
					"       )) AS date_value\n",
					"   )\n",
					"   SELECT \n",
					"       date_value, \n",
					"       dayofweek(date_value) AS day_of_week,\n",
					"       -- Calculate which week of the 2-week cycle this date falls into\n",
					"       -- Using a reference date (e.g., '2024-01-01' as Week 1 start)\n",
					"       -- You may need to adjust this reference date based on your business rules\n",
					"       CASE \n",
					"           WHEN ((datediff(date_value, '2024-01-01') / 7) % 2) = 0 THEN 1\n",
					"           ELSE 2\n",
					"       END AS week_in_cycle\n",
					"   FROM date_sequence\n",
					"   WHERE dayofweek(date_value) BETWEEN 2 AND 6  -- Only include weekdays (Monday to Friday)\n",
					"   \"\"\")\n",
					"   LoggingUtil().log_info(\"Date dimension table created with 2-week cycle logic\")\n",
					"   \n",
					"   # Step 3: Expand absence records into individual days using a join with date_dimension\n",
					"   LoggingUtil().log_info(\"Expanding absence records into individual days with 2-week schedule\")\n",
					"   spark.sql(\"\"\"\n",
					"   INSERT INTO odw_harmonised_db.sap_hr_fact_Absence_All_TEMP (\n",
					"       absencedate,\n",
					"       absencehours,\n",
					"       staffnumber,\n",
					"       WorkScheduleRule,\n",
					"       AbsType,\n",
					"       SicknessGroup,\n",
					"       AttendanceorAbsenceType,\n",
					"       Leave,\n",
					"       PSGroup,\n",
					"       PersonnelArea,\n",
					"       PersonnelSubarea,    \n",
					"       SourceSystemID,\n",
					"       IngestionDate,\n",
					"       ValidTo,\n",
					"       RowID,\n",
					"       IsActive\n",
					"   )\n",
					"   SELECT DISTINCT\n",
					"       CAST(d.date_value AS DATE) AS absencedate,\n",
					"       ROUND(CASE\n",
					"           WHEN (CAST(aa.Days AS DOUBLE) BETWEEN 0.01 AND 0.99) THEN \n",
					"               CASE \n",
					"                   WHEN d.week_in_cycle = 1 THEN\n",
					"                       CASE d.day_of_week\n",
					"                           WHEN 2 THEN CAST(ws.MoWk1 AS DOUBLE) * CAST(aa.Days AS DOUBLE)\n",
					"                           WHEN 3 THEN CAST(ws.TuWk1 AS DOUBLE) * CAST(aa.Days AS DOUBLE)\n",
					"                           WHEN 4 THEN CAST(ws.WeWk1 AS DOUBLE) * CAST(aa.Days AS DOUBLE)\n",
					"                           WHEN 5 THEN CAST(ws.ThWk1 AS DOUBLE) * CAST(aa.Days AS DOUBLE)\n",
					"                           WHEN 6 THEN CAST(ws.FrWk1 AS DOUBLE) * CAST(aa.Days AS DOUBLE)\n",
					"                           ELSE 0 \n",
					"                       END\n",
					"                   ELSE -- week_in_cycle = 2\n",
					"                       CASE d.day_of_week\n",
					"                           WHEN 2 THEN CAST(ws.MoWk2 AS DOUBLE) * CAST(aa.Days AS DOUBLE)\n",
					"                           WHEN 3 THEN CAST(ws.TuWk2 AS DOUBLE) * CAST(aa.Days AS DOUBLE)\n",
					"                           WHEN 4 THEN CAST(ws.WeWk2 AS DOUBLE) * CAST(aa.Days AS DOUBLE)\n",
					"                           WHEN 5 THEN CAST(ws.ThWk2 AS DOUBLE) * CAST(aa.Days AS DOUBLE)\n",
					"                           WHEN 6 THEN CAST(ws.FrWk2 AS DOUBLE) * CAST(aa.Days AS DOUBLE)\n",
					"                           ELSE 0 \n",
					"                       END\n",
					"               END\n",
					"           ELSE \n",
					"               CASE \n",
					"                   WHEN d.week_in_cycle = 1 THEN\n",
					"                       CASE d.day_of_week\n",
					"                           WHEN 2 THEN CAST(ws.MoWk1 AS DOUBLE)\n",
					"                           WHEN 3 THEN CAST(ws.TuWk1 AS DOUBLE)\n",
					"                           WHEN 4 THEN CAST(ws.WeWk1 AS DOUBLE)\n",
					"                           WHEN 5 THEN CAST(ws.ThWk1 AS DOUBLE)\n",
					"                           WHEN 6 THEN CAST(ws.FrWk1 AS DOUBLE)\n",
					"                           ELSE 0 \n",
					"                       END\n",
					"                   ELSE -- week_in_cycle = 2\n",
					"                       CASE d.day_of_week\n",
					"                           WHEN 2 THEN CAST(ws.MoWk2 AS DOUBLE)\n",
					"                           WHEN 3 THEN CAST(ws.TuWk2 AS DOUBLE)\n",
					"                           WHEN 4 THEN CAST(ws.WeWk2 AS DOUBLE)\n",
					"                           WHEN 5 THEN CAST(ws.ThWk2 AS DOUBLE)\n",
					"                           WHEN 6 THEN CAST(ws.FrWk2 AS DOUBLE)\n",
					"                           ELSE 0 \n",
					"                       END\n",
					"               END \n",
					"       END, 2) AS absencehours,\n",
					"       CASE\n",
					"           WHEN lpad(CAST(aa.StaffNumber AS STRING), 8, '0') LIKE '004%' THEN \n",
					"               '50' || substring(lpad(CAST(aa.StaffNumber AS STRING), 8, '0'), 3)\n",
					"           WHEN lpad(CAST(aa.StaffNumber AS STRING), 8, '0') LIKE '006%' THEN \n",
					"               '60' || substring(lpad(CAST(aa.StaffNumber AS STRING), 8, '0'), 3)\n",
					"           WHEN lpad(CAST(aa.StaffNumber AS STRING), 8, '0') LIKE '5%' OR\n",
					"                lpad(CAST(aa.StaffNumber AS STRING), 8, '0') LIKE '005%' THEN\n",
					"               lpad(CAST(aa.StaffNumber AS STRING), 8, '0') \n",
					"           ELSE \n",
					"               lpad(CAST(aa.StaffNumber AS STRING), 8, '0')\n",
					"       END AS staffnumber,\n",
					"       aa.WorkScheduleRule,\n",
					"       aa.AbsType,\n",
					"       aa.SicknessGroup,\n",
					"       aa.AttendanceorAbsenceType,\n",
					"       ROUND(CASE\n",
					"           WHEN (aa.AbsType = 'CB01' AND CAST(aa.Days AS DOUBLE) = 0) THEN 1\n",
					"           WHEN (aa.AbsType = 'MT01' AND CAST(aa.Days AS DOUBLE) = 0) THEN 1\n",
					"           WHEN (aa.AttendanceorAbsenceType = 'Spec u/p-up to 3mths' AND CAST(aa.Days AS DOUBLE) = 0) THEN 1\n",
					"           WHEN CAST(aa.Caldays AS DOUBLE) = 0 THEN CAST(aa.Hrs AS DOUBLE) / CAST(aa.HrsDay AS DOUBLE)\n",
					"           ELSE 1 \n",
					"       END, 2) as Leave,\n",
					"       NULL AS PSGroup,\n",
					"       NULL AS PersonnelArea,\n",
					"       NULL AS PersonnelSubarea,\n",
					"       aa.SourceSystemID,\n",
					"       aa.IngestionDate,\n",
					"       aa.ValidTo,\n",
					"       aa.RowID,\n",
					"       aa.IsActive\n",
					"   FROM \n",
					"       odw_harmonised_db.sap_hr_absence_All aa\n",
					"   JOIN \n",
					"       date_dimension d\n",
					"       ON d.date_value BETWEEN CAST(aa.StartDate AS DATE) AND CAST(aa.EndDate AS DATE)\n",
					"   LEFT JOIN \n",
					"       odw_harmonised_db.sap_hr_workschedulerule ws\n",
					"       ON REPLACE(LOWER(aa.WorkScheduleRule), ' ', '') = REPLACE(LOWER(ws.WorkScheduleRule), ' ', '')\n",
					"   WHERE \n",
					"       -- Calculate working hours directly within the query for 2-week cycle\n",
					"       COALESCE(\n",
					"           CASE \n",
					"               WHEN d.week_in_cycle = 1 THEN\n",
					"                   CASE d.day_of_week\n",
					"                       WHEN 2 THEN CAST(ws.MoWk1 AS DOUBLE)\n",
					"                       WHEN 3 THEN CAST(ws.TuWk1 AS DOUBLE)\n",
					"                       WHEN 4 THEN CAST(ws.WeWk1 AS DOUBLE)\n",
					"                       WHEN 5 THEN CAST(ws.ThWk1 AS DOUBLE)\n",
					"                       WHEN 6 THEN CAST(ws.FrWk1 AS DOUBLE)\n",
					"                       ELSE 0\n",
					"                   END\n",
					"               ELSE -- week_in_cycle = 2\n",
					"                   CASE d.day_of_week\n",
					"                       WHEN 2 THEN CAST(ws.MoWk2 AS DOUBLE)\n",
					"                       WHEN 3 THEN CAST(ws.TuWk2 AS DOUBLE)\n",
					"                       WHEN 4 THEN CAST(ws.WeWk2 AS DOUBLE)\n",
					"                       WHEN 5 THEN CAST(ws.ThWk2 AS DOUBLE)\n",
					"                       WHEN 6 THEN CAST(ws.FrWk2 AS DOUBLE)\n",
					"                       ELSE 0\n",
					"                   END\n",
					"           END, 0) >= 0\n",
					"   \"\"\")\n",
					"   LoggingUtil().log_info(\"Records expanded into temporary table\")\n",
					"   \n",
					"   # Step 4: Remove duplicates and insert into final table\n",
					"   LoggingUtil().log_info(\"Removing duplicates and inserting into final table\")\n",
					"   spark.sql(\"\"\"\n",
					"   WITH Duplicate_CTE AS (\n",
					"       SELECT \n",
					"           *,\n",
					"           ROW_NUMBER() OVER(\n",
					"               PARTITION BY absencedate, staffnumber \n",
					"               ORDER BY CASE WHEN AttendanceorAbsenceType = 'NA Time (Inspectors)' THEN 0 ELSE 1 END DESC\n",
					"           ) AS Rno\n",
					"       FROM \n",
					"           odw_harmonised_db.sap_hr_fact_Absence_All_TEMP\n",
					"   )\n",
					"   INSERT INTO odw_harmonised_db.sap_hr_fact_Absence_All (\n",
					"       absencedate,\n",
					"       absencehours,\n",
					"       staffnumber,\n",
					"       WorkScheduleRule,\n",
					"       AbsType,\n",
					"       SicknessGroup,\n",
					"       AttendanceorAbsenceType,\n",
					"       Leave,\n",
					"       PSGroup,\n",
					"       PersonnelArea,\n",
					"       PersonnelSubarea,\n",
					"       SourceSystemID,\n",
					"       IngestionDate,\n",
					"       ValidTo,\n",
					"       RowID,\n",
					"       IsActive\n",
					"   )\n",
					"   SELECT \n",
					"       CAST(absencedate AS DATE),\n",
					"       CAST(ROUND(absencehours, 2) AS DECIMAL(10,2)) AS absencehours,\n",
					"       staffnumber,\n",
					"       WorkScheduleRule,\n",
					"       AbsType,\n",
					"       SicknessGroup,\n",
					"       AttendanceorAbsenceType,\n",
					"       CAST(ROUND(Leave, 2) AS DECIMAL(10,2)) AS Leave,\n",
					"       NULL AS PSGroup,\n",
					"       NULL AS PersonnelArea,\n",
					"       NULL AS PersonnelSubarea,\n",
					"       SourceSystemID,\n",
					"       IngestionDate,\n",
					"       ValidTo,\n",
					"       RowID,\n",
					"       IsActive\n",
					"   FROM \n",
					"       Duplicate_CTE\n",
					"   WHERE \n",
					"       Rno = 1\n",
					"   \"\"\")\n",
					"   LoggingUtil().log_info(\"Data inserted into final table\")\n",
					"   \n",
					"   # Step 5: Delete rows for staff numbers that do not exist in SAP_HR data history\n",
					"   LoggingUtil().log_info(\"Deleting rows for non-existent staff numbers\")\n",
					"   spark.sql(\"\"\"\n",
					"   DELETE FROM odw_harmonised_db.sap_hr_fact_Absence_All\n",
					"   WHERE staffnumber IN ('50410587', '50422294')\n",
					"   \"\"\")\n",
					"   LoggingUtil().log_info(\"Deleted rows for non-existent staff numbers\")\n",
					"   \n",
					"   # Step 6: Create a staging table with best HR data matches\n",
					"   LoggingUtil().log_info(\"Creating HR data staging table\")\n",
					"   spark.sql(\"\"\"\n",
					"   CREATE OR REPLACE TEMPORARY VIEW hr_data_staging AS\n",
					"   SELECT \n",
					"       a.staffnumber,\n",
					"       a.absencedate,\n",
					"       -- For each absence record, find the most recent HR data\n",
					"       FIRST_VALUE(h.PSGroup) OVER (\n",
					"           PARTITION BY a.staffnumber, a.absencedate\n",
					"           ORDER BY \n",
					"               -- First prioritize records where the absence date falls within the reporting period\n",
					"               CASE WHEN a.absencedate BETWEEN date_add(date_add(h.Report_MonthEnd_Date, 1), -30) AND h.Report_MonthEnd_Date THEN 0 ELSE 1 END,\n",
					"               -- Then get the most recent HR record\n",
					"               h.Report_MonthEnd_Date DESC\n",
					"       ) AS PSGroup,\n",
					"       FIRST_VALUE(h.PersonnelArea) OVER (\n",
					"           PARTITION BY a.staffnumber, a.absencedate\n",
					"           ORDER BY \n",
					"               CASE WHEN a.absencedate BETWEEN date_add(date_add(h.Report_MonthEnd_Date, 1), -30) AND h.Report_MonthEnd_Date THEN 0 ELSE 1 END,\n",
					"               h.Report_MonthEnd_Date DESC\n",
					"       ) AS PersonnelArea,\n",
					"       FIRST_VALUE(h.PersonnelSubarea) OVER (\n",
					"           PARTITION BY a.staffnumber, a.absencedate\n",
					"           ORDER BY \n",
					"               CASE WHEN a.absencedate BETWEEN date_add(date_add(h.Report_MonthEnd_Date, 1), -30) AND h.Report_MonthEnd_Date THEN 0 ELSE 1 END,\n",
					"               h.Report_MonthEnd_Date DESC\n",
					"       ) AS PersonnelSubarea\n",
					"   FROM \n",
					"       odw_harmonised_db.sap_hr_fact_Absence_All a\n",
					"   JOIN \n",
					"       odw_harmonised_db.Hist_SAP_HR h\n",
					"   ON \n",
					"       a.staffnumber = h.PersNo\n",
					"   \"\"\")\n",
					"   LoggingUtil().log_info(\"HR data staging table created\")\n",
					"   \n",
					"   # Step 7: Deduplicate the staging data to ensure one row per staffnumber/absencedate\n",
					"   LoggingUtil().log_info(\"Deduplicating HR data\")\n",
					"   spark.sql(\"\"\"\n",
					"   CREATE OR REPLACE TEMPORARY VIEW hr_data_final AS\n",
					"   SELECT \n",
					"       staffnumber,\n",
					"       absencedate,\n",
					"       PSGroup,\n",
					"       PersonnelArea,\n",
					"       PersonnelSubarea\n",
					"   FROM (\n",
					"       SELECT \n",
					"           *,\n",
					"           ROW_NUMBER() OVER (PARTITION BY staffnumber, absencedate ORDER BY absencedate) AS rn\n",
					"       FROM \n",
					"           hr_data_staging\n",
					"   ) t\n",
					"   WHERE \n",
					"       rn = 1\n",
					"   \"\"\")\n",
					"   LoggingUtil().log_info(\"HR data deduplication completed\")\n",
					"   \n",
					"   # Step 8: Use MERGE to update the target table\n",
					"   LoggingUtil().log_info(\"Merging HR data into final absence table\")\n",
					"   spark.sql(\"\"\"\n",
					"   MERGE INTO odw_harmonised_db.sap_hr_fact_Absence_All a\n",
					"   USING hr_data_final s\n",
					"   ON a.staffnumber = s.staffnumber AND a.absencedate = s.absencedate\n",
					"   WHEN MATCHED THEN\n",
					"   UPDATE SET\n",
					"       PSGroup = s.PSGroup,\n",
					"       PersonnelArea = s.PersonnelArea,\n",
					"       PersonnelSubarea = s.PersonnelSubarea\n",
					"   \"\"\")\n",
					"   LoggingUtil().log_info(\"HR data merge completed\")\n",
					"   \n",
					"   LoggingUtil().log_info(\"Absence data processing completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"   # Log the exception in detail\n",
					"   LoggingUtil().log_error(f\"Error in absence data processing: {str(e)}\")\n",
					"   logException(e)\n",
					"   \n",
					"   # Re-raise the exception to ensure the notebook fails properly\n",
					"   raise e\n",
					"finally:\n",
					"   # Always flush logs regardless of success or failure\n",
					"   LoggingUtil().log_info(\"Flushing logs\")\n",
					"   LoggingUtil().flush_logging()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import col, to_date\n",
					"import json\n",
					"\n",
					"# Initialize result object\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"try:\n",
					"    # Create the filtered view\n",
					"    spark.sql(\"\"\"\n",
					"        CREATE OR REPLACE TEMPORARY VIEW absences_without_holidays AS\n",
					"        SELECT a.*\n",
					"        FROM odw_harmonised_db.sap_hr_fact_absence_all a\n",
					"        LEFT JOIN (\n",
					"            SELECT DISTINCT to_date(HolidayDate, 'dd/MM/yyyy') AS holiday_date\n",
					"            FROM odw_standardised_db.Live_Holidays\n",
					"        ) h ON to_date(a.absencedate) = h.holiday_date\n",
					"        WHERE h.holiday_date IS NULL\n",
					"    \"\"\")\n",
					"    \n",
					"    # Get the counts\n",
					"    original_count = spark.sql(\"SELECT COUNT(*) FROM odw_harmonised_db.sap_hr_fact_absence_all\").collect()[0][0]\n",
					"    filtered_count = spark.sql(\"SELECT COUNT(*) FROM absences_without_holidays\").collect()[0][0]\n",
					"    \n",
					"    print(f\"Original count: {original_count}\")\n",
					"    print(f\"After holiday filtering: {filtered_count}\")\n",
					"    print(f\"Records to be removed: {original_count - filtered_count}\")\n",
					"    \n",
					"    # Method 1: Try using Delta Lake operations\n",
					"    try:\n",
					"        from delta.tables import DeltaTable\n",
					"        \n",
					"        # Get the list of holiday dates\n",
					"        holiday_dates = spark.sql(\"\"\"\n",
					"            SELECT DISTINCT to_date(HolidayDate, 'dd/MM/yyyy') AS holiday_date\n",
					"            FROM odw_standardised_db.Live_Holidays\n",
					"        \"\"\").collect()\n",
					"        \n",
					"        absence_delta = DeltaTable.forName(spark, \"odw_harmonised_db.sap_hr_fact_absence_all\")\n",
					"        \n",
					"        # Process each holiday date individually to avoid syntax errors\n",
					"        for row in holiday_dates:\n",
					"            holiday = row.holiday_date.strftime('%Y-%m-%d')\n",
					"            absence_delta.delete(f\"to_date(absencedate) = '{holiday}'\")\n",
					"            print(f\"Deleted records for holiday: {holiday}\")\n",
					"        \n",
					"        # Verify final count\n",
					"        final_count = spark.sql(\"SELECT COUNT(*) FROM odw_harmonised_db.sap_hr_fact_absence_all\").collect()[0][0]\n",
					"        result[\"record_count\"] = final_count\n",
					"        print(f\"Final count after deletion: {final_count}\")\n",
					"        print(f\"Total records removed: {original_count - final_count}\")\n",
					"        \n",
					"    except Exception as e:\n",
					"        print(f\"Delta operations failed: {e}\")\n",
					"        print(\"Trying alternative method...\")\n",
					"        \n",
					"        # Method 2: Save to temporary table and replace\n",
					"        try:\n",
					"            # First, save the filtered data to a temporary location\n",
					"            filtered_df = spark.sql(\"SELECT * FROM absences_without_holidays\")\n",
					"            temp_path = \"/tmp/filtered_absences\"\n",
					"            \n",
					"            filtered_df.write.format(\"delta\").mode(\"overwrite\").save(temp_path)\n",
					"            \n",
					"            # Then truncate and reload\n",
					"            spark.sql(\"DELETE FROM odw_harmonised_db.sap_hr_fact_absence_all\")\n",
					"            \n",
					"            # Read from temp location and insert\n",
					"            temp_df = spark.read.format(\"delta\").load(temp_path)\n",
					"            temp_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"odw_harmonised_db.sap_hr_fact_absence_all\")\n",
					"            \n",
					"            # Verify final count\n",
					"            final_count = spark.sql(\"SELECT COUNT(*) FROM odw_harmonised_db.sap_hr_fact_absence_all\").collect()[0][0]\n",
					"            result[\"record_count\"] = final_count\n",
					"            print(f\"Final count after replacement: {final_count}\")\n",
					"            print(f\"Total records removed: {original_count - final_count}\")\n",
					"            \n",
					"        except Exception as e2:\n",
					"            print(f\"Alternative method failed: {e2}\")\n",
					"            print(\"Trying simplest approach...\")\n",
					"            \n",
					"            # Method 3: INSERT OVERWRITE approach\n",
					"            try:\n",
					"                # Use INSERT OVERWRITE instead of TRUNCATE + INSERT\n",
					"                spark.sql(\"\"\"\n",
					"                    INSERT OVERWRITE TABLE odw_harmonised_db.sap_hr_fact_absence_all\n",
					"                    SELECT * FROM absences_without_holidays\n",
					"                \"\"\")\n",
					"                \n",
					"                # Verify final count\n",
					"                final_count = spark.sql(\"SELECT COUNT(*) FROM odw_harmonised_db.sap_hr_fact_absence_all\").collect()[0][0]\n",
					"                result[\"record_count\"] = final_count\n",
					"                print(f\"Final count after INSERT OVERWRITE: {final_count}\")\n",
					"                print(f\"Total records removed: {original_count - final_count}\")\n",
					"                \n",
					"            except Exception as e3:\n",
					"                print(f\"All methods failed. Final error: {e3}\")\n",
					"                # Try to get current count even in case of error\n",
					"                try:\n",
					"                    error_count = spark.sql(\"SELECT COUNT(*) FROM odw_harmonised_db.sap_hr_fact_absence_all\").collect()[0][0]\n",
					"                    result[\"record_count\"] = error_count\n",
					"                except:\n",
					"                    result[\"record_count\"] = 0\n",
					"                \n",
					"                result[\"status\"] = \"failed\"\n",
					"                result[\"error_message\"] = str(e3)\n",
					"                print(\"Please contact your database administrator for assistance.\")\n",
					"    \n",
					"except Exception as e:\n",
					"    print(f\"Error in initial setup: {e}\")\n",
					"    # Try to get current count even in case of error\n",
					"    try:\n",
					"        error_count = spark.sql(\"SELECT COUNT(*) FROM odw_harmonised_db.sap_hr_fact_absence_all\").collect()[0][0]\n",
					"        result[\"record_count\"] = error_count\n",
					"    except:\n",
					"        result[\"record_count\"] = 0\n",
					"    \n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = str(e)\n",
					"\n",
					"finally:\n",
					"    \n",
					"    print(f\"RESULT: {json.dumps(result)}\")\n",
					"\n",
					"mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": null
			}
		]
	}
}