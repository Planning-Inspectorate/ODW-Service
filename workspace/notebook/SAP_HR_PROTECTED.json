{
	"name": "SAP_HR_PROTECTED",
	"properties": {
		"folder": {
			"name": "odw-harmonised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "8792f658-669f-430f-9969-4d80d9c6c6b9"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Entity Name : Protected Data</u>\n",
					"###### Author: Prathap A\n",
					"###### Date: 25/02/2025\n",
					"\n",
					"###### version : 0001\n",
					"###### <u>Description</u>:\n",
					"This template is designed to facilitate the monthly processing and harmonization of Protected data. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The template ensures that Protected data is accurately transformed, stored, and made available for reporting and analysis."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Intialisations"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np\n",
					"\n",
					""
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"SET spark.sql.ansi.enabled = false;"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"drop table  odw_harmonised_db.SAP_HR_PC"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"describe odw_harmonised_db.SAP_HR_PC"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"select * from odw_standardised_db.sap_protected_monthly"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"delete from odw_harmonised_db.SAP_HR_PC ;\n",
					"\n",
					"insert into odw_harmonised_db.SAP_HR_PC(\n",
					"\n",
					"\n",
					"\n",
					"SELECT \n",
					"\t   RefNo\n",
					"      ,NULLIF(EthnicOrigin, '')\n",
					"      ,NULLIF(ReligiousDenominationKey, '')\n",
					"      ,NULLIF(SxO, '')\n",
					"      ,Grade\n",
					"      ,NULLIF(DisabilityText, '')\n",
					"      ,cast(Report_MonthEnd_Date as date)  as Report_MonthEnd_Date\n",
					"      , 'saphr' AS SourceSystemID\n",
					"    ,CURRENT_DATE() AS IngestionDate\n",
					"   , CURRENT_TIMESTAMP() AS ValidTo\n",
					"   , NULL AS RowID,\n",
					"    'Y' AS IsActive\n",
					"\n",
					"from odw_standardised_db.sap_protected_monthly\n",
					"\n",
					")"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"UPDATE odw_harmonised_db.SAP_HR_PC\n",
					"SET RowID = md5(\n",
					"    concat_ws('|',\n",
					"\t   RefNo\n",
					"      ,EthnicOrigin\n",
					"      ,ReligiousDenominationKey\n",
					"      ,SxO\n",
					"      ,Grade\n",
					"      ,DisabilityText\n",
					"      )\n",
					"      )"
				],
				"execution_count": 17
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# PC_INSERT_DELETE"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"WITH pc AS (\n",
					"    SELECT \n",
					"        Report_MonthEnd_Date,\n",
					"        COUNT(*) AS s_count\n",
					"    FROM odw_harmonised_db.SAP_HR_PC\n",
					"    WHERE RefNo IS NOT NULL\n",
					"    GROUP BY Report_MonthEnd_Date\n",
					"),\n",
					"x2 AS (\n",
					"    SELECT \n",
					"        Report_MonthEnd_Date, \n",
					"        COUNT(*) AS history_rows\n",
					"    FROM \n",
					"        odw_harmonised_db.hist_SAP_HR h \n",
					"    WHERE \n",
					"        Report_MonthEnd_Date > '2019-03' \n",
					"        AND h.PayrollArea = 'PINS'\n",
					"    GROUP BY \n",
					"        Report_MonthEnd_Date\n",
					"),\n",
					"to_delete AS (\n",
					"    SELECT pc.Report_MonthEnd_Date\n",
					"    FROM pc \n",
					"    INNER JOIN x2 \n",
					"        ON pc.Report_MonthEnd_Date = x2.Report_MonthEnd_Date\n",
					"    WHERE pc.s_count = x2.history_rows\n",
					")\n",
					"\n",
					"-- Use MERGE to delete and insert rows\n",
					"MERGE INTO odw_harmonised_db.sap_hr_protected_data AS target\n",
					"USING (\n",
					"    SELECT \n",
					"        SAP_HR_PC.*,\n",
					"        CASE \n",
					"            WHEN to_delete.Report_MonthEnd_Date IS NOT NULL THEN 'DELETE'\n",
					"            ELSE 'INSERT'\n",
					"        END AS action\n",
					"    FROM odw_harmonised_db.SAP_HR_PC\n",
					"    LEFT JOIN to_delete \n",
					"        ON SAP_HR_PC.Report_MonthEnd_Date = to_delete.Report_MonthEnd_Date\n",
					") AS source\n",
					"ON target.Report_MonthEnd_Date = source.Report_MonthEnd_Date\n",
					"WHEN MATCHED AND source.action = 'DELETE' THEN DELETE\n",
					"WHEN NOT MATCHED AND source.action = 'INSERT' THEN INSERT *;"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"UPDATE odw_harmonised_db.sap_hr_protected_data\n",
					"SET RowID = md5(\n",
					"    concat_ws('|',\n",
					"\t   RefNo\n",
					"      ,EthnicOrigin\n",
					"      ,ReligiousDenominationKey\n",
					"      ,SxO\n",
					"      ,Grade\n",
					"      ,DisabilityText\n",
					"      )\n",
					"      )"
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"    SELECT \n",
					"        Report_MonthEnd_Date, \n",
					"        COUNT(*) AS history_rows\n",
					"    FROM \n",
					"        odw_harmonised_db.hist_SAP_HR h \n",
					"    WHERE \n",
					"        Report_MonthEnd_Date > '2019-03' \n",
					"        AND h.PayrollArea = 'PINS'\n",
					"    GROUP BY \n",
					"        Report_MonthEnd_Date"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import col, expr, substring, regexp_replace, count, date_format, to_date\n",
					"\n",
					"# Initialize Spark session\n",
					"spark = SparkSession.builder.appName(\"SAP_HR_Processing\").getOrCreate()\n",
					"\n",
					"# Step 1: Create the `pc` DataFrame\n",
					"pc_df = (\n",
					"    spark.table(\"odw_harmonised_db.SAP_HR_PC\")\n",
					"    .filter(col(\"RefNo\").isNotNull())\n",
					"    .withColumn(\"file_name\", date_format(\n",
					"        to_date(substring(regexp_replace(col(\"file_name\"), \".xlsx\", \"\"), -8, 8), \"yyyyMMdd\"), \"yyyy-MM\"\n",
					"    ))\n",
					"    .groupBy(\"file_name\", \"file\")\n",
					"    .agg(count(\"*\").alias(\"s_count\"))\n",
					")\n",
					"\n",
					"# Step 2: Create the `x2` DataFrame\n",
					"x2_df = (\n",
					"    spark.table(\"odw_harmonised_db.SAP_HR\")\n",
					"    .filter((substring(col(\"Report_MonthEnd_Date\").cast(\"string\"), 1, 7) > \"2019-03\")\n",
					"    .filter(col(\"PayrollArea\") == \"PINS\")\n",
					"    .withColumn(\"r_month\", substring(col(\"Report_MonthEnd_Date\").cast(\"string\"), 1, 7))\n",
					"    .groupBy(\"r_month\")\n",
					"    .agg(count(\"*\").alias(\"history_rows\"))\n",
					")\n",
					"\n",
					"# Step 3: Perform the DELETE operation\n",
					"# Identify rows to delete\n",
					"delete_files_df = (\n",
					"    pc_df.alias(\"pc\")\n",
					"    .join(x2_df.alias(\"x2\"), col(\"pc.file_name\") == col(\"x2.r_month\"))\n",
					"    .filter(col(\"pc.s_count\") == col(\"x2.history_rows\"))\n",
					"    .select(\"pc.file\")\n",
					")\n",
					"\n",
					"# Delete rows from `protected_data`\n",
					"protected_data_df = spark.table(\"odw_harmonised_db.protected_data\")\n",
					"protected_data_df = protected_data_df.join(delete_files_df, on=\"file_name\", how=\"left_anti\")\n",
					"\n",
					"# Overwrite the `protected_data` table with the filtered data\n",
					"protected_data_df.write.mode(\"overwrite\").saveAsTable(\"odw_harmonised_db.protected_data\")\n",
					"\n",
					"# Step 4: Perform the INSERT operation\n",
					"# Identify rows to insert\n",
					"insert_files_df = (\n",
					"    pc_df.alias(\"pc\")\n",
					"    .join(x2_df.alias(\"x2\"), col(\"pc.file_name\") == col(\"x2.r_month\"))\n",
					"    .filter(col(\"pc.s_count\") == col(\"x2.history_rows\"))\n",
					"    .select(\"pc.file\")\n",
					")\n",
					"\n",
					"# Filter rows from `SAP_HR_PC` to insert\n",
					"insert_data_df = (\n",
					"    spark.table(\"odw_harmonised_db.SAP_HR_PC\")\n",
					"    .join(insert_files_df, on=\"file_name\", how=\"inner\")\n",
					")\n",
					"\n",
					"# Insert rows into `protected_data`\n",
					"insert_data_df.write.mode(\"append\").saveAsTable(\"odw_harmonised_db.protected_data\")"
				],
				"execution_count": null
			}
		]
	}
}