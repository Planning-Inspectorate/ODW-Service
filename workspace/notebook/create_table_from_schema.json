{
	"name": "create_table_from_schema",
	"properties": {
		"folder": {
			"name": "utils"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "a154493d-6fc8-49cf-b0df-f4560bcc35b3"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Example of creating a table from a json schema in the data-model repo"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Notes\r\n",
					"\r\n",
					"Json fields described as Integer may need converting to LongType  \r\n",
					"\r\n",
					"Initial creation of an empty table will result in all fields being nullable. Therefore the schema will not exactly match the data model in this respect. Do we need to worry about this?  \r\n",
					"\r\n",
					"Rather than use py_get_schema_from_url notebook to define additional fields, we should add the additional fields from the config directly, i.e. create schema from the data model then get additional fields from config as needed, depending on database. Curated will match the data model exactly without additional fields."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def delete_table(db_name, table_name):\n",
					"    import logging\n",
					"    logger = logging.getLogger(__name__)\n",
					"    from pyspark.sql import SparkSession\n",
					"    from notebookutils import mssparkutils\n",
					"    spark = SparkSession.builder.getOrCreate()\n",
					"    if spark._jsparkSession.catalog().tableExists(db_name, table_name):\n",
					"        table_details = spark.sql(f\"DESCRIBE DETAIL {db_name}.{table_name}\").toPandas()\n",
					"        if len(table_details) > 1:\n",
					"            raise RuntimeError(\"too many locations associated with the table!\")\n",
					"        else:\n",
					"            loc = table_details['location'][0]\n",
					"            mssparkutils.fs.rm(loc, True)\n",
					"            spark.sql(f\"DROP TABLE IF EXISTS {db_name}.{table_name}\")\n",
					"    else:\n",
					"        logger.info(\"Table does not exist\")\n",
					"\n",
					"delete_table('odw_harmonised_db', 'appeal_document') "
				],
				"execution_count": 80
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"db_name = 'odw_harmonised_db'\r\n",
					"entity_name = 'appeal-document'"
				],
				"execution_count": 81
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"table_name = entity_name.replace(\"-\", \"_\")\r\n",
					"full_table_name = f\"{db_name}.{table_name}\""
				],
				"execution_count": 82
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pprint\r\n",
					"from pyspark.sql.types import *\r\n",
					"import json\r\n",
					"from pyspark.sql.functions import col\r\n",
					"from pyspark.sql import DataFrame"
				],
				"execution_count": 83
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def test_table_exists(db_name: str, table_name: str) -> bool:\r\n",
					"    spark.sql(f\"USE {db_name}\")\r\n",
					"    tables_df = spark.sql(\"SHOW TABLES\")\r\n",
					"    table_names = [row['tableName'] for row in tables_df.collect()]\r\n",
					"    return table_name in table_names"
				],
				"execution_count": 84
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"test_table_exists(db_name, table_name)"
				],
				"execution_count": 85
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_incremental_key():\n",
					"    # getting the incremental key from the odw-config/orchestration\n",
					"    storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"    path_to_orchestration_file = \"abfss://odw-config@\"+storage_account+\"orchestration/orchestration.json\"\n",
					"    df = spark.read.option(\"multiline\",\"true\").json(path_to_orchestration_file)\n",
					"    definitions = json.loads(df.toJSON().first())['definitions']\n",
					"\n",
					"    definition = next((d for d in definitions if entity_name == d['Source_Filename_Start']), None)\n",
					"    return definition['Harmonised_Incremental_Key'] if definition and 'Harmonised_Incremental_Key' in definition else None"
				],
				"execution_count": 86
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def create_spark_schema(db_name: str, entity_name: str) -> StructType:\r\n",
					"    incremental_key = get_incremental_key() if db_name == 'odw_harmonised_db' else None\r\n",
					"    json_schema = mssparkutils.notebook.run(\"/py_get_schema_from_url\", 30, {\"db_name\": db_name, \"entity_name\": entity_name, \"incremental_key\": incremental_key})\r\n",
					"    spark_schema = StructType.fromJson(json.loads(json_schema))\r\n",
					"    return spark_schema"
				],
				"execution_count": 87
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def create_spark_dataframe() -> DataFrame:\r\n",
					"    spark_dataframe = spark.createDataFrame([], schema=create_spark_schema(db_name, entity_name))\r\n",
					"    return spark_dataframe"
				],
				"execution_count": 88
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def create_spark_table(db_name: str, table_name: str, spark_dataframe: DataFrame) -> None:\r\n",
					"    spark.sql(f\"drop table if exists `{db_name}`.`{table_name}`\")\r\n",
					"    spark_dataframe.write.saveAsTable(f\"{db_name}.{table_name}\")\r\n",
					"    print(\"Table created\")"
				],
				"execution_count": 89
			},
			{
				"cell_type": "code",
				"source": [
					"spark_dataframe = create_spark_dataframe()\r\n",
					"create_spark_table(db_name, table_name, spark_dataframe)"
				],
				"execution_count": 90
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"test_table_exists(db_name, table_name)"
				],
				"execution_count": 91
			}
		]
	}
}