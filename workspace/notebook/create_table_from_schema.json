{
	"name": "create_table_from_schema",
	"properties": {
		"folder": {
			"name": "utils"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "080d1eb1-2f8f-4061-9539-e4e54078436a"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Example of creating a table from a json schema in the data-model repo"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Notes\r\n",
					"\r\n",
					"Json fields described as Integer may need converting to LongType  \r\n",
					"\r\n",
					"Initial creation of an empty table will result in all fields being nullable. Therefore the schema will not exactly match the data model in this respect. Do we need to worry about this?  \r\n",
					"\r\n",
					"Rather than use py_get_schema_from_url notebook to define additional fields, we should add the additional fields from the config directly, i.e. create schema from the data model then get additional fields from config as needed, depending on database. Curated will match the data model exactly without additional fields."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"db_name = ''\r\n",
					"entity_name = ''"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"table_name = entity_name.replace(\"-\", \"_\")\r\n",
					"full_table_name = f\"{db_name}.{table_name}\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pprint\r\n",
					"from pyspark.sql.types import *\r\n",
					"import json\r\n",
					"from pyspark.sql.functions import col\r\n",
					"from pyspark.sql import DataFrame"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def test_table_exists(db_name: str, table_name: str) -> bool:\r\n",
					"    spark.sql(f\"USE {db_name}\")\r\n",
					"    tables_df = spark.sql(\"SHOW TABLES\")\r\n",
					"    table_names = [row['tableName'] for row in tables_df.collect()]\r\n",
					"    return table_name in table_names"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"test_table_exists(db_name, table_name)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def create_spark_schema(db_name: str, entity_name: str) -> StructType:\r\n",
					"    json_schema = mssparkutils.notebook.run(\"/py_get_schema_from_url\", 30, {\"db_name\": db_name, \"entity_name\": entity_name})\r\n",
					"    spark_schema = StructType.fromJson(json.loads(json_schema))\r\n",
					"    return spark_schema"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def create_spark_dataframe() -> DataFrame:\r\n",
					"    spark_dataframe = spark.createDataFrame([], schema=create_spark_schema(db_name, entity_name))\r\n",
					"    return spark_dataframe"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def create_spark_table(db_name: str, table_name: str, spark_dataframe: DataFrame) -> None:\r\n",
					"    spark.sql(f\"drop table if exists `{db_name}`.`{table_name}`\")\r\n",
					"    spark_dataframe.write.saveAsTable(f\"{db_name}.{table_name}\")\r\n",
					"    print(\"Table created\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"spark_dataframe = create_spark_dataframe()\r\n",
					"create_spark_table(db_name, table_name, spark_dataframe)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"test_table_exists(db_name, table_name)"
				],
				"execution_count": null
			}
		]
	}
}