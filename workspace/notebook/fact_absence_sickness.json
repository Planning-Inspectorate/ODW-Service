{
	"name": "fact_absence_sickness",
	"properties": {
		"folder": {
			"name": "odw-harmonised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "3d78c861-43d3-4843-ae76-ffa47c1d539a"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Entity : fact_absence_sickness\n",
					"###### Author: Prathap A\n",
					"###### Date: 25/02/2025\n",
					"\n",
					"###### version : 0001\n",
					"###### <u>Description</u>:\n",
					"This template is designed to facilitate the monthly processing and harmonization of Sickness data. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The template ensures that Sickness data is accurately transformed, stored, and made available for reporting and analysis."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Sickness Data"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"editable": true,
					"run_control": {
						"frozen": false
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"-- First, clean the target table\n",
					"DELETE FROM odw_harmonised_db.sap_hr_fact_absence_sickness;\n",
					"\n",
					"-- Step 1: Create the initial sickness view with base data\n",
					"CREATE OR REPLACE TEMP VIEW sickness AS\n",
					"SELECT \n",
					"    a.StaffNumber,\n",
					"    CAST(a.StartDate AS DATE) AS sickness_start,\n",
					"    CAST(a.EndDate AS DATE) AS sickness_end,\n",
					"    a.Days,\n",
					"    a.SicknessGroup,\n",
					"    a.WorkScheduleRule,\n",
					"    ds.FY AS FY_start,\n",
					"    de.FY AS FY_end\n",
					"FROM odw_harmonised_db.sap_hr_absence_all a\n",
					"LEFT JOIN odw_harmonised_db.live_dim_date ds ON CAST(a.StartDate AS DATE) = ds.date\n",
					"LEFT JOIN odw_harmonised_db.live_dim_date de ON CAST(a.EndDate AS DATE) = de.date\n",
					"WHERE a.AttendanceorAbsenceType = 'Sickness';\n",
					"\n",
					"-- Step 2: Create a view with row numbers to help identify unique records\n",
					"CREATE OR REPLACE TEMP VIEW sickness_with_row_numbers AS\n",
					"SELECT \n",
					"    ROW_NUMBER() OVER (ORDER BY StaffNumber, sickness_start) AS row_num,\n",
					"    StaffNumber,\n",
					"    sickness_start,\n",
					"    sickness_end,\n",
					"    Days,\n",
					"    SicknessGroup,\n",
					"    WorkScheduleRule,\n",
					"    FY_start,\n",
					"    FY_end,\n",
					"    -- Add lead and lag information for contiguity detection\n",
					"    LEAD(sickness_start) OVER (PARTITION BY StaffNumber ORDER BY sickness_start DESC) AS prev_sickness_start,\n",
					"    LEAD(sickness_end) OVER (PARTITION BY StaffNumber ORDER BY sickness_start DESC) AS prev_sickness_end\n",
					"\n",
					"FROM sickness;\n",
					"\n",
					"-- Step 3: Identify contiguous periods using a date function that respects working days\n",
					"CREATE OR REPLACE TEMP VIEW sickness_with_contiguity AS\n",
					"SELECT \n",
					"    row_num,\n",
					"    StaffNumber,\n",
					"    sickness_start,\n",
					"    sickness_end,\n",
					"    Days,\n",
					"    SicknessGroup,\n",
					"    WorkScheduleRule,\n",
					"    FY_start, \n",
					"    FY_end,\n",
					"    prev_sickness_start,\n",
					"    prev_sickness_end,\n",
					"    -- This determines if the current sickness period is contiguous with the previous one\n",
					"    -- We're using DATE_ADD to mimic the DATEADDNOWKNOBH function\n",
					"    CASE \n",
					"        WHEN DATE_ADD(prev_sickness_end, 1) = sickness_start THEN UUID()\n",
					"        ELSE NULL \n",
					"    END AS contiguous_sickness,\n",
					"    DATE_ADD(prev_sickness_end, 1) AS next_working_day_after_previous_sickness\n",
					"FROM sickness_with_row_numbers;\n",
					"\n",
					"-- Step 4: Create a view that assigns sickness IDs\n",
					"CREATE OR REPLACE TEMP VIEW sickness_with_ids AS\n",
					"SELECT \n",
					"    s.*,\n",
					"    -- This is a cumulative sum that increases by 1 whenever we encounter a non-contiguous sickness period\n",
					"    SUM(CASE \n",
					"            WHEN contiguous_sickness IS NULL OR sickness_start <> next_working_day_after_previous_sickness \n",
					"            THEN 1 \n",
					"            ELSE 0 \n",
					"        END) OVER (PARTITION BY StaffNumber ORDER BY sickness_start) AS sickness_id\n",
					"FROM sickness_with_contiguity s;\n",
					"\n",
					"-- Step 5: Group the results properly by sickness ID\n",
					"CREATE OR REPLACE TEMP VIEW sickness_final AS\n",
					"SELECT \n",
					"    StaffNumber,\n",
					"    sickness_id,\n",
					"    SUM(Days) AS Total_Days,\n",
					"    MIN(sickness_start) AS sickness_start,\n",
					"    MAX(sickness_end) AS sickness_end,\n",
					"    MAX(FY_start) AS FY,\n",
					"    CASE \n",
					"        WHEN MAX(sickness_end) < DATE_SUB(CURRENT_DATE(), 2 * 365) THEN 'Older FY'\n",
					"        WHEN MAX(sickness_end) BETWEEN DATE_SUB(CURRENT_DATE(), 2 * 365) AND DATE_SUB(CURRENT_DATE(), 1 * 365) THEN 'Previous FY'\n",
					"        WHEN MAX(sickness_end) BETWEEN DATE_SUB(CURRENT_DATE(), 1 * 365) AND CURRENT_DATE() THEN 'Current FY'\n",
					"        ELSE 'Next FY'\n",
					"\n",
					"    END AS financial_year,\n",
					"    CASE \n",
					"        WHEN MAX(sickness_end) < DATE_SUB(CURRENT_DATE(), 2 * 365) THEN 'Older CY'\n",
					"        WHEN MAX(sickness_end) BETWEEN DATE_SUB(CURRENT_DATE(), 2 * 365) AND DATE_SUB(CURRENT_DATE(), 1 * 365) THEN 'Previous CY'\n",
					"        WHEN MAX(sickness_end) BETWEEN DATE_SUB(CURRENT_DATE(), 1 * 365) AND CURRENT_DATE() THEN 'Current CY'\n",
					"        ELSE 'Future CY'\n",
					"\n",
					"    END AS calendar_year\n",
					"FROM sickness_with_ids\n",
					"GROUP BY StaffNumber, sickness_id;\n",
					"\n",
					"-- Step 6: Insert the final results into the target table\n",
					"INSERT OVERWRITE TABLE odw_harmonised_db.sap_hr_fact_absence_sickness\n",
					"\n",
					"SELECT \n",
					"    sickness_id,\n",
					"    CASE\n",
					"        WHEN TRIM(LEADING '0' FROM LPAD(StaffNumber, 8, '0')) LIKE '4%' THEN \n",
					"            '50' || TRIM(LEADING '0' FROM LPAD(StaffNumber, 8, '0'))\n",
					"        WHEN TRIM(LEADING '0' FROM LPAD(StaffNumber, 8, '0')) LIKE '5%' THEN \n",
					"            '00' || TRIM(LEADING '0' FROM LPAD(StaffNumber, 8, '0'))\n",
					"        WHEN TRIM(LEADING '0' FROM LPAD(StaffNumber, 8, '0')) LIKE '6%' THEN \n",
					"            '60' || TRIM(LEADING '0' FROM LPAD(StaffNumber, 8, '0'))\n",
					"        ELSE LPAD(StaffNumber, 7, '0')\n",
					"    END  AS StaffNumber,\n",
					"\n",
					"    Total_Days AS Days,\n",
					"    sickness_start,\n",
					"    sickness_end,\n",
					"    FY,\n",
					"    financial_year,\n",
					"    calendar_year,\n",
					"    'saphr' AS SourceSystemID,\n",
					"    CURRENT_DATE() AS IngestionDate,\n",
					"    CURRENT_TIMESTAMP() AS ValidFrom,\n",
					"    CURRENT_TIMESTAMP() AS ValidTo,\n",
					"    NULL AS RowID,\n",
					"    'Y' AS IsActive,\n",
					"    CURRENT_TIMESTAMP() AS LastUpdated\n",
					"FROM sickness_final;"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"UPDATE odw_harmonised_db.sap_hr_fact_absence_sickness\n",
					"SET RowID = md5(\n",
					"    concat_ws('|',\n",
					"    sickness_id,\n",
					"    StaffNumber,\n",
					"    Days ,\n",
					"    sickness_start,\n",
					"    sickness_end,\n",
					"    FY,\n",
					"    financial_year,\n",
					"    calendar_year\n",
					" \n",
					"\n",
					"    )\n",
					")"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"\n",
					"select  * from odw_harmonised_db.sap_hr_fact_absence_sickness where StaffNumber = '50410538'"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"spark.sql(\"\"\"\n",
					"CREATE OR REPLACE TEMPORARY VIEW holidays_view AS\n",
					"SELECT \n",
					"    HolidayDate\n",
					"FROM \n",
					"    odw_standardised_db.Live_Holidays;\n",
					"\"\"\")\n",
					"\n",
					"# Step 2: Use MERGE to delete rows where absencedate matches a holiday\n",
					"spark.sql(\"\"\"\n",
					"MERGE INTO odw_harmonised_db.sap_hr_fact_absence_sickness AS target\n",
					"USING holidays_view AS source\n",
					"ON CAST(target.sickness_start AS DATE) = cast(source.HolidayDate as date)\n",
					"WHEN MATCHED THEN DELETE;\n",
					"\"\"\")"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}