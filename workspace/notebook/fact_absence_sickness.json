{
	"name": "fact_absence_sickness",
	"properties": {
		"folder": {
			"name": "odw-harmonised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "fe1336ef-c6ec-4027-99f9-425c01c87179"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import udf\n",
					"from pyspark.sql.types import DateType\n",
					"from datetime import datetime, timedelta\n",
					"\n",
					"# Initialize Spark session\n",
					"spark = SparkSession.builder.appName(\"FactAbsenceSickness\").getOrCreate()\n",
					"\n",
					"# Define the function to add business days\n",
					"def add_business_days(start_date, num_days, holidays):\n",
					"    \"\"\"\n",
					"    Adds business days to a given date, excluding weekends and holidays.\n",
					"    \n",
					"    :param start_date: The starting date (datetime object).\n",
					"    :param num_days: The number of business days to add (int).\n",
					"    :param holidays: A list of holiday dates (list of datetime objects).\n",
					"    :return: The resulting date after adding business days (datetime object).\n",
					"    \"\"\"\n",
					"    current_date = start_date\n",
					"    while num_days > 0:\n",
					"        current_date += timedelta(days=1)\n",
					"        # Skip weekends (Saturday=5, Sunday=6)\n",
					"        if current_date.weekday() >= 5:\n",
					"            continue\n",
					"        # Skip holidays\n",
					"        if current_date in holidays:\n",
					"            continue\n",
					"        num_days -= 1\n",
					"    return current_date\n",
					"\n",
					"# Register the UDF\n",
					"add_business_days_udf = udf(add_business_days, DateType())\n",
					"spark.udf.register(\"DATEADDNOWKNOBH\", add_business_days_udf)"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"# Load necessary tables into DataFrames\n",
					"absence_all_df = spark.table(\"sap_hr.absence_all\")\n",
					"dim_date_df = spark.table(\"Live.dim_date\")\n",
					"holidays_df = spark.table(\"Live.Holidays\")  # Assuming you have a table for holidays\n",
					"\n",
					"# Convert holidays to a list\n",
					"holidays_list = [row.HolidayDate for row in holidays_df.collect()]\n",
					"\n",
					"# Register the holidays list as a temporary view\n",
					"holidays_df.createOrReplaceTempView(\"holidays\")\n",
					"\n",
					"# Use Spark SQL to perform the logic\n",
					"result_df = spark.sql(f\"\"\"\n",
					"    WITH sickness AS (\n",
					"        SELECT \n",
					"            a.Staff_Number,\n",
					"            ds.FY AS FY_start,\n",
					"            de.FY AS FY_end,\n",
					"            CAST(a.Start_Date AS DATE) AS sickness_start,\n",
					"            CAST(a.End_Date AS DATE) AS sickness_end,\n",
					"            a.Days,\n",
					"            a.Sickness_Group,\n",
					"            a.Work_Schedule_Rule\n",
					"        FROM\n",
					"            sap_hr.absence_all a\n",
					"            LEFT JOIN Live.dim_date ds ON CAST(a.Start_Date AS DATE) = CAST(ds.date AS DATE)\n",
					"            LEFT JOIN Live.dim_date de ON CAST(a.End_Date AS DATE) = CAST(de.date AS DATE)\n",
					"        WHERE\n",
					"            a.[Attendance or Absence Type] = 'Sickness'\n",
					"    ),\n",
					"    sickness2 AS (\n",
					"        SELECT\n",
					"            s.Staff_Number,\n",
					"            s.Days,\n",
					"            LEAD(s.sickness_start) OVER (PARTITION BY s.Staff_Number ORDER BY s.sickness_start) AS prev_sickness_start,\n",
					"            LEAD(s.sickness_end) OVER (PARTITION BY s.Staff_Number ORDER BY s.sickness_start) AS prev_sickness_end,\n",
					"            CASE\n",
					"                WHEN DATEADD(DAY, 1, s.sickness_end) = LEAD(s.sickness_start) OVER (PARTITION BY s.Staff_Number ORDER BY s.sickness_start) THEN UUID()\n",
					"                WHEN s.sickness_end = LAG(s.sickness_start) OVER (PARTITION BY s.Staff_Number ORDER BY s.sickness_start) THEN UUID()\n",
					"                ELSE NULL\n",
					"            END AS contiguous_sickness,\n",
					"            s.sickness_start,\n",
					"            s.sickness_end,\n",
					"            CASE\n",
					"                WHEN s.sickness_end >= DATEADD(YEAR, -2, CURRENT_DATE) AND s.sickness_end < DATEADD(YEAR, 0, CURRENT_DATE) THEN 'Previous FY'\n",
					"                WHEN s.sickness_end < DATEADD(YEAR, -1, CURRENT_DATE) THEN 'Older FY'\n",
					"                WHEN s.sickness_end >= DATEADD(YEAR, -1, CURRENT_DATE) AND s.sickness_end < CURRENT_DATE THEN 'Current FY'\n",
					"                ELSE 'Next FY'\n",
					"            END AS financial_year,\n",
					"            CASE\n",
					"                WHEN s.sickness_end >= DATEADD(YEAR, -2, CURRENT_DATE) AND s.sickness_end < DATEADD(YEAR, -1, CURRENT_DATE) THEN 'Previous CY'\n",
					"                WHEN s.sickness_end < DATEADD(YEAR, -2, CURRENT_DATE) THEN 'Older CY'\n",
					"                WHEN s.sickness_end >= DATEADD(YEAR, -1, CURRENT_DATE) AND s.sickness_end < CURRENT_DATE THEN 'Current CY'\n",
					"                ELSE 'Future CY'\n",
					"            END AS calendar_year,\n",
					"            s.Sickness_Group,\n",
					"            s.Work_Schedule_Rule\n",
					"        FROM\n",
					"            sickness s\n",
					"    )\n",
					"    SELECT \n",
					"        ROW_NUMBER() OVER(ORDER BY Staff_Number, sickness_start) AS row_num,\n",
					"        Staff_Number,\n",
					"        Days,\n",
					"        Sickness_Group,\n",
					"        contiguous_sickness,\n",
					"        DATEADDNOWKNOBH(sickness_end, 1, ARRAY({', '.join([f\"CAST('{h}' AS DATE)\" for h in holidays_list]})) AS next_working_day_after_previous_sickness,\n",
					"        sickness_start,\n",
					"        sickness_end,\n",
					"        prev_sickness_start,\n",
					"        prev_sickness_end,\n",
					"        FY_start AS FY,\n",
					"        financial_year,\n",
					"        calendar_year,\n",
					"        Work_Schedule_Rule,\n",
					"        NULL AS sickness_id\n",
					"    FROM\n",
					"        sickness2\n",
					"\"\"\")\n",
					"\n",
					"# Show the result\n",
					"#result_df.show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import when, col, lit\n",
					"\n",
					"# Define a window specification\n",
					"window_spec = Window.partitionBy(\"Staff_Number\").orderBy(\"row_num\")\n",
					"\n",
					"# Assign sickness IDs\n",
					"result_df = result_df.withColumn(\"sickness_id\", lit(1))\n",
					"\n",
					"# Use a loop-like approach to assign sickness IDs\n",
					"for i in range(2, result_df.count() + 1):\n",
					"    result_df = result_df.withColumn(\"sickness_id\", \n",
					"        when(\n",
					"            (col(\"row_num\") == i) & (col(\"contiguous_sickness\").isNull() | (col(\"sickness_start\") != col(\"next_working_day_after_previous_sickness\"))),\n",
					"            col(\"sickness_id\") + 1\n",
					"        ).otherwise(col(\"sickness_id\"))\n",
					"    )\n",
					"\n",
					"# Group by staff_number and sickness_id\n",
					"final_df = result_df.groupBy(\"Staff_Number\", \"sickness_id\") \\\n",
					"    .agg(\n",
					"        sum(\"Days\").alias(\"Days\"),\n",
					"        min(\"sickness_start\").alias(\"sickness_start\"),\n",
					"        max(\"sickness_end\").alias(\"sickness_end\"),\n",
					"        max(\"FY\").alias(\"FY\"),\n",
					"        max(\"financial_year\").alias(\"financial_year\"),\n",
					"        max(\"calendar_year\").alias(\"calendar_year\")\n",
					"    )\n",
					"\n",
					"# Write the result to the fact table\n",
					"final_df.write.mode(\"overwrite\").saveAsTable(\"sap_hr.fact_absence_sickness\")"
				],
				"execution_count": null
			}
		]
	}
}