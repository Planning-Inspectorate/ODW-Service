{
	"name": "py_Inspector_Specialisms",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b43f03de-0834-4681-a2b4-513de350b662"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Standardised layer and build a table for Curated Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;25-Feb-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This template is designed to facilitate the monthly processing and harmonization of Inspector Specialism. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The template ensures that Inspector Specialism data is accurately transformed, stored, and made available for reporting and analysis."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Intialisations"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from odw.core.util.logging_util import LoggingUtil\n",
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# "
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Entity Name : inspector_Specialisms"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"\n",
					"\n",
					"LoggingUtil().log_info(\"Starting inspector specialisms data processing\")\n",
					"\n",
					"# Step 1: Delete all records from the transform table\n",
					"LoggingUtil().log_info(\"Step 1: Deleting records from transform_inspector_Specialisms\")\n",
					"spark.sql(\"\"\"\n",
					"DELETE FROM odw_harmonised_db.transform_inspector_Specialisms\n",
					"\"\"\")\n",
					"LoggingUtil().log_info(\"Records deleted from transform table\")\n",
					"\n",
					"# Step 2: Insert new records into the transform table\n",
					"LoggingUtil().log_info(\"Step 2: Inserting records into transform_inspector_Specialisms\")\n",
					"spark.sql(\"\"\"\n",
					"INSERT INTO odw_harmonised_db.transform_inspector_Specialisms (\n",
					"    StaffNumber,\n",
					"    Firstname,\n",
					"    Lastname,\n",
					"    QualificationName,\n",
					"    Proficien,\n",
					"    SourceSystemID,\n",
					"    IngestionDate,\n",
					"    ValidTo,\n",
					"    RowID,\n",
					"    IsActive\n",
					")\n",
					"SELECT \n",
					"    -- Format StaffNumber based on length and prefix\n",
					"    CASE \n",
					"        WHEN LENGTH(StaffNumber) = 6 THEN\n",
					"            CASE \n",
					"                WHEN StaffNumber LIKE '50%' THEN CONCAT('00', StaffNumber)\n",
					"                WHEN StaffNumber LIKE '42%' THEN CONCAT('50', StaffNumber)\n",
					"                ELSE StaffNumber\n",
					"            END\n",
					"        WHEN LENGTH(StaffNumber) = 8 THEN StaffNumber\n",
					"        ELSE StaffNumber\n",
					"    END AS StaffNumber,\n",
					"    Firstname,\n",
					"    Lastname,\n",
					"    QualificationName,\n",
					"    Proficien,\n",
					"    'saphr' AS SourceSystemID,\n",
					"    CURRENT_DATE() AS IngestionDate,\n",
					"    CURRENT_DATE() AS ValidTo,\n",
					"    -- Generate RowID during insert instead of separate update\n",
					"    md5(concat_ws('|', \n",
					"        coalesce(cast(\n",
					"            CASE \n",
					"                WHEN LENGTH(StaffNumber) = 6 THEN\n",
					"                    CASE \n",
					"                        WHEN StaffNumber LIKE '50%' THEN CONCAT('00', StaffNumber)\n",
					"                        WHEN StaffNumber LIKE '42%' THEN CONCAT('50', StaffNumber)\n",
					"                        ELSE StaffNumber\n",
					"                    END\n",
					"                WHEN LENGTH(StaffNumber) = 8 THEN StaffNumber\n",
					"                ELSE StaffNumber\n",
					"            END as string), ''), \n",
					"        coalesce(cast(Firstname as string), ''), \n",
					"        coalesce(cast(Lastname as string), ''), \n",
					"        coalesce(cast(QualificationName as string), ''), \n",
					"        coalesce(cast(Proficien as string), '')\n",
					"    )) AS RowID,\n",
					"    'Y' AS IsActive\n",
					"FROM \n",
					"    odw_standardised_db.inspector_specialisms_monthly t1\n",
					"WHERE \n",
					"    StaffNumber IS NOT NULL\n",
					"\"\"\")\n",
					"\n",
					"# Get count of inserted records\n",
					"record_count = spark.sql(\"SELECT COUNT(*) as record_count FROM odw_harmonised_db.transform_inspector_Specialisms\").collect()[0]['record_count']\n",
					"LoggingUtil().log_info(f\"Inserted {record_count} records into transform_inspector_Specialisms\")\n",
					"\n",
					"LoggingUtil().log_info(\"Inspector specialisms data processing completed successfully\")\n",
					"\n",
					"# Ensure logs are flushed\n",
					"flushLogging()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Update Statements"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"try:\n",
					"    LoggingUtil().log_info(\"Starting insertion of new inspector specialisms records\")\n",
					"    \n",
					"    # First get count of potential new records\n",
					"    new_records_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"    LEFT OUTER JOIN odw_harmonised_db.sap_hr_inspector_Specialisms tarSpe\n",
					"        ON souSpe.StaffNumber = tarSpe.StaffNumber\n",
					"        AND tarSpe.Current = 1\n",
					"        AND souSpe.QualificationName = tarSpe.QualificationName\n",
					"    WHERE tarSpe.StaffNumber IS NULL\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    LoggingUtil().log_info(f\"Found {new_records_count} new inspector specialisms to insert\")\n",
					"    \n",
					"    # Insert new records that don't exist in the target table\n",
					"    LoggingUtil().log_info(\"Inserting new records into sap_hr_inspector_Specialisms\")\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.sap_hr_inspector_Specialisms (\n",
					"        StaffNumber, \n",
					"        Firstname, \n",
					"        Lastname, \n",
					"        QualificationName, \n",
					"        Proficien, \n",
					"        SourceSystemID, \n",
					"        IngestionDate, \n",
					"        ValidFrom, \n",
					"        ValidTo, \n",
					"        Current, \n",
					"        RowID, \n",
					"        IsActive, \n",
					"        LastUpdated\n",
					"    )\n",
					"    SELECT \n",
					"        souSpe.StaffNumber, \n",
					"        souSpe.Firstname, \n",
					"        souSpe.Lastname, \n",
					"        souSpe.QualificationName, \n",
					"        souSpe.Proficien, \n",
					"        'saphr', \n",
					"        current_date(), \n",
					"        current_date(), \n",
					"        '9999-12-31', \n",
					"        1, \n",
					"        souSpe.RowID, \n",
					"        'Y', \n",
					"        current_date()\n",
					"    FROM odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"    LEFT OUTER JOIN odw_harmonised_db.sap_hr_inspector_Specialisms tarSpe\n",
					"        ON souSpe.StaffNumber = tarSpe.StaffNumber\n",
					"        AND tarSpe.Current = 1\n",
					"        AND souSpe.QualificationName = tarSpe.QualificationName\n",
					"    WHERE tarSpe.StaffNumber IS NULL\n",
					"    \"\"\")\n",
					"    \n",
					"    # Verify the insertion was successful\n",
					"    actual_inserted = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count \n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE IngestionDate = current_date()\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    LoggingUtil().log_info(f\"Successfully inserted {actual_inserted} new inspector specialisms records\")\n",
					"    \n",
					"    if actual_inserted != new_records_count:\n",
					"        LoggingUtil().log_info(f\"Note: Expected to insert {new_records_count} records but actually inserted {actual_inserted}\")\n",
					"    \n",
					"    LoggingUtil().log_info(\"Inspector specialisms insertion completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail\n",
					"    logError(f\"Error inserting inspector specialisms records: {str(e)}\")\n",
					"    logException(e)\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    LoggingUtil().log_info(\"Flushing logs\")\n",
					"    flushLogging()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"try:\n",
					"    LoggingUtil().log_info(\"Starting update of obsolete inspector specialisms records\")\n",
					"    \n",
					"    # First, let's check for potential duplicates in the source data\n",
					"    LoggingUtil().log_info(\"Checking for duplicate records in source data\")\n",
					"    duplicate_check = spark.sql(\"\"\"\n",
					"    SELECT \n",
					"        StaffNumber, \n",
					"        QualificationName,\n",
					"        COUNT(*) as duplicate_count\n",
					"    FROM (\n",
					"        SELECT \n",
					"            oldSpe.StaffNumber, \n",
					"            oldSpe.QualificationName\n",
					"        FROM \n",
					"            odw_harmonised_db.sap_hr_inspector_Specialisms oldSpe\n",
					"        LEFT OUTER JOIN \n",
					"            odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"        ON \n",
					"            oldSpe.StaffNumber = souSpe.StaffNumber\n",
					"            AND oldSpe.QualificationName = souSpe.QualificationName\n",
					"        WHERE \n",
					"            souSpe.StaffNumber IS NULL\n",
					"            AND oldSpe.Current = 1\n",
					"    ) source_data\n",
					"    GROUP BY StaffNumber, QualificationName\n",
					"    HAVING COUNT(*) > 1\n",
					"    ORDER BY duplicate_count DESC\n",
					"    \"\"\")\n",
					"    \n",
					"    duplicate_count = duplicate_check.count()\n",
					"    if duplicate_count > 0:\n",
					"        LoggingUtil().log_info(f\"Found {duplicate_count} duplicate combinations in source data - using DISTINCT to resolve\")\n",
					"        # Show some examples of duplicates for debugging\n",
					"        duplicate_examples = duplicate_check.limit(5).collect()\n",
					"        for row in duplicate_examples:\n",
					"            LoggingUtil().log_info(f\"Duplicate: StaffNumber={row['StaffNumber']}, QualificationName={row['QualificationName']}, Count={row['duplicate_count']}\")\n",
					"    else:\n",
					"        LoggingUtil().log_info(\"No duplicates found in source data\")\n",
					"    \n",
					"    # Get count of records to be updated (using DISTINCT to match our MERGE logic)\n",
					"    LoggingUtil().log_info(\"Counting obsolete inspector specialisms records to update\")\n",
					"    obsolete_records_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM (\n",
					"        SELECT DISTINCT\n",
					"            oldSpe.StaffNumber, \n",
					"            oldSpe.QualificationName\n",
					"        FROM \n",
					"            odw_harmonised_db.sap_hr_inspector_Specialisms oldSpe\n",
					"        LEFT OUTER JOIN \n",
					"            odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"        ON \n",
					"            oldSpe.StaffNumber = souSpe.StaffNumber\n",
					"            AND oldSpe.QualificationName = souSpe.QualificationName\n",
					"        WHERE \n",
					"            souSpe.StaffNumber IS NULL\n",
					"            AND oldSpe.Current = 1\n",
					"    ) distinct_obsolete\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    LoggingUtil().log_info(f\"Found {obsolete_records_count} obsolete inspector specialisms to update\")\n",
					"    \n",
					"    if obsolete_records_count == 0:\n",
					"        LoggingUtil().log_info(\"No obsolete records found to update. Skipping MERGE operation.\")\n",
					"    else:\n",
					"        # Update records that are no longer present in the source data\n",
					"        LoggingUtil().log_info(\"Updating obsolete records in sap_hr_inspector_Specialisms using MERGE with DISTINCT source\")\n",
					"        \n",
					"        merge_result = spark.sql(\"\"\"\n",
					"        MERGE INTO odw_harmonised_db.sap_hr_inspector_Specialisms AS target\n",
					"        USING (\n",
					"            SELECT DISTINCT\n",
					"                oldSpe.StaffNumber, \n",
					"                oldSpe.QualificationName\n",
					"            FROM \n",
					"                odw_harmonised_db.sap_hr_inspector_Specialisms oldSpe\n",
					"            LEFT OUTER JOIN \n",
					"                odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"            ON \n",
					"                oldSpe.StaffNumber = souSpe.StaffNumber\n",
					"                AND oldSpe.QualificationName = souSpe.QualificationName\n",
					"            WHERE \n",
					"                souSpe.StaffNumber IS NULL\n",
					"                AND oldSpe.Current = 1\n",
					"        ) AS source\n",
					"        ON \n",
					"            target.StaffNumber = source.StaffNumber\n",
					"            AND target.QualificationName = source.QualificationName\n",
					"            AND target.Current = 1\n",
					"        WHEN MATCHED THEN\n",
					"        UPDATE SET \n",
					"            target.Current = 0,\n",
					"            target.ValidTo = CURRENT_DATE()\n",
					"        \"\"\")\n",
					"        \n",
					"        LoggingUtil().log_info(\"MERGE operation completed successfully\")\n",
					"        \n",
					"        # Verify the update was successful\n",
					"        LoggingUtil().log_info(\"Verifying update results\")\n",
					"        updated_records = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as count \n",
					"        FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"        WHERE ValidTo = CURRENT_DATE() AND Current = 0\n",
					"        \"\"\").collect()[0]['count']\n",
					"        \n",
					"        LoggingUtil().log_info(f\"Total records now marked as obsolete today: {updated_records}\")\n",
					"        \n",
					"        # Additional verification - check if the expected records were updated\n",
					"        remaining_obsolete = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as count\n",
					"        FROM (\n",
					"            SELECT DISTINCT\n",
					"                oldSpe.StaffNumber, \n",
					"                oldSpe.QualificationName\n",
					"            FROM \n",
					"                odw_harmonised_db.sap_hr_inspector_Specialisms oldSpe\n",
					"            LEFT OUTER JOIN \n",
					"                odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"            ON \n",
					"                oldSpe.StaffNumber = souSpe.StaffNumber\n",
					"                AND oldSpe.QualificationName = souSpe.QualificationName\n",
					"            WHERE \n",
					"                souSpe.StaffNumber IS NULL\n",
					"                AND oldSpe.Current = 1\n",
					"        ) still_current_obsolete\n",
					"        \"\"\").collect()[0]['count']\n",
					"        \n",
					"        if remaining_obsolete == 0:\n",
					"            LoggingUtil().log_info(\"✓ All expected obsolete records have been successfully updated\")\n",
					"        else:\n",
					"            logError(f\"⚠ Warning: {remaining_obsolete} obsolete records are still marked as Current = 1\")\n",
					"        \n",
					"        # Log summary statistics\n",
					"        total_current_records = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as count \n",
					"        FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"        WHERE Current = 1\n",
					"        \"\"\").collect()[0]['count']\n",
					"        \n",
					"        total_obsolete_records = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as count \n",
					"        FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"        WHERE Current = 0\n",
					"        \"\"\").collect()[0]['count']\n",
					"        \n",
					"        LoggingUtil().log_info(f\"Final state - Current records: {total_current_records}, Obsolete records: {total_obsolete_records}\")\n",
					"    \n",
					"    LoggingUtil().log_info(\"Obsolete inspector specialisms update completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail with context\n",
					"    error_msg = f\"Error updating obsolete inspector specialisms records: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    logError(f\"Exception type: {type(e).__name__}\")\n",
					"    \n",
					"    # If it's a Spark SQL exception, try to extract more meaningful error info\n",
					"    if hasattr(e, 'java_exception') and e.java_exception is not None:\n",
					"        java_exception = str(e.java_exception)\n",
					"        if \"DeltaUnsupportedOperationException\" in java_exception:\n",
					"            logError(\"This appears to be a Delta Lake MERGE conflict error\")\n",
					"            logError(\"Possible causes: duplicate source rows, concurrent operations, or schema issues\")\n",
					"        elif \"multipleSourceRowMatchingTargetRowInMergeException\" in java_exception:\n",
					"            logError(\"Multiple source rows are matching the same target row - source data needs deduplication\")\n",
					"    \n",
					"    # Log the full stack trace for debugging\n",
					"    logException(e)\n",
					"    \n",
					"    # Provide recovery suggestions\n",
					"\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"    \n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    LoggingUtil().log_info(\"Flushing logs and cleaning up\")\n",
					"    flushLogging()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"try:\n",
					"    LoggingUtil().log_info(\"Starting insertion of changed inspector specialisms records\")\n",
					"    \n",
					"    # First get count of changed records to be inserted\n",
					"    changed_records_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"    INNER JOIN odw_harmonised_db.sap_hr_inspector_Specialisms tarSpe\n",
					"        ON souSpe.StaffNumber = tarSpe.StaffNumber\n",
					"        AND tarSpe.Current = 1\n",
					"        AND souSpe.QualificationName = tarSpe.QualificationName\n",
					"    WHERE tarSpe.RowID <> souSpe.RowID\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    LoggingUtil().log_info(f\"Found {changed_records_count} changed inspector specialisms to insert\")\n",
					"    \n",
					"    # Insert new versions of records that have changed (different RowID)\n",
					"    LoggingUtil().log_info(\"Inserting changed records into sap_hr_inspector_Specialisms\")\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.sap_hr_inspector_Specialisms (\n",
					"        StaffNumber, \n",
					"        Firstname, \n",
					"        Lastname, \n",
					"        QualificationName, \n",
					"        Proficien, \n",
					"        SourceSystemID, \n",
					"        IngestionDate, \n",
					"        ValidFrom, \n",
					"        ValidTo, \n",
					"        Current, \n",
					"        RowID, \n",
					"        IsActive, \n",
					"        LastUpdated\n",
					"    )\n",
					"    SELECT \n",
					"        souSpe.StaffNumber, \n",
					"        souSpe.Firstname, \n",
					"        souSpe.Lastname, \n",
					"        souSpe.QualificationName, \n",
					"        souSpe.Proficien, \n",
					"        'saphr', \n",
					"        current_date(), \n",
					"        current_date(), \n",
					"        '9999-12-31', \n",
					"        1, \n",
					"        souSpe.RowID, \n",
					"        'Y', \n",
					"        current_date()\n",
					"    FROM odw_harmonised_db.transform_inspector_Specialisms souSpe\n",
					"    LEFT OUTER JOIN odw_harmonised_db.sap_hr_inspector_Specialisms tarSpe\n",
					"        ON souSpe.StaffNumber = tarSpe.StaffNumber\n",
					"        AND tarSpe.Current = 1\n",
					"        AND souSpe.QualificationName = tarSpe.QualificationName\n",
					"    WHERE tarSpe.RowID <> souSpe.RowID\n",
					"    \"\"\")\n",
					"    \n",
					"    # Verify the insertion was successful\n",
					"    inserted_records = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count \n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE IngestionDate = CURRENT_DATE() AND Current = 1\n",
					"    \"\"\").collect()[0]['count'] - spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count \n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE ValidFrom = CURRENT_DATE() AND Current = 1\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    LoggingUtil().log_info(f\"Successfully inserted {inserted_records} changed inspector specialisms records\")\n",
					"    \n",
					"    if inserted_records != changed_records_count:\n",
					"        LoggingUtil().log_info(f\"Note: Expected to insert {changed_records_count} records but actual count differs\")\n",
					"    \n",
					"    LoggingUtil().log_info(\"Changed inspector specialisms insertion completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Log the exception in detail\n",
					"    logError(f\"Error inserting changed inspector specialisms records: {str(e)}\")\n",
					"    logException(e)\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    LoggingUtil().log_info(\"Flushing logs\")\n",
					"    flushLogging()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"import json\n",
					"\n",
					"# Initialize result dictionary\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"try:\n",
					"    LoggingUtil().log_info(\"Starting enhanced MERGE operation for inspector specialisms with comprehensive record tracking\")\n",
					"    \n",
					"    # Get baseline counts before any operations\n",
					"    LoggingUtil().log_info(\"Capturing baseline record counts\")\n",
					"    baseline_current = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE Current = 1\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    baseline_obsolete = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE Current = 0\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    LoggingUtil().log_info(f\"Baseline - Current records: {baseline_current}, Obsolete records: {baseline_obsolete}\")\n",
					"    \n",
					"    # Check for duplicates in the source data that could cause MERGE conflicts\n",
					"    LoggingUtil().log_info(\"Checking for duplicate records in source data\")\n",
					"    duplicate_records = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM (\n",
					"        SELECT \n",
					"            StaffNumber,\n",
					"            QualificationName,\n",
					"            COUNT(*) as duplicate_count\n",
					"        FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"        WHERE Current = 1\n",
					"        AND ValidFrom = current_date()\n",
					"        GROUP BY StaffNumber, QualificationName\n",
					"        HAVING COUNT(*) > 1\n",
					"    ) duplicates\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    if duplicate_records > 0:\n",
					"        LoggingUtil().log_info(f\"Found {duplicate_records} duplicate combinations in source data - using ROW_NUMBER() to resolve\")\n",
					"    else:\n",
					"        LoggingUtil().log_info(\"No duplicates found in source data\")\n",
					"    \n",
					"    # Get count of records to be updated (current records with today's date)\n",
					"    records_to_update_count = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE Current = 1\n",
					"    AND ValidFrom = current_date()\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    LoggingUtil().log_info(f\"Found {records_to_update_count} current inspector specialisms records to potentially update\")\n",
					"    \n",
					"    if records_to_update_count == 0:\n",
					"        LoggingUtil().log_info(\"No records found to update. Skipping MERGE operation.\")\n",
					"        records_updated_in_operation = 0\n",
					"    else:\n",
					"        # Get count of records that were already updated today (before this operation)\n",
					"        pre_operation_today_count = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as count \n",
					"        FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"        WHERE Current = 0 AND ValidTo = CURRENT_DATE()\n",
					"        \"\"\").collect()[0]['count']\n",
					"        \n",
					"        LoggingUtil().log_info(f\"Records already marked obsolete today before operation: {pre_operation_today_count}\")\n",
					"        \n",
					"        # Perform the MERGE operation\n",
					"        LoggingUtil().log_info(\"Performing MERGE operation with CTE deduplication\")\n",
					"        spark.sql(\"\"\"\n",
					"        -- Step 1: Deduplicate the source table (newSpe)\n",
					"        WITH DeduplicatedSource AS (\n",
					"            SELECT \n",
					"                StaffNumber,\n",
					"                QualificationName,\n",
					"                ValidFrom,\n",
					"                Current,\n",
					"                ROW_NUMBER() OVER (\n",
					"                    PARTITION BY StaffNumber, QualificationName \n",
					"                    ORDER BY ValidFrom DESC\n",
					"                ) AS row_num\n",
					"            FROM \n",
					"                odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"            WHERE \n",
					"                Current = 1\n",
					"                AND ValidFrom = current_date()\n",
					"        )\n",
					"\n",
					"        -- Step 2: Perform the MERGE operation using the deduplicated source\n",
					"        MERGE INTO odw_harmonised_db.sap_hr_inspector_Specialisms AS oldSpe\n",
					"        USING DeduplicatedSource AS newSpe\n",
					"        ON \n",
					"            oldSpe.StaffNumber = newSpe.StaffNumber\n",
					"            AND oldSpe.QualificationName = newSpe.QualificationName\n",
					"            AND oldSpe.Current = 1\n",
					"            AND oldSpe.ValidFrom < current_date()\n",
					"            AND newSpe.row_num = 1 -- Ensure only one row per combination\n",
					"        WHEN MATCHED THEN\n",
					"        UPDATE SET \n",
					"            oldSpe.Current = 0,\n",
					"            oldSpe.ValidTo = current_date()\n",
					"        \"\"\")\n",
					"        \n",
					"        LoggingUtil().log_info(\"MERGE operation completed successfully\")\n",
					"        \n",
					"        # Get post-operation counts\n",
					"        post_operation_today_count = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as count \n",
					"        FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"        WHERE Current = 0 AND ValidTo = CURRENT_DATE()\n",
					"        \"\"\").collect()[0]['count']\n",
					"        \n",
					"        # Calculate records updated in this specific operation\n",
					"        records_updated_in_operation = post_operation_today_count - pre_operation_today_count\n",
					"        \n",
					"        LoggingUtil().log_info(f\"Records updated in this operation: {records_updated_in_operation}\")\n",
					"        LoggingUtil().log_info(f\"Total records marked obsolete today: {post_operation_today_count}\")\n",
					"        \n",
					"        # Verify the update count matches expectations\n",
					"        if records_updated_in_operation != records_to_update_count:\n",
					"            # This might be expected if not all current records have matching older records\n",
					"            LoggingUtil().log_info(f\"Note: Expected to potentially update {records_to_update_count} records but actually updated {records_updated_in_operation}\")\n",
					"            LoggingUtil().log_info(\"This is normal if some current records don't have corresponding older records to update\")\n",
					"        else:\n",
					"            LoggingUtil().log_info(\"✓ Update count matches expected count\")\n",
					"    \n",
					"    # Get final record counts for comprehensive tracking\n",
					"    final_current = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE Current = 1\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    final_obsolete = spark.sql(\"\"\"\n",
					"    SELECT COUNT(*) as count\n",
					"    FROM odw_harmonised_db.sap_hr_inspector_Specialisms\n",
					"    WHERE Current = 0\n",
					"    \"\"\").collect()[0]['count']\n",
					"    \n",
					"    # Set record_count to total records in the table (standard metric for tracking)\n",
					"    total_record_count = final_current + final_obsolete\n",
					"    result[\"record_count\"] = total_record_count\n",
					"    \n",
					"    LoggingUtil().log_info(f\"Total record count in table: {total_record_count}\")\n",
					"    \n",
					"    # Validate total record count consistency\n",
					"    baseline_total = baseline_current + baseline_obsolete\n",
					"    final_total = final_current + final_obsolete\n",
					"    \n",
					"    if baseline_total != final_total:\n",
					"        warning_msg = f\"Warning: Total record count changed from {baseline_total} to {final_total}\"\n",
					"        LoggingUtil().log_info(warning_msg)\n",
					"        result[\"status\"] = \"warning\"\n",
					"    \n",
					"    LoggingUtil().log_info(f\"Final state - Current records: {final_current}, Obsolete records: {final_obsolete}\")\n",
					"    LoggingUtil().log_info(f\"Net change - Current: {final_current - baseline_current}, Obsolete: {final_obsolete - baseline_obsolete}\")\n",
					"    \n",
					"    LoggingUtil().log_info(\"Enhanced MERGE operation for inspector specialisms completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Capture error information and limit to 300 characters\n",
					"    error_msg = f\"Error during MERGE operation for inspector specialisms: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    \n",
					"    # Enhanced error context for common issues\n",
					"    if \"DeltaUnsupportedOperationException\" in str(e):\n",
					"        logError(\"Delta Lake MERGE conflict detected - check for duplicate source rows\")\n",
					"        error_msg += \" [Delta MERGE conflict]\"\n",
					"    elif \"AnalysisException\" in str(e):\n",
					"        logError(\"SQL analysis error - check table structure and column references\")\n",
					"        error_msg += \" [SQL analysis error]\"\n",
					"    elif \"multipleSourceRowMatchingTargetRowInMergeException\" in str(e):\n",
					"        logError(\"Multiple source rows matching target - deduplication logic may need adjustment\")\n",
					"        error_msg += \" [Multiple source matches]\"\n",
					"    \n",
					"    logException(e)\n",
					"    \n",
					"    # Update result for error case\n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = error_msg[:300]  # Truncate to 300 characters\n",
					"    result[\"record_count\"] = -1  # Indicate failure with -1 count\n",
					"    \n",
					"    # Log recovery suggestions\n",
					"\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    LoggingUtil().log_info(\"Flushing logs and preparing final result\")\n",
					"    flushLogging()\n",
					"    \n",
					"    # Log the final result for debugging\n",
					"    LoggingUtil().log_info(f\"Final result summary:\")\n",
					"    LoggingUtil().log_info(f\"  Status: {result['status']}\")\n",
					"    LoggingUtil().log_info(f\"  Total record count: {result['record_count']}\")\n",
					"    LoggingUtil().log_info(f\"  Error message: {result['error_message']}\")\n",
					"    \n",
					"    # Output the result as JSON for ADF to capture\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": null
			}
		]
	}
}