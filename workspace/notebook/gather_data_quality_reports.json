{
	"name": "gather_data_quality_reports",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "c9658474-31d0-4621-88cb-360d72b3fd88"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from notebookutils import mssparkutils\n",
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false,
					"tags": [
						"parameters"
					]
				},
				"source": [
					"delta_lake_container = 'abfss://odw-standardised@' + storage_account\r\n",
					"staging_container = 'abfss://odw-workspace@' + storage_account\r\n",
					"delta_lake_folder = 'sap-load-tables'\r\n",
					"delta_lake_table_name = 'sap-hr'\r\n",
					"delta_table_path = delta_lake_container + '/' + delta_lake_folder + '/' + delta_lake_table_name"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"deltatableDF=spark.read.format('delta').load(delta_table_path)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pydeequ\r\n",
					"from pydeequ.analyzers import *\r\n",
					"\r\n",
					"analysisResult = AnalysisRunner(spark).onData(deltatableDF).addAnalyzer(Size()).addAnalyzer(Completeness(\"b\")).run()\r\n",
					"\r\n",
					"analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)\r\n",
					"analysisResult_df.show()\r\n",
					""
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pydeequ.profiles import *\r\n",
					"\r\n",
					"result = ColumnProfilerRunner(spark) \\\r\n",
					"    .onData(deltatableDF) \\\r\n",
					"    .run()\r\n",
					"\r\n",
					"for col, profile in result.profiles.items():\r\n",
					"    print(profile)"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pydeequ.checks import *\r\n",
					"from pydeequ.verification import *\r\n",
					"\r\n",
					"check = Check(spark, CheckLevel.Warning, \"Review Check\")\r\n",
					"\r\n",
					"checkResult = VerificationSuite(spark) \\\r\n",
					"    .onData(deltatableDF) \\\r\n",
					"    .addCheck(\r\n",
					"        check.hasSize(lambda x: x >= 3) \\\r\n",
					"        .hasMin(\"b\", lambda x: x == 0) \\\r\n",
					"        .isComplete(\"c\")  \\\r\n",
					"        .isUnique(\"a\")  \\\r\n",
					"        .isContainedIn(\"a\", [\"foo\", \"bar\", \"baz\"]) \\\r\n",
					"        .isNonNegative(\"b\")) \\\r\n",
					"    .run()\r\n",
					"\r\n",
					"checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\r\n",
					"checkResult_df.show()"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pydeequ.repository import *\r\n",
					"from pydeequ.analyzers import *\r\n",
					"\r\n",
					"metrics_file = FileSystemMetricsRepository.helper_metrics_file(spark, 'metrics.json')\r\n",
					"repository = FileSystemMetricsRepository(spark, metrics_file)\r\n",
					"key_tags = {'tag': 'pydeequ hello world'}\r\n",
					"resultKey = ResultKey(spark, ResultKey.current_milli_time(), key_tags)\r\n",
					"\r\n",
					"analysisResult = AnalysisRunner(spark) \\\r\n",
					"    .onData(deltatableDF) \\\r\n",
					"    .addAnalyzer(ApproxCountDistinct('b')) \\\r\n",
					"    .useRepository(repository) \\\r\n",
					"    .saveOrAppendResult(resultKey) \\\r\n",
					"    .run()"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"result_metrep_df = repository.load() \\\r\n",
					"    .before(ResultKey.current_milli_time()) \\\r\n",
					"    .forAnalyzers([ApproxCountDistinct('b')]) \\\r\n",
					"    .getSuccessMetricsAsDataFrame()"
				],
				"execution_count": 26
			}
		]
	}
}