{
	"name": "gather_data_quality_reports",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinsodwspark",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "f513ace3-daea-4cad-83ee-4874e0d10f25"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-odw-data-dev-rg/providers/Microsoft.Synapse/workspaces/pins-odw-data-dev-syn-ws/bigDataPools/pinsodwspark",
				"name": "pinsodwspark",
				"type": "Spark",
				"endpoint": "https://pins-odw-data-dev-syn-ws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinsodwspark",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"delta_lake_container = 'abfss://standardised@dlsbdpdatalake.dfs.core.windows.net'\r\n",
					"staging_container = 'abfss://landing@dlsbdpdatalake.dfs.core.windows.net'\r\n",
					"delta_lake_folder = 'standardised'\r\n",
					"delta_lake_table_name = 'sap_hr'\r\n",
					"delta_table_path = delta_lake_container + '/' + delta_lake_folder + '/' + delta_lake_table_name\r\n",
					"deltatableDF=spark.read.format('delta').load(delta_table_path)"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pydeequ\r\n",
					"from pydeequ.analyzers import *\r\n",
					"\r\n",
					"analysisResult = AnalysisRunner(spark).onData(deltatableDF).addAnalyzer(Size()).addAnalyzer(Completeness(\"b\")).run()\r\n",
					"\r\n",
					"analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)\r\n",
					"analysisResult_df.show()\r\n",
					""
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pydeequ.profiles import *\r\n",
					"\r\n",
					"result = ColumnProfilerRunner(spark) \\\r\n",
					"    .onData(deltatableDF) \\\r\n",
					"    .run()\r\n",
					"\r\n",
					"for col, profile in result.profiles.items():\r\n",
					"    print(profile)"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pydeequ.checks import *\r\n",
					"from pydeequ.verification import *\r\n",
					"\r\n",
					"check = Check(spark, CheckLevel.Warning, \"Review Check\")\r\n",
					"\r\n",
					"checkResult = VerificationSuite(spark) \\\r\n",
					"    .onData(deltatableDF) \\\r\n",
					"    .addCheck(\r\n",
					"        check.hasSize(lambda x: x >= 3) \\\r\n",
					"        .hasMin(\"b\", lambda x: x == 0) \\\r\n",
					"        .isComplete(\"c\")  \\\r\n",
					"        .isUnique(\"a\")  \\\r\n",
					"        .isContainedIn(\"a\", [\"foo\", \"bar\", \"baz\"]) \\\r\n",
					"        .isNonNegative(\"b\")) \\\r\n",
					"    .run()\r\n",
					"\r\n",
					"checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\r\n",
					"checkResult_df.show()"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pydeequ.repository import *\r\n",
					"from pydeequ.analyzers import *\r\n",
					"\r\n",
					"metrics_file = FileSystemMetricsRepository.helper_metrics_file(spark, 'metrics.json')\r\n",
					"repository = FileSystemMetricsRepository(spark, metrics_file)\r\n",
					"key_tags = {'tag': 'pydeequ hello world'}\r\n",
					"resultKey = ResultKey(spark, ResultKey.current_milli_time(), key_tags)\r\n",
					"\r\n",
					"analysisResult = AnalysisRunner(spark) \\\r\n",
					"    .onData(deltatableDF) \\\r\n",
					"    .addAnalyzer(ApproxCountDistinct('b')) \\\r\n",
					"    .useRepository(repository) \\\r\n",
					"    .saveOrAppendResult(resultKey) \\\r\n",
					"    .run()"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"result_metrep_df = repository.load() \\\r\n",
					"    .before(ResultKey.current_milli_time()) \\\r\n",
					"    .forAnalyzers([ApproxCountDistinct('b')]) \\\r\n",
					"    .getSuccessMetricsAsDataFrame()"
				],
				"execution_count": 26
			}
		]
	}
}