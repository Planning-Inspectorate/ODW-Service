{
	"name": "py_utils_curated_validate_general",
	"properties": {
		"description": "Validation notebook for validating curated layer tables against the json schema.",
		"folder": {
			"name": "archive/utils/data-validation"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "7086f55b-b5b3-41a2-b7f9-916a4b79028a"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 32,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Validation notebook for validating curated layer tables against the json schema\r\n",
					"\r\n",
					"#### NB: PLEASE ATTACH TO THE SPARK POOL \"pinssynspodw34\" AS THIS IS RUNNING PYTHON 3.10 WHICH IS NEEDED"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Install packages"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%pip install --quiet --upgrade pip\r\n",
					"%pip install --quiet jsonschema==4.20.0 iso8601==2.1.0 git+https://github.com/Planning-Inspectorate/data-model@main#egg=pins_data_model"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Imports"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pandas as pd\r\n",
					"import numpy as np\r\n",
					"import pyarrow as pa\r\n",
					"import json\r\n",
					"from jsonschema import validate, FormatChecker, Draft202012Validator\r\n",
					"from iso8601 import parse_date, ParseError\r\n",
					"from pins_data_model import load_schemas\r\n",
					"from notebookutils import mssparkutils\r\n",
					"from pyspark.sql.types import StructType,ArrayType\r\n",
					"from pyspark.sql.functions import col"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"db_name='odw_curated_db'\n",
					"table_name=''\n",
					"entity_name=''\n",
					"primary_key=''"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Function to validate a list of rows of data."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def is_iso8601_date_time(instance):\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"    Function to check if a date matches ISO-8601 format\r\n",
					"    \"\"\"\r\n",
					"    if instance is None:\r\n",
					"        return True\r\n",
					"    try:\r\n",
					"        parse_date(instance)\r\n",
					"        return True\r\n",
					"    except ParseError:\r\n",
					"        return False"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def validate_data(data: list, schema: dict, primary_key: str) -> str:\n",
					"\n",
					"    \"\"\"\n",
					"    Function to validate a list of rows of data.\n",
					"    Validation includes a format check against ISO-8601.\n",
					"\n",
					"    Args:\n",
					"        data: list\n",
					"        schema: dictionary\n",
					"        primary_key: string\n",
					"\n",
					"    Returns:\n",
					"        success: string\n",
					"        \"Success\" or \"Failed\" for the result of the validation\n",
					"    \"\"\"\n",
					"    \n",
					"    format_checker = FormatChecker()\n",
					"    format_checker.checks(\"date-time\")(is_iso8601_date_time)\n",
					"    validator = Draft202012Validator(schema, format_checker=format_checker)\n",
					"\n",
					"    success = \"Success\"\n",
					"\n",
					"    for index, row in enumerate(data, start=1):\n",
					"        errors = list(validator.iter_errors(row))\n",
					"        if errors:\n",
					"            success = \"Failed\"\n",
					"            print(f\"{len(errors)} error(s) found in row {index}\")\n",
					"            key_value = row.get(primary_key)\n",
					"            print(f\"{primary_key}: {key_value}\")\n",
					"            for error in errors:\n",
					"                print(error.message)\n",
					"                print(\"-\"*100)\n",
					"            print(\"#\"*100)\n",
					"            print(\"#\"*100)\n",
					"\n",
					"    return success"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Define constants"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Get variables from schema_file\r\n",
					"SCHEMAS = load_schemas.load_all_schemas()[\"schemas\"]\r\n",
					"SCHEMA = SCHEMAS[f\"{entity_name}.schema.json\"]\r\n",
					"TABLE = table_name\r\n",
					"DATABASE = db_name\r\n",
					"PRIMARY_KEY = primary_key\r\n",
					"QUERY = f\"SELECT * FROM `{DATABASE}`.`{TABLE}`\"\r\n",
					"OUTPUT_FILE = f\"{TABLE}.txt\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Create dataframe of the data from the table\r\n",
					"\r\n",
					"For certain tables it may be necessary to rename a column or two in order to create the array and avoid duplicate field names.\r\n",
					"\r\n",
					"It's also necessary to convert None and NaT values to empty strings. A NULL value in SQL is shown as None for strings or NaT for timestamps in pandas so converting to an empty string will match the expected schema as we're validating using a pandas dataframe.\r\n",
					"\r\n",
					"The replace() function is used to convert NaT values to None and then these are converted to empty strings using fillna()."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"spark.sql(f\"REFRESH TABLE {DATABASE}.{TABLE}\")\r\n",
					"df = spark.sql(QUERY)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def find_array_columns(df):\r\n",
					"    array_columns = []\r\n",
					"    struct_array_columns = []\r\n",
					"\r\n",
					"    for field in df.schema.fields:\r\n",
					"        if isinstance(field.dataType, ArrayType):  # Check if it's an ArrayType\r\n",
					"            array_columns.append(field.name)\r\n",
					"            if isinstance(field.dataType.elementType, StructType):  # Check if the array contains StructType\r\n",
					"                struct_array_columns.append(field.name)\r\n",
					"    \r\n",
					"    return array_columns, struct_array_columns\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"array_columns, struct_array_columns =  find_array_columns(df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def flatten_struct(df):\r\n",
					"    # List to hold column names for the flattened DataFrame\r\n",
					"    flat_columns = []\r\n",
					"    \r\n",
					"    # Iterate over the schema to find struct fields\r\n",
					"    for field in df.schema.fields:\r\n",
					"        if isinstance(field.dataType, StructType):\r\n",
					"            # For each struct type field, flatten it\r\n",
					"            for sub_field in field.dataType.fields:\r\n",
					"                flat_columns.append(col(f\"{field.name}.{sub_field.name}\").alias(f\"{field.name}_{sub_field.name}\"))\r\n",
					"        else:\r\n",
					"            # For non-struct fields, just add them as they are\r\n",
					"            # flat_columns.append(col(field.name))\r\n",
					"            flat_columns.append(field.name)\r\n",
					"    \r\n",
					"    # Return the DataFrame with flattened struct columns\r\n",
					"    return df.select(*flat_columns)\r\n",
					"\r\n",
					"# Example usage\r\n",
					"df_flat = flatten_struct(df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"data = df_flat.toPandas()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Replace the Nat and np.nan values\r\n",
					"for column in data:\r\n",
					"    data[column].replace({pd.NaT: None, np.nan: None},inplace=True)\r\n",
					"\r\n",
					"# Make sure all array columns are in defined as lists\r\n",
					"for col in array_columns:\r\n",
					"    data[col] = data[col].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\r\n",
					"\r\n",
					"json_data = data.to_dict(orient='records')"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Validate against the schema\r\n",
					"\r\n",
					"%%capture magic command is used to capture the cell output and assign it to the variable \"output\". This can then be used later as the data to be written to file.\r\n",
					"\r\n",
					"The validate variable can be used and returned as the exit value of the notebook."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%%capture output\r\n",
					"\r\n",
					"validate = validate_data(data=json_data, \r\n",
					"                        schema=SCHEMA,\r\n",
					"                        primary_key = PRIMARY_KEY)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Print the validation output\r\n",
					"\r\n",
					"Output is sent to storage so commented out but can be printed here if need be"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"result = output.stdout\r\n",
					"print(result)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Set notebook exit value to the validate variable above\r\n",
					"\r\n",
					"Success for successful validation\r\n",
					"Failed for faield validation"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"mssparkutils.notebook.exit(validate)"
				],
				"execution_count": null
			}
		]
	}
}