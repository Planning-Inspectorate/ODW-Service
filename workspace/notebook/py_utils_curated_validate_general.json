{
	"name": "py_utils_curated_validate_general",
	"properties": {
		"description": "Validation notebook for validating curated layer tables against the json schema.",
		"folder": {
			"name": "utils/data-validation"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "37729d70-3de2-4dce-aa30-60774d404f16"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Validation notebook for validating curated layer tables against the json schema\r\n",
					"\r\n",
					"#### NB: PLEASE ATTACH TO THE SPARK POOL \"PINSSYNSPODW\" AS THIS IS RUNNING PYTHON 3.10 WHICH IS NEEDED"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Install packages"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%pip install --quiet --upgrade pip\r\n",
					"%pip install --quiet jsonschema==4.20.0 iso8601==2.1.0 git+https://github.com/Planning-Inspectorate/data-model@main#egg=pins_data_model"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Imports"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pandas as pd\r\n",
					"import numpy as np\r\n",
					"import json\r\n",
					"from collections import defaultdict\r\n",
					"import pprint\r\n",
					"from jsonschema import validate, FormatChecker, ValidationError, Draft202012Validator\r\n",
					"from iso8601 import parse_date, ParseError\r\n",
					"from pins_data_model import load_schemas"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Function to validate a list of rows of data."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def validate_data(data: list, schema: dict, primary_key: str) -> str:\n",
					"\n",
					"    \"\"\"\n",
					"    Function to validate a list of rows of data.\n",
					"    Validation includes a format check against ISO-8601.\n",
					"\n",
					"    Args:\n",
					"        data: list\n",
					"        schema: dictionary\n",
					"        primary_key: string\n",
					"\n",
					"    Returns:\n",
					"        success: string\n",
					"        \"Success\" or \"Failed\" for the result of the validation\n",
					"    \"\"\"\n",
					"    \n",
					"    format_checker = FormatChecker()\n",
					"    format_checker.checks(\"date-time\")(is_iso8601_date_time)\n",
					"    validator = Draft202012Validator(schema, format_checker=format_checker)\n",
					"\n",
					"    success = \"Success\"\n",
					"\n",
					"    for index, row in enumerate(data, start=1):\n",
					"        errors = list(validator.iter_errors(row))\n",
					"        if errors:\n",
					"            success = \"Failed\"\n",
					"            print(f\"{len(errors)} error(s) found in row {index}\")\n",
					"            key_value = row.get(primary_key)\n",
					"            print(f\"{primary_key}: {key_value}\")\n",
					"            for error in errors:\n",
					"                print(error.message)\n",
					"                print(\"-\"*100)\n",
					"            print(\"#\"*100)\n",
					"            print(\"#\"*100)\n",
					"\n",
					"    return success"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Define constants"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"SCHEMA_DICT = load_schemas.load_all_schemas()[\"schemas\"]\r\n",
					"\r\n",
					"for key in SCHEMA_DICT:\r\n",
					"    # list all the keys\r\n",
					"    SCHEMA_DICT[key] =[SCHEMA_DICT[key]]\r\n",
					"    \r\n",
					"# add in the dictionary the name of the table as well - that could be incorrect eg.  s51-advice entity, nsip_s51_advice\r\n",
					"\r\n",
					"for key in SCHEMA_DICT:\r\n",
					"    SCHEMA_DICT[key].append(f\"{key}\".replace('-', '_').split('.')[0])"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"schema_file = 's51-advice.schema.json'"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"SCHEMA = SCHEMA_DICT[schema_file][0]\r\n",
					"TABLE = SCHEMA_DICT[schema_file][1] \r\n",
					"DATABASE = \"odw_curated_db\"\r\n",
					"QUERY = f\"SELECT * FROM `{DATABASE}`.`{TABLE}`\"\r\n",
					"OUTPUT_FILE = f\"{TABLE}.txt\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"####PREVIOUS VERSION\r\n",
					"\r\n",
					"SCHEMAS = load_schemas.load_all_schemas()[\"schemas\"]\r\n",
					"SCHEMA = SCHEMAS[\"s51-advice.schema.json\"]\r\n",
					"PRIMARY_KEY = \"adviceId\"\r\n",
					"DATABASE = \"odw_curated_db\"\r\n",
					"# TABLE = \"nsip_s51_advice\"\r\n",
					"QUERY = f\"SELECT * FROM `{DATABASE}`.`{TABLE}`\"\r\n",
					"OUTPUT_FILE = f\"{TABLE}.txt\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Create dataframe of the data from the table\r\n",
					"\r\n",
					"For certain tables it may be necessary to rename a column or two in order to create the array and avoid duplicate field names.\r\n",
					"\r\n",
					"It's also necessary to convert None and NaT values to empty strings. A NULL value in SQL is shown as None for strings or NaT for timestamps in pandas so converting to an empty string will match the expected schema as we're validating using a pandas dataframe.\r\n",
					"\r\n",
					"The replace() function is used to convert NaT values to None and then these are converted to empty strings using fillna()."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"spark.sql(f\"REFRESH TABLE {DATABASE}.{TABLE}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df = spark.sql(QUERY)\r\n",
					"data = df.toPandas()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create a list of the columns in the dataFrame that have array dtype\r\n",
					"columns_with_arrays = data.columns[data.iloc[0].map(lambda x: isinstance(x, np.ndarray))].tolist()\r\n",
					"\r\n",
					"#Apply list on the array columns\r\n",
					"\r\n",
					"for col in columns_with_arrays:\r\n",
					"    data[col] = data[col].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# propbaly we can use the line above as well.\r\n",
					"df = spark.sql(QUERY)\r\n",
					"data = df.toPandas()\r\n",
					"for column in data:\r\n",
					"    data[column].replace({pd.NaT: None, np.nan: None},inplace=True)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"####PREVIOUS VERSION\r\n",
					"df = spark.sql(QUERY)\r\n",
					"data = df.toPandas()\r\n",
					"for column in data:\r\n",
					"    data[column].replace({pd.NaT: None, np.nan: None},inplace=True)\r\n",
					"\r\n",
					"data['attachmentIds'] = data['attachmentIds'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\r\n",
					"json_data = data.to_dict(orient='records')"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Validate against the schema\r\n",
					"\r\n",
					"%%capture magic command is used to capture the cell output and assign it to the variable \"output\". This can then be used later as the data to be written to file.\r\n",
					"\r\n",
					"The validate variable can be used and returned as the exit value of the notebook."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%%capture output\r\n",
					"\r\n",
					"validate = validate_data(data=json_data, \r\n",
					"                        schema=SCHEMA,\r\n",
					"                        primary_key = PRIMARY_KEY)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Print the validation output\r\n",
					"\r\n",
					"Output is sent to storage so commented out but can be printed here if need be"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"result = output.stdout\r\n",
					"print(result)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Write output to storage"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"write_file_to_storage(path, OUTPUT_FILE, result)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Set notebook exit value to the validate variable above\r\n",
					"\r\n",
					"Success for successful validation\r\n",
					"Failed for faield validation"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"mssparkutils.notebook.exit(validate)"
				],
				"execution_count": null
			}
		]
	}
}