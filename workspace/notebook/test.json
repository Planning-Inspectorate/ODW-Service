{
	"name": "test",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "3cc91bf0-f17e-4168-b5d4-fb723159c79d"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# delta_table_path = \"abfss://odw-harmonised@pinsstodwdevuks9h80mb.dfs.core.windows.net/sb_s51_advice/\"\n",
					"# from delta.tables import DeltaTable\n",
					"# delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
					"# # Perform the update\n",
					"# delta_table.update(\n",
					"#     condition=\"\"\"\n",
					"#         (adviceId = '986' AND NSIPAdviceID IN ('100000070')) OR\n",
					"#         (adviceId = '987' AND NSIPAdviceID = '100000083')\n",
					"#     \"\"\",\n",
					"#     set={\n",
					"#         \"adviceId\": \"NSIPAdviceID\",\n",
					"#         \"NSIPAdviceID\": \"adviceId\"\n",
					"#     }\n",
					"# )"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					},
					"collapsed": false
				},
				"source": [
					"%%pyspark\n",
					"query = \"\"\"\n",
					"WITH std_filtered AS (\n",
					"    SELECT adviceId, adviceReference, caseId, caseReference, title, titleWelsh,\n",
					"           `from`, agent, method, enquiryDate, enquiryDetails, enquiryDetailsWelsh,\n",
					"           adviceGivenBy, adviceDate, adviceDetails, adviceDetailsWelsh,\n",
					"           status, redactionStatus, attachmentIds\n",
					"    FROM `odw_standardised_db`.`sb_s51_advice`\n",
					"    WHERE message_type in ('Create', 'update', 'Update', 'Publish', 'Unpublish', 'Delete')\n",
					"),\n",
					"hrm AS (\n",
					"    SELECT adviceId, adviceReference, caseId, caseReference, title, titleWelsh,\n",
					"           `from`, agent, method, enquiryDate, enquiryDetails, enquiryDetailsWelsh,\n",
					"           adviceGivenBy, adviceDate, adviceDetails, adviceDetailsWelsh,\n",
					"           status, redactionStatus, attachmentIds\n",
					"    FROM `odw_harmonised_db`.`sb_s51_advice`\n",
					"),\n",
					"missing_in_std AS (\n",
					"    SELECT * FROM hrm\n",
					"    EXCEPT\n",
					"    SELECT * FROM std_filtered\n",
					"),\n",
					"missing_in_hrm AS (\n",
					"    SELECT * FROM std_filtered\n",
					"    EXCEPT\n",
					"    SELECT * FROM hrm\n",
					")\n",
					"SELECT 'Missing in Standardised' AS difference_type, * FROM missing_in_std\n",
					"UNION ALL\n",
					"SELECT 'Missing in Harmonised' AS difference_type, * FROM missing_in_hrm\n",
					"\"\"\"\n",
					"\n",
					"df = spark.sql(query)\n",
					"display(df)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import DataFrame\n",
					"\n",
					"# Define table names as strings\n",
					"std_table_full: str = \"odw_standardised_db.sb_s51_advice\"\n",
					"hrm_table_full: str = \"odw_harmonised_db.sb_s51_advice\"\n",
					"\n",
					"# Load the tables\n",
					"std_df: DataFrame = spark.table(std_table_full)\n",
					"hrm_df: DataFrame = spark.table(hrm_table_full)\n",
					"\n",
					"# Define allowed message types\n",
					"allowed_message_types = ['Create', 'update', 'Update', 'Publish', 'Unpublish', 'Delete']\n",
					"\n",
					"# Filter standardised DataFrame if message_type column exists\n",
					"if \"message_type\" in std_df.columns:\n",
					"    std_df = std_df.filter(std_df.message_type.isin(allowed_message_types))\n",
					"\n",
					"# Count records\n",
					"std_count: int = std_df.count()\n",
					"hrm_count: int = hrm_df.count()\n",
					"\n",
					"# Print counts\n",
					"print(f\"Standardised count: {std_count}\")\n",
					"print(f\"Harmonised count: {hrm_count}\")\n",
					""
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"std_columns = set(std_df.columns)\n",
					"hrm_columns = set(hrm_df.columns)\n",
					"\n",
					"print(\"Columns only in Standardised:\", std_columns - hrm_columns)\n",
					"print(\"Columns only in Harmonised:\", hrm_columns - std_columns)\n",
					""
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"common_columns = list(std_columns & hrm_columns)\n",
					"\n",
					"std_common = std_df.select(common_columns).dropDuplicates()\n",
					"hrm_common = hrm_df.select(common_columns).dropDuplicates()\n",
					"\n",
					"# Records in standardised but not in harmonised\n",
					"diff_std = std_common.subtract(hrm_common)\n",
					"print(f\"Records in Standardised not in Harmonised: {diff_std.count()}\")\n",
					"diff_std.show(truncate=False)\n",
					"\n",
					"# Records in harmonised but not in standardised\n",
					"diff_hrm = hrm_common.subtract(std_common)\n",
					"print(f\"Records in Harmonised not in Standardised: {diff_hrm.count()}\")\n",
					"# diff_hrm.show(truncate=False)\n",
					"display(diff_hrm)\n",
					""
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import functions as F\n",
					"\n",
					"# Check for nulls in key columns\n",
					"std_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in common_columns])\n",
					"hrm_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in common_columns]).show()\n",
					""
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"from pyspark.sql import functions as F\n",
					"\n",
					"allowed_message_types = ['Create', 'update', 'Update', 'Publish', 'Unpublish', 'Delete']\n",
					"std_table_full: str = \"odw_standardised_db.sb_s51_advice\"\n",
					"hrm_table_full: str = \"odw_harmonised_db.sb_s51_advice\"\n",
					"\n",
					"# Load the tables\n",
					"std_df: DataFrame = spark.table(std_table_full)\n",
					"hrm_df: DataFrame = spark.table(hrm_table_full)\n",
					"\n",
					"# Define allowed message types\n",
					"allowed_message_types = ['Create', 'update', 'Update', 'Publish', 'Unpublish', 'Delete']\n",
					"\n",
					"# Filter standardised DataFrame if message_type column exists\n",
					"if \"message_type\" in std_df.columns:\n",
					"    std_df = std_df.filter(std_df.message_type.isin(allowed_message_types))\n",
					"\n",
					"# Group by adviceId and count in each table\n",
					"std_grouped = std_df.groupBy(\"adviceId\").agg(F.count(\"*\").alias(\"std_count\"))\n",
					"hrm_grouped = hrm_df.groupBy(\"adviceId\").agg(F.count(\"*\").alias(\"hrm_count\"))\n",
					"\n",
					"# Join the grouped results on adviceId\n",
					"joined_counts = std_grouped.join(hrm_grouped, on=\"adviceId\", how=\"outer\")\n",
					"\n",
					"# Show differences where counts don't match or one side is missing\n",
					"mismatched = joined_counts.filter(\n",
					"    (F.col(\"std_count\") != F.col(\"hrm_count\")) |\n",
					"    F.col(\"std_count\").isNull() |\n",
					"    F.col(\"hrm_count\").isNull()\n",
					")\n",
					"\n",
					"display(mismatched)\n",
					"# Display mismatched counts\n",
					"# mismatched.show(truncate=False)"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"from pyspark.sql import functions as F\n",
					"\n",
					"# Load the full table\n",
					"df = spark.table(\"odw_standardised_db.sb_s51_advice\")\n",
					"\n",
					"# Group by relevant columns and count\n",
					"grouped_df = df.groupBy(\n",
					"    \"adviceId\",\n",
					"    \"caseReference\",\n",
					"    \"title\",\n",
					"    \"titleWelsh\",\n",
					"    \"from\",\n",
					"    \"agent\",\n",
					"    \"method\",\n",
					"    \"enquiryDate\",\n",
					"    \"enquiryDetails\",\n",
					"    \"enquiryDetailsWelsh\",\n",
					"    \"adviceGivenBy\",\n",
					"    \"adviceDate\",\n",
					"    \"adviceDetails\",\n",
					"    \"adviceDetailsWelsh\",\n",
					"    \"status\",\n",
					"    \"redactionStatus\",\n",
					"    \"attachmentIds\"\n",
					").agg(F.count(\"*\").alias(\"duplicate_count\"))\n",
					"\n",
					"# Filter for duplicates\n",
					"duplicates_df = grouped_df.filter(F.col(\"duplicate_count\") > 1)\n",
					"\n",
					"# Get list of duplicate adviceIds\n",
					"duplicate_ids_df = duplicates_df.select(\"adviceId\").distinct()\n",
					"\n",
					"# Join back to original table to get full records\n",
					"full_duplicate_records = df.join(duplicate_ids_df, on=\"adviceId\", how=\"inner\")\n",
					"\n",
					"# Order by adviceId\n",
					"full_duplicate_records = full_duplicate_records.orderBy(\"adviceId\")\n",
					"\n",
					"# Display the result\n",
					"display(full_duplicate_records)\n",
					""
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"source": [
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"raw_container = \"abfss://odw-raw@\" + storage_account+'ServiceBus / s51-advice/'\n",
					"print(raw_container)"
				],
				"execution_count": 29
			}
		]
	}
}