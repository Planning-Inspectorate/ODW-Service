{
	"name": "py_create_tables_new",
	"properties": {
		"description": "Create Tables from JSON Schema",
		"folder": {
			"name": "odw-harmonised"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "158c7a3c-e96d-4f20-946d-173a9110054b"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": []
				},
				"source": [
					"from notebookutils import mssparkutils\n",
					"storage_account=mssparkutils.notebook.run('/utils/py_utils_get_storage_account')"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"tags": [
						"parameters"
					]
				},
				"source": [
					"harmonised_storage_container = 'abfss://odw-harmonised@' + storage_account\r\n",
					"json_schema_folder='config/json_files/new/'\r\n",
					"harmonised_storage_delta_folder = 'lib'\r\n",
					"delta_lake_database_name = \"odw_harmonised_db\""
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from notebookutils import mssparkutils\n",
					"\n",
					"json_schema_path = harmonised_storage_container + '/' + json_schema_folder\n",
					"\n",
					"print('json_schema_path '+json_schema_path)\n",
					"\n",
					"files = mssparkutils.fs.ls(json_schema_path)\n",
					"\n",
					"json_schema_files = []\n",
					"\n",
					"for fi in files:\n",
					"  json_schema_files.append(fi.name)\n",
					""
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"json_string_list=[]\n",
					"\n",
					"for file_name in json_schema_files:\n",
					"    json_schema_file_path = json_schema_path + '/' + file_name\n",
					"    df=spark.read.text(json_schema_file_path)\n",
					"    table = [x[\"value\"] for x in df.rdd.collect()]\n",
					"    json_string = \"\"\n",
					"\n",
					"    for item in table:\n",
					"        json_string += item\n",
					"\n",
					"    json_string_list.append(json_string)\n",
					""
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.types import StructType, DataType, StructField, StringType, IntegerType\n",
					"from pyspark.sql.types import StructType, StructField, IntegerType, StringType ,DoubleType\n",
					"from delta.tables import DeltaTable\n",
					"from notebookutils import mssparkutils\n",
					"import sys, traceback\n",
					"import json\n",
					"\n",
					"schema_list=[]\n",
					"i=0\n",
					"\n",
					"archive_folder = harmonised_storage_container + 'config/json_files/old' \n",
					"\n",
					"for json_string in json_string_list:\n",
					"    json_file = ''\n",
					"    try:\n",
					"        json_file = json_schema_path+json_schema_files[i]\n",
					"        table_name= json_schema_files[i].split('.')[0]\n",
					"        print('---------creating table '+table_name)\n",
					"        print('---------from file '+json_file)\n",
					"        schema = StructType.fromJson(json.loads(json_string))\n",
					"        harmonised_storage_delta_table_path = harmonised_storage_container + '/' + harmonised_storage_delta_folder + '/' + table_name\n",
					"\n",
					"        if not (DeltaTable.isDeltaTable(spark,harmonised_storage_delta_table_path)):\n",
					"            harmonisationInspectordf = spark.createDataFrame([], schema)\n",
					"            harmonisationInspectordf.write.option(\"mergeSchema\", \"true\").format('delta').mode('overwrite').save(harmonised_storage_delta_table_path)\n",
					"            spark.sql(\"CREATE TABLE {0}.{1} USING DELTA LOCATION '{2}'\".format(delta_lake_database_name,table_name,harmonised_storage_delta_table_path))\n",
					"            mssparkutils.fs.mv(json_file, archive_folder)\n",
					"    except Exception as e:\n",
					"        print('------------Error in File '+json_file)\n",
					"        traceback.print_exc(file=sys.stdout)\n",
					"    i += 1\n",
					""
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_secure_info_fact;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_diversity_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_disability_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_religion_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_sxo_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_record_fact;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_employee_fact;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_employee_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_work_schedule_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_absence_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_contract_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_costcenter_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_employee_hr_hierarchy_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_leave_entitlement_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_organisation_unit_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_payband_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_personnel_area_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_personnel_sub_area_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_pins_location_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_position_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_specialism_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_payroll_area_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_employee_leavers_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.hr_employeegroup_dim;\")\r\n",
					"\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.legacy_mwr_record_fact;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.legacy_mwr_fact;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.legacy_mwr_lines_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.legacy_mwr_inspector_join;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.legacy_mwr_status_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.legacy_mwr_submission_date_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.timesheets_record_fact;\")\r\n",
					"\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.timesheets_info_fact;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.timesheets_minutes_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.timesheets_segment_type_reference_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.timesheets_work_segment_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.timesheets_work_segment_lock_dim;\")\r\n",
					"\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.casework_legacy_rights_of_way_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.casework_contacts_organisation_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.casework_all_appeals_dim;\")\r\n",
					"# spark.sql(f\"drop table if exists odw_harmonised_db.casework_picasso_dim;\")\r\n",
					""
				],
				"execution_count": 12
			}
		]
	}
}