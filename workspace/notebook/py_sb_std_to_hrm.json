{
	"name": "py_sb_std_to_hrm",
	"properties": {
		"folder": {
			"name": "service-bus"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "1f32fb40-9f1b-4dc0-9573-926d173ecdb6"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw",
				"name": "pinssynspodw",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 1200
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": 17
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The entity name\r\n",
					"\r\n",
					"This is populated by the pipeline parameter but for testing can be entered manually here"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"entity_name = ''"
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"from notebookutils import mssparkutils\r\n",
					"import json\r\n",
					"from datetime import datetime, date\r\n",
					"import pprint\r\n",
					"from pyspark.sql import DataFrame\r\n",
					"from pyspark.sql.types import *\r\n",
					"from pyspark.sql.functions import col, when, lit, row_number, desc\r\n",
					"from pyspark.sql.window import Window\r\n",
					"from pyspark.sql.types import LongType"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run service-bus/py_spark_df_ingestion_functions"
				],
				"execution_count": 20
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Create spark session and read the orchestration file and table definition from config"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"logInfo(\"Reading orchestration config\")\n",
					"spark: SparkSession = SparkSession.builder.getOrCreate()\n",
					"storage_account: str = mssparkutils.notebook.run('/utils/py_utils_get_storage_account')\n",
					"harmonised_container: str = f\"abfss://odw-harmonised@{storage_account}lib/\"\n",
					"path_to_orchestration_file: str = \"abfss://odw-config@\"+storage_account+\"orchestration/orchestration.json\"\n",
					"df: DataFrame = spark.read.option(\"multiline\",\"true\").json(path_to_orchestration_file)\n",
					"definitions: list = json.loads(df.toJSON().first())['definitions']\n",
					"definition: dict = next((d for d in definitions if entity_name == d['Source_Filename_Start']), None)\n",
					"if not definition:\n",
					"    logError(f\"Failed to read definition for {entity_name}\")\n",
					"    mssparkutils.notebook.exit(f\"Definition not found for {entity_name}\")\n",
					"logInfo(\"Done reading orchestration config\")"
				],
				"execution_count": 21
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Define variables for database, tables and keys"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"std_db: str = \"odw_standardised_db\"\n",
					"hrm_db: str = \"odw_harmonised_db\"\n",
					"\n",
					"std_table: str = definition[\"Standardised_Table_Name\"]\n",
					"hrm_table: str = definition[\"Harmonised_Table_Name\"]\n",
					"\n",
					"print(std_table)\n",
					"print(hrm_table)\n",
					"\n",
					"hrm_incremental_key: str = definition[\"Harmonised_Incremental_Key\"]\n",
					"entity_primary_key: str = definition[\"Entity_Primary_Key\"]"
				],
				"execution_count": 22
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Utility functions\n",
					"\n",
					"`set_harmonised_master_columns`: Adds the master columns for the harmonised schema and drops the master columns of the standardised schema\n",
					"\n",
					"`insert_rows_in_df`: Insert new rows in an existing df while incrementing the incremental_key"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\n",
					"def set_harmonised_master_columns(df: DataFrame) -> DataFrame:\n",
					"    source_system_df: DataFrame = spark.sql(f\"SELECT * FROM {hrm_db}.main_sourcesystem_fact WHERE Description = 'Casework' AND IsActive = 'Y'\")\n",
					"    \n",
					"    # Set source system ID\n",
					"    df: DataFrame = df.withColumn(\"SourceSystemID\", lit(source_system_df.select('SourceSystemID').collect()[0][0]))\n",
					"    \n",
					"    # Set master column values\n",
					"    df: DataFrame = df.withColumn(\"RowID\", lit('').cast(\"string\"))\n",
					"    df: DataFrame = df.withColumn(\"migrated\", lit('1').cast(\"string\"))\n",
					"    df: DataFrame = df.withColumn(\"ODTSourceSystem\", lit('ODT').cast(\"string\"))\n",
					"    df: DataFrame = df.withColumn(\"ValidTo\", lit('').cast(\"string\"))\n",
					"    df: DataFrame = df.withColumn(\"IsActive\", lit('Y').cast(\"string\"))\n",
					"    df: DataFrame = df.withColumn(hrm_incremental_key, lit(None).cast(LongType()))\n",
					"    df: DataFrame = df.withColumn(\"IngestionDate\", col(\"message_enqueued_time_utc\").cast(\"string\"))\n",
					"    \n",
					"    # drop standardised columns from DataFrame\n",
					"    df: DataFrame = df.drop(\"message_enqueued_time_utc\")\n",
					"    df: DataFrame = df.drop(\"expected_from\")\n",
					"    df: DataFrame = df.drop(\"expected_to\")\n",
					"    df: DataFrame = df.drop(\"input_file\")\n",
					"    \n",
					"    return df"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\r\n",
					"def insert_rows_in_df(source_df: DataFrame, target_df: DataFrame, incremental_key: str) -> DataFrame:\r\n",
					"    # appending the incremental key\r\n",
					"    max_id: int = target_df.count()\r\n",
					"\r\n",
					"    w: Window = Window.orderBy(lit(1))\r\n",
					"    source_df: DataFrame = source_df.withColumn(incremental_key, (max_id + row_number().over(w)).cast(LongType()))\r\n",
					"\r\n",
					"    return target_df.union(source_df)"
				],
				"execution_count": 24
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Get the latest ingested data in Standardised"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Get the latest data from the standardised table and create dataframes containing:\r\n",
					"\r\n",
					"All of the data\r\n",
					"Created messages\r\n",
					"Updated messages\r\n",
					"Deleted messages\r\n",
					"\r\n",
					"And a further dataframe containing the harmonised data which is the target to be merged with"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\n",
					"def get_latest_ingested_data() -> tuple:\n",
					"    latest_ingestion = spark.sql(f\"SELECT * FROM {std_db}.{std_table} WHERE message_id not in (SELECT DISTINCT message_id FROM {hrm_db}.{hrm_table} where message_id IS NOT NULL) ORDER BY message_enqueued_time_utc\")\n",
					"\n",
					"    if latest_ingestion.count() == 0:\n",
					"        logError(f\"Standardised data not available for {entity_name}\")\n",
					"        mssparkutils.notebook.exit(f\"Standardised data not available for {entity_name}\")\n",
					"\n",
					"    # remove the column ingested_datetime and then remove duplicates\n",
					"    latest_ingestion = latest_ingestion.drop(\"ingested_datetime\").dropDuplicates()\n",
					"    latest_ingestion = set_harmonised_master_columns(latest_ingestion)\n",
					"    # create dataframes for each message type and the target dataframe (the harmonised target table)\n",
					"    create_df: DataFrame = latest_ingestion[latest_ingestion[\"message_type\"] == \"Create\"]\n",
					"    update_df: DataFrame = latest_ingestion[latest_ingestion[\"message_type\"].isin([\"update\", \"Update\", \"Publish\", \"Unpublish\"])]\n",
					"    delete_df: DataFrame = latest_ingestion[latest_ingestion[\"message_type\"] == \"Delete\"]\n",
					"    target_df: DataFrame = spark.table(f\"{hrm_db}.{hrm_table}\").drop(\"input_file\")\n",
					"\n",
					"\n",
					"    return (latest_ingestion,\n",
					"            create_df, \n",
					"            update_df,\n",
					"            delete_df,\n",
					"            target_df)"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"logInfo(\"Reading ingested data\")\n",
					"latest_ingestion, create_df, update_df, delete_df, target_df = get_latest_ingested_data()\n",
					"logInfo(\"Read ingested data\")"
				],
				"execution_count": 26
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Drop the `message_type` column since it's not needed in the table, compare and merge the schema of latest ingestion and existing harmonised table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"latest_ingestion: DataFrame = latest_ingestion.drop(\"message_type\")\n",
					"create_df: DataFrame = create_df.drop(\"message_type\").select(target_df.columns)\n",
					"update_df: DataFrame = update_df.drop(\"message_type\").select(target_df.columns)\n",
					"delete_df: DataFrame = delete_df.drop(\"message_type\").select(target_df.columns)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"logInfo(\"Comparing and merging schemas\")\n",
					"compare_and_merge_schema(latest_ingestion, f\"{hrm_db}.{hrm_table}\")\n",
					"logInfo(\"Compared and merged schemas\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Print counts of rows in each dataframe for reference"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Get the data we need again incase the schema has changed due to a merge\r\n",
					"latest_ingestion, create_df, update_df, delete_df, target_df = get_latest_ingested_data()\r\n",
					"latest_ingestion: DataFrame = latest_ingestion.drop(\"message_type\")\r\n",
					"create_df: DataFrame = create_df.drop(\"message_type\").select(target_df.columns)\r\n",
					"update_df: DataFrame = update_df.drop(\"message_type\").select(target_df.columns)\r\n",
					"delete_df: DataFrame = delete_df.drop(\"message_type\").select(target_df.columns)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"print(f\"Latest ingestion: {latest_ingestion.count()}\")\r\n",
					"print(f\"Create: {create_df.count()}\")\r\n",
					"print(f\"Update: {update_df.count()}\") \r\n",
					"print(f\"Delete: {delete_df.count()}\")\r\n",
					"print(f\"Target: {target_df.count()}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Drop any duplicates"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"logInfo(\"Dropping dupes\")\r\n",
					"if create_df.count() > 0:\r\n",
					"    create_df = create_df.dropDuplicates().orderBy(\"IngestionDate\")\r\n",
					"\r\n",
					"if update_df.count() > 0:\r\n",
					"    update_df = update_df.dropDuplicates().orderBy(\"IngestionDate\")\r\n",
					"\r\n",
					"if delete_df.count() > 0:\r\n",
					"    delete_df = delete_df.dropDuplicates().orderBy(\"IngestionDate\")\r\n",
					"\r\n",
					"if target_df.count() > 0:\r\n",
					"    target_df = target_df.dropDuplicates().orderBy(\"IngestionDate\")\r\n",
					"    \r\n",
					"logInfo(\"Finished dropping dupes\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"print(f\"Latest ingestion: {latest_ingestion.count()}\")\r\n",
					"print(f\"Create: {create_df.count()}\")\r\n",
					"print(f\"Update: {update_df.count()}\") \r\n",
					"print(f\"Delete: {delete_df.count()}\")\r\n",
					"print(f\"Target: {target_df.count()}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Handling new rows"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if create_df.count() > 0:\n",
					"    target_df = insert_rows_in_df(create_df, target_df, hrm_incremental_key)\n",
					"\n",
					"logInfo(f\"New rows: {create_df.count()}\\nTarget dataframe: {target_df.count()}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Handling updated rows"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"if update_df.count() > 0:\n",
					"    logInfo(\"Handling updated rows\")\n",
					"    update_df = update_df.dropDuplicates().orderBy(\"IngestionDate\")\n",
					"   \n",
					"    #Assume we have multiple updates for the same record to be applied in a single load. We need to deal with that to make sure we're handling it correctly\n",
					"    window_spec = Window.partitionBy(f\"{entity_primary_key}\").orderBy(update_df[\"IngestionDate\"].desc())\n",
					"    update_df = update_df.withColumn('Submitted', row_number().over(window_spec))\n",
					"\n",
					"    #The first pot of data we need are the straight inserts of the most recent data\n",
					"    latest_updates = update_df.filter(update_df.Submitted == 1)\n",
					"    latest_updates = latest_updates.select(target_df.columns)\n",
					"\n",
					"    target_df = insert_rows_in_df(latest_updates, target_df, hrm_incremental_key)\n",
					"\n",
					"    #Now we are likely to have multiple records in target_df that have IsActive = 'Y' we need to deal with\n",
					"    #First split the target into active and inactive\n",
					"    inactive = target_df.filter(target_df.IsActive == 'N')\n",
					"    active = target_df.filter(target_df.IsActive == 'Y')\n",
					"\n",
					"    #We will need to assume that we have more than 1 active record for each primary key, we need to deal with this and rebuild the target dataframe\n",
					"    window_spec = Window.partitionBy(f\"{entity_primary_key}\").orderBy(active[\"IngestionDate\"].desc())\n",
					"    active = active.withColumn('Reverse_order_submitted', row_number().over(window_spec))\n",
					"\n",
					"    #We know that we don't need to touch the inactive pot and the pot that are the truely latest records. We'll start by combining these into the new dataframe\n",
					"    latest = active.filter(active.Reverse_order_submitted == 1).select(inactive.columns)\n",
					"    target_df_new = latest.union(inactive)\n",
					"\n",
					"    #Next we need to deal with the records that aren't active to set the ValidTo correctly\n",
					"    reasign = active.filter(active.Reverse_order_submitted > 1).drop(\"IsActive\", \"ValidTo\")\n",
					"    valid_to = active.selectExpr(f\"{entity_primary_key} AS Pk\", \"IngestionDate AS ValidTo\", \"Reverse_order_submitted + 1 AS Next_record\")\n",
					"    reasign = reasign.join(valid_to, (reasign[f\"{entity_primary_key}\"] == valid_to[f\"Pk\"]) &  (reasign[\"Reverse_order_submitted\"] == valid_to[\"Next_record\"]))\n",
					"\n",
					"    #Add the IsActive flag back in, set it correctly, and select the columns we're interested in\n",
					"    reasign = reasign.withColumn(\"IsActive\", lit('N').cast(\"string\")).select(target_df_new.columns)\n",
					"    target_df_new = target_df_new.union(reasign)\n",
					"    target_df = target_df_new\n",
					"\n",
					"    #Next, we have the group of data that is inbound but we have had a subsequent update in the same day\n",
					"    raw = update_df.filter(update_df.Submitted > 1)\n",
					"    raw = raw.withColumn(\"next_submitted\", raw['Submitted']-1)\n",
					"    raw = raw.drop(\"IsActive\").drop(\"ValidTo\").drop(\"Submitted\")\n",
					"\n",
					"    next_received = update_df.select(f\"{entity_primary_key}\",\"IngestionDate\", \"Submitted\")\n",
					"    next_received = next_received.withColumn(\"IsActive\", lit('N')).withColumnRenamed(\"IngestionDate\", \"ValidTo\").withColumnRenamed(f\"{entity_primary_key}\", \"Key\")\n",
					"    \n",
					"    raw = raw.join(next_received, (raw[f\"{entity_primary_key}\"] == next_received[\"Key\"]) & (raw[\"next_submitted\"] == next_received[\"Submitted\"]))\n",
					"\n",
					"    previous_updates = raw.select(target_df.columns)   \n",
					"    target_df = insert_rows_in_df(previous_updates, target_df, hrm_incremental_key)\n",
					"    logInfo(\"Done handling updated rows\")\n",
					"\n",
					"logInfo(f\"Updated rows: {update_df.count()}\\nTarget dataframe: {target_df.count()}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Handling deleted rows"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if delete_df.count() > 0:\r\n",
					"    logInfo(\"Handling deleted rows\")\r\n",
					"else:\r\n",
					"    logInfo(\"No deleted messages\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###### Set the ValidTo of the deleted records to the IngestionDate and IsActive to N"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if delete_df.count() > 0:\r\n",
					"    delete_df = delete_df \\\r\n",
					"    .withColumn(\"ValidTo\", col(\"IngestionDate\")) \\\r\n",
					"    .withColumn(\"IsActive\", lit(\"N\"))\r\n",
					"    display(delete_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###### Set the ValidTo date of the target DataFrame records\r\n",
					"\r\n",
					"For all deleted primary keys, set the ValidTo date to the IngestionDate from the deleted DataFrame if the record is active (only 1 should be active in the target and we don't want to reset all the other ValidTo dates as these have already been set when they were first inserted).  "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if delete_df.count() > 0:\r\n",
					"    target_df = (\r\n",
					"        target_df.join(\r\n",
					"            delete_df.select(\r\n",
					"                entity_primary_key, \r\n",
					"                col(\"IngestionDate\").alias(\"delete_IngestionDate\")\r\n",
					"            ),\r\n",
					"            on=entity_primary_key,\r\n",
					"            how=\"left\"\r\n",
					"        )\r\n",
					"        .withColumn(\r\n",
					"            \"ValidTo\",\r\n",
					"            when(col(\"IsActive\") == \"Y\", col(\"delete_IngestionDate\"))\r\n",
					"            .otherwise(col(\"ValidTo\"))\r\n",
					"        )\r\n",
					"        .drop(\"delete_IngestionDate\")\r\n",
					"    )"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###### Update the IsActive flag\r\n",
					"\r\n",
					"For all primary keys in the deleted DataFrame, set the IsActive flag to N for those primary keys in the target DataFrame.   "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if delete_df.count() > 0:\r\n",
					"    deleted_primary_keys = delete_df.select(entity_primary_key).distinct()\r\n",
					"    target_df = target_df.withColumn(\r\n",
					"        \"IsActive\", \r\n",
					"        when(col(entity_primary_key).isin([getattr(row, entity_primary_key) for row in deleted_primary_keys.collect()]), lit(\"N\"))\r\n",
					"        .otherwise(col(\"IsActive\"))\r\n",
					"    )"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###### Update the target DataFrame with the deleted records"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if delete_df.count() > 0:\r\n",
					"    target_df = insert_rows_in_df(delete_df, target_df, hrm_incremental_key)\r\n",
					"    logInfo(f\"Deleted rows: {delete_df.count()}\\nTarget dataframe: {target_df.count()}\")\r\n",
					"    logInfo(\"Done handling deleted rows\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### De-dupe the target dataframe based on a subset of columns, i.e. all columns apart from the incremental key and ValidTo"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"columns_to_consider = [c for c in target_df.columns if c not in [hrm_incremental_key, \"ValidTo\"]]\n",
					"target_df = target_df.orderBy(hrm_incremental_key).dropDuplicates(subset=columns_to_consider).orderBy(\"IngestionDate\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Apply the target dataframe to the harmonised table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"logInfo(\"Applying new dataframe to table\")\n",
					"apply_df_to_table(target_df, hrm_db, hrm_table)\n",
					"logInfo(\"Done applying new dataframe to table\")"
				],
				"execution_count": null
			}
		]
	}
}