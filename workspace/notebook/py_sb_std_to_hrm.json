{
	"name": "py_sb_std_to_hrm",
	"properties": {
		"folder": {
			"name": "service-bus"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "09e99b08-3d22-422f-ab6a-be3a42698258"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this pyspark notebook is to read service bus data from owb_standarsied_db Delta table and appends into single owb_harmonised_db Delta table based on passing the table name as a parameter.\r\n",
					"\r\n",
					"**Description**  \r\n",
					"The purpose of this pyspark notebook is to read service bus data from owb_standarsied_db Delta table and appends into single owb_harmonised_db Delta tables based on passing the table names as a parameter\r\n",
					"\r\n",
					"**Spark Cluster Configuration** -> Apache Spark Version- 3.4, Python Version \t\t- 3.10, Delta Lake Version \t- 2.4\r\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The entity name\r\n",
					"\r\n",
					"This is populated by the pipeline parameter but for testing can be entered manually here"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"entity_name = ''"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from odw.core.util.util import Util\n",
					"from pyspark.sql import SparkSession\r\n",
					"from notebookutils import mssparkutils\r\n",
					"import json\r\n",
					"from datetime import datetime, date\r\n",
					"import pprint\r\n",
					"from pyspark.sql import DataFrame\r\n",
					"from pyspark.sql.types import *\r\n",
					"from pyspark.sql.functions import col, when, lit, row_number, desc\r\n",
					"from pyspark.sql.window import Window\r\n",
					"from pyspark.sql.types import LongType"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/py_logging_decorator"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Initialize Application Insight Logging functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run utils/py_utils_common_logging_output"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run service-bus/py_spark_df_ingestion_functions"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Create spark session and read the orchestration file and table definition from config"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"logInfo(\"Reading orchestration config\")\r\n",
					"spark: SparkSession = SparkSession.builder.getOrCreate()\r\n",
					"storage_account: str = Util.get_storage_account()\r\n",
					"harmonised_container: str = f\"abfss://odw-harmonised@{storage_account}lib/\"\r\n",
					"path_to_orchestration_file: str = \"abfss://odw-config@\"+storage_account+\"orchestration/orchestration.json\"\r\n",
					"df: DataFrame = spark.read.option(\"multiline\",\"true\").json(path_to_orchestration_file)\r\n",
					"definitions: list = json.loads(df.toJSON().first())['definitions']\r\n",
					"definition: dict = next((d for d in definitions if entity_name == d['Source_Filename_Start']), None)\r\n",
					"if not definition:\r\n",
					"    logError(f\"Failed to read definition for {entity_name}\")\r\n",
					"    error_meesage = f\"Failed to read definition for {entity_name}\"\r\n",
					"    end_exec_time = str(datetime.now())\r\n",
					"    app_insight_logger.add_table_result(\r\n",
					"        delta_table_name = table_name,\r\n",
					"        insert_count = insert_count,\r\n",
					"        update_count = update_count, \r\n",
					"        delete_count = delete_count, \r\n",
					"        table_result = \"failed\",\r\n",
					"        start_exec_time = start_exec_time, \r\n",
					"        end_exec_time = end_exec_time,\r\n",
					"        total_exec_time = \"\",\r\n",
					"        error_message = error_message\r\n",
					"    )\r\n",
					"    # Exit with the JSON result\r\n",
					"    mssparkutils.notebook.exit(app_insight_logger.generate_processing_results())\r\n",
					"logInfo(\"Done reading orchestration config\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Define variables for database, tables and keys"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"std_db: str = \"odw_standardised_db\"\r\n",
					"hrm_db: str = \"odw_harmonised_db\"\r\n",
					"\r\n",
					"std_table: str = definition[\"Standardised_Table_Name\"]\r\n",
					"hrm_table: str = definition[\"Harmonised_Table_Name\"]\r\n",
					"\r\n",
					"print(std_table)\r\n",
					"print(hrm_table)\r\n",
					"\r\n",
					"hrm_incremental_key: str = definition[\"Harmonised_Incremental_Key\"]\r\n",
					"entity_primary_key: str = definition[\"Entity_Primary_Key\"]\r\n",
					"\r\n",
					"start_exec_time = str(datetime.now())\r\n",
					"insert_count = 0\r\n",
					"update_count = 0\r\n",
					"delete_count = 0\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Utility functions\r\n",
					"\r\n",
					"`set_harmonised_master_columns`: Adds the master columns for the harmonised schema and drops the master columns of the standardised schema\r\n",
					"\r\n",
					"`insert_rows_in_df`: Insert new rows in an existing df while incrementing the incremental_key"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\r\n",
					"def set_harmonised_master_columns(df: DataFrame) -> DataFrame:\r\n",
					"    try:\r\n",
					"        source_system_df: DataFrame = spark.sql(f\"SELECT * FROM {hrm_db}.main_sourcesystem_fact WHERE Description = 'Casework' AND IsActive = 'Y'\")\r\n",
					"        \r\n",
					"        # Set source system ID\r\n",
					"        df: DataFrame = df.withColumn(\"SourceSystemID\", lit(source_system_df.select('SourceSystemID').collect()[0][0]))\r\n",
					"        \r\n",
					"        # Set master column values\r\n",
					"        df: DataFrame = df.withColumn(\"RowID\", lit('').cast(\"string\"))\r\n",
					"        df: DataFrame = df.withColumn(\"migrated\", lit('1').cast(\"string\"))\r\n",
					"        df: DataFrame = df.withColumn(\"ODTSourceSystem\", lit('ODT').cast(\"string\"))\r\n",
					"        df: DataFrame = df.withColumn(\"ValidTo\", lit('').cast(\"string\"))\r\n",
					"        df: DataFrame = df.withColumn(\"IsActive\", lit('Y').cast(\"string\"))\r\n",
					"        df: DataFrame = df.withColumn(hrm_incremental_key, lit(None).cast(LongType()))\r\n",
					"        df: DataFrame = df.withColumn(\"IngestionDate\", col(\"message_enqueued_time_utc\").cast(\"string\"))\r\n",
					"        \r\n",
					"        # drop standardised columns from DataFrame\r\n",
					"        df: DataFrame = df.drop(\"message_enqueued_time_utc\")\r\n",
					"        df: DataFrame = df.drop(\"expected_from\")\r\n",
					"        df: DataFrame = df.drop(\"expected_to\")\r\n",
					"        df: DataFrame = df.drop(\"input_file\")\r\n",
					"    except Exception as e:\r\n",
					"        logError(f\"Error setting up Audit columns:\\n{e}\")\r\n",
					"        error_message = app_insight_logger.format_error_message(e, max_length=800)\r\n",
					"        \r\n",
					"        end_exec_time = str(datetime.now())\r\n",
					"        app_insight_logger.add_table_result(\r\n",
					"            delta_table_name = table_name,\r\n",
					"            insert_count = insert_count,\r\n",
					"            update_count = update_count, \r\n",
					"            delete_count = delete_count, \r\n",
					"            table_result = \"failed\",\r\n",
					"            start_exec_time = start_exec_time, \r\n",
					"            end_exec_time = end_exec_time,\r\n",
					"            total_exec_time = \"\",\r\n",
					"            error_message = error_message\r\n",
					"        )\r\n",
					"        # Exit with the JSON result\r\n",
					"        mssparkutils.notebook.exit(app_insight_logger.generate_processing_results())\r\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"@logging_to_appins\r\n",
					"def insert_rows_in_df(source_df: DataFrame, target_df: DataFrame, incremental_key: str) -> DataFrame:\r\n",
					"    try:\r\n",
					"        # appending the incremental key\r\n",
					"        max_id: int = target_df.count()\r\n",
					"\r\n",
					"        w: Window = Window.orderBy(lit(1))\r\n",
					"        source_df: DataFrame = source_df.withColumn(incremental_key, (max_id + row_number().over(w)).cast(LongType()))\r\n",
					"    except Exception as e:\r\n",
					"        logError(f\"Error in creating source data frame :\\n{e}\")\r\n",
					"        error_message = app_insight_logger.format_error_message(e, max_length=800)\r\n",
					"        \r\n",
					"        end_exec_time = str(datetime.now())\r\n",
					"        app_insight_logger.add_table_result(\r\n",
					"            delta_table_name = table_name,\r\n",
					"            insert_count = insert_count,\r\n",
					"            update_count = update_count, \r\n",
					"            delete_count = delete_count, \r\n",
					"            table_result = \"failed\",\r\n",
					"            start_exec_time = start_exec_time, \r\n",
					"            end_exec_time = end_exec_time,\r\n",
					"            total_exec_time = \"\",\r\n",
					"            error_message = error_message\r\n",
					"        )\r\n",
					"        # Exit with the JSON result\r\n",
					"        mssparkutils.notebook.exit(app_insight_logger.generate_processing_results())\r\n",
					"    return target_df.union(source_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Get the latest ingested data in Standardised"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Get the latest data from the standardised table and create dataframes containing:\r\n",
					"\r\n",
					"All of the data\r\n",
					"Created messages\r\n",
					"Updated messages\r\n",
					"Deleted messages\r\n",
					"\r\n",
					"And a further dataframe containing the harmonised data which is the target to be merged with"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"@logging_to_appins\r\n",
					"def get_latest_ingested_data() -> tuple:\r\n",
					"    # Initialize defaults\r\n",
					"    latest_ingestion = None\r\n",
					"    create_df = spark.createDataFrame([], schema=StructType([]))\r\n",
					"    update_df = spark.createDataFrame([], schema=StructType([]))\r\n",
					"    delete_df = spark.createDataFrame([], schema=StructType([]))\r\n",
					"    target_df = spark.createDataFrame([], schema=StructType([]))\r\n",
					"    \r\n",
					"    try:\r\n",
					"        latest_ingestion = spark.sql(f\"SELECT * FROM {std_db}.{std_table} WHERE message_id not in (SELECT DISTINCT message_id FROM {hrm_db}.{hrm_table} where message_id IS NOT NULL) ORDER BY message_enqueued_time_utc\")\r\n",
					"\r\n",
					"        if latest_ingestion.count() == 0:\r\n",
					"            logError(f\"Standardised data not available for {entity_name}\")\r\n",
					"\r\n",
					"            error_msg = f\"Standardised data not available for {entity_name}\"\r\n",
					"            error_message = app_insight_logger.format_error_message(error_msg, max_length=800)\r\n",
					"            end_exec_time = str(datetime.now())\r\n",
					"            \r\n",
					"            app_insight_logger.add_table_result(\r\n",
					"                delta_table_name=f\"{hrm_db}.{hrm_table}\",\r\n",
					"                insert_count=insert_count, \r\n",
					"                update_count=update_count, \r\n",
					"                delete_count=delete_count, \r\n",
					"                table_result=\"failed\",\r\n",
					"                start_exec_time=start_exec_time, \r\n",
					"                end_exec_time=end_exec_time,\r\n",
					"                total_exec_time=\"\",\r\n",
					"                error_message=error_message\r\n",
					"            )\r\n",
					"            \r\n",
					"            # Return defaults instead of exiting - let the calling code decide what to do\r\n",
					"            return (latest_ingestion, create_df, update_df, delete_df, target_df)\r\n",
					"\r\n",
					"        # Process the data if available\r\n",
					"        latest_ingestion = latest_ingestion.drop(\"ingested_datetime\").dropDuplicates()\r\n",
					"        latest_ingestion = set_harmonised_master_columns(latest_ingestion)\r\n",
					"        \r\n",
					"        create_df = latest_ingestion[latest_ingestion[\"message_type\"] == \"Create\"]\r\n",
					"        update_df = latest_ingestion[latest_ingestion[\"message_type\"].isin([\"update\", \"Update\", \"Publish\", \"Unpublish\"])]\r\n",
					"        delete_df = latest_ingestion[latest_ingestion[\"message_type\"] == \"Delete\"]\r\n",
					"        target_df = spark.table(f\"{hrm_db}.{hrm_table}\").drop(\"input_file\")\r\n",
					"        \r\n",
					"        return (latest_ingestion, create_df, update_df, delete_df, target_df)\r\n",
					"\r\n",
					"    except Exception as e:\r\n",
					"        logError(f\"Error in getting latest ingestion date from data frames :\\n{e}\")\r\n",
					"        error_message = app_insight_logger.format_error_message(e, max_length=800)\r\n",
					"        \r\n",
					"        end_exec_time = str(datetime.now())        \r\n",
					"        app_insight_logger.add_table_result(\r\n",
					"            delta_table_name=f\"{hrm_db}.{hrm_table}\",\r\n",
					"            insert_count=0,\r\n",
					"            table_result=\"failed\",\r\n",
					"            start_exec_time=start_exec_time, \r\n",
					"            end_exec_time=end_exec_time,\r\n",
					"            error_message=error_message\r\n",
					"        )\r\n",
					"        # Exit with the JSON result\r\n",
					"        mssparkutils.notebook.exit(app_insight_logger.generate_processing_results())\r\n",
					"        \r\n",
					"        return (latest_ingestion, create_df, update_df, delete_df, target_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"editable": true,
					"run_control": {
						"frozen": false
					}
				},
				"source": [
					"logInfo(\"Reading ingested data\")\r\n",
					"latest_ingestion, create_df, update_df, delete_df, target_df = (\r\n",
					"    get_latest_ingested_data()\r\n",
					"    )\r\n",
					"logInfo(\"Read ingested data successfully\")\r\n",
					"\r\n",
					"# Check if there is valid data available\r\n",
					"if latest_ingestion is None or latest_ingestion.count() == 0:\r\n",
					"    logError(\"No data available for processing. Exiting notebook.\")\r\n",
					"    error_message = \"No data available for processing. Exiting notebook.\"\r\n",
					"    \r\n",
					"    end_exec_time = str(datetime.now())        \r\n",
					"    app_insight_logger.add_table_result(\r\n",
					"        delta_table_name=f\"{hrm_db}.{hrm_table}\",\r\n",
					"        insert_count=0,\r\n",
					"        table_result=\"failed\",\r\n",
					"        start_exec_time=start_exec_time, \r\n",
					"        end_exec_time=end_exec_time,\r\n",
					"        error_message=error_message\r\n",
					"    )\r\n",
					"    # Exit with the JSON result    \r\n",
					"    mssparkutils.notebook.exit(app_insight_logger.generate_processing_results())"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Drop the `message_type` column since it's not needed in the table, compare and merge the schema of latest ingestion and existing harmonised table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"latest_ingestion: DataFrame = latest_ingestion.drop(\"message_type\")\r\n",
					"create_df: DataFrame = create_df.drop(\"message_type\").select(target_df.columns)\r\n",
					"update_df: DataFrame = update_df.drop(\"message_type\").select(target_df.columns)\r\n",
					"delete_df: DataFrame = delete_df.drop(\"message_type\").select(target_df.columns)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"logInfo(\"Comparing and merging schemas\")\r\n",
					"compare_and_merge_schema(latest_ingestion, f\"{hrm_db}.{hrm_table}\")\r\n",
					"logInfo(\"Compared and merged schemas\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Print counts of rows in each dataframe for reference"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Get the data we need again incase the schema has changed due to a merge\r\n",
					"latest_ingestion, create_df, update_df, delete_df, target_df = get_latest_ingested_data()\r\n",
					"latest_ingestion: DataFrame = latest_ingestion.drop(\"message_type\")\r\n",
					"create_df: DataFrame = create_df.drop(\"message_type\").select(target_df.columns)\r\n",
					"update_df: DataFrame = update_df.drop(\"message_type\").select(target_df.columns)\r\n",
					"delete_df: DataFrame = delete_df.drop(\"message_type\").select(target_df.columns)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"print(f\"Latest ingestion: {latest_ingestion.count()}\")\r\n",
					"print(f\"Create: {create_df.count()}\")\r\n",
					"print(f\"Update: {update_df.count()}\") \r\n",
					"print(f\"Delete: {delete_df.count()}\")\r\n",
					"print(f\"Target: {target_df.count()}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Drop any duplicates"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"logInfo(\"Dropping dupes\")\r\n",
					"if create_df.count() > 0:\r\n",
					"    create_df = create_df.dropDuplicates().orderBy(\"IngestionDate\")\r\n",
					"\r\n",
					"if update_df.count() > 0:\r\n",
					"    update_df = update_df.dropDuplicates().orderBy(\"IngestionDate\")\r\n",
					"\r\n",
					"if delete_df.count() > 0:\r\n",
					"    delete_df = delete_df.dropDuplicates().orderBy(\"IngestionDate\")\r\n",
					"\r\n",
					"if target_df.count() > 0:\r\n",
					"    target_df = target_df.dropDuplicates().orderBy(\"IngestionDate\")\r\n",
					"    \r\n",
					"logInfo(\"Finished dropping dupes\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Process record counts"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"print(f\"Latest ingestion: {latest_ingestion.count()}\")\r\n",
					"print(f\"Create: {create_df.count()}\")\r\n",
					"print(f\"Update: {update_df.count()}\") \r\n",
					"print(f\"Delete: {delete_df.count()}\")\r\n",
					"print(f\"Target: {target_df.count()}\")\r\n",
					"\r\n",
					"start_exec_time = str(datetime.now())\r\n",
					"\r\n",
					"insert_count = create_df.count()\r\n",
					"update_count = update_df.count()\r\n",
					"delete_count = delete_df.count()\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Handling new rows"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if create_df.count() > 0:\r\n",
					"    target_df = insert_rows_in_df(create_df, target_df, hrm_incremental_key)\r\n",
					"\r\n",
					"logInfo(f\"New rows: {create_df.count()}\\nTarget dataframe: {target_df.count()}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Handling updated rows"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"if update_df.count() > 0:    \r\n",
					"    try:        \r\n",
					"        logInfo(\"Handling updated rows\")\r\n",
					"        update_df = update_df.dropDuplicates().orderBy(\"IngestionDate\")\r\n",
					"\r\n",
					"        #Assume we have multiple updates for the same record to be applied in a single load. We need to deal with that to make sure we're handling it correctly\r\n",
					"        window_spec = Window.partitionBy(f\"{entity_primary_key}\").orderBy(update_df[\"IngestionDate\"].desc())\r\n",
					"        update_df = update_df.withColumn('Submitted', row_number().over(window_spec))\r\n",
					"\r\n",
					"        #The first pot of data we need are the straight inserts of the most recent data\r\n",
					"        # latest_updates = update_df.filter(update_df.Submitted == 1)\r\n",
					"        latest_updates = update_df\r\n",
					"        latest_updates = latest_updates.select(target_df.columns)\r\n",
					"\r\n",
					"        target_df = insert_rows_in_df(latest_updates, target_df, hrm_incremental_key)\r\n",
					"\r\n",
					"        #Now we are likely to have multiple records in target_df that have IsActive = 'Y' we need to deal with\r\n",
					"        #First split the target into active and inactive\r\n",
					"        inactive = target_df.filter(target_df.IsActive == 'N')\r\n",
					"        active = target_df.filter(target_df.IsActive == 'Y')\r\n",
					"\r\n",
					"        #We will need to assume that we have more than 1 active record for each primary key, we need to deal with this and rebuild the target dataframe\r\n",
					"        window_spec = Window.partitionBy(f\"{entity_primary_key}\").orderBy(active[\"IngestionDate\"].desc())\r\n",
					"        active = active.withColumn('Reverse_order_submitted', row_number().over(window_spec))\r\n",
					"\r\n",
					"        #We know that we don't need to touch the inactive pot and the pot that are the truely latest records. We'll start by combining these into the new dataframe\r\n",
					"        latest = active.filter(active.Reverse_order_submitted == 1).select(inactive.columns)\r\n",
					"        target_df_new = latest.union(inactive)\r\n",
					"\r\n",
					"        #Next we need to deal with the records that aren't active to set the ValidTo correctly\r\n",
					"        reasign = active.filter(active.Reverse_order_submitted > 1).drop(\"IsActive\", \"ValidTo\")\r\n",
					"        valid_to = active.selectExpr(f\"{entity_primary_key} AS Pk\", \"IngestionDate AS ValidTo\", \"Reverse_order_submitted + 1 AS Next_record\")\r\n",
					"        reasign = reasign.join(valid_to, (reasign[f\"{entity_primary_key}\"] == valid_to[f\"Pk\"]) &  (reasign[\"Reverse_order_submitted\"] == valid_to[\"Next_record\"]))\r\n",
					"\r\n",
					"        #Add the IsActive flag back in, set it correctly, and select the columns we're interested in\r\n",
					"        reasign = reasign.withColumn(\"IsActive\", lit('N').cast(\"string\")).select(target_df_new.columns)\r\n",
					"        target_df = target_df_new.union(reasign)\r\n",
					"\r\n",
					"        logInfo(\"Done handling updated rows\")\r\n",
					"    except Exception as e:\r\n",
					"        logError(f\"Error in data frames processing steps: \\n{e}\")\r\n",
					"        error_message = app_insight_logger.format_error_message(e, max_length=800)\r\n",
					"        \r\n",
					"        end_exec_time = str(datetime.now())        \r\n",
					"        app_insight_logger.add_table_result(\r\n",
					"            delta_table_name=f\"{hrm_db}.{hrm_table}\",\r\n",
					"            insert_count=0,\r\n",
					"            table_result=\"failed\",\r\n",
					"            start_exec_time=start_exec_time, \r\n",
					"            end_exec_time=end_exec_time,\r\n",
					"            error_message=error_message\r\n",
					"        )\r\n",
					"        # Exit with the JSON result    \r\n",
					"        mssparkutils.notebook.exit(app_insight_logger.generate_processing_results())        \r\n",
					"\r\n",
					"logInfo(f\"Updated rows: {update_df.count()}\\nTarget dataframe: {target_df.count()}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Handling deleted rows"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if delete_df.count() > 0:\r\n",
					"    logInfo(\"Handling deleted rows\")\r\n",
					"else:\r\n",
					"    logInfo(\"No deleted messages\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###### Set the ValidTo of the deleted records to the IngestionDate and IsActive to N"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"if delete_df.count() > 0:\r\n",
					"    delete_df = delete_df \\\r\n",
					"    .withColumn(\"ValidTo\", col(\"IngestionDate\")) \\\r\n",
					"    .withColumn(\"IsActive\", lit(\"N\"))\r\n",
					"    display(delete_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###### Set the ValidTo date of the target DataFrame records\r\n",
					"\r\n",
					"For all deleted primary keys, set the ValidTo date to the IngestionDate from the deleted DataFrame if the record is active (only 1 should be active in the target and we don't want to reset all the other ValidTo dates as these have already been set when they were first inserted).  "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if delete_df.count() > 0:\r\n",
					"    target_df = (\r\n",
					"        target_df.join(\r\n",
					"            delete_df.select(\r\n",
					"                entity_primary_key, \r\n",
					"                col(\"IngestionDate\").alias(\"delete_IngestionDate\")\r\n",
					"            ),\r\n",
					"            on=entity_primary_key,\r\n",
					"            how=\"left\"\r\n",
					"        )\r\n",
					"        .withColumn(\r\n",
					"            \"ValidTo\",\r\n",
					"            when(col(\"IsActive\") == \"Y\", col(\"delete_IngestionDate\"))\r\n",
					"            .otherwise(col(\"ValidTo\"))\r\n",
					"        )\r\n",
					"        .drop(\"delete_IngestionDate\")\r\n",
					"    )"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###### Update the IsActive flag\r\n",
					"\r\n",
					"For all primary keys in the deleted DataFrame, set the IsActive flag to N for those primary keys in the target DataFrame.   "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if delete_df.count() > 0:\r\n",
					"    deleted_primary_keys = delete_df.select(entity_primary_key).distinct()\r\n",
					"    target_df = target_df.withColumn(\r\n",
					"        \"IsActive\", \r\n",
					"        when(col(entity_primary_key).isin([getattr(row, entity_primary_key) for row in deleted_primary_keys.collect()]), lit(\"N\"))\r\n",
					"        .otherwise(col(\"IsActive\"))\r\n",
					"    )"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###### Update the target DataFrame with the deleted records"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if delete_df.count() > 0:\r\n",
					"    target_df = insert_rows_in_df(delete_df, target_df, hrm_incremental_key)\r\n",
					"    logInfo(f\"Deleted rows: {delete_df.count()}\\nTarget dataframe: {target_df.count()}\")\r\n",
					"    logInfo(\"Done handling deleted rows\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### De-dupe the target dataframe based on a subset of columns, i.e. all columns apart from the incremental key and ValidTo"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"columns_to_consider = [c for c in target_df.columns if c not in [hrm_incremental_key, \"ValidTo\"]]\r\n",
					"target_df = target_df.orderBy(hrm_incremental_key).dropDuplicates(subset=columns_to_consider).orderBy(\"IngestionDate\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Apply the target dataframe to the harmonised table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"try:\r\n",
					"    logInfo(\"Applying new dataframe to table\")\r\n",
					"    apply_df_to_table(\r\n",
					"        target_df, \r\n",
					"        hrm_db, \r\n",
					"        hrm_table\r\n",
					"        )\r\n",
					"    \r\n",
					"    end_exec_time = str(datetime.now())\r\n",
					"    app_insight_logger.add_table_result(                    \r\n",
					"        delta_table_name = f\"{hrm_db}.{hrm_table}\",\r\n",
					"        insert_count = insert_count, \r\n",
					"        update_count = update_count, \r\n",
					"        delete_count = delete_count, \r\n",
					"        table_result = \"success\",\r\n",
					"        start_exec_time = start_exec_time, \r\n",
					"        end_exec_time = end_exec_time,\r\n",
					"        total_exec_time = \"\",\r\n",
					"        error_message = \"\"\r\n",
					"    )\r\n",
					"\r\n",
					"    logInfo(\"Done applying new dataframe to table\")\r\n",
					"\r\n",
					"except Exception as e:\r\n",
					"        logError(f\"Error appending data to the harmonised layer table :\\n{e}\")\r\n",
					"        error_message = app_insight_logger.format_error_message(e, max_length=800)\r\n",
					"        end_exec_time = str(datetime.now())\r\n",
					"        app_insight_logger.add_table_result(\r\n",
					"            delta_table_name = f\"{hrm_db}.{hrm_table}\",\r\n",
					"            insert_count = insert_count, \r\n",
					"            update_count = update_count, \r\n",
					"            delete_count = delete_count, \r\n",
					"            table_result = \"failed\",\r\n",
					"            start_exec_time = start_exec_time, \r\n",
					"            end_exec_time = end_exec_time,\r\n",
					"            total_exec_time = \"\",\r\n",
					"            error_message = error_message\r\n",
					"        )\r\n",
					"        # Exit with the JSON result\r\n",
					"        mssparkutils.notebook.exit(app_insight_logger.generate_processing_results())"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Produce Json formatted output"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Exit with the JSON result\r\n",
					"mssparkutils.notebook.exit(app_insight_logger.generate_processing_results())"
				],
				"execution_count": null
			}
		]
	}
}