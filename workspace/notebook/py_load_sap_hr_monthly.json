{
	"name": "py_load_sap_hr_monthly",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "0b43af84-d4b2-4f35-b3c7-21d8521090d4"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Stadardised layer and build a table.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;01-April-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This Notebook is designed to facilitate the monthly processing and harmonization of SAP HR data. It includes steps for initializing the environment, creating and managing Delta tables, and inserting data into harmonized tables. The Notebook ensures that HR data is accurately transformed, stored, and made available for reporting and analysis."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"from odw.core.util.logging_util import LoggingUtil"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"import json\n",
					"\n",
					"# Initialize result dictionary\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"try:\n",
					"    # Set time parser policy\n",
					"    LoggingUtil().log_info(\"Setting legacy time parser policy\")\n",
					"    spark.sql(\"SET spark.sql.legacy.timeParserPolicy = LEGACY\")\n",
					"    LoggingUtil().log_info(\"Legacy time parser policy set successfully\")\n",
					"    \n",
					"    # Delete existing data\n",
					"    LoggingUtil().log_info(\"Starting deletion of all rows from odw_harmonised_db.load_sap_hr_monthly\")\n",
					"    spark.sql(\"DELETE FROM odw_harmonised_db.load_sap_hr_monthly\")\n",
					"    LoggingUtil().log_info(\"Successfully deleted all rows from odw_harmonised_db.load_sap_hr_monthly\")\n",
					"    \n",
					"    # Insert data from source table\n",
					"    LoggingUtil().log_info(\"Starting data insertion into odw_harmonised_db.load_sap_hr_monthly from sap_hr_history_monthly\")\n",
					"    spark.sql(\"\"\"\n",
					"    INSERT INTO odw_harmonised_db.load_sap_hr_monthly (\n",
					"        PersNo,\n",
					"        Firstname,\n",
					"        Lastname,\n",
					"        EmployeeNo,\n",
					"        CoCd,\n",
					"        CompanyCode,\n",
					"        PA,\n",
					"        PersonnelArea,\n",
					"        PSubarea,\n",
					"        PersonnelSubarea,\n",
					"        Orgunit,\n",
					"        OrganizationalUnit,\n",
					"        Organizationalkey,\n",
					"        OrganizationalKey1,\n",
					"        WorkC,\n",
					"        WorkContract,\n",
					"        CT,\n",
					"        ContractType,\n",
					"        PSgroup,\n",
					"        PayBandDescription,\n",
					"        FTE,\n",
					"        Wkhrs,\n",
					"        IndicatorPartTimeEmployee,\n",
					"        S,\n",
					"        EmploymentStatus,\n",
					"        GenderKey,\n",
					"        TRAStartDate,\n",
					"        TRAEndDate,\n",
					"        TRAStatus,\n",
					"        TRAGrade,\n",
					"        PrevPersNo,\n",
					"        ActR,\n",
					"        ReasonforAction,\n",
					"        Position,\n",
					"        Position1,\n",
					"        CostCtr,\n",
					"        CostCentre,\n",
					"        CivilServiceStart,\n",
					"        DatetoCurrentJob,\n",
					"        SeniorityDate,\n",
					"        DatetoSubstGrade,\n",
					"        PersNo1,\n",
					"        NameofManagerOM,\n",
					"        ManagerPosition,\n",
					"        ManagerPositionText,\n",
					"        CounterSignManager,\n",
					"        Loc,\n",
					"        Location,\n",
					"        OrgStartDate,\n",
					"        FixTermEndDate,\n",
					"        LoanStartDate,\n",
					"        LoanEndDate,\n",
					"        EEGrp,\n",
					"        EmployeeGroup,\n",
					"        Annualsalary,\n",
					"        Curr,\n",
					"        NInumber,\n",
					"        Birthdate,\n",
					"        Ageofemployee,\n",
					"        EO,\n",
					"        Ethnicorigin,\n",
					"        NID,\n",
					"        Rel,\n",
					"        ReligiousDenominationKey,\n",
					"        SxO,\n",
					"        WageType,\n",
					"        EmployeeSubgroup,\n",
					"        LOAAbsType,\n",
					"        LOAAbsenceTypeText,\n",
					"        Schemereference,\n",
					"        PensionSchemeName,\n",
					"        DisabilityCode,\n",
					"        DisabilityText,\n",
					"        DisabilityCodeDescription,\n",
					"        PArea,\n",
					"        PayrollArea,\n",
					"        AssignmentNumber,\n",
					"        FTE2,\n",
					"        Report_MonthEnd_Date,\n",
					"        PDAC_ETL_Date,\n",
					"        SourceSystemID,\n",
					"        IngestionDate,\n",
					"        ValidTo,\n",
					"        RowID,\n",
					"        IsActive\n",
					"    )\n",
					"    SELECT\n",
					"        PersNo,\n",
					"        Firstname,\n",
					"        Lastname,\n",
					"        EmployeeNo,\n",
					"        CoCd,\n",
					"        CompanyCode,\n",
					"        PA,\n",
					"        PersonnelArea,\n",
					"        PSubarea,\n",
					"        PersonnelSubarea,\n",
					"        Orgunit,\n",
					"        OrganizationalUnit,\n",
					"        Organizationalkey,\n",
					"        OrganizationalKey1,\n",
					"        WorkC,\n",
					"        WorkContract,\n",
					"        CT,\n",
					"        ContractType,\n",
					"        PSgroup,\n",
					"        PayBandDescription,\n",
					"        CASE \n",
					"    WHEN TRIM(FTE) = '' OR TRIM(FTE) = 'NULL' THEN NULL \n",
					"    ELSE FORMAT_NUMBER(TRY_CAST(FTE AS FLOAT), 2)\n",
					"END AS FTE ,\n",
					"        CASE WHEN Wkhrs = '' THEN NULL ELSE \n",
					"        FORMAT_NUMBER(TRY_CAST(Wkhrs AS FLOAT), 2) END AS Wkhrs,\n",
					"        CASE WHEN IndicatorPartTimeEmployee = '' THEN NULL ELSE   IndicatorPartTimeEmployee end as IndicatorPartTimeEmployee ,\n",
					"        S,\n",
					"        EmploymentStatus,\n",
					"        GenderKey,\n",
					"        TRAStartDate AS TRAStartDate,\n",
					"        TRAEndDate AS TRAEndDate,\n",
					"        TRAStatus,\n",
					"        TRAGrade,\n",
					"        PrevPersNo,\n",
					"        ActR,\n",
					"        ReasonforAction,\n",
					"        Position,\n",
					"        Position1,\n",
					"        CostCtr,\n",
					"        CostCentre,\n",
					"        cast(to_timestamp(CivilServiceStart, \"dd/MM/yyyy\") as date) as CivilServiceStart,\n",
					"        cast(to_timestamp(DatetoCurrentJob, 'dd/MM/yyyy') as date) AS DatetoCurrentJob,\n",
					"        cast(to_timestamp(SeniorityDate, 'dd/MM/yyyy') as date) AS SeniorityDate,\n",
					"        cast(to_timestamp(DatetoSubstGrade, 'dd/MM/yyyy') as date) as DatetoSubstGrade,\n",
					"        PersNo1,\n",
					"        NameofManagerOM,\n",
					"        ManagerPosition,\n",
					"        ManagerPositionText,\n",
					"        CounterSignManager,\n",
					"        Loc,\n",
					"        Location,\n",
					"        cast(to_timestamp(OrgStartDate, 'dd/MM/yyyy') as date) as OrgStartDate,\n",
					"        FixTermEndDate AS FixTermEndDate,\n",
					"        LoanStartDate AS LoanStartDate,\n",
					"        LoanEndDate AS LoanEndDate,\n",
					"        EEGrp,\n",
					"        EmployeeGroup,\n",
					"        CASE WHEN Annualsalary = '' THEN NULL ELSE TRY_CAST(Annualsalary AS DOUBLE) END AS Annualsalary,\n",
					"        Curr,\n",
					"        null as NInumber,\n",
					"        to_date(Birthdate, 'dd/MM/yyyy') as Birthdate,\n",
					"        Ageofemployee,\n",
					"        EO,\n",
					"        Ethnicorigin,\n",
					"        NID,\n",
					"        Rel,\n",
					"        ReligiousDenominationKey,\n",
					"        SxO,\n",
					"        WageType,\n",
					"        EmployeeSubgroup,\n",
					"        LOAAbsType,\n",
					"        LOAAbsenceTypeText,\n",
					"        Schemereference,\n",
					"        PensionSchemeName,\n",
					"        DisabilityCode,\n",
					"        DisabilityText,\n",
					"        DisabilityCodeDescription,\n",
					"        PArea,\n",
					"        PayrollArea,\n",
					"        AssignmentNumber,\n",
					"        CASE \n",
					"    WHEN TRIM(FTE2) = '' OR TRIM(FTE2) = 'NULL' THEN NULL \n",
					"    ELSE FORMAT_NUMBER(TRY_CAST(FTE2 AS FLOAT), 2)\n",
					"END AS   FTE2,\n",
					"        cast(to_timestamp(Report_MonthEnd_Date , 'dd/MM/yyyy') as date) as Report_MonthEnd_Date,\n",
					"        CURRENT_TIMESTAMP() AS PDAC_ETL_Date,\n",
					"        'saphr' AS SourceSystemID,\n",
					"        CURRENT_DATE() AS IngestionDate,\n",
					"        CURRENT_TIMESTAMP() AS ValidTo,\n",
					"        NULL AS RowID,\n",
					"        'Y' AS IsActive\n",
					"    FROM odw_standardised_db.sap_hr_history_monthly\n",
					"    \"\"\")\n",
					"    \n",
					"    # Check the number of rows inserted\n",
					"    inserted_rows = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.load_sap_hr_monthly\").collect()[0]['count']\n",
					"    result[\"record_count\"] = inserted_rows\n",
					"    LoggingUtil().log_info(f\"Successfully inserted {inserted_rows} rows into odw_harmonised_db.load_sap_hr_monthly\")\n",
					"    \n",
					"    # Update RowID with MD5 hash\n",
					"    LoggingUtil().log_info(\"Starting RowID update with MD5 hash values\")\n",
					"    spark.sql(\"\"\"\n",
					"    UPDATE odw_harmonised_db.load_sap_hr_monthly\n",
					"    SET RowID = md5(concat_ws('|', \n",
					"        PersNo, Firstname, Lastname, EmployeeNo, CoCd, CompanyCode, PA, PersonnelArea, PSubarea, PersonnelSubarea, \n",
					"        Orgunit, OrganizationalUnit, Organizationalkey, OrganizationalKey1, WorkC, WorkContract, CT, ContractType, \n",
					"        PSgroup, PayBandDescription, FTE, Wkhrs, IndicatorPartTimeEmployee, S, EmploymentStatus, GenderKey, \n",
					"        TRAStartDate, TRAEndDate, TRAStatus, TRAGrade, PrevPersNo, ActR, ReasonforAction, Position, Position1, \n",
					"        CostCtr, CostCentre, CivilServiceStart, DatetoCurrentJob, SeniorityDate, DatetoSubstGrade, PersNo1, \n",
					"        NameofManagerOM, ManagerPosition, ManagerPositionText, CounterSignManager, Loc, Location, OrgStartDate, \n",
					"        FixTermEndDate, LoanStartDate, LoanEndDate, EEGrp, EmployeeGroup, Annualsalary, Curr, NInumber, Birthdate, \n",
					"        Ageofemployee, EO, Ethnicorigin, NID, Rel, ReligiousDenominationKey, SxO, WageType, EmployeeSubgroup, \n",
					"        LOAAbsType, LOAAbsenceTypeText, Schemereference, PensionSchemeName, DisabilityCode, DisabilityText, \n",
					"        DisabilityCodeDescription, PArea, PayrollArea, AssignmentNumber, FTE2\n",
					"    ))\n",
					"    \"\"\")\n",
					"    LoggingUtil().log_info(\"Successfully updated all RowID values with MD5 hashes\")\n",
					"    \n",
					"    # Verify data quality\n",
					"    null_rowid_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.load_sap_hr_monthly WHERE RowID IS NULL\").collect()[0]['count']\n",
					"    if null_rowid_count > 0:\n",
					"        LoggingUtil().log_error(f\"Data quality issue: {null_rowid_count} rows have NULL RowID values\")\n",
					"        result[\"status\"] = \"warning\"\n",
					"    else:\n",
					"        LoggingUtil().log_info(\"Data quality check passed: No NULL RowID values found\")\n",
					"    \n",
					"    # Final success message\n",
					"    LoggingUtil().log_info(\"SAP HR monthly data processing completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Capture error information and limit to 300 characters\n",
					"    error_msg = f\"Error in SAP HR monthly data processing: {str(e)}\"\n",
					"    LoggingUtil().log_error(error_msg)\n",
					"    LoggingUtil().log_exception(e)\n",
					"    \n",
					"    # Truncate error message if it exceeds 300 characters\n",
					"    truncated_error = (error_msg[:297] + '...') if len(error_msg) > 300 else error_msg\n",
					"    \n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = truncated_error\n",
					"    result[\"record_count\"] = -1  # Indicate failure with -1 count\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    LoggingUtil().log_info(\"Flushing logs\")\n",
					"    LoggingUtil().flush_logging()\n",
					"    \n",
					"    # Output the result as JSON for ADF to capture\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": null
			}
		]
	}
}