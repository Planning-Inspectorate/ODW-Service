{
	"name": "py_sap_hr_protected_data",
	"properties": {
		"folder": {
			"name": "odw-harmonised/saphr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodw34",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "fb5e89ae-de3a-4f66-9569-73dff107df28"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ff442a29-fc06-4a13-8e3e-65fd5da513b3/resourceGroups/pins-rg-data-odw-dev-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-dev-uks/bigDataPools/pinssynspodw34",
				"name": "pinssynspodw34",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodw34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The purpose of this notebook is to read data from Harmoised layer and build a table for Curated Layer.\n",
					"\n",
					"**Author** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   **Created Date** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Description**  \n",
					"Prathap Adicherla &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;25-02-2025 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This Notebook is designed to facilitate Enriches data with HR attributes."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Entity Name : Protected Data</u>\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Intialisations"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import udf, col, lit, when, coalesce, concat, sum, avg, max, min, count, countDistinct, date_format, to_date, datediff, months_between, year, month,  hour, minute, second, expr, asc, desc\n",
					"from pyspark.sql.types import DateType, TimestampType, StringType, IntegerType, FloatType, DoubleType, BooleanType, StructType, StructField, ArrayType, MapType\n",
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, ntile\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql.avro.functions import from_avro, to_avro\n",
					"from pyspark.sql.streaming import DataStreamReader, DataStreamWriter\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"from pyspark.sql.catalog import Catalog\n",
					"from pyspark.sql.column import Column\n",
					"from pyspark.sql.group import GroupedData\n",
					"from pyspark.sql.pandas.functions import pandas_udf\n",
					"\n",
					"from datetime import datetime, timedelta\n",
					"import pandas as pd\n",
					"import numpy as np\n",
					"\n",
					""
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"%run utils/py_logging_decorator"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Data Load into SAP_HR_PC\n",
					"\n",
					"###### handling of sensitive information (such as ethnic origin and disability status) is crucial for compliance with regulations like GDPR or equal employment laws. Ensuring that this data is accurately recorded and updated helps mitigate legal risks"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"\n",
					"\n",
					"try:\n",
					"   logInfo(\"Starting SAP HR Protected Characteristics data processing\")\n",
					"   \n",
					"   # Disable ANSI SQL mode for more flexible handling\n",
					"   logInfo(\"Disabling ANSI SQL mode\")\n",
					"   spark.sql(\"\"\"\n",
					"   SET spark.sql.ansi.enabled = false\n",
					"   \"\"\")\n",
					"   logInfo(\"ANSI SQL mode disabled successfully\")\n",
					"   \n",
					"   # Clear the target table\n",
					"   logInfo(\"Clearing target table odw_harmonised_db.SAP_HR_PC\")\n",
					"   spark.sql(\"\"\"\n",
					"   DELETE FROM odw_harmonised_db.SAP_HR_PC\n",
					"   \"\"\")\n",
					"   logInfo(\"Target table cleared successfully\")\n",
					"   \n",
					"   # Insert data with transformations\n",
					"   logInfo(\"Inserting transformed data into odw_harmonised_db.SAP_HR_PC\")\n",
					"   spark.sql(\"\"\"\n",
					"   INSERT INTO odw_harmonised_db.SAP_HR_PC\n",
					"   SELECT \n",
					"       RefNo,\n",
					"       NULLIF(EthnicOrigin, '') AS EthnicOrigin,\n",
					"       NULLIF(ReligiousDenominationKey, '') AS ReligiousDenominationKey,\n",
					"       NULLIF(SxO, '') AS SxO,\n",
					"       Grade,\n",
					"       NULLIF(DisabilityText, '') AS DisabilityText,\n",
					"       CAST(to_timestamp(Report_MonthEnd_Date, \"dd/MM/yyyy\") AS DATE) AS Report_MonthEnd_Date,\n",
					"       'saphr' AS SourceSystemID,\n",
					"       CURRENT_DATE() AS IngestionDate,\n",
					"       CURRENT_TIMESTAMP() AS ValidTo,\n",
					"       md5(concat_ws('|', RefNo, EthnicOrigin, ReligiousDenominationKey, SxO, Grade, DisabilityText)) AS RowID,\n",
					"       'Y' AS IsActive\n",
					"   FROM odw_standardised_db.sap_protected_monthly\n",
					"   \"\"\")\n",
					"   \n",
					"   # Get count of records inserted\n",
					"   record_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.SAP_HR_PC\").collect()[0]['count']\n",
					"   logInfo(f\"Successfully inserted {record_count} records into odw_harmonised_db.SAP_HR_PC\")\n",
					"   \n",
					"   logInfo(\"SAP HR Protected Characteristics data processing completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"   # Log the exception in detail\n",
					"   logError(f\"Error in SAP HR Protected Characteristics data processing: {str(e)}\")\n",
					"   logException(e)\n",
					"   \n",
					"   # Re-raise the exception to ensure the notebook fails properly\n",
					"   raise e\n",
					"finally:\n",
					"   # Always flush logs regardless of success or failure\n",
					"   logInfo(\"Flushing logs\")\n",
					"   flushLogging()"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## PC_INSERT_DELETE"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"import json\n",
					"\n",
					"# Initialize result dictionary\n",
					"result = {\n",
					"    \"status\": \"success\",\n",
					"    \"record_count\": 0,\n",
					"    \"error_message\": None\n",
					"}\n",
					"\n",
					"try:\n",
					"    logInfo(\"Starting complex MERGE operation for protected data\")\n",
					"    \n",
					"    # Get baseline count before operation\n",
					"    baseline_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.sap_hr_protected_data\").collect()[0]['count']\n",
					"    logInfo(f\"Baseline target record count: {baseline_count}\")\n",
					"    \n",
					"    # Get source record count for validation\n",
					"    source_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.SAP_HR_PC\").collect()[0]['count']\n",
					"    logInfo(f\"Source record count: {source_count}\")\n",
					"    \n",
					"    if source_count == 0:\n",
					"        logInfo(\"Warning: No source records found in SAP_HR_PC table\")\n",
					"        result[\"status\"] = \"warning\"\n",
					"        result[\"record_count\"] = baseline_count\n",
					"    else:\n",
					"        # Complex MERGE operation with data preparation\n",
					"        logInfo(\"Executing MERGE to update sap_hr_protected_data\")\n",
					"        spark.sql(\"\"\"\n",
					"        -- First, ensure SAP_HR_PC is properly formatted with dates\n",
					"        WITH source_data AS (\n",
					"            SELECT \n",
					"                RefNo,\n",
					"                NULLIF(EthnicOrigin, '') AS EthnicOrigin,\n",
					"                NULLIF(ReligiousDenominationKey, '') AS ReligiousDenominationKey,\n",
					"                NULLIF(SxO, '') AS SxO,\n",
					"                Grade,\n",
					"                NULLIF(DisabilityText, '') AS DisabilityText,\n",
					"                cast(to_timestamp(Report_MonthEnd_Date, \"dd/MM/yyyy\") as date) AS Report_MonthEnd_Date,\n",
					"                'saphr' AS SourceSystemID,\n",
					"                CURRENT_DATE() AS IngestionDate,\n",
					"                CURRENT_TIMESTAMP() AS ValidTo,\n",
					"                NULL AS RowID,\n",
					"                'Y' AS IsActive,\n",
					"                -- Calculate hash in the source directly\n",
					"                md5(concat(\n",
					"                    coalesce(RefNo, ''),\n",
					"                    coalesce(NULLIF(EthnicOrigin, ''), ''),\n",
					"                    coalesce(NULLIF(ReligiousDenominationKey, ''), ''),\n",
					"                    coalesce(NULLIF(SxO, ''), ''),\n",
					"                    coalesce(Grade, ''),\n",
					"                    coalesce(NULLIF(DisabilityText, ''), '')\n",
					"                )) AS record_hash\n",
					"            FROM odw_harmonised_db.SAP_HR_PC\n",
					"        ),\n",
					"        -- Deduplicate source data to ensure only one row per key\n",
					"        deduplicated_source AS (\n",
					"            SELECT *,\n",
					"                   ROW_NUMBER() OVER (\n",
					"                       PARTITION BY RefNo, Report_MonthEnd_Date \n",
					"                       ORDER BY IngestionDate DESC\n",
					"                   ) AS row_num\n",
					"            FROM source_data\n",
					"        )\n",
					"        \n",
					"        -- Use MERGE with deduplicated source\n",
					"        MERGE INTO odw_harmonised_db.sap_hr_protected_data target\n",
					"        USING (SELECT * FROM deduplicated_source WHERE row_num = 1) source\n",
					"        ON source.RefNo = target.RefNo AND source.Report_MonthEnd_Date = target.Report_MonthEnd_Date\n",
					"        WHEN MATCHED AND \n",
					"             md5(concat(\n",
					"                coalesce(target.RefNo, ''),\n",
					"                coalesce(target.EthnicOrigin, ''),\n",
					"                coalesce(target.ReligiousDenominationKey, ''),\n",
					"                coalesce(target.SxO, ''),\n",
					"                coalesce(target.Grade, ''),\n",
					"                coalesce(target.DisabilityText, '')\n",
					"             )) != source.record_hash \n",
					"        THEN UPDATE SET\n",
					"            EthnicOrigin = source.EthnicOrigin,\n",
					"            ReligiousDenominationKey = source.ReligiousDenominationKey,\n",
					"            SxO = source.SxO,\n",
					"            Grade = source.Grade,\n",
					"            DisabilityText = source.DisabilityText,\n",
					"            SourceSystemID = source.SourceSystemID,\n",
					"            IngestionDate = source.IngestionDate,\n",
					"            ValidTo = source.ValidTo,\n",
					"            IsActive = source.IsActive\n",
					"        WHEN NOT MATCHED THEN\n",
					"        INSERT (\n",
					"            RefNo, EthnicOrigin, ReligiousDenominationKey, SxO, Grade, \n",
					"            DisabilityText, Report_MonthEnd_Date, SourceSystemID, \n",
					"            IngestionDate, ValidTo, RowID, IsActive\n",
					"        )\n",
					"        VALUES (\n",
					"            source.RefNo, source.EthnicOrigin, source.ReligiousDenominationKey, \n",
					"            source.SxO, source.Grade, source.DisabilityText, source.Report_MonthEnd_Date, \n",
					"            source.SourceSystemID, source.IngestionDate, source.ValidTo, \n",
					"            source.RowID, source.IsActive\n",
					"        )\n",
					"        \"\"\")\n",
					"        \n",
					"        logInfo(\"MERGE operation completed successfully\")\n",
					"        \n",
					"        # Get final record count after operation\n",
					"        final_count = spark.sql(\"SELECT COUNT(*) as count FROM odw_harmonised_db.sap_hr_protected_data\").collect()[0]['count']\n",
					"        result[\"record_count\"] = final_count\n",
					"        \n",
					"        logInfo(f\"Operation summary - Source records: {source_count}, Final target records: {final_count}\")\n",
					"        logInfo(f\"Net change in target table: {final_count - baseline_count} records\")\n",
					"        \n",
					"        # Validate operation results\n",
					"        if final_count < baseline_count:\n",
					"            warning_msg = f\"Warning: Target record count decreased from {baseline_count} to {final_count}\"\n",
					"            logInfo(warning_msg)\n",
					"            result[\"status\"] = \"warning\"\n",
					"        elif final_count == baseline_count:\n",
					"            logInfo(\"No net change in record count - all records may have been updates rather than inserts\")\n",
					"        else:\n",
					"            logInfo(f\"✓ Successfully processed data - {final_count - baseline_count} net new records added\")\n",
					"        \n",
					"        # Additional validation - check for any records with null RefNo (data quality check)\n",
					"        null_refno_count = spark.sql(\"\"\"\n",
					"        SELECT COUNT(*) as count \n",
					"        FROM odw_harmonised_db.sap_hr_protected_data \n",
					"        WHERE RefNo IS NULL\n",
					"        \"\"\").collect()[0]['count']\n",
					"        \n",
					"        if null_refno_count > 0:\n",
					"            warning_msg = f\"Warning: Found {null_refno_count} records with null RefNo\"\n",
					"            logInfo(warning_msg)\n",
					"            if result[\"status\"] != \"warning\":\n",
					"                result[\"status\"] = \"partial_success\"\n",
					"        else:\n",
					"            logInfo(\"✓ Data quality check passed - no null RefNo values found\")\n",
					"    \n",
					"    logInfo(\"Complex MERGE operation for protected data completed successfully\")\n",
					"\n",
					"except Exception as e:\n",
					"    # Capture error information and limit to 300 characters\n",
					"    error_msg = f\"Error in complex MERGE operation for protected data: {str(e)}\"\n",
					"    logError(error_msg)\n",
					"    \n",
					"    # Enhanced error context for common Delta Lake issues\n",
					"    if \"DeltaUnsupportedOperationException\" in str(e):\n",
					"        logError(\"Delta Lake MERGE conflict detected - check for duplicate source rows\")\n",
					"        error_msg += \" [Delta MERGE conflict]\"\n",
					"    elif \"AnalysisException\" in str(e):\n",
					"        logError(\"SQL analysis error - check table structure and column references\")\n",
					"        error_msg += \" [SQL analysis error]\"\n",
					"    elif \"ParseException\" in str(e):\n",
					"        logError(\"SQL parsing error - check MERGE syntax and column names\")\n",
					"        error_msg += \" [SQL parse error]\"\n",
					"    elif \"Path does not exist\" in str(e):\n",
					"        logError(\"Table path does not exist - check table configuration\")\n",
					"        error_msg += \" [Table not found]\"\n",
					"    \n",
					"    logException(e)\n",
					"    \n",
					"    # Update result for error case\n",
					"    result[\"status\"] = \"failed\"\n",
					"    result[\"error_message\"] = error_msg[:300]  # Truncate to 300 characters\n",
					"    result[\"record_count\"] = -1  # Indicate failure with -1 count\n",
					"    \n",
					"    # Log recovery suggestions\n",
					"\n",
					"    \n",
					"    # Re-raise the exception to ensure the notebook fails properly\n",
					"    raise e\n",
					"\n",
					"finally:\n",
					"    # Always flush logs regardless of success or failure\n",
					"    logInfo(\"Flushing logs and preparing final result\")\n",
					"    flushLogging()\n",
					"    \n",
					"    # Log the final result for debugging\n",
					"    logInfo(f\"Final result summary:\")\n",
					"    logInfo(f\"  Status: {result['status']}\")\n",
					"    logInfo(f\"  Total record count: {result['record_count']}\")\n",
					"    logInfo(f\"  Error message: {result['error_message']}\")\n",
					"    \n",
					"    # Output the result as JSON for ADF to capture\n",
					"    mssparkutils.notebook.exit(json.dumps(result))"
				],
				"execution_count": 4
			}
		]
	}
}