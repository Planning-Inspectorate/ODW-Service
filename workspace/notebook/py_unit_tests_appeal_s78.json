{
	"name": "py_unit_tests_appeal_s78",
	"properties": {
		"folder": {
			"name": "utils/unit-tests"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pinssynspodwpr",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4edfaff4-65e5-4a66-b306-acd5c2aba095"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/a82fd28d-5989-4e06-a0bb-1a5d859f9e0c/resourceGroups/pins-rg-data-odw-prod-uks/providers/Microsoft.Synapse/workspaces/pins-synw-odw-prod-uks/bigDataPools/pinssynspodwpr",
				"name": "pinssynspodwpr",
				"type": "Spark",
				"endpoint": "https://pins-synw-odw-dev-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pinssynspodwpr",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### This unit test notebook needs more modification once we ingest all horizon tables as well as we  need tests on `odw_harmonised_db.appeal_s78` to trace both service bus and horizon records. There are not tests for this table `odw_harmonised_db.appeal_s78` yet as of today (6/03/2025)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import json\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql import functions as F\n",
					"import pprint"
				],
				"execution_count": 48
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"entity_name: str = 'appeal-s78'\n",
					"folder_name: str = 'appeal-s78'\n",
					"std_db_name: str = 'odw_standardised_db'\n",
					"hrm_db_name: str = 'odw_harmonised_db'\n",
					"curated_db_name: str = 'odw_curated_db'"
				],
				"execution_count": 49
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"std_table_name: str = 'sb_appeal_s78'\n",
					"std_table_name_final : str = 'horizon_appeal_s78_final'\n",
					"hrm_table_name: str = 'sb_appeal_s78'\n",
					"curated_table_name: str = 'appeal_s78'"
				],
				"execution_count": 50
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\n",
					"sb_std_df = spark.table(f\"{std_db_name}.{std_table_name}\")\n",
					"sb_hrm_df = spark.table(f\"{hrm_db_name}.{hrm_table_name}\")\n",
					"curated_df = spark.table(f\"{hrm_db_name}.{hrm_table_name}\")\n",
					""
				],
				"execution_count": 51
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"sb_std_careference = sb_std_df.select(\"caseReference\")\n",
					"sb_hrm_careference = sb_hrm_df.select(\"caseReference\")\n",
					"curated_careference = curated_df.select(\"caseReference\")"
				],
				"execution_count": 52
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#keep track of the exitCodes, if the exit code is not zero then we've had failures, we flip the boolean\n",
					"exitCode: int = 0"
				],
				"execution_count": 53
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run /utils/unit-tests/py_unit_tests_functions"
				],
				"execution_count": 54
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"sb_std_schema = create_spark_schema(std_db_name, entity_name)\n",
					"sb_std_table_schema = spark.table(f\"{std_db_name}.{std_table_name}\").schema\n",
					"sb_hrm_schema = create_spark_schema(hrm_db_name, entity_name)\n",
					"sb_hrm_table_schema = spark.table(f\"{hrm_db_name}.{hrm_table_name}\").schema\n",
					"curated_schema = create_spark_schema(curated_db_name, entity_name)\n",
					"curated_table_schema = spark.table(f\"{curated_db_name}.{curated_table_name}\").schema"
				],
				"execution_count": 56
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"data_model_columns = [field.name for field in curated_table_schema.fields]"
				],
				"execution_count": 57
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Compare schemas"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"std_schema_correct: bool = test_compare_schemas(sb_std_schema, sb_std_table_schema)\n",
					"exitCode += int(not std_schema_correct)\n",
					"print(f\"Service bus standardised schema correct: {std_schema_correct}\\nTable: {std_db_name}.{std_table_name}\\nDifferences shown above (if any)\")\n",
					"hrm_schema_correct: bool = test_compare_schemas(sb_hrm_schema, sb_hrm_table_schema)\n",
					"print(f\"Service bus harmonised schema correct: {hrm_schema_correct}\\nTable: {hrm_db_name}.{hrm_table_name}\\nDifferences shown above (if any)\")\n",
					"exitCode += int(not hrm_schema_correct)\n",
					"cur_schema_correct: bool = test_compare_schemas(curated_schema, curated_table_schema)\n",
					"print(f\"Curated schema correct: {cur_schema_correct}\\nTable: {curated_db_name}.{curated_table_name}\\nDifferences shown above (if any)\")\n",
					"exitCode += int(not cur_schema_correct)"
				],
				"execution_count": 16
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Compare service bus standardised with harmonised\n",
					"As of now ( 4/3/2025) we havent horizon and service bus merged yet."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"standardised_count, harmonised_count, counts_match = test_std_same_rows_hrm(std_table_name, hrm_table_name)\n",
					"print(f\"Standardised Count: {standardised_count: ,}\\nHarmonised Count: {harmonised_count: ,}\\nCounts match: {counts_match}\")\n",
					"\n",
					"if standardised_count > harmonised_count:\n",
					"    exitCode += 1\n",
					"    print(f\"{standardised_count - harmonised_count} rows from Standardised are missing in Harmonised.\" )\n",
					"    differentiate_std_and_hrm(f\"{std_db_name}.{std_table_name}\", f\"{hrm_db_name}.{hrm_table_name}\", data_model_columns)"
				],
				"execution_count": 19
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Compare final harmonised table (if combined with Horizon) with curated table\n",
					"Comparing where IsActive = Y in harmonised = curated row count"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					" ###### As of now (4/3/2025) this harmonised and curated table has only service bus data"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"harmonised_final_count, curated_count, counts_match = test_curated_row_count(hrm_table_name, curated_table_name, data_model_columns)\n",
					"print(f\"Harmonised Final Count: {harmonised_final_count: ,}\\nCurated Count: {curated_count: ,}\\nCounts match: {counts_match}\")\n",
					"exitCode += int(not counts_match)"
				],
				"execution_count": 20
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### This needs to be changed once we have horizon and service bus together in harmonised."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Check if there is data coming from ODT (back-office-appeals) and Horizon\n",
					"# automated version of previous visual check which is now moved to bottom of notebook\n",
					"\n",
					"check_ODT_and_horizon_data_query = \"\"\"\n",
					"(SELECT DISTINCT\n",
					"*\n",
					"FROM odw_harmonised_db.appeal_s78\n",
					"WHERE  ODTsourcesystem = 'ODT'\n",
					"ORDER BY ingestiondate DESC)\n",
					"\"\"\"\n",
					"\n",
					"ODT_and_horizon_dataframe = get_dataframe(check_ODT_and_horizon_data_query)\n",
					"\n",
					"if ODT_and_horizon_dataframe.isEmpty():\n",
					"    print(\"Failed: data coming from ODT (back-office-appeals) and Horizon\")\n",
					"    exitCode += 1"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if test_sb_std_to_sb_hrm_no_dropping_records(std_table_name, hrm_table_name, 'caseReference'):\n",
					"    pass  \n",
					"else:\n",
					"    print(\"Failed: test_sb_std_to_sb_hrm_no_dropping_records\")\n",
					"    exitCode += 1"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# To be changed once we have a hrm_table_final\n",
					"if test_sb_hrm_to_hrm_final_no_dropping_records(hrm_table_name, hrm_table_name, 'caseReference'):\n",
					"    pass\n",
					"else:\n",
					"    print(\"Failed: test_sb_hrm_to_hrm_final_no_dropping_records\")\n",
					"    exitCode += 1"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# To be changed once we have a hrm_table_final\n",
					"if test_hrm_to_curated_no_dropping_records(hrm_table_name, curated_table_name, 'caseReference'):\n",
					"    pass\n",
					"else:\n",
					"    print(\"Failed: test_hrm_to_curated_no_dropping_records\")\n",
					"    exitCode += 1"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if test_sb_std_to_hrm_no_message_id_dropped(f\"{std_db_name}.{std_table_name}\", f\"{hrm_db_name}.{hrm_table_name}\"):\n",
					"    pass\n",
					"else:\n",
					"    print(\"Failed: test_sb_std_to_hrm_no_message_id_dropped\")\n",
					"    exitCode += 1"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if test_sb_std_to_hrm_no_active_deleted_record(f\"{std_db_name}.{std_table_name}\", f\"{hrm_db_name}.{hrm_table_name}\"):\n",
					"    pass\n",
					"else:\n",
					"    print(\"Failed: test_sb_std_to_hrm_no_active_deleted_record\")\n",
					"    exitCode += 1"
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"mssparkutils.notebook.exit(exitCode)"
				],
				"execution_count": 27
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Visual checks moved to below notebook exit\n",
					"This is so they won't run when notebook is called during automation run"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Below you will find a bit of EDA on horizon data as of now ( 4/3/2025)"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def check_count_match(hzn_table: str, hzn_table_2: str, primary_k_1:str, primary_k_2:str) -> tuple: \n",
					"    df_hzn_1: Dataframe = spark.sql(f\"\"\"\n",
					"                                SELECT DISTINCT {primary_k_1}\n",
					"                                FROM odw_standardised_db.{hzn_table}\n",
					"                            \"\"\")\n",
					"\n",
					"    df_hzn_2: Dataframe = spark.sql(f\"\"\"\n",
					"                                SELECT DISTINCT {primary_k_2}\n",
					"                                FROM odw_standardised_db.{hzn_table_2}\n",
					"                                \n",
					"                            \"\"\")\n",
					"\n",
					"    return df_hzn_1.count(), df_hzn_2.count(), df_hzn_1.count() == df_hzn_2.count()\n",
					""
				],
				"execution_count": 31
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### All tables are expected to not have the same amount of records from the master table `horizon_Cases_s78` eg. for every caseunique id in case specialism there could be from 0-3 records.\n",
					"##### All tables that have primary key = casenodeid they have same amount records and are expected to have duplicates as they are pivot tables confirmed with Gareth. (10/02/2025)\n",
					"##### Also will not check if the count macth for typeofprocedure and horizon_typeofreasonforcase as they are reference tables and also they are not directly connected with horizon cases_s78."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"count_hzn_1, count_hzn_2, count_records = check_count_match('horizoncases_s78','cases_specialisms','casereference','caseReference')\n",
					"print(f\"Count of horizon_cases_s78: {count_hzn_1}, Count of cases_specialisms: {count_hzn_2}, Does the count match?: {count_records}\")\n",
					"\n",
					"count_hzn_1, count_hzn_2, count_records = check_count_match('horizoncases_s78','casesitestrings','caseuniqueid','casenodeid')\n",
					"print(f\"Count of horizon_cases_s78: {count_hzn_1}, Count of casesitestrings: {count_hzn_2}, Does the count match?: {count_records}\")\n",
					"\n",
					"count_hzn_1, count_hzn_2, count_records = check_count_match('horizoncases_s78','casedocumentdatesdates','caseuniqueid','casenodeid')\n",
					"print(f\"Count of horizon_cases_s78: {count_hzn_1}, Count of casedocumentdatesdates: {count_hzn_2}, Does the count match?: {count_records}\")\n",
					"\n",
					"count_hzn_1, count_hzn_2, count_records = check_count_match('horizoncases_s78','vw_case_dates','caseuniqueid','casenodeid')\n",
					"print(f\"Count of horizon_cases_s78: {count_hzn_1}, Count of vw_case_dates: {count_hzn_2}, Does the count match?: {count_records}\")\n",
					"\n",
					"\n",
					"count_hzn_1, count_hzn_2, count_records = check_count_match('horizoncases_s78','vw_addadditionaldata','caseuniqueid','AppealRefNumber')\n",
					"print(f\"Count of horizon_cases_s78: {count_hzn_1}, Count of vw_addadditionaldata: {count_hzn_2}, Does the count match?: {count_records}\")\n",
					"\n",
					"\n",
					"count_hzn_1, count_hzn_2, count_records = check_count_match('horizoncases_s78','vw_additionalfields','caseuniqueid','AppealRefNumber')\n",
					"print(f\"Count of horizon_cases_s78: {count_hzn_1}, Count of vw_additionalfields: {count_hzn_2}, Does the count match?: {count_records}\")\n",
					"\n",
					""
				],
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"source": [
					"def check_dupl_records(hzn_table:str, primary_k:str)->True:\n",
					"        df: Dataframe = spark.sql(f\"\"\"\n",
					"                                SELECT distinct 1\n",
					"                                FROM odw_standardised_db.{hzn_table} hzn_tbl\n",
					"                                where hzn_tbl.ingested_datetime = (SELECT MAX(hzn_tbl.ingested_datetime) from odw_standardised_db.{hzn_table} hzn_tbl)\n",
					"                                GROUP BY {primary_k}\n",
					"                                HAVING COUNT(*) > 1 \n",
					"                                Limit 1\n",
					"                            \"\"\")\n",
					"        # If df is empty, no duplicates; otherwise, duplicates exist\n",
					"        return  df.count() != 0"
				],
				"execution_count": 61
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Expected to have duplicate records in cases_specialism"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"print('Does cases_specialism have duplicates?:', check_dupl_records('cases_specialisms','caseReference'))\n",
					"print('Does horizoncases_s78 have duplicates?:',check_dupl_records('horizoncases_s78','casereference'))\n",
					"print('Does casesitestrings have duplicates?:', check_dupl_records('casesitestrings','casenodeid'))\n",
					"print('Does casedocumentdatesdates have duplicates?:',check_dupl_records('casedocumentdatesdates','casenodeid'))\n",
					"print('Does vw_case_dates have duplicates?:', check_dupl_records('vw_case_dates','casenodeid'))\n",
					"print('Does vw_addadditionaldata have duplicates?:',check_dupl_records('vw_addadditionaldata','AppealRefNumber'))\n",
					"print('Does vw_additionalfields have duplicates?:',check_dupl_records('vw_additionalfields','AppealRefNumber'))\n",
					"print('Does typeofprocedure have duplicates?:',check_dupl_records('typeofprocedure','Name'))\n",
					"print('Does horizon_typeofreasonforcase have duplicates?:',check_dupl_records('horizon_typeofreasonforcase','Id'))"
				],
				"execution_count": 34
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Trace some records to see if they exists in all horizon tables in phase 1. Below you will see SQL queries.\n",
					"###### We are going to take `casereference = 'APP/B3030/W/17/3184391'` or  `'APP/L2630/W/16/3155500'` from horizon_cases_s78 and check if records exists in the following tables. Each table I am checking I am using their primary key to check accordingly."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"SELECT *\n",
					"FROM odw_standardised_db.horizoncases_s78\n",
					"-- where casereference = 'APP/B3030/W/17/3184391'\n",
					"where casereference = 'APP/L2630/W/16/3155500'\n",
					"and ingested_datetime = (select max(ingested_datetime) from odw_standardised_db.horizoncases_s78)\n",
					"\n",
					"\n",
					"\n",
					""
				],
				"execution_count": 63
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"\n",
					"%%sql\n",
					"-- expected to see multiple rows for each casereference.\n",
					"SELECT DISTINCT *\n",
					"FROM odw_standardised_db.cases_specialisms\n",
					"WHERE CASEREFERENCE = '3155500' and ingested_datetime = (select max(ingested_datetime) from odw_standardised_db.cases_specialisms)\n",
					"\n",
					"    -- where casereference = '3184391'"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"SELECT *\n",
					"FROM odw_standardised_db.casesitestrings\n",
					"--where casenodeid = '23849934'\n",
					"where casenodeid = '17248610'\n",
					"and ingested_datetime = (select max(ingested_datetime) from odw_standardised_db.casesitestrings)\n",
					"    "
				],
				"execution_count": 68
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"SELECT *\n",
					"FROM odw_standardised_db.casedocumentdatesdates\n",
					"--where casenodeid = '23849934'\n",
					"where casenodeid = '17248610'\n",
					"and ingested_datetime = (select max(ingested_datetime) from odw_standardised_db.casedocumentdatesdates)\n",
					""
				],
				"execution_count": 67
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"SELECT *\n",
					"FROM odw_standardised_db.vw_case_dates\n",
					"-- where casenodeid = '23849934'\n",
					"where casenodeid = '17248610'\n",
					"and ingested_datetime = (select max(ingested_datetime) from odw_standardised_db.vw_case_dates)\n",
					""
				],
				"execution_count": 64
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"SELECT *\n",
					"FROM odw_standardised_db.vw_addadditionaldata\n",
					"-- where appealrefnumber = '3184391'\n",
					"where appealrefnumber = '3155500'\n",
					"and ingested_datetime = (select max(ingested_datetime) from odw_standardised_db.vw_addadditionaldata)\n",
					""
				],
				"execution_count": 66
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"SELECT *\n",
					"FROM odw_standardised_db.vw_additionalfields\n",
					"-- where appealrefnumber = '3184391'\n",
					" where appealrefnumber = '3155500'\n",
					"and ingested_datetime = (select max(ingested_datetime) from odw_standardised_db.vw_additionalfields)\n",
					""
				],
				"execution_count": 65
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### This table `horizon_appeal_s78_final` is the merged table that includes all horizon data tables. For this `casereference = 'APP/L2630/W/16/3155500'` we are expected to see 2 records per casereference as in `casespecialism` table there are 2 different info for casespecialism ( 1 to many relationship)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"SELECT *\n",
					"FROM odw_standardised_db.horizon_appeal_s78_final\n",
					"-- where casereference = 'APP/B3030/W/17/3184391'\n",
					"where casereference = 'APP/L2630/W/16/3155500'"
				],
				"execution_count": 42
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The below two tables are reference tables and thats why we dont have that many records. There will be joining in the next phase with other tables. After that we can do some record tracing.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					}
				},
				"source": [
					"%%sql\n",
					"SELECT *\n",
					"FROM odw_standardised_db.typeofprocedure\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					}
				},
				"source": [
					"%%sql\n",
					"SELECT *\n",
					"FROM odw_standardised_db.horizon_typeofreasonforcase"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Trace service bus data from standardised to harmonised\n",
					"\n",
					"We need to make sure the data has loaded through correctly. To do this efficiently, we will select a sample record and check that the data is maintained as it moves through the medallian architecture"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"SELECT * FROM  odw_standardised_db.sb_appeal_s78\n",
					"where casereference in(select distinct casereference from FROM odw_standardised_db.horizon_appeal_s78_final)"
				],
				"execution_count": 45
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"SELECT * FROM  odw_harmonised_db.sb_appeal_s78\n",
					"where caseReference = ''\n",
					"order by IngestionDate"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Check if there is data coming from ODT (back-office-appeals) and Horizon"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"SELECT DISTINCT\n",
					"*\n",
					"FROM odw_harmonised_db.sb_appeal_s78\n",
					"\n",
					"-- where  horizonfolderid  is null and \n",
					"where ODTsourcesystem = 'ODT'\n",
					"order BY\n",
					"    ingestiondate DESC"
				],
				"execution_count": 19
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"This is for later on once we have merged horizon and service bus :\n",
					"\n",
					"Horizon and Service Bus data successfully combined and flags set appropriately"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_sb_hrm = spark.sql(\"select * from odw_harmonised_db.sb_appeal_s78 where caseReference = '' order by IngestionDate\")\n",
					"\n",
					"df_cur = spark.sql(\"select * from odw_curated_db.sb_appeal_s78 where caseReference = ''\")\n",
					"\n",
					"\n",
					"\n",
					"display(df_sb_hrm)\n",
					"\n",
					"display(df_cur)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Data updated in curated correctly"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"SELECT\n",
					"    *\n",
					"FROM\n",
					"    odw_curated_db.sb_appeal_s78\n",
					"WHERE\n",
					"     caseReference = ''"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"data: DataFrame = spark.sql(\"SELECT * FROM odw_curated_db.sb_appeal_s78\")\n",
					"data.printSchema()"
				],
				"execution_count": null
			}
		]
	}
}