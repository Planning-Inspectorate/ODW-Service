###
# Pipeline to copy Synapse data lake config files from the repo main branch 
# to the Azure Storage account
###

---

# Create parameters
parameters:
- name: environment
  displayName: Environment
  type: string
  default: Dev
  values:
  - Dev
- name: failover_deployment
  displayName: 'Failover Deployment'
  type: boolean
  default: false

# Create variables
variables:
- group: Terraform ${{ parameters.environment }}

- name: environment
  value: ${{ lower(parameters.environment) }}

# Set service connection for the environment to deploy to, e.g. ODW Dev, ODW Test, ODW Prod
- name: armServiceConnectionName
  value: ${{ format('Azure DevOps Pipelines - ODW {0} - Infrastructure', upper(parameters.environment)) }}

- name: poolName
  value: 'pins-agent-pool-odw-dev-uks'

- name: source_folder
  value: '$(System.DefaultWorkingDirectory)/infrastructure/configuration/data-lake'

- name: target_folder
  value: '$(Build.ArtifactStagingDirectory)'

- name: config_files_path
  value: '**/infrastructure/configuration/data-lake/*'

- name: storage_account_name
  value: 'pinsstodwdevuks9h80mb'

- name: storage_container
  value: 'data-lake-config'

# Github branch to trigger the running of this pipeline.
# Any code change in this branch will trigger this pipeline.
trigger:
  branches:
    include:
      - 'main'

# Only code in the below paths will trigger the pipeline.
# All files in the functions folder but not files in subfolders.
  paths:
    include:
      - '$(config_files_path)'

# Disable pull request triggers, i.e. it will not be triggered by any pull requests.
pr: none

# Specify the Microsoft hosted image we want to use.
pool: 
  # vmImage: ubuntu-22.04

  '$(poolName)'

jobs:

- job: BuildAndPackage
  displayName: 'Build and Package'

  steps:

  # Checkout the Github repo, in this case ODW-Service.
  - checkout: self@feat/ODW-1035-DevOps-move-synapse-files-out-of-infrastructure-folder
    fetchDepth: '0'
    displayName: 'Checkout repo'

  - script: |

      sourceFolder="$(source_folder)"
      targetFolder="$(target_folder)"
      changedFiles=$(git diff --name-only --relative=$sourceFolder HEAD HEAD~1)
      for file in $changedFiles; do
        mkdir -p "$(dirname $targetFolder/$file)"
        cp "$sourceFolder/$file" "$targetFolder/$file"
      done

      echo "Listing target folder files"
      ls -R "$targetFolder"

    displayName: 'Copy amended files to staging directory'

  # - task: CopyFiles@2
  #   displayName: 'Copy files to staging directory'
  #   inputs:
  #     SourceFolder: '$(source_folder)'
  #     Contents: '**'
  #     TargetFolder: '$(target_folder)'

  # - script: |
  #     echo 'Listing files...'
  #     ls -R '$(Build.ArtifactStagingDirectory)'

  #   displayName: 'Listing files...'

  # - template: steps/azure-login.yaml

  - task: AzureCLI@2
    displayName: 'Send to Azure storage'
    inputs:
      azureSubscription: '$(armServiceConnectionName)'
      scriptType: 'bash'
      scriptLocation: 'inlineScript'
      inlineScript: |
        az storage blob upload-batch \
        --account-name $(storage_account_name) \
        --destination $(storage_container) \
        --source $(Build.ArtifactStagingDirectory) \
        --pattern '*' \
        --overwrite true \
        --auth-mode login