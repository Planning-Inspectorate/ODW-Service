###
# Pipeline to copy Synapse data lake config files from the repo main branch 
# to the Azure Storage account
###

---

# Create parameters
parameters:
- name: environment
  displayName: Environment
  type: string
  default: Dev
  values:
  - Dev
- name: failover_deployment
  displayName: 'Failover Deployment'
  type: boolean
  default: false

# Create variables
variables:
- group: Terraform ${{ parameters.environment }}

- name: environment
  value: ${{ lower(parameters.environment) }}

# Set service connection for the environment to deploy to, e.g. ODW Dev, ODW Test, ODW Prod
- name: armServiceConnectionName
  value: ${{ format('Azure DevOps Pipelines - ODW {0} - Infrastructure', upper(parameters.environment)) }}

- name: poolName
  value: 'pins-agent-pool-odw-dev-uks'

- name: source_folder
  value: '$(System.DefaultWorkingDirectory)/infrastructure/configuration/data-lake'

- name: target_folder
  value: '$(Build.ArtifactStagingDirectory)'

- name: config_files_path
  value: '**/infrastructure/configuration/data-lake/*'

- name: storage_account_name
  value: 'pinsstodwdevuks9h80mb'

- name: storage_container
  value: 'data-lake-config'

# Github branch to trigger the running of this pipeline.
# Any code change in this branch will trigger this pipeline.
trigger: none
  # branches:
  #   include:
  #     - 'main'

# Only code in the below paths will trigger the pipeline.
  # paths:
  #   include:
  #     - '$(config_files_path)'

# Disable pull request triggers, i.e. it will not be triggered by any pull requests.
pr: none

# Specify the Microsoft hosted image we want to use.
pool: 
  # vmImage: ubuntu-22.04

  '$(poolName)'

resources:
  repositories:
  - repository: config
    type: github
    endpoint: Planning-Inspectorate
    name: Planning-Inspectorate/ODW-Service
    ref: main

jobs:

- job: BuildAndPackage
  displayName: 'Build and Package'

  steps:

  # Checkout the Github repo, in this case ODW-Service.
  - checkout: config
    fetchDepth: '0'
    displayName: 'Checkout repo'

  # - script: |

  #     sourceFolder="$(source_folder)"
  #     targetFolder="$(target_folder)"
  #     changedFiles=$(git diff --name-only --relative=$sourceFolder HEAD HEAD~1)
  #     for file in $changedFiles; do
  #       mkdir -p "$(dirname $targetFolder/$file)"
  #       cp "$sourceFolder/$file" "$targetFolder/$file"
  #     done

  #     echo "Listing target folder files"
  #     ls -R "$targetFolder"

  #   displayName: 'Copy amended files to staging directory'

  - task: CopyFiles@2
    displayName: 'Copy files to staging directory'
    inputs:
      SourceFolder: '$(source_folder)'
      Contents: '**'
      TargetFolder: '$(target_folder)'

  - script: |
      echo 'Listing files...'
      ls -R '$(Build.ArtifactStagingDirectory)'

    displayName: 'Listing files...'

  - task: AzureCLI@2
    displayName: 'Send to Azure storage'
    inputs:
      azureSubscription: '$(armServiceConnectionName)'
      scriptType: 'bash'
      scriptLocation: 'inlineScript'
      inlineScript: |
        az storage blob upload-batch \
        --account-name $(storage_account_name) \
        --destination $(storage_container) \
        --source $(Build.ArtifactStagingDirectory) \
        --pattern '*' \
        --overwrite true \
        --auth-mode login